{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a454567",
   "metadata": {},
   "source": [
    "# Text Mining Process for Face Recognition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dbc38e",
   "metadata": {},
   "source": [
    "## Getting the desired text from the patents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8238380",
   "metadata": {},
   "source": [
    "We start from parsing the files containing the patents and look for the ones which can give us useful information about the topic we are interested into.\n",
    "This is going to take some minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cedf75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting files...\n",
      "START!\n",
      "END!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "path = 'C:\\\\Users\\\\Stefano\\\\Desktop\\\\Stefano\\\\Business\\\\Project\\\\Material\\\\MyPatents'\n",
    "print(\"Getting files...\")\n",
    "# getting all files from the directory given by the path\n",
    "files = os.listdir(path)\n",
    "# moving to the desired directory\n",
    "os.chdir(path)\n",
    "text = \"\"\n",
    "print(\"START!\")\n",
    "for filename in files:\n",
    "    file = open(filename, encoding=\"utf-8\")\n",
    "    text+=file.read();\n",
    "    #print(\"A file! \"+filename)\n",
    "    #    print(\"\\n\")\n",
    "    #print(text)\n",
    "print(\"END!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2fa90",
   "metadata": {},
   "source": [
    "After obtaining the whole text of the file of our interest, we separate the sections containing the abstract and the claims, which are the two sections we are interested on for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d6ba1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['An electronic apparatus including an image capturing device, a storage device and a processor and an operation method thereof are provided. The image capturing device captures an image for a user, and the storage device records a plurality of modules. The processor is coupled to the image capturing device and the storage device and is configured to: configure the image capturing device to capture a head image of a user; perform a face recognition operation to obtain a face region; detect a plurality of facial landmarks within the face region; estimate a head posture angle of the user according to the facial landmarks; calculate a gaze position where the user gazes on the screen according to the head posture angle, a plurality of rotation reference angle, and a plurality of predetermined calibration positions; and configure the screen to display a corresponding visual effect according to the gaze position.', 'The present disclosure provides a computation method and product thereof. The computation method adopts a fusion method to perform machine learning computations. Technical effects of the present disclosure include fewer computations and less power consumption.', \"A method for detecting body information on passengers of a vehicle based on humans' status recognition is provided. The method includes steps of: a passenger body information-detecting device, (a) inputting an interior image of the vehicle into a face recognition network, to detect faces of the passengers and output passenger feature information, and inputting the interior image into a body recognition network, to detect bodies and output body-part length information; and (b) retrieving specific height mapping information by referring to a height mapping table of ratios of segment body portions of human groups to heights per the human groups, acquiring a specific height of the specific passenger, retrieving specific weight mapping information from a weight mapping table of correlations between the heights and weights per the human groups, and acquiring a weight of the specific passenger by referring to the specific height.\", 'Techniques related to improved video coding based on face detection, region extraction, and tracking are discussed. Such techniques may include performing a facial search of a video frame to determine candidate face regions in the video frame, testing the candidate face regions based on skin tone information to determine valid and invalid face regions, rejecting invalid face regions, and encoding the video frame based on valid face regions to generate a coded bitstream.', 'A method for managing a smart database which stores facial images for face recognition is provided. The method includes steps of: a managing device (a) counting specific facial images corresponding to a specific person in the smart database where new facial images are continuously stored, and determining whether a first counted value, representing a count of the specific facial images, satisfies a first set value; and (b) if the first counted value satisfies the first set value, inputting the specific facial images into a neural aggregation network, to generate quality scores of the specific facial images by aggregation of the specific facial images, and, if a second counted value, representing a count of specific quality scores among the quality scores from a highest during counting thereof, satisfies a second set value, deleting part of the specific facial images, corresponding to the uncounted quality scores, from the smart database.', 'A system capable of determining which recognition algorithms should be applied to regions of interest within digital representations is presented. A preprocessing module utilizes one or more feature identification algorithms to determine regions of interest based on feature density. The preprocessing modules leverages the feature density signature for each region to determine which of a plurality of diverse recognition modules should operate on the region of interest. A specific embodiment that focuses on structured documents is also presented. Further, the disclosed approach can be enhanced by addition of an object classifier that classifies types of objects found in the regions of interest.', 'Disclosed is a mobile terminal. The mobile terminal may include a front camera obtaining a 2D face image of a user, a glance sensor tilted by a certain angle and disposed adjacent to the front camera to obtain metadata of the 2D face image, and a controller obtaining a distance between the glance sensor and the front camera, the distance enabling an area of an overlap region, where a first region representing a range photographable by the front camera overlaps a second region representing a range photographable by the glance sensor, to be the maximum.', \"This disclosure provides systems, methods and apparatus, including computer programs encoded on computer storage media for intelligent routing of notifications related to media programming. In one aspect, a smart television (TV) can be implemented to track a user's TV watching behavior, and anticipate programming based on that behavior. In some other aspects, the smart TV can be implemented to detect a user's presence, and based on that detection, can automatically change the TV channel to media programming analyzed to be desirable to the user. In some further aspects, the smart TV can be implemented to transmit notification instructions to electronic devices within a network in an attempt to alert the user to upcoming media programming. Additionally, the smart TV can be implemented to transmit detection instructions to the electronic devices within the network, whereby the electronic devices attempt to detect a user's presence through voice or facial recognition.\", 'A camera is configured to output a test depth+multi-spectral image including a plurality of pixels. Each pixel corresponds to one of the plurality of sensors of a sensor array of the camera and includes at least a depth value and a spectral value for each spectral light sub-band of a plurality of spectral illuminators of the camera. A face recognition machine is previously trained with a set of labeled training depth+multi-spectral images having a same structure as the test depth+multi-spectral image. The face recognition machine is configured to output a confidence value indicating a likelihood that the test depth+multi-spectral image includes a face.', 'Embodiments of the present disclosure relate to an image processing method and apparatus, and an electronic device. The method includes: acquiring a photo album obtained from face clustering; collecting face information of respective images in the photo album, and acquiring a face parameter of each image according to the face information; selecting a cover image according to the face parameter of each image; and taking a face-region image from the cover image, and setting the face-region image as a cover of the photo album.', 'Techniques described herein provide location-based access control to secured resources. Generally described, configurations disclosed herein enable a system to dynamically modify access to secured resources based on one or more location-related actions. For example, techniques disclosed herein can enable a computing system to control access to resources such as computing devices, display devices, secured locations, and secured data. In some configurations, the techniques disclosed herein can enable controlled access to secured resources based, at least in part, on an invitation associated with a location and positioning data indicating a location of a user.', 'One embodiment provides a method comprising receiving a piece of content and salient moments data for the piece of content. The method further comprises, based on the salient moments data, determining a first path for a viewport for the piece of content. The method further comprises displaying the viewport on a display device. Movement of the viewport is based on the first path during playback of the piece of content. The method further comprises generating an augmentation for a salient moment occurring in the piece of content, and presenting the augmentation in the viewport during a portion of the playback. The augmentation comprises an interactive hint for guiding the viewport to the salient moment.', 'A computer-implemented method, system, and computer program product are provided for facial recognition. The method includes receiving, by a processor device, a plurality of images. The method also includes extracting, by the processor device with a feature extractor utilizing a convolutional neural network (CNN) with an enlarged intra-class variance of long-tail classes, feature vectors for each of the plurality of images. The method additionally includes generating, by the processor device with a feature generator, discriminative feature vectors for each of the feature vectors. The method further includes classifying, by the processor device utilizing a fully connected classifier, an identity from the discriminative feature vector. The method also includes control an operation of a processor-based machine to react in accordance with the identity.', 'Some embodiments of the invention provide efficient, expressive machine-trained networks for performing machine learning. The machine-trained (MT) networks of some embodiments use novel processing nodes with novel activation functions that allow the MT network to efficiently define with fewer processing node layers a complex mathematical expression that solves a particular problem (e.g., face recognition, speech recognition, etc.). In some embodiments, the same activation function (e.g., a cup function) is used for numerous processing nodes of the MT network, but through the machine learning, this activation function is configured differently for different processing nodes so that different nodes can emulate or implement two or more different functions (e.g., two or more Boolean logical operators, such as XOR and AND). The activation function in some embodiments is a periodic function that can be configured to implement different functions (e.g., different sinusoidal functions).', 'Methods and systems may provide for facial recognition of at least one input image utilizing hierarchical feature learning and pair-wise classification. Receptive field theory may be used on the input image to generate a pre-processed multi-channel image. Channels in the pre-processed image may be activated based on the amount of feature rich details within the channels. Similarly, local patches may be activated based on the discriminant features within the local patches. Features may be extracted from the local patches and the most discriminant features may be selected in order to perform feature matching on pair sets. The system may utilize patch feature pooling, pair-wise matching, and large-scale training in order to quickly and accurately perform facial recognition at a low cost for both system memory and computation.', 'A method for controlling a terminal is provided. The terminal includes a capturing apparatus and at least one processor. An image is acquired by the capturing apparatus. A motion parameter of the terminal is obtained. Image processing on the acquired image is controlled to be performed based on the motion parameter being equal to or less than a preset parameter threshold, and skipped based on the motion parameter being greater than the preset parameter threshold.', 'A drive-through order processing method and apparatus are disclosed. The drive-through order processing method includes receiving customer information detected through vision recognition, providing product information based on the customer information, and processing a product order of a customer. According to the present disclosure, it is possible to rapidly process an order using customer information based on customer recognition using an artificial intelligence (AI) model of machine learning through a 5G network.', 'An image processing method performed at a computing device includes: identifying, using face recognition, one or more faces, each face corresponding to a respective person captured in a first image; for each identified face: extracting a set of profile parameters of a corresponding person in the first image; and selecting, from a plurality of image tiles, a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile; generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles; and sharing the first image and the second image in a predefined order via a group chat session.', 'In one embodiment, the artificial reality system determines that a performance metric of an eye tracking system is below a first performance threshold. The eye tracking system is associated with a head-mounted display worn by a user. The artificial reality system receives first inputs associated with the body of a user and determines a region that the user is looking at within a field of view of a head-mounted display based on the received first inputs. The system determines a vergence distance of the user based at least on the first inputs associated with the body of the user, the region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display. The system adjusts one or more configurations of the head-mounted display based on the determined vergence distance of the user.', 'A computer-implemented method is provided for image-based, self-guided object detection. The method includes receiving, by a processor device, a set of images. Each of the images has a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data. The method further includes training, by the processor device, a grid-based object detector using the grid level label data. The method also includes determining, by the processor device, a respective bounding box for the respective object in each of the images, by applying local segmentation to each of the images. The method additionally includes training, by the processor device, a Region-based Convolutional Neural Network (RCNN) for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the RCNN.', 'A system and method of face recognition comprising multiple phases implemented in a parallel architecture. The first phase is a normalization phase whereby a captured image is normalized to the same size, orientation, and illumination of stored images in a preexisting database. The second phase is a feature extraction/distance matrix phase where a distance matrix is generated for the captured image. In a coarse recognition phase, the generated distance matrix is compared with distance matrices in the database using Euclidean distance matches to create candidate lists, and in a detailed recognition phase, multiple face recognition algorithms are applied to the candidate lists to produce a final result. The distance matrices in the normalized database may be broken into parallel lists for parallelization in the feature extraction/distance matrix phase, and the candidate lists may also be grouped according to a dissimilarity algorithm for parallel processing in the detailed recognition phase.', 'An imaging device including a pixel matrix and a processor is provided. The pixel matrix includes a plurality of phase detection pixels and a plurality of regular pixels. The processor performs autofocusing according to pixel data of the phase detection pixels, and determines an operating resolution of the regular pixels according to autofocused pixel data of the phase detection pixels, wherein the phase detection pixels are always-on pixels and the regular pixels are selectively turned on after the autofocusing is accomplished.', 'An apparatus includes a first camera module providing a first image of an object with a first field of view, a second camera module providing a second image of the object with a second field of view different from the first field of view, a first depth map generator that generates a first depth map of the first image based on the first image and the second image, and a second depth map generator that generates a second depth map of the second image based on the first image, the second image, and the first depth map.', 'Methods, systems, and apparatus, including computer programs encoded on computer storage media, for a payment based on a face recognition are provided. One of the methods includes: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including determining whether an angle of rotation in each preset direction is less than an angle threshold and whether a probability value that a user gazes at a payment screen is greater than a probability threshold; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition.', 'A novel method and apparatus for face authentication is disclosed. The disclosed method comprises detecting a motion by a subject within a predetermined area of view, assigning a unique session identification number to the subject detected within a predetermined area of view, detecting a facial area of the subject detected within a predetermined area of view, generating an image of the facial area of the subject, assessing a quality of the image of the facial area of the subject, conducing an incremental training of the image of the facial area of the subject, determining an identity of the subject based on the image of the facial area of the subject, identifying an intent of the subject, and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject.', 'Disclosed herein is a robot and an electronic device for acquiring video, and a method for acquiring video using the robot. The robot includes a camera configured to rotate in the lateral direction and tilt in the vertical direction, and controls at least one of a direction of the rotation of the camera, an angle of the tilt of the camera, and a focal distance of the camera by recognizing and tracking users in a video acquired by the camera.', 'Systems and methods are disclosed for inferring topics from a file containing both audio and video, for example a multimodal or multimedia file, in order to facilitate video indexing. A set of entities is extracted from the file and linked to produce a graph, and reference information is also obtained for the set of entities. Entities may be drawn, for example, from Wikipedia categories, or other large ontological data sources. Analysis of the graph, using unsupervised learning, permits determining clusters in the graph. Extracting features from the clusters, possibly using supervised learning, provides for selection of topic identifiers. The topic identifiers are then used for indexing the file.', 'A face recognition method, a neural network training method, an apparatus, and an electronic device. The method comprises: obtaining a first face image by means of a first camera (101); extracting a first face feature of the first face image (102); comparing the first face feature with a pre-stored second face feature to obtain a reference similarity, the second face feature being obtained by extracting a feature of a second face image obtained by a second camera, and the second camera and the first camera being different types of cameras (103); and determining, according to the reference similarity, whether the first face feature and the second face feature correspond to a same person (104).', 'The present invention discloses a technique for alerting on vision impairment. The system comprises a processing unit configured and operable for receiving scene data being indicative of a scene of at least one consumer in an environment, identifying in the scene data a certain consumer, identifying an event being indicative of a behavioral compensation for vision impairment, and, upon identification of such an event, sending a notification relating to the vision impairment.']\n",
      "ABSTRACT TEXT OBTAINED!\n"
     ]
    }
   ],
   "source": [
    "p1 = \"\"\n",
    "p1=re.findall(r'(?<=<abstract>\\n)(?s:.*?)(?=\\n</abstract>)',text)\n",
    "print(p1)\n",
    "print(\"ABSTRACT TEXT OBTAINED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68fc0ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. An electronic device (10), configured to make a screen (110) to display a plurality of image frames, comprising: an image capturing device (120); a storage device (130), storing a plurality of modules; and a processor (14), coupled to the image capturing device (140) and the storage device (130), configured to execute the modules in the storage device (130) to: configure the screen (110) to display a plurality of marker objects at a plurality of predetermined calibration positions; configure the image capturing device (120) to capture a plurality of first head images when a user is looking at the predetermined calibration positions; (S301) perform a plurality of first face recognition operations on the first head images to obtain a plurality of first face regions corresponding to the predetermined calibration positions; (S302) detect a plurality of first facial landmarks corresponding to the first face regions; (S303) calculate a plurality of rotation reference angles of the user looking at the predetermined calibration positions according to the first facial landmarks; configure the image capturing device (120) to capture a second head image of the user; perform a second face recognition operation on the second head image to obtain a second face region; detect a plurality of second facial landmarks within the second face region; (S304) estimate a head posture angle of the user according to the second facial landmarks; calculate a gaze position of the user on the screen (110) according to the head posture angle, the rotation reference angles, and the predetermined calibration positions; and configure the screen (110) to display a corresponding visual effect according to the gaze position.\\n2. The electronic device (10) according to claim 1, wherein the gaze position comprises a first coordinate value in a first axial direction and a second coordinate value in a second axial direction.\\n3. The electronic device (10) according to claim 2, wherein the head posture angles comprise a head pitch angle and a head yaw angle, and the rotation reference angles comprise a first pitch angle, a second pitch angle, a first yaw angle, and a second yaw angle corresponding to the predetermined calibration positions.\\n4. The electronic device (10) according to claim 3, wherein the processor (140) performs interpolation operation or extrapolation operation according to the first yaw angle, the second yaw angle, a first position corresponding to the first yaw angle among the predetermined calibration positions, a second position corresponding to the second yaw angle among the predetermined calibration positions and the head yaw angle, thereby obtaining the first coordinate value of the gaze position; and\\nthe processor (140) performs interpolation operation or extrapolation operation according to the first pitch angle, the second pitch angle, a third position corresponding to the first pitch angle among the predetermined calibration positions, a fourth position corresponding to the second pitch angle among the predetermined calibration positions and the head pitch angle, thereby obtaining the second coordinate value of the gaze position.\\n5. The electronic device (10) according to claim 1, wherein the processor (140) calculates a plurality of first viewing distances between the user and the screen (110) according to the first facial landmarks;\\nThe processor (140) estimates a second viewing distance between the user and the screen (110) according to the second facial landmarks; and\\nthe processor (140) adjusts the rotation reference angles or the gaze position according to the second viewing distance and the first viewing distances.\\n6. The electronic device (10) according to claim 1, wherein the processor (140) maps a plurality of two-dimensional position coordinates of the second facial landmarks under a plane coordinate system to a plurality of three-dimensional position coordinates under a three-dimensional coordinate system; and\\nthe processor (140) estimates the head posture angle according to the three-dimensional position coordinates of the second facial landmarks.\\n7. The electronic device (10) according to claim 1, wherein the second head image comprises a wearable device, and the second facial landmarks do not comprise a plurality of third facial landmarks of the user covered by the wearable device.\\n8. The electronic device (10) according to claim 1, wherein the second head image comprises a wearable device, and the second facial landmarks comprise one or more simulated landmarks marked by the wearable device.\\n9. An operating method, adapted for an electronic device (10) comprising an image capturing device (120) and making a screen (110) to display a plurality of image frames, the method comprising: configuring the screen (110) to display a plurality of marker objects at a plurality of predetermined calibration positions; configuring the image capturing device (120) to capture a plurality of first head images when a user is looking at the predetermined calibration positions; (S301) performing a plurality of first face recognition operations on the first head images to obtain a plurality of first face regions corresponding to the predetermined calibration positions; (S302) detecting a plurality of first facial landmarks corresponding to the first face regions; (S303) calculating a plurality of rotation reference angles of the user looking at the predetermined calibration positions according to the first facial landmarks; configuring the image capturing device (120) to capture a second head image of the user; performing a second face recognition operation on the second head image to obtain a second face region; (S304) detecting a plurality of second facial landmarks within the second face region; estimating a head posture angle of the user according to the second facial landmarks; calculating a gaze position of the user on the screen (110) according to the head posture angle, the rotation reference angles, and the predetermined calibration positions; and (S305) configuring the screen (110) to display a corresponding visual effect according to the gaze position.\\n10. The operation method according to claim 9, wherein the gaze position comprises a first coordinate value in a first axial direction and a second coordinate value in a second axial direction.\\n11. The operation method according to claim 10, wherein the head posture angles comprise a head pitch angle and a head yaw angle, and the rotation reference angles comprise a first pitch angle, a second pitch angle, a first yaw angle, and a second yaw angle corresponding to the predetermined calibration positions.\\n12. The operation method according to claim 11, wherein the step of calculating the gaze position of the user on the screen (110) according to the head posture angle, the rotation reference angles and the predetermined calibration positions comprises: performing interpolation operation or extrapolation operation according to the first yaw angle, the second yaw angle, a first position corresponding to the first yaw angle among the predetermined calibration positions, a second position corresponding to the second yaw angle among the predetermined calibration positions and the head yaw angle, thereby obtaining the first coordinate value of the gaze position; and performing interpolation operation or extrapolation operation according to the first pitch angle, the second pitch angle, a third position corresponding to the first pitch angle among the predetermined calibration positions, a fourth position corresponding to the second pitch angle among the predetermined calibration positions and the head pitch angle, thereby obtaining the second coordinate value of the gaze position.\\n13. The operation method according to claim 9, wherein the method further comprises: calculating a plurality of first viewing distances between the user and the screen (110) according to the first facial landmarks; estimating a second viewing distance between the user and the screen (110) according to the second facial landmarks; and adjusting the rotation reference angles or the gaze position according to the second viewing distance and the first viewing distances.\\n14. The operation method according to claim 9, wherein the method further comprises: mapping a plurality of two-dimensional position coordinates of the second facial landmarks under a plane coordinate system to a plurality of three-dimensional position coordinates under a three-dimensional coordinate system; and estimating the head posture angle according to the three-dimensional position coordinates of the second facial landmarks.\\n15. The operation method according to claim 9, wherein the second head image comprises a wearable device, and the second facial landmarks do not comprise a plurality of third facial landmarks of the user covered by the wearable device.\\n16. The operation method according to claim 9, wherein the second head image comprises a wearable device, and the second facial landmarks comprise one or more simulated landmarks marked by the wearable device.', '1. A computation method applied to a computing system, wherein the computing system comprises: a control unit, a computation group, and a general storage unit, wherein the control unit comprises: a first memory, a decoding logic, and a controller, wherein the computation group comprises: a group controller and a plurality of computing units; the general storage unit is configured to store data; and the computation method comprises: receiving, by the controller, a first level instruction sequence, and partitioning, by the decoding logic, the first level instruction sequence into a plurality of second level instruction sequences, creating, by the controller, M threads for the plurality of second level instruction sequences, and allocating, by the controller, an independent register as well as configuring an independent addressing function for each thread of the M threads, wherein M is an integer greater than or equal to 1; and obtaining, by the group controller, a plurality of computation types of the plurality of second level instruction sequences, obtaining a corresponding fusion computation manner of the computation types according to the plurality of computation types, and adopting, by the plurality of computing units, the fusion computation manner to call the M threads for performing computations on the plurality of second level instruction sequences to obtain a final result.\\n2. The method of claim 1, wherein, the obtaining, by the group controller, a plurality of computation types of the plurality of second level instruction sequences, obtaining a corresponding fusion computation manner of the computation types according to the plurality of computation types, and adopting, by the plurality of computing units, the fusion computation manner to call the M threads for performing computations on the plurality of second instruction sequences to obtain a final result:\\nif the computation types represent computation operations of the same type, the group controller calls a combined computation manner in which single instruction multiple data of the same type is in combination with single instruction multiple threads, and uses the M threads to perform the combined computation manner to obtain a final result, which includes:\\npartitioning, by the decoding logic, the M threads into N wraps for allocating to the the plurality of computing units, converting, by the group controller, the plurality of second instruction sequences into a plurality of second control signals and sending the second control signals to the plurality of computing units, calling, by the plurality of computing units, wraps that are allocated to the computing units and the second control signals to fetch corresponding data according to the independent addressing function, performing, by the plurality of computing units, computations on the data to obtain a plurality of intermediate results, and splicing the plurality of intermediate results to obtain a final result.\\n3. The method of claim 1, wherein, the obtaining, by the group controller, a plurality of computation types of the plurality of second level instruction sequences, obtaining a corresponding fusion computation manner of the computation types according to the plurality of computation types, and adopting, by the plurality of computing units, the fusion computation manner to call the M threads for performing computations on the plurality of second instruction sequences to obtain a final result:\\nif the computation types represent computation operations of different types, the group controller calls simultaneous multi-threading and the M threads to perform computations to obtain a final result, which includes:\\npartitioning, by the decoding logic, the M threads into N wraps, converting the plurality of second instruction sequences into a plurality of second control signals, obtaining, by the group controller, computation types supported by the plurality of computing units, allocating, by the controller, the N wraps and the plurality of second control signals to corresponding computing units that support computation types of the wraps and the second control signals, calling, by the plurality of computing units, wraps that are allocated to the computing units and the second control signals, fetching, by the plurality of computing units, corresponding data, performing, by the plurality of computing units, computations on the data to obtain a plurality of intermediate results, and splicing all the intermediate results to obtain a final result.\\n4. The method of claim 2 or 3, further comprising:\\nif a wrap A in the plurality of wraps is blocked, adding the wrap A to a waiting queue, and if data of the wrap A are already fetched, adding the wrap A to a preparation queue, wherein the preparation queue is a queue where a wrap to be scheduled for executing is located when a computing resource is idle.\\n5. The method of claim 1, wherein\\nthe first level instruction sequence includes a very long instruction, and the second level instruction sequence includes an instruction sequence.\\n6. The method of claim 1, wherein the computing system further includes: a tree module, wherein the tree module includes: a root port and a plurality of branch ports, wherein the root port of the tree module is connected to the group controller, and the plurality of branch ports of the tree module are connected to a computing unit of the plurality of computing units respectively; and\\nthe tree module is configured to forward data blocks, wraps, or instruction sequences between the group controller and the plurality of computing units.\\n7. The method of claim 6, wherein the tree module is an n-ary tree, wherein n is an integer greater than or equal to 2.\\n8. The method of claim 1, wherein the computing system further includes a branch processing circuit,\\nwherein the branch processing circuit is connected between the group controller and the plurality of computing units; and\\nthe branch processing circuit is configured to forward data, wraps, or instruction sequences between the group controller and the plurality of computing units.\\n9. A computing system, comprising: a control unit, a computation group, and a general storage unit, wherein the control unit includes: a first memory, a decoding logic, and a controller, the computation group includes: a group controller and a plurality of computing units; the general storage unit is configured to store data;\\nthe controller is configured to receive a first level instruction sequence and control the first memory and the decoding logic;\\nthe decoding logic is configured to partition the first level instruction sequence into a plurality of second level instruction sequences;\\nthe the controller is further configured to create M threads for the plurality of second level instruction sequences, and allocate an independent register and configure an independent addressing function for each thread of the M threads; M is an integer greater than or equal to 1; and the controller is further configured to convert the plurality of second instruction sequences into a plurality of control signals for sending to the group controller;\\nthe group controller is configured to receive the plurality of control signals, obtain a plurality of computational types if the plurality of control signals, divide the M threads into N wraps, and allocate the N wraps and the plurality of control signals to the plurality of computing units according to the plurality of computational types;\\nthe plurality of computing units are configured to fetch data from the general storage unit through allocated wraps and control signals, and perform computations to obtain an intermediate result; and\\nthe group controller is configured to splice all intermediate results to obtain a final computation result.\\n10. The computing system of claim 9, wherein\\nthe plurality of computing units includes: an addition computing unit, a multiplication computing unit, an activation computing unit, or a dedicated computing unit.\\n11. The computing system of claim 9, wherein\\nthe dedicated computing unit includes: a face recognition computing unit, a graphics computing unit, a fingerprint computing unit, or a neural network computing unit.\\n12. The computing system of claim 11, wherein\\nthe group controller is configured to, if computation types of a plurality of control signals are graphics computations, fingerprint identification, face recognition, or neural network operations, allocate the plurality of control signals to the face recognition computing unit, the graphics computing unit, the fingerprint computing unit, or the neural network computing unit respectively.\\n13. The computing system of claim 9, wherein\\nthe first level instruction sequence includes a very long instruction, and the second level instruction sequence includes an instruction sequence.\\n14. The computing system of claim 9, further comprising a tree module, wherein the tree module includes: a root port and a plurality of branch ports, wherein the root port of the tree module is connected to the group controller, and the plurality of branch ports of the tree module are connected to a computing unit of the plurality of computing units respectively; and\\nthe tree module is configured to forward data blocks, wraps, or instruction sequences between the group controller and the plurality of computing units.\\n15. The computing system of claim 14, wherein the tree module is an n-ary tree, wherein n is an integer greater than or equal to 2.\\n16. The computing system of claim 9, wherein the computing system includes a branch processing circuit,\\nthe branch processing circuit is connected between the group controller and the plurality of computing units; and\\nthe branch processing circuit is configured to forward data, wraps, or instruction sequences between the group controller and the plurality of computing units.\\n17. A computer program product, comprising a non-instant computer readable storage medium, wherein a computer program is stored in the non-instant computer readable storage medium, and the computer program is capable of causing a computer to perform the method of any of claims 1-8 through operations.', \"1. A method for detecting body information on one or more passengers of a vehicle based on humans' status recognition, comprising steps of: (a) if at least one interior image of an interior of the vehicle is acquired, a passenger body information-detecting device performing (i) a process of inputting the interior image into a face recognition network, to thereby allow the face recognition network to detect each of faces of each of the passengers from the interior image, and thus to output multiple pieces of passenger feature information corresponding to each of the detected faces, and (ii) a process of inputting the interior image into a body recognition network, to thereby allow the body recognition network to detect each of bodies of each of the passengers from the interior image, and thus to output body-part length information of each of the detected bodies; and (b) the passenger body information-detecting device performing a process of retrieving specific height mapping information corresponding to specific passenger feature information on a specific passenger from a height mapping table which stores height mapping information representing respective one or more predetermined ratios of one or more segment body portions of each of human groups to each of heights per each of the human groups, a process of acquiring a specific height of the specific passenger from the specific height mapping information by referring to specific body-part length information of the specific passenger, a process of retrieving specific weight mapping information corresponding to the specific passenger feature information from a weight mapping table which stores multiple pieces of weight mapping information representing predetermined correlations between each of the heights and each of weights per each of the human groups, and a process of acquiring a weight of the specific passenger from the specific weight mapping information by referring to the specific height of the specific passenger.\\n2. The method of Claim 1, wherein, at the step of (a), the passenger body information-detecting device performs a process of inputting the interior image into the body recognition network, to thereby allow the body recognition network to (i) output one or more feature tensors with one or more channels corresponding to the interior image via a feature extraction network, (ii) generate at least one keypoint heatmap and at least one part affinity field with one or more channels corresponding to each of the feature tensors via a keypoint heatmap & part affinity field extractor, and (iii) extract keypoints from the keypoint heatmap via a keypoint detector, to group the extracted keypoints by referring to the part affinity field, and thus to generate body parts per the passengers, and as a result, allow the body recognition network to output multiple pieces of body-part length information on each of the passengers by referring to the body parts per the passengers.\\n3. The method of Claim 2, wherein the feature extraction network includes at least one convolutional layer and applies at least one convolution operation to the interior image, to thereby output the feature tensors.\\n4. The method of Claim 2, wherein the keypoint heatmap & part affinity field extractor includes one of a fully convolutional network and a 1×1 convolutional layer, and applies a fully-convolution operation or 1×1 convolution operation to the feature tensors, to thereby generate the keypoint heatmap and the part affinity field.\\n5. The method of Claim 2, wherein the keypoint detector connects, by referring to the part affinity field, pairs respectively having highest mutual connection probabilities of being connected among the extracted keypoints, to thereby group the extracted keypoints.\\n6. The method of Claim 2, wherein the feature extraction network and the keypoint heatmap & part affinity field extractor have been learned by a learning device performing (i) a process of inputting at least one training image including one or more objects for training into the feature extraction network, to thereby allow the feature extraction network to generate one or more feature tensors for training having one or more channels by applying at least one convolutional operation to the training image, (ii) a process of inputting the feature tensors for training into the keypoint heatmap & part affinity field extractor, to thereby allow the keypoint heatmap & part affinity field extractor to generate one or more keypoint heatmaps for training and one or more part affinity fields for training having one or more channels for each of the feature tensors for training, (iii) a process of inputting the keypoint heatmaps for training and the part affinity fields for training into the keypoint detector, to thereby allow the keypoint detector to extract keypoints for training from each of the keypoint heatmaps for training and a process of grouping the extracted keypoints for training by referring to each of the part affinity fields for training, to thereby detect keypoints per each of the objects for training, and (iv) a process of allowing a loss layer to calculate one or more losses by referring to the keypoints per each of the objects for training and their corresponding ground truths, to thereby adjust one or more parameters of the feature extraction network and the keypoint heatmap & part affinity field extractor such that the losses are minimized by backpropagation using the losses.\\n7. The method of Claim 1, wherein, at the step of (a), the passenger body information-detecting device performs a process of inputting the interior image into the face recognition network, to thereby allow the face recognition network to detect each of the faces of each of the passengers located in the interior image via a face detector, and to output multiple pieces of the passenger feature information on each of the facial images via a facial feature classifier.\\n8. The method of Claim 1, wherein, at the step of (a), the passenger body information-detecting device performs a process of inputting the interior image into the face recognition network, to thereby allow the face recognition network to (i) apply at least one convolution operation to the interior image and thus to output at least one feature map corresponding to the interior image via at least one convolutional layer, (ii) output one or more proposal boxes, where the passengers are estimated as located, on the feature map, via a region proposal network, (iii) apply pooling operation to one or more regions, corresponding to the proposal boxes, on the feature map and thus to output at least one feature vector via a pooling layer, and (iv) apply fully-connected operation to the feature vector, and thus to output the multiple pieces of the passenger feature information corresponding to each of the faces of each of the passengers corresponding to each of the proposal boxes via a fully connected layer.\\n9. The method of Claim 1, wherein the multiple pieces of the passenger feature information include each of ages, each of genders and each of races corresponding to each of the passengers.\\n10. A passenger body information-detecting device for detecting body information on one or more passengers of a vehicle based on humans' status recognition, comprising: at least one memory that stores instructions; and at least one processor configured to execute the instructions to perform or support another device to perform: (I) if at least one interior image of an interior of the vehicle is acquired, (i) a process of inputting the interior image into a face recognition network, to thereby allow the face recognition network to detect each of faces of each of the passengers from the interior image, and thus to output multiple pieces of passenger feature information corresponding to each of the detected faces, and (ii) a process of inputting the interior image into a body recognition network, to thereby allow the body recognition network to detect each of bodies of each of the passengers from the interior image, and thus to output body-part length information of each of the detected bodies, and (II) a process of retrieving specific height mapping information corresponding to specific passenger feature information on a specific passenger from a height mapping table which stores height mapping information representing respective one or more predetermined ratios of one or more segment body portions of each of human groups to each of heights per each of the human groups, a process of acquiring a specific height of the specific passenger from the specific height mapping information by referring to specific body-part length information of the specific passenger, a process of retrieving specific weight mapping information corresponding to the specific passenger feature information from a weight mapping table which stores multiple pieces of weight mapping information representing predetermined correlations between each of the heights and each of weights per each of the human groups, and a process of acquiring a weight of the specific passenger from the specific weight mapping information by referring to the specific height of the specific passenger.\\n11. The passenger body information-detecting device of Claim 10, wherein, at the process of (I), the processor performs a process of inputting the interior image into the body recognition network, to thereby allow the body recognition network to (i) output one or more feature tensors with one or more channels corresponding to the interior image via a feature extraction network, (ii) generate at least one keypoint heatmap and at least one part affinity field with one or more channels corresponding to each of the feature tensors via a keypoint heatmap & part affinity field extractor, and (iii) extract keypoints from the keypoint heatmap via a keypoint detector, to group the extracted keypoints by referring to the part affinity field, and thus to generate body parts per the passengers, and as a result, allow the body recognition network to output multiple pieces of body-part length information on each of the passengers by referring to the body parts per the passengers.\\n12. The passenger body information-detecting device of Claim 11, wherein the keypoint heatmap & part affinity field extractor includes one of a fully convolutional network and a 1×1 convolutional layer, and applies a fully-convolution operation or 1×1 convolution operation to the feature tensors, to thereby generate the keypoint heatmap and the part affinity field.\\n13. The passenger body information-detecting device of Claim 11, wherein the keypoint detector connects, by referring to the part affinity field, pairs respectively having highest mutual connection probabilities of being connected among the extracted keypoints, to thereby group the extracted keypoints.\\n14. The passenger body information-detecting device of Claim 11, wherein the feature extraction network and the keypoint heatmap & part affinity field extractor have been learned by a learning device performing (i) a process of inputting at least one training image including one or more objects for training into the feature extraction network, to thereby allow the feature extraction network to generate one or more feature tensors for training having one or more channels by applying at least one convolutional operation to the training image, (ii) a process of inputting the feature tensors for training into the keypoint heatmap & part affinity field extractor, to thereby allow the keypoint heatmap & part affinity field extractor to generate one or more keypoint heatmaps for training and one or more part affinity fields for training having one or more channels for each of the feature tensors for training, (iii) a process of inputting the keypoint heatmaps for training and the part affinity fields for training into the keypoint detector, to thereby allow the keypoint detector to extract keypoints for training from each of the keypoint heatmaps for training and a process of grouping the extracted keypoints for training by referring to each of the part affinity fields for training, to thereby detect keypoints per each of the objects for training, and (iv) a process of allowing a loss layer to calculate one or more losses by referring to the keypoints per each of the objects for training and their corresponding ground truths, to thereby adjust one or more parameters of the feature extraction network and the keypoint heatmap & part affinity field extractor such that the losses are minimized by backpropagation using the losses.\\n15. The passenger body information-detecting device of Claim 10, wherein, at the process of (I), the processor performs a process of inputting the interior image into the face recognition network, to thereby allow the face recognition network to (i) apply at least one convolution operation to the interior image and thus to output at least one feature map corresponding to the interior image via at least one convolutional layer, (ii) output one or more proposal boxes, where the passengers are estimated as located, on the feature map, via a region proposal network, (iii) apply pooling operation to one or more regions, corresponding to the proposal boxes, on the feature map and thus to output at least one feature vector via a pooling layer, and (iv) apply fully-connected operation to the feature vector, and thus to output the multiple pieces of the passenger feature information corresponding to each of the faces of each of the passengers corresponding to each of the proposal boxes via a fully connected layer.\", '1. A computer implemented method for performing video coding based on face detection comprising: receiving a video frame comprising one of a plurality of video frames of a video sequence; determining the video frame is a key frame of the video sequence; performing, in response to the video frame being a key frame of the video sequence, a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame; testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region; rejecting the second candidate face region and outputting the first candidate face region; and encoding the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream.\\n2. The method of claim 1, wherein the skin tone information comprises a skin probability map.\\n3. The method of claim 1, wherein said testing the first and second candidate face regions based on skin tone information is performed in response to the video frame being a key frame of the video sequence.\\n4. The method of claim 1, wherein the first candidate face region comprises a rectangular region, the method further comprising: determining a free form shape face region corresponding to the first candidate face region, wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy.\\n5. The method of claim 4, wherein determining the free form shape face region comprises: generating an enhanced skip probability map corresponding to the first candidate face region; binarizing the enhanced skip probability map; and overlaying the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region.\\n6. The method of claim 4, wherein a second video frame comprises a non-key frame of the video sequence, the method further comprising performing face detection in the second video frame of the video sequence based on the free form shape face region.\\n7. The method of claim 6, further comprising: tracking a second free form shape face region in the second video frame based on the free form shape face region in the video frame.\\n8. The method of claim 7, wherein tracking the second free form shape face region comprises determining a location of a second valid face region in the second video frame based on a displacement offset with respect to the first candidate face region.\\n9. The method of claim 8, further comprising: determining the displacement offset based on an offset between a centroid of a bounding box around a skin enhanced region corresponding to the first candidate face region and a second centroid of a second bounding box around a second skin enhanced region in the second video frame.\\n10. The method of claim 1, wherein encoding the video frame based at least in part on the first candidate face region being a valid face region comprises at least one of reducing a quantization parameter corresponding to the first candidate face region, adjusting a lambda value for the first candidate face region, or disabling skip coding for the first candidate face region.\\n11. The method of claim 1, wherein the bitstream comprises at least one of an H.264/Advanced Video Coding (AVC) compliant bitstream, an H.265/High Efficiency Video Coding (HEVC) compliant bitstream, a VP9 compliant bitstream, a VP10 compliant bitstream, or an Alliance for Open Media (AOM) AV1 compliant bitstream.\\n12. A computer implemented method for performing face detection comprising: receiving a video frame of a sequence of video frames; performing a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame; testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region; rejecting the second candidate face region and outputting the first candidate face region as a valid face region for further processing; and providing an index indicative of a person being present in the video frame based on the valid face region.\\n13. The method of claim 12, wherein the sequence of video frames comprises a sequence of surveillance video frames, the method further comprising: performing face recognition in the surveillance video frames based on the valid face region.\\n14. The method of claim 12, wherein the sequence of video frames comprises a sequence of decoded video frames, the method further comprising: adding a marker corresponding to the received video frame to perform face recognition on the received video frame based on the valid face region.\\n15. The method of claim 12, wherein the sequence of video frames is received during a device login attempt, the method further comprising: performing face recognition based on the valid face region; and allowing access to the device if a secured face is recognized.\\n16. The method of claim 12, wherein the sequence of video frames comprises a sequence of videoconferencing frames, the method further comprising: encoding the video frame based at least in part on the valid face region to generate a coded bitstream.\\n17. The method of claim 16, wherein encoding the video frame comprises not encoding a background region of the video frame into the bitstream.\\n18. The method of claim 12, further comprising: encoding the video frame based at least in part on the valid face region to generate a coded bitstream, wherein encoding the video frame comprises including metadata corresponding to the valid face region in the bitstream.\\n19. The method of claim 18, further comprising: decoding the coded bitstream to generate a decoded video frame and to determine the metadata corresponding to the valid face region in the bitstream.\\n20. The method of claim 19, further comprising at least one of replacing the valid face region based on the decoded metadata, cropping and displaying image data corresponding only to the valid face region based on the decoded metadata, or indexing the decoded video frame based on the decoded metadata.\\n21. A system for performing video coding based on face detection comprising: a memory configured to store a video frame comprising one of a plurality of video frames of a video sequence; and a processor coupled to the memory, the processor to receive the video frame, to determine the video frame is a key frame of the video sequence; to perform, in response to the video frame being a key frame of the video sequence, a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame, to test the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region, to reject the second candidate face region and outputting the first candidate face region, and to encode the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream.\\n22. The system of claim 21, wherein the skin tone information comprises a skin probability map.\\n23. The system of claim 21, wherein the first candidate face region comprises a rectangular region, the processor further to determine a free form shape face region corresponding to the first candidate face region, wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy.\\n24. The system of claim 23, wherein the processor to determine the free form shape face region comprises the processor to generate an enhanced skip probability map corresponding to the first candidate face region, to binarize the enhanced skip probability map, and to overlay the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region.\\n25. The system of claim 23, wherein a second video frame comprises a non-key frame of the video sequence, and the processor is further to perform face detection in the second video frame of the video sequence based on the free form shape face region.\\n26. The system of claim 25, wherein the processor is further to track a second free form shape face region in the second video frame based on the free form shape face region in the video frame.\\n27. The system of claim 21, wherein to encode the video frame based at least in part on the first candidate face region being a valid face region comprises the processor to reduce a quantization parameter corresponding to the first candidate face region, adjust a lambda value for the first candidate face region, or disable skip coding for the first candidate face region.\\n28. At least one non-transitory machine readable medium comprising a plurality of instructions that, in response to being executed on a device, cause the device to perform video coding based on face detection by: receiving a video frame comprising one of a plurality of video frames of a video sequence; determining the video frame is a key frame of the video sequence; performing, in response to the video frame being a key frame of the video sequence, a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame; testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region; rejecting the second candidate face region and outputting the first candidate face region; and encoding the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream.\\n29. The non-transitory machine readable medium of claim 28, wherein the skin tone information comprises a skin probability map.\\n30. The non-transitory machine readable medium of claim 28, wherein the first candidate face region comprises a rectangular region, the machine readable medium comprising further instructions that, in response to being executed on the device, cause the device to perform video coding based on face detection by: determining a free form shape face region corresponding to the first candidate face region, wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy.\\n31. The non-transitory machine readable medium of claim 30, wherein determining the free form shape face region comprises: generating an enhanced skip probability map corresponding to the first candidate face region; binarizing the enhanced skip probability map; and overlaying the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region.\\n32. The non-transitory machine readable medium of claim 30, wherein a second video frame comprises a non-key frame of the video sequence, the machine readable medium comprising further instructions that, in response to being executed on the device, cause the device to perform video coding based on face detection by performing face detection in the second video frame of the video sequence based on the free form shape face region.\\n33. The non-transitory machine readable medium of claim 32, the machine readable medium comprising further instructions that, in response to being executed on the device, cause the device to perform video coding based on face detection by: tracking a second free form shape face region in the second video frame based on the free form shape face region in the video frame.\\n34. The non-transitory machine readable medium of claim 28, wherein encoding the video frame based at least in part on the first candidate face region being a valid face region comprises at least one of reducing a quantization parameter corresponding to the first candidate face region, adjusting a lambda value for the first candidate face region, or disabling skip coding for the first candidate face region.', '1. A method for managing a smart database which stores facial images for face recognition, comprising steps of: (a) a managing device performing a process of counting one or more specific facial images corresponding to at least one specific person stored in the smart database where new facial images for the face recognition are continuously stored, and a process of determining whether a first counted value representing a count of the specific facial images satisfies a preset first set value; and (b) if the first counted value is determined as satisfying the first set value, the managing device performing a process of inputting the specific facial images into a neural aggregation network, to thereby allow the neural aggregation network to generate each of quality scores of each of the specific facial images by aggregation of the specific facial images, and a process of sorting the quality scores corresponding to the specific facial images in a descending order of the quality scores, a process of counting the sorted specific facial images in the descending order until a second counted value which represents the number of a counted part of the specific facial images becomes equal to a preset second set value, and a process of deleting an uncounted part of the specific facial images from the smart database.\\n2. The method of claim 1, further comprising a step of: (c) the managing device performing a process of generating at least one optimal feature by weighted summation of one or more features of the specific facial images using the counted part of the quality scores and a process of setting the optimal feature as a representative face corresponding to the specific person.\\n3. The method of claim 1, wherein, at the step of (b), the managing device performs a process of inputting the specific facial images into a CNN of the neural aggregation network, to thereby allow the CNN to generate one or more features corresponding to each of the specific facial images, and a process of inputting at least one feature vector, where the features are embedded, into an aggregation module including at least two attention blocks, to thereby allow the aggregation module to generate each of the quality scores of each of the features.\\n4. The method of claim 1, wherein, at the step of (b), the managing device performs a process of matching (i) (i-1) one or more features corresponding to each of the specific facial images stored in the smart database and (i-2) the quality scores with (ii) the specific person, and a process of storing the matched features and the matched quality scores in the smart database.\\n5. The method of claim 1, further comprising a step of: (d) the managing device performing one of (i) a process of learning a face recognition system by using the specific facial images corresponding to the specific person stored in the smart database and (ii) a process of transmitting the specific facial images, corresponding to the specific person, to a learning device corresponding to the face recognition system, to thereby allow the learning device to learn the face recognition system using the specific facial images.\\n6. The method of claim 1, wherein the neural aggregation network has been learned by a learning device repeating more than once (i) a process of inputting multiple facial images for training corresponding to an image set of a single face or a video of the single face into a CNN of the neural aggregation network, to thereby allow the CNN to generate one or more features for training by applying at least one convolution operation to the facial images for training, (ii) a process of inputting at least one feature vector for training, where the features for training are embedded, into an aggregation module, including at least two attention blocks, of the neural aggregation network, to thereby allow the aggregation module to generate each of quality scores for training of each of the features for training by aggregation of the features for training using one or more attention parameters learned in a previous iteration, (iii) a process of outputting at least one optimal feature for training by weighted summation of the features for training using the quality scores for training, and (iv) a process of updating the attention parameters learned in the previous iteration of the at least two attention blocks such that one or more losses are minimized which are outputted from a loss layer by referring to the optimal feature for training and its corresponding ground truth.\\n7. A managing device for managing a smart database which stores facial images for face recognition, comprising: at least one memory that stores instructions; and at least one processor configured to execute the instructions to perform or support another device to perform: (I) a process of counting one or more specific facial images corresponding to at least one specific person stored in the smart database where new facial images for the face recognition are continuously stored, and a process of determining whether a first counted value representing a count of the specific facial images satisfies a preset first set value, and (II) if the first counted value is determined as satisfying the first set value, a process of inputting the specific facial images into a neural aggregation network, to thereby allow the neural aggregation network to generate each of quality scores of each of the specific facial images by aggregation of the specific facial images, and a process of sorting the quality scores corresponding to the specific facial images in a descending order of the quality scores, a process of counting the sorted specific facial images in the descending order until a second counted value which represents the number of a counted part of the specific facial images becomes equal to a preset second set value, and a process of deleting an uncounted part of the specific facial images from the smart database.\\n8. The managing device of claim 7, wherein the processor further performs: (III) a process of generating at least one optimal feature by weighted summation of one or more features of the specific facial images using the counted part of the quality scores and a process of setting the optimal feature as a representative face corresponding to the specific person.\\n9. The managing device of claim 7, wherein, at the process of (II), the processor performs a process of inputting the specific facial images into a CNN of the neural aggregation network, to thereby allow the CNN to generate one or more features corresponding to each of the specific facial images, and a process of inputting at least one feature vector, where the features are embedded, into an aggregation module including at least two attention blocks, to thereby allow the aggregation module to generate each of the quality scores of each of the features.\\n10. The managing device of claim 7, wherein, at the process of (II), the processor performs a process of matching (i) (i-1) one or more features corresponding to each of the specific facial images stored in the smart database and (i-2) the quality scores with (ii) the specific person, and a process of storing the matched features and the matched quality scores in the smart database.\\n11. The managing device of claim 7, wherein the processor further performs: (IV) one of (i) a process of learning a face recognition system by using the specific facial images corresponding to the specific person stored in the smart database and (ii) a process of transmitting the specific facial images, corresponding to the specific person, to a learning device corresponding to the face recognition system, to thereby allow the learning device to learn the face recognition system using the specific facial images.\\n12. The managing device of claim 7, wherein the neural aggregation network has been learned by a learning device repeating more than once (i) a process of inputting multiple facial images for training corresponding to an image set of a single face or a video of the single face into a CNN of the neural aggregation network, to thereby allow the CNN to generate one or more features for training by applying at least one convolution operation to the facial images for training, (ii) a process of inputting at least one feature vector for training, where the features for training are embedded, into an aggregation module, including at least two attention blocks, of the neural aggregation network, to thereby allow the aggregation module to generate each of quality scores for training of each of the features for training by aggregation of the features for training using one or more attention parameters learned in a previous iteration, (iii) a process of outputting at least one optimal feature for training by weighted summation of the features for training using the quality scores for training, and (iv) a process of updating the attention parameters learned in the previous iteration of the at least two attention blocks such that one or more losses are minimized which are outputted from a loss layer by referring to the optimal feature for training and its corresponding ground truth.', '1. An object data processing system comprising: at least one processor configured to execute: at least one implementation of a plurality of recognition algorithms stored on at least one non-transitory computer-readable storage medium, each recognition algorithm having feature density selection criteria; and data preprocessing code executed by at least one processor, the data preprocessing code comprising an invariant feature identification algorithm and configured to: obtain a digital representation of a scene, the scene comprising one or more textual media; generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation; cluster the set of invariant features into regions of interest in the digital representation of the scene, each region of interest having a region feature density; classify, by region classifier code, at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation, wherein the at least one of the classified regions of interest corresponds to text; and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type, wherein the another of the regions of interest corresponds to a region of interest for images.\\n2. The system of claim 1, wherein preprocessing code, based on the feature density selection criteria, determines that an OCR algorithm is applicable to the text, and that other recognition algorithms are applicable to aspects of the photographs and to logos.\\n3. The system of claim 1, wherein a user creates a user profile for a camera-equipped smartphone that includes the information that the user is visually impaired, which causes prioritized execution of the OCR algorithm such that a text reader program begins reading the text to the user as quickly as possible.\\n4. The system of claim 3, further comprising an audio or tactile feedback mechanism that helps the user to position the smart phone relative to the text.\\n5. The system of claim 4, further comprising a \"hold still\" audio feedback signal that is sent to the user when the text is at the center of the captured scene.\\n6. The system of claim 1, wherein the digital representation comprises at least one of the following types of digital data: image data, video data, and audio data.\\n7. The system of claim 1, wherein invariant feature identification algorithm comprises at least one of the following feature identification algorithms: FAST, SIFT, FREAK, BRISK, Harris, DAISY, and MSER.\\n8. The system of claim 1, wherein the invariant feature identification algorithm includes at least one of the following: edge detection algorithm, corner detection algorithm, saliency map algorithm, curve detection algorithm, a texton identification algorithm, and wavelets algorithm.\\n9. The system of claim 1, wherein at least one region of interest represents at least one physical object in the scene.\\n10. The system of claim 1, wherein at least one region of interest represents at least one textual media in the scene.\\n11. The system of claim 10, wherein the region of interest represents a document as the textual media.\\n12. The system of claim 11, wherein the region of interest represents a financial document.\\n13. The system of claim 11, wherein the region of interest represents a structured document.\\n14. The system of claim 1, wherein at least one implementation of a plurality of recognition algorithms includes at least one of the following: a template driven algorithm, a face recognition algorithm, an optical character recognition algorithm, a speech recognition algorithm, and an object recognition algorithm.\\n15. The system of claim 1, wherein data preprocessing code is further configured to assign each region of interest at least one recognition algorithm as a function of a scene context derived from the digital representation.\\n16. The system of claim 15, wherein the scene context includes at least one of the following types of data: a location, a position, a time, a user identity, a news event, a medical event, and a promotion.\\n17. The system of claim 1, further comprising a mobile device comprising at least one implementation of a plurality of recognition algorithms and data preprocessing code.\\n18. The system of claim 17, wherein the mobile device comprises at least one of the following: a smart phone, a tablet, wearable glass, a toy, a vehicle, a computer, and a phablet.\\n19. The system of claim 1, further comprising a network-accessible server device comprising at least one implementation of a plurality of recognition algorithms and data preprocessing code.\\n20. The system of claim 1, wherein the object type includes at least one of the following: a face, an animal, a vehicle, a document, a plant, a building, an appliance, clothing, a body part, and a toy.\\n21. An object data processing system comprising: at least one processor configured to execute: at least one implementation of a plurality of recognition algorithms stored on at least one non-transitory computer-readable storage medium, each recognition algorithm having feature density selection criteria; and data preprocessing code executed by at least one processor, the data preprocessing code comprising an invariant feature identification algorithm and configured to: obtain a digital representation of a scene, the scene comprising one or more textual media; generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation; cluster the set of invariant features into regions of interest in the digital representation of the scene, each region of interest having a region feature density; classify, by region classifier code, at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation; wherein the at least one of the classified regions of interest corresponds to text; and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type, wherein the another of the regions of interest corresponds to a region of interest for images; assign each region of interest at least one recognition algorithm from at least one implementation of a plurality of diverse recognition algorithms as a function of the region feature density of each region of interest and the feature density selection criteria of the at least one implementation of a plurality of diverse recognition algorithms; and configure the assigned recognition algorithms to process their respective regions of interest, wherein preprocessing code, based on the feature density selection criteria, determines that an OCR algorithm is applicable to the text, and that other recognition algorithms are applicable to aspects of the photographs and to logos.\\n22. A device comprising: at least one processor configured to execute: at least one implementation of a plurality of recognition algorithms stored on at least one non-transitory computer-readable storage medium, each recognition algorithm having feature density selection criteria; and data preprocessing code executed by at least one processor, the data preprocessing code comprising an invariant feature identification algorithm and configured to: obtain a digital representation of a scene, the scene comprising one or more textual media; generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation; cluster the set of invariant features into regions of interest in the digital representation of the scene, each region of interest having a region feature density; and classify, by region classifier code, at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation, wherein the at least one of the classified regions of interest corresponds to text; and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type, wherein the another of the regions of interest corresponds to a region of interest for images.', '1. A mobile terminal comprising: a front camera configured to obtain a two-dimensional (2D) face image of a user; a glance sensor tilted by a certain angle and disposed adjacent to the front camera to obtain metadata of the 2D face image; and a controller obtaining a distance between the glance sensor and the front camera, the distance enabling an area of an overlap region, where a first region representing a range photographable by the front camera overlaps a second region representing a range photographable by the glance sensor, to be the maximum.\\n2. The mobile terminal of claim 1, wherein the controller is configured to obtain the distance, enabling the area of the overlap region to be the maximum, between the glance sensor and the front camera by varying a tilting angle of the glance sensor.\\n3. The mobile terminal of claim 2, wherein the controller is configured to set the distance, enabling the area of the overlap region to be the maximum, between the glance sensor and the front camera and the tilting angle of the glance sensor as an optimal disposition location of the glance sensor.\\n4. The mobile terminal of claim 3, wherein the controller is configured to set a disposition location of the front camera as an original point and calculates coordinates of a first triangle representing the first region, based on a field of view of the front camera and a maximum photographing distance of the front camera.\\n5. The mobile terminal of claim 4, wherein the controller is configured to calculate coordinates of a second triangle representing the second region, based on a field of view of the glance sensor, a maximum photographing distance of the glance sensor, a distance between the front camera and the glance sensor, and a tilting angle of the glance sensor.\\n6. The mobile terminal of claim 5, wherein before the glance sensor is tilted, the controller is configured to calculate coordinates of a third triangle representing a third region photographable by the glance sensor, and the controller is configured to rotation-convert the coordinates of the third triangle, based on the tilting angle of the glance sensor and calculate the coordinates of the second triangle.\\n7. The mobile terminal of claim 6, wherein the controller is configured to calculate coordinates of the overlap region, based on the coordinates of the first triangle and the coordinates of the second triangle and calculates the area of the overlap region, based on the coordinates of the overlap region.\\n8. The mobile terminal of claim 1, wherein the controller is configured to generate three-dimensional (3D) face information, based on the 2D face image obtained by the front camera and metadata obtained by the glance sensor.\\n9. The mobile terminal of claim 8, wherein the metadata comprises one or more of an angle of a face of the user, a size of the face, and a location of the face.\\n10. The mobile terminal of claim 9, wherein the angle of the face comprises an angle by which the face is rotated about one or more of a pitch axis, a roll axis, and a yaw axis.\\n11. The mobile terminal of claim 8, further comprising a memory storing the generated 3D face information, wherein the controller is configured to performs a user authentication process by comparing the stored 3D face information with 3D face information obtained for user authentication.\\n12. The mobile terminal of claim 1, wherein the glance sensor is controlled to be permanently activated with a low power to obtain a front image and metadata of the front image.\\n13. The mobile terminal of claim 1, wherein the front camera and the glance sensor are disposed on the same line in an upper end of the mobile terminal.\\n14. The mobile terminal of claim 1, wherein the glance sensor is tilted in one direction of an up direction, a down direction, a left direction, and a right direction.\\n15. The mobile terminal of claim 1, wherein the metadata is data which is changed when the mobile terminal is tilted by an external physical force.', '1. A method, comprising: receiving, by a smart television (TV), an indication of upcoming media programming, wherein the upcoming media programming is based on a user profile; identifying one or more devices in communication with the smart TV, each of the one or more devices including at least one of a microphone or a camera; instructing at least one identified device to detect audio signals using its respective microphone, or to detect visual signals using its respective camera; selecting at least one device of the one or more devices based on the detected audio signal or detected visual signal; and providing instructions to the selected device to output a notification related to the upcoming media programming.\\n2. The method of claim 1, wherein the upcoming media programming is one of a live television program, a recorded television program, a broadcast television program, or an application-provided program.\\n3. The method of claim 1, wherein selecting the first device based on the detected audio signal includes recognizing a voice.\\n4. The method of claim 3, further comprising determining a distance to the recognized voice, and wherein selecting the first device is further based on the determined distance.\\n5. The method of claim 1, wherein selecting the first device based on the detected visual signals includes recognizing a face.\\n6. The method of claim 5, wherein recognizing the face includes a face recognition technique.\\n7. The method of claim 1, further comprising presenting, on the smart TV, the upcoming media programming in a favorite channel list.\\n8. The method of claim 7, further comprising: obtaining media programming viewing data, wherein the media programming viewing data includes at least one of a historical time and a historical date that one or more media programs were viewed; obtaining at least one of a current time and a current date; processing the media programming viewing data to determine a probability of the one or more media programs being viewed based on at least one of the current time and the current date; and presenting the favorite channel list based on the determined probability of the one or more media programs being viewed.\\n9. The method of claim 8, wherein processing the media programming viewing data includes employing a neural network model.\\n10. The method of claim 9, wherein employing the neural network model comprises: determining a duration that the one or more media programs were viewed for each of the at least one of the historical time and the historical date; setting a threshold time duration; comparing the determined duration to the threshold time duration; and filtering out the one or more media programs viewed below the threshold time duration.\\n11. A smart television (TV), comprising: a network interface; a non-transitory computer-readable medium; and a processor in communication with the network interface, and the non-transitory computer-readable medium, and capable of executing processor-executable program code stored in the non-transitory computer-readable medium, to cause the smart TV to: receive an indication of upcoming media programming, wherein the upcoming media programming is based on a user profile; identify one or more devices in communication with the smart TV, each of the one or more devices including at least one of a microphone or a camera; instruct at least one identified device to detect audio signals using its respective microphone, or to detect visual signals using its respective camera; select at least one device of the one or more devices based on the detected audio signal or detected visual signal; and provide instructions to the selected device to output a notification related to the upcoming media programming.\\n12. The smart TV of claim 11, wherein selecting the first device based on the detected audio signal includes recognizing a voice.\\n13. The smart TV of claim 12, wherein the processor is further capable of executing processor-executable program code to: determine a distance to the recognized voice, and wherein selecting the first device is further based on the determined distance.\\n14. The smart TV of claim 11, wherein selecting the first device based on the detected visual signals includes detecting the presence of a user.\\n15. The smart TV of claim 14, wherein detecting the presence of the user includes employing one or more of a camera, a microphone, or a fingerprint sensor associated with at least one of the smart TV a mobile device, a smartphone, a laptop computer, a tablet device, a wearable device, an Internet of Things (IoT) device, an Internet of Everything (IoE) device, an IoT hub, or an IoE hub.\\n16. A smart television (TV), comprising: means for receiving an indication of upcoming media programming, wherein the upcoming media programming is based on a user profile; means for identifying one or more devices in communication with the smart TV, each of the one or more devices including at least one of a microphone or a camera; means for instructing at least one identified device to detect audio signals using its respective microphone, or to detect visual signals using its respective camera; means for selecting at least one device of the one or more devices based on the detected audio signal or detected visual signal; and means for providing instructions to the selected device to output a notification related to the upcoming media programming.\\n17. The smart TV of claim 16, wherein the one or more devices includes at least one of a mobile device, a smartphone, a laptop computer, a tablet device, a wearable device, an Internet of Things (IoT) device, an Internet of Everything (IoE) device, an IoT hub, an IoE hub, or another smart TV.\\n18. The smart TV of claim 16, wherein the upcoming media programming is one of a live television program, a recorded television program, a broadcast television program, or an application-provided program.\\n19. The smart TV of claim 16, wherein the notification includes at least one of a push message, a SMS message, a Way2SMS message, an audio alert, an audio message, or an email message.\\n20. The smart TV of claim 16, further comprising presenting the upcoming media programming in a favorite channel list.\\n21. The smart TV of claim 20, further comprising: means for obtaining media programming viewing data, wherein the media programming viewing data includes at least one of a historical time and a historical date that one or more media programs were viewed on the smart TV; means for obtaining at least one of a current time and a current date; means for processing the media programming viewing data to determine a probability of the one or more media programs being viewed on the smart TV based on at least one of the current time and the current date; and means for presenting the favorite channel list based on the determined probability of the one or more media programs being viewed.\\n22. The smart TV of claim 21, wherein the means for processing the media programming viewing data includes employing a neural network model.\\n23. The smart TV of claim 22, wherein employing the neural network model comprises: determining a duration that the one or more media programs were viewed on the smart TV for each of the at least one of the historical time and the historical date; setting a threshold time duration; comparing the determined duration to the threshold time duration; and filtering out the one or more media programs viewed below the threshold time duration.\\n24. The smart TV of claim 21, further comprising: means for adjusting at least one of a volume or a brightness of the smart TV, wherein the adjusting is based on at least one of the historical time and the historical date.\\n25. The smart TV of claim 21, further comprising means for restricting access to one or more media programs.\\n26. A non-transitory computer-readable medium comprising processor-executable program code configured to cause a processor of a smart television (TV) to: receive an indication of upcoming media programming, wherein the upcoming media programming is based on a user profile; identify one or more devices in communication with the smart TV, each of the one or more devices including at least one of a microphone or a camera; instruct at least one identified device to detect audio signals using its respective microphone, or to detect visual signals using its respective camera; select at least one device of the one or more devices based on the detected audio signal or detected visual signal; and provide instructions to the selected device to output a notification related to the upcoming media programming.\\n27. The non-transitory computer-readable medium of claim 26, wherein selecting the first device based on the detected audio signal includes recognizing a voice.\\n28. The non-transitory computer-readable medium of claim 27, wherein the processor is further capable of executing processor-executable program code to: determine a distance to the recognized voice, and wherein selecting the first device is further based on the determined distance.\\n29. The non-transitory computer-readable medium of claim 26, wherein selecting the first device based on the detected visual signals includes recognizing a face.\\n30. The non-transitory computer-readable medium of claim 29, wherein recognizing the face includes a face recognition technique.', '1. A camera comprising: a sensor array including a plurality of sensors; an infrared (IR) illuminator configured to emit active IR light in an IR light sub-band; a plurality of spectral illuminators, each spectral illuminator configured to emit active spectral light in a different spectral light sub-band; a depth controller machine configured to determine a depth value for each of the plurality of sensors based on the active IR light, a spectral controller machine configured to, for each of the plurality of sensors, determine a spectral value for each spectral light sub-band of the plurality of spectral illuminators; and an output machine configured to output a test depth+multi-spectral image including a plurality of pixels, each pixel corresponding to one of the plurality of sensors of the sensor array and including at least: a depth value, and a spectral value for each spectral light sub-band of the plurality of spectral illuminators; a face recognition machine previously trained with a set of labeled training depth+multi-spectral images having a same structure as the test depth+multi-spectral image, the face recognition machine configured to output a confidence value indicating a likelihood that the test depth+multi-spectral image includes a face.\\n2. The camera of claim 1, wherein each spectral value is calculated based on the depth value determined for the sensor that corresponds to the pixel.\\n3. The camera of claim 1, wherein the face recognition machine is configured to use a convolutional neural network to determine the confidence value.\\n4. The camera of claim 3, wherein the face recognition machine includes a plurality of input nodes, wherein each input node is configured to receive a pixel value array corresponding to a different pixel of the plurality of pixels of the test depth+multi-spectral image, and wherein the pixel value array includes the depth value and the plurality of multi-spectral values for the pixel.\\n5. The camera of claim 4, wherein the plurality of multi-spectral values for the pixel include more than three spectral values.\\n6. The camera of claim 4, wherein the output machine is configured to output a surface normal for each pixel of the test depth+multi-spectral image, and wherein the pixel value array includes the surface normal.\\n7. The camera of claim 4, wherein the output machine is configured to output a curvature for each pixel of the test depth+multi-spectral image, and wherein the pixel value array includes the curvature.\\n8. The camera of claim 3, wherein the face recognition machine is configured to use a plurality of models to determine the confidence value, wherein the plurality of models includes a plurality of channel-specific models, wherein each channel-specific model is configured to process a different pixel parameter for the plurality of pixels of the test depth+multi-spectral image, wherein each channel-specific model includes a plurality of input nodes, and wherein, for each channel-specific model, each input node is configured to receive a pixel parameter value for a different pixel of the plurality of pixels of the test depth+multi-spectral image.\\n9. The camera of claim 1, wherein the face recognition machine is configured to use a statistical model to determine the confidence value.\\n10. The camera of claim 9, wherein the statistical model includes a nearest neighbor algorithm.\\n11. The camera of claim 9, wherein the statistical model includes a support vector machine.\\n12. The camera of claim 1, wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of a bounding box around a recognized face.\\n13. The camera of claim 1, wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of an identified two-dimensional (2D) facial feature of a recognized face.\\n14. The camera of claim 1, wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of an identified three-dimensional (3D) facial feature of a recognized face.\\n15. The camera of claim 1, wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of an identified spectral feature on a recognized face.\\n16. The camera of claim 1, wherein the face recognition machine is further configured to output, for each pixel of the test depth+multi-spectral image, a confidence value indicating a likelihood that the pixel is included in a face.\\n17. The camera of claim 1, wherein the face recognition machine is further configured to output an identity of a face recognized in the test depth+multi-spectral image.\\n18. The camera of claim 1, wherein the plurality of sensors of the sensor array are differential sensors, and wherein each spectral value is determined based on a depth value and a differential measurement for that differential sensor.\\n19. A camera comprising: a sensor array including a plurality of sensors; an infrared (IR) illuminator configured to emit active IR light in an IR light sub-band; a plurality of spectral illuminators, each spectral illuminator configured to emit active spectral light in a different spectral light sub-band; a depth controller machine configured to determine a depth value for each of the plurality of sensors based on the active IR light, a spectral controller machine configured to, for each of the plurality of sensors, determine a spectral value for each spectral light sub-band of the plurality of spectral illuminators, wherein each spectral value is calculated based on the depth value determined for the sensor that corresponds to the pixel; and an output machine configured to output a test depth+multi-spectral image including a plurality of pixels, each pixel corresponding to one of the plurality of sensors of the sensor array and including at least: a depth value, and a spectral value for each spectral light sub-band of the plurality of spectral illuminators; and a face recognition machine including a convolutional neural network previously trained with a set of labeled training depth+multi-spectral images having a same structure as the test depth+multi-spectral image, the face recognition machine configured to output a confidence value indicating a likelihood that the test depth+multi-spectral image includes a face.', '1. An image processing method, comprising: acquiring a photo album obtained from face clustering; collecting face information of respective images in the photo album, and acquiring a face parameter of each image according to the face information; selecting a cover image according to the face parameter of each image; and taking a face-region image from the cover image, and setting the face-region image as a cover of the photo album; wherein selecting the cover image according to the face parameter of each image comprises: performing calculation on the face parameter of each image in a preset way, to obtain a cover score of each image; selecting the image with a highest cover score as the cover image; wherein selecting the image with the highest cover score as the cover image comprises: acquiring a source of each image; and selecting the image with the highest cover score in images coming from a preset source as the cover image.\\n2. The method according to claim 1, wherein selecting the image with the highest cover score as the cover image comprises: acquiring the number of faces contained in each image; determining single-person images according to the number of faces; and selecting the single-person image with the highest cover score as the cover image.\\n3. The method according to claim 2, wherein selecting the image with the highest cover score as the cover image further comprises: when there is no single-person image in the photo album, determining images including two faces from the photo album; and selecting the image with the highest cover score from the images including two faces as the cover image.\\n4. The method according to claim 1, wherein the face information comprises face feature points, and the face parameter comprises a face turning angle; acquiring the face parameter of each image according to the face information comprises: acquiring coordinate values of the face feature points; determining distances and angles between the face feature points; and determining the face turning angle according to the distances and the angles.\\n5. The method according to claim 1, wherein the face parameter comprises a face ratio; acquiring the face parameter of each image according to the face information comprises: determining a face region of the image according to the face information; and calculating a ratio of an area of the face region to an area of the image to obtain the face ratio.\\n6. The method according to claim 5, wherein calculating the face ratio comprises: when there is more than one face in the image, subtracting an area occupied faces other than a face corresponding to the photo album from the face region to obtain a remaining area; and calculating a ratio of the remaining area to the area of the image to obtain the face ratio.\\n7. The method according to claim 1, wherein collecting face information of respective images in the photo album comprises: acquiring image identifications of images in the photo album; extracting face information corresponding to the image identifications from a face database, the face database being stored with face recognition results of images, the face recognition results including the face information.\\n8. An image processing apparatus, comprising: a processor; and a memory, configured to store instructions executable by the processor, wherein the processor is configured to run a program corresponding to the instructions by reading the instructions stored in the memory, so as to perform: acquiring a photo album obtained from face clustering; collecting face information of each image in the photo album; acquiring a face parameter of each image according to the face information; selecting a cover image according to the face parameter of each image; taking a face-region image from the cover image, and setting the face-region image as a cover of the photo album; wherein the processor is configured to: perform calculation on the face parameter of each image in a preset way, to obtain a cover score of each image; and select the image with a highest cover score as the cover image; and wherein the processor is configured to: acquire a source of each image; and select the image with the highest cover score in images coming from a preset source as the cover image.\\n9. The apparatus according to claim 8, wherein the processor is configured to: acquire the number of faces contained in each image; determine single-person images according to the number of faces; and select the single-person image with the highest cover score as the cover image.\\n10. The apparatus according to claim 9, wherein the processor is further configured to: when there is no single-person image in the photo album, determine images including two faces from the photo album; and select the image with the highest cover score from the images including two faces as the cover image.\\n11. The apparatus according to claim 8, wherein the face information comprises face feature points, and the face parameter comprises a face turning angle; the processor is configured to: acquire coordinate values of the face feature points; determine distances and angles between the face feature points; and determine the face turning angle according to the distances and the angles.\\n12. The apparatus according to claim 8, wherein the face parameter comprises a face ratio; the processor is configured to: determine a face region of the image according to the face information; and calculate a ratio of an area of the face region to an area of the image to obtain the face ratio.\\n13. The apparatus according to claim 12, wherein the processor is configured to: when there is more than one face in the image, subtract an area occupied faces other than a face corresponding to the photo album from the face region to obtain a remaining area; and calculate a ratio of the remaining area to the area of the image to obtain the face ratio.\\n14. The apparatus according to claim 8, wherein the processor is configured to: acquire image identifications of images in the photo album; extract face information corresponding to the image identifications from a face database, the face database being stored with face recognition results of images, the face recognition results including the face information.\\n15. An electronic device, comprising a processor, a memory, a display screen and an input device connected via a system bus, wherein the memory is stored with computer programs that, when executed by the processor, cause the processor to implement an image processing method, the image processing method comprising: acquiring a photo album obtained from face clustering; collecting face information of respective images in the photo album, and acquiring a face parameter of each image according to the face information; selecting a cover image according to the face parameter of each image; and taking a face-region image from the cover image, and setting the face-region image as a cover of the photo album; wherein selecting the cover image according to the face parameter of each image comprises: performing calculation on the face parameter of each image in a preset way, to obtain a cover score of each image; and selecting the image with a highest cover score as the cover image; and wherein selecting the image with the highest cover score as the cover image comprises: acquiring a source of each image; and selecting the image with the highest cover score in images coming from a preset source as the cover image.\\n16. The electronic device according to claim 15, wherein the electronic device comprises at least one of a mobile phone, a tablet computer, a personal digital assistant and a wearable device.', '1. A computer-implemented method, comprising: receiving, at a computing device, a meeting invitation identifying a location and at least one invitee, the meeting invitation configured to provide the at least one invitee with physical access to the location, wherein the meeting invitation causes a system to control a pathway allowing physical access to the location; providing, based on the meeting invitation, the at least one invitee with physical access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location wherein the positioning data is based in part on a face recognition camera system identifying the at least one invitee; receiving the positioning data from the face recognition camera system identifying the at least one invitee, wherein the positioning data indicates a pattern of movement of the at least one invitee; determining that the pattern of movement indicates that the at least one invitee has exited the location; and revoking physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway, in response to determining that the pattern of movement indicates that the at least one invitee has exited the location.\\n2. The computer-implemented method of claim 1, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location in a predetermined direction.\\n3. The computer-implemented method of claim 1, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has moved through an area in a predetermined direction.\\n4. The computer-implemented method of claim 1, wherein the positioning data indicates a second pattern of movement of the at least one invitee and, wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement.\\n5. The computer-implemented method of claim 1, further comprising: collating secured data and public data to generate resource data; and communicating the resource data to a client computing device associated with the at least one invitee when access of the location is provided.\\n6. The computer-implemented method of claim 1, wherein the positioning data indicates that the at least one invitee is at the predetermined location when the at least one invitee passes through the predetermined location.\\n7. The computer-implemented method of claim 1, wherein the positioning data indicates that the at least one invitee is at the predetermined location when the at least one invitee passes through the predetermined location near the location in a predetermined direction.\\n8. A system, comprising: a processor; and a memory in communication with the processor, the memory having computer-readable instructions stored thereupon that, when executed by the processor, cause the processor to: receive a meeting invitation indicating a location and an identity, the meeting invitation configured to provide at least one invitee with physical access to the location, wherein the meeting invitation causes the system to control a pathway allowing physical access to the location; provide the at least one invitee associated with the identity access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location, wherein the positioning data is based in part on a face recognition camera system identifying the at least one invitee; receive the positioning data from the face recognition camera system identifying the at least one invitee, wherein the positioning data indicates a pattern of movement of the at least one invitee; determine that the pattern of movement indicates that the at least one invitee has exited the location; and revoke physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway, in response to determining that the pattern of movement indicates that the at least one invitee has exited the location.\\n9. The system of claim 8, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location.\\n10. The system of claim 8, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has moved through an area in a predetermined direction.\\n11. The system of claim 8, wherein the positioning data indicates a second pattern of movement of the at least one invitee and wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement.\\n12. The system of claim 8, wherein the instructions further cause the processor to: collate secured data and public data to generate resource data; and communicate the resource data to a client computing device associated with the at least one invitee when access of the location is provided.\\n13. A non-transitory computer-readable storage medium having computer-executable instructions stored thereupon which, when executed by one or more processors of a computing device, cause the one or more processors of the computing device to: receive a meeting invitation indicating a location and an identity, the meeting invitation configured to provide at least one invitee with physical access to the location, wherein the meeting invitation causes a system to control a pathway allowing physical access to the location; provide the at least one invitee associated with the identity access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location, wherein the positioning data is based in part on a face recognition camera system identifying the at least one invitee; receive the positioning data from the face recognition camera system identifying the at least one invitee, wherein the positioning data indicates a pattern of movement of the at least one invitee; determine that the pattern of movement indicates that the at least one invitee has exited the location; and revoke physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway, in response to determining that the pattern of movement indicates that the at least one invitee has exited the location.\\n14. The non-transitory computer-readable storage medium of claim 13, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location.\\n15. The non-transitory computer-readable storage medium of claim 13, wherein the positioning data indicates a second pattern of movement of the at least one invitee and wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement.\\n16. The non-transitory computer-readable storage medium of claim 13, wherein the instructions further cause the one or more processors to: collate secured data and public data to generate resource data; and communicate the resource data to a client computing device associated with the at least one invitee when access of the location is provided.', '1. A method, comprising: receiving a piece of content and salient data for the piece of content; based on the salient data, determining a first path for a viewport for the piece of content, wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content; providing the viewport on a display device, wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback; detecting an additional salient event in the piece of content that is not included in the first path for the viewport; and providing an indication for the additional salient event in the viewport during the playback.\\n2. The method of claim 1, wherein the salient data identifies each salient event in the piece of content, and the salient data indicates, for each salient event in the piece of content, a corresponding point location of the salient event in the piece of content and a corresponding time at which the salient event occurs during the playback.\\n3. The method of claim 2, wherein the salient data further indicates, for each salient event in the piece of content, a corresponding type of the salient event and a corresponding strength value of the salient event.\\n4. The method of claim 1, wherein the first path for the viewport controls the movement of the viewport to put the different salient events in a view of the viewport at the different times during the playback.\\n5. The method of claim 1, further comprising: detecting one or more salient events in the piece of content based on at least one of the following: visual data of the piece of content, audio data of the piece of content, or content consumption experience data for the piece of content; wherein the salient data is indicative of each salient event detected.\\n6. The method of claim 1, further comprising: detecting one or more salient events in the piece of content based on at least one of the following: face recognition, facial emotion recognition, object recognition, motion recognition, or metadata of the piece of content; wherein the salient data is indicative of each salient event detected.\\n7. The method of claim 1, further comprising: detecting user interaction with the indication, wherein the indication comprises an interactive hint; and in response to detecting the user interaction: adapting the first path for the viewport to a second path for the viewport based on the user interaction, wherein the second path for the viewport includes the additional salient event; and providing an updated viewport for the piece of content on the display device, wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback, and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport.\\n8. The method of claim 7, further comprising: changing a weight assigned to the additional salient event and one or more other salient events in the piece of content having the same type as the additional salient event.\\n9. The method of claim 7, wherein the second path for the viewport includes one or more other salient events in the piece of content having the same type as the additional salient event.\\n10. A system, comprising: at least one processor; and a non-transitory processor-readable memory device storing instructions that when executed by the at least one processor causes the at least one processor to perform operations including: receiving a piece of content and salient data for the piece of content; based on the salient data, determining a first path for a viewport for the piece of content, wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content; providing the viewport on a display device, wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback; detecting an additional salient event in the piece of content that is not included in the first path for the viewport; and providing an indication for the additional salient event in the viewport during the playback.\\n11. The system of claim 10, wherein the salient data identifies each salient event in the piece of content, and the salient data indicates, for each salient event in the piece of content, a corresponding point location of the salient event in the piece of content and a corresponding time at which the salient event occurs during the playback.\\n12. The system of claim 11, wherein the salient data further indicates, for each salient event in the piece of content, a corresponding type of the salient event and a corresponding strength value of the salient event.\\n13. The system of claim 10, wherein the salient data is generated offline on a server.\\n14. The system of claim 10, the operations further comprising: detecting one or more salient events in the piece of content based on at least one of the following: visual data of the piece of content, audio data of the piece of content, or content consumption experience data for the piece of content; wherein the salient data is indicative of each salient event detected.\\n15. The system of claim 10, the operations further comprising: detecting one or more salient events in the piece of content based on at least one of the following: face recognition, facial emotion recognition, object recognition, motion recognition, or metadata of the piece of content; wherein the salient data is indicative of each salient event detected.\\n16. The system of claim 10, the operations further comprising: detecting user interaction with the indication, wherein the indication comprises an interactive hint; and in response to detecting the user interaction: adapting the first path for the viewport to a second path for the viewport based on the user interaction, wherein the second path for the viewport includes the additional salient event; and providing an updated viewport for the piece of content on the display device, wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback, and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport.\\n17. The system of claim 16, the operations further comprising: changing a weight assigned to the additional salient event and one or more other salient events in the piece of content having the same type as the additional salient event.\\n18. The system of claim 16, wherein the second path for the viewport includes one or more other salient events in the piece of content having the same type as the additional salient event.\\n19. A non-transitory computer readable storage medium including instructions to perform a method comprising: receiving a piece of content and salient data for the piece of content; based on the salient data, determining a first path for a viewport for the piece of content, wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content; providing the viewport on a display device, wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback; detecting an additional salient event in the piece of content that is not included in the first path for the viewport; and providing an indication for the additional salient event in the viewport during the playback.\\n20. The computer readable storage medium of claim 19, the method further comprising: detecting user interaction with the indication, wherein the indication comprises an interactive hint; and in response to detecting the user interaction: adapting the first path for the viewport to a second path for the viewport based on the user interaction, wherein the second path for the viewport includes the additional salient event; and providing an updated viewport for the piece of content on the display device, wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback, and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport.', '1. A mobile device with facial recognition, the mobile device comprising: one or more cameras; a processor device and memory coupled to the processor device, the processing system programmed to: receive a plurality of images from the one or more cameras; extract, with a feature extractor utilizing a convolutional neural network (CNN) with an enlarged intra-class variance of long-tail classes, feature vectors from each of the plurality of images; generate, with a feature generator, discriminative feature vectors for each of the feature vectors; classify, with a fully connected classifier, an identity from the discriminative feature vectors; and control an operation of the mobile device to react in accordance with the identity.\\n2. The mobile device as recited in claim 1, further includes a communication system.\\n3. The mobile device as recited in claim 1, wherein the operation tags the video with the identity and uploads the video to social media.\\n4. The mobile device as recited in claim 1, wherein the operation tags the video with the identity and sends the video to a user.\\n5. The mobile device as recited in claim 1, wherein the mobile device is a smart phone.\\n6. The mobile device as recited in claim 1, wherein the mobile device is a body cam.\\n7. The mobile device as recited in claim 1, further programmed to train the feature extractor, the feature generator, and the fully connected classifier with an alternative bi-stage strategy.\\n8. The mobile device as recited in claim 1, wherein the feature extractor shares covariance matrices across all classes to transfer intra-class variance from regular classes to the long-tail classes.\\n9. The mobile device as recited in claim 1, wherein the feature generator optimizes a softmax loss by joint regularization of weights and features through a magnitude of an inner product of the weights and features.\\n10. The mobile device as recited in claim 1, wherein the feature extractor averages the feature vector with a flipped feature vector, the flipped feature vector being generated from a horizontally flipped frame from one of the plurality of images.\\n11. The mobile device as recited in claim 1, wherein each of the plurality of images is selected from the group consisting of an image, a video, and a frame from the video.\\n12. The mobile device as recited in claim 2, wherein the communication system connects to a remote server that includes a facial recognition network.\\n13. The mobile device as recited in claim 7, wherein one stage of the alternative bi-stage strategy fixes the feature extractor and applies the feature generator to generate new transferred features that are more diverse and violate a decision boundary.\\n14. The mobile device as recited in claim 7, wherein one stage of the alternative bi-stage strategy fixes the fully connected classifier and updates the feature extractor and the feature generator.\\n15. A computer program product for a mobile device with facial recognition, the computer program product comprising a non-transitory computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computer to cause the computer to perform a method comprising: receiving, by a processor device, a plurality of images; extracting, by the processor device with a feature extractor utilizing a convolutional neural network (CNN) with an enlarged intra-class variance of long-tail classes, feature vectors for each of the plurality of images; generating, by the processor device with a feature generator, discriminative feature vectors for each of the feature vectors; classifying, by the processor device utilizing a fully connected classifier, an identity from the discriminative feature vector; and controlling an operation of the mobile device to react in accordance with the identity.\\n16. A computer-implemented method for facial recognition in a mobile device, the method comprising: receiving, by a processor device, a plurality of images; extracting, by the processor device with a feature extractor utilizing a convolutional neural network (CNN) with an enlarged intra-class variance of long-tail classes, feature vectors for each of the plurality of images; generating, by the processor device with a feature generator, discriminative feature vectors for each of the feature vectors; classifying, by the processor device utilizing a fully connected classifier, an identity from the discriminative feature vector; and controlling an operation of the mobile device to react in accordance with the identity.\\n17. The computer-implemented method as recited in claim 16, wherein controlling includes tagging the video with the identity and uploading the video to social media.\\n18. The computer-implemented method as recited in claim 16, wherein controlling includes tagging the video with the identity and sending the video to a user.\\n19. The computer-implemented method as recited in claim 16, wherein extracting includes sharing covariance matrices across all classes to transfer intra-class variance from regular classes to the long-tail classes.', '1. A computing device comprising: a non-transitory machine readable medium storing a machine trained (MT) network comprising a plurality of layers of processing nodes, each processing node configured to: compute a first output value by combining a set of output values from a set of processing nodes, and use a piecewise linear cup function to compute a second output value from the first output value of the processing node, wherein the piecewise linear cup function prior to training of the MT network comprises at least (i) a first linear section with a first slope, followed by (ii) a second linear section with a negative second slope, followed by (iii) a third linear section with a negative third slope that is different from the second slope, followed by (iv) a fourth linear section with a positive fourth slope, followed by (v) a fifth linear section with a positive fifth slope that is different from the fourth slope, followed by (vi) a sixth linear section with a sixth slope, wherein the piecewise linear cup function is symmetric about a vertical axis between the third and fourth linear sections prior to training of the MT network; a content capturing circuit for capturing content for processing by the MT network; and a set of processing units for executing the processing nodes to process content captured by the content capturing circuit, wherein by training a set of parameters that define the piecewise linear cup function of each node in first and second pluralities of processing nodes, (i) each processing node in the first plurality of processing nodes is configured to emulate a Boolean AND operator such that an output value of the processing node is in a range associated with a \"1\" value only when a set of inputs to the processing node have a set of values in a range associated with \"1\" and (ii) each processing node in the second plurality of processing nodes is configured to emulate a Boolean XNOR operator such that an output value of the processing node is in the range associated with \"1\" only when (a) a set of inputs to the node have a set of values in a range associated with \"1\" or (b) the set of inputs to the node have a set of values in a range associated with a \"0\" value.\\n2. The computing device of claim 1, wherein the third linear section of the piecewise linear cup function of a first processing node in the MT network has a different slope from the third linear section of a second processing node in the MT network.\\n3. The computing device of claim 1, wherein the length of the third section of a piecewise linear cup function of a first processing node in the MT network is different from the length of the third section of a piecewise linear cup function of a second processing node in the MT network.\\n4. The computing device of claim 1, wherein the sets of parameters are trained in part by a back propagating module for back propagating errors in output values of later layers of processing nodes to earlier layers of processing nodes by adjusting the set of parameters that define the piecewise linear cup functions of the earlier layers of processing nodes.\\n5. The computing device of claim 4, wherein each processing node uses a linear function that is defined by a set of parameters to compute the first output value of the processing node, wherein the back propagating module back propagates errors in output values of later layers of processing nodes to earlier layers of processing nodes by adjusting the set of parameters that define the linear functions of the earlier layers of processing nodes.\\n6. The computing device of claim 1, wherein the first plurality of processing nodes that emulate the Boolean AND operator and the second plurality of processing nodes that emulate the Boolean XNOR operator enable the MT network to implement mathematical problems.\\n7. The computing device of claim 1, wherein each of a plurality of processing node layers has a plurality of processing nodes that receive as input values the output values from a plurality of processing nodes in a set of prior layers.\\n8. The computing device of claim 7, wherein each processing node uses a linear function to compute the first output value of the processing node, wherein each processing node\\'s piecewise linear cup function is defined along first and second axes, the first axis defining a range of output values from the processing node\\'s linear function, and the second axis defining a range of output values produced by the piecewise linear cup function for the range of output values from the processing node\\'s linear function.\\n9. The computing device of claim 1, further comprising: a content output circuit for presenting an output based on the processing of the content by the MT network.\\n10. The computing device of claim 9, wherein the captured content is one of an image and an audio segment, and wherein the presented output is an output display on a display screen of the computing device or an audio presentation output on a speaker of the computing device.\\n11. The computing device of claim 10, wherein the computing device is a mobile device.\\n12. The computing device of claim 1, wherein the MT network is a MT neural network and the processing nodes are MT neurons.\\n13. The computing device of claim 1, wherein the set of parameters configured through training for a plurality of the processing nodes comprise at least one of the negative second and third slopes for the second and third linear sections, the positive fourth and fifth slopes for the fourth and fifth linear sections, a first intercept for the second linear section, a second intercept for the fifth linear section, and a set of lengths for at least the second, third, fourth, and fifth sections.\\n14. The computing device of claim 1, wherein the trained set of parameters that define the piecewise linear cup function of each node comprise a plurality of output values.\\n15. The computing device of claim 1, wherein the first and sixth slopes are zero.', 'We claim:\\n1. A system comprising: a memory device to store an input image; a processor including, an image input interface to receive the input image, a pre-processor to model the input image to yield a multi-channel image, a feature extractor to extract a set of features based on the multi-channel image, a feature selector to select one or more features from the set of features of the multi-channel image, wherein the one or more features are selected based on an ability to differentiate features, a feature matcher to match the one or more features to a learned feature set, and a similarity detector to determine whether the one or more features meet a pre-defined similarity threshold.\\n2. The system of claim 1, wherein the pre-processor further is to activate one or more channels of the multi-channel image to yield one or more activated channels.\\n3. The system of claim 2, wherein the one or more activated channels are to be determined based on their ability to differentiate features.\\n4. The system of claim 2, wherein the pre-processor further is to activate one or more local patches of the one or more activated channels.\\n5. The system of claim 4, wherein the one or more local patches are to be determined based on their ability to differentiate features.\\n6. The system of claim 1, wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching.\\n7. An apparatus comprising: an image input interface to receive an input image; a pre-processor to model the input image to yield a multi-channel image; a feature extractor to extract a set of features based on the multi-channel image; a feature selector to select one or more features from the set of features of the multi-channel image, wherein the one or more features are selected based on an ability to differentiate features; a feature matcher to match the one or more features to a learned feature set; and a similarity detector to determine whether the one or more features meet a pre-defined similarity threshold.\\n8. The apparatus of claim 7, wherein the pre-processor further is to activate one or more channels of the multi-channel image to yield one or more activated channels.\\n9. The apparatus of claim 8, wherein the one or more activated channels are to be determined based on their ability to differentiate features.\\n10. The apparatus of claim 8, wherein the pre-processor further is to activate one or more local patches of the one or more activated channels.\\n11. The apparatus of claim 10, wherein the one or more local patches are to be determined based on their ability to differentiate features.\\n12. The apparatus of claim 7, wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching.\\n13. A method comprising: modeling an input image to yield a multi-channel image; extracting a set of features based on the multi-channel image; selecting one or more features from the set of features of the multi-channel image, wherein the one or more features are selected based on an ability to differentiate features; matching the one or more features to a learned feature set; and determining whether the one or more features meet a pre-defined similarity threshold.\\n14. The method of claim 13, wherein modeling the input image further is to include activating one or more channels of the multi-channel image to yield one or more activated channels.\\n15. The method of claim 14, wherein the one or more activated channels are to be determined based on their ability to differentiate features.\\n16. The method of claim 13, wherein extracting features of the input image further is to include activating one or more local patches of the one or more activated channels.\\n17. The method of claim 16, wherein the one or more local patches are to be determined based on their ability to differentiate features.\\n18. The method of claim 13, wherein the feature matcher utilizes a large-scale data learning process to perform the feature matching.\\n19. At least one non-transitory computer readable storage medium comprising a set of instructions which, when executed by a computing device, cause the computing device to: model an input image to yield a multi-channel image, extract a set of features based on the multi-channel image, select one or more features from the set of features of the multi-channel image, wherein the features are selected based on an ability to differentiate features, match the one or more features to a learned feature set, and determine whether the one or more features meet a pre-defined similarity threshold.\\n20. The at least one non-transitory computer readable storage medium of claim 19, wherein the instructions, when executed, cause a computing device to activate one or more channels of the multi-channel image to yield one or more activated channels.\\n21. The at least one non-transitory computer readable storage medium of claim 20, wherein the instructions, when executed, cause a computing device to determine the one or more activated channels based on their ability to differentiate features.\\n22. The at least one non-transitory computer readable storage medium of claim 20, wherein extracting features of the input image is to further include activating one or more local patches of the one or more activated channels.\\n23. The at least one non-transitory computer readable storage medium of claim 22, wherein the one or more local patches are to be determined based on their ability to differentiate features.\\n24. The at least one non-transitory computer readable storage medium of claim 19, wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching.\\n25. An apparatus comprising: means for modeling an input image to yield a multi-channel image, means for extracting a set of features based on the multi-channel image, means for selecting one or more features from the set of features of the multi-channel image, wherein the one or more features are selected based on an ability to differentiate features, means for matching the one or more features to a learned feature set, and means for determining whether the one or more features meet a pre-defined similarity threshold.', '1. A method for controlling a terminal, the terminal comprising a capturing apparatus and at least one processor, the method comprising: acquiring, by the capturing apparatus, an image; obtaining, by the at least one processor, a motion parameter of the terminal, the motion parameter comprising at least one of a motion frequency or a motion time, and two or more parameters from among an acceleration, an angular velocity, a motion amplitude, the motion frequency, and the motion time; transmitting, by the at least one processor, a parameter threshold obtaining request to a data management server, the parameter threshold obtaining request comprising configuration information of the terminal; receiving corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request; comparing the two or more parameters with the corresponding preset thresholds; and controlling, by the at least one processor, not to perform image processing on the acquired image based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds, wherein the acquiring comprises acquiring the image in real time, and the obtaining comprises obtaining the motion parameter of the terminal in real time, the method further comprising: in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, obtaining the motion parameter of the terminal again; and in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds, performing the image processing on the image acquired at the latest time.\\n2. The method according to claim 1, wherein the acquiring comprises: controlling, by the at least one processor, to turn on the capturing apparatus based on a face recognition instruction; and acquiring, by the capturing apparatus, a face image when the capturing apparatus is turned on.\\n3. The method according to claim 2, wherein the controlling not to perform the image processing comprises: skipping performing face recognition on the acquired face image based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds.\\n4. The method according to claim 1, wherein the obtaining comprises at least one of: obtaining the acceleration of the terminal by using an acceleration sensor; or obtaining the angular velocity of the terminal by using a gyro sensor.\\n5. The method according to claim 1, wherein the transmitting comprises: transmitting the parameter threshold obtaining request to the data management server according to a preset time period.\\n6. The method according to claim 1, further comprising: generating prompt information based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, the prompt information being used for prompting the terminal to stop moving.\\n7. The method according to claim 1, wherein the motion parameter comprises the motion frequency and the motion time.\\n8. A terminal comprising: a capturing apparatus; at least one memory configured to store program code; and at least one processor configured to access the at least one memory and operate according to the program code, the program code comprising: motion parameter obtaining code configured to cause the at least one processor to acquire an image by using the capturing apparatus and obtain a motion parameter of the terminal, the motion parameter comprising at least one of a motion frequency or a motion time, and two or more parameters from among an acceleration, an angular velocity, a motion amplitude, the motion frequency, and the motion time; request transmitting code configured to cause the at least one processor to transmit a parameter threshold obtaining request to a data management server, the parameter threshold obtaining request comprising configuration information of the terminal; parameter threshold receiving code configured to cause the at least one processor to receive corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request; comparing code configured to cause the at least one processor to compare the two or more parameters with the corresponding preset thresholds; and control code configured to cause the at least one processor not to perform image processing on the acquired image based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds, wherein the motion parameter obtaining code causes the at least one processor to: acquire the image in real time and obtain the motion parameter of the terminal in real time, and in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, obtain the motion parameter of the terminal again, and wherein the control code causes the at least one processor to, in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds, perform the image processing on the image acquired at the latest time.\\n9. The terminal according to claim 8, wherein the program code further comprises face instruction receiving code configured to cause the at least one processor to receive a face recognition instruction, wherein the motion parameter obtaining code causes the at least one processor to control, according to the face recognition instruction, the capturing apparatus to turn on, and acquire a face image by using the capturing apparatus when the capturing apparatus is turned on; and wherein the control code causes the at least one processor to skip performing face recognition on the acquired face image based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds.\\n10. The terminal according to claim 8, wherein the request transmitting code causes the at least one processor to transmit the parameter threshold obtaining request to the data management server according to a preset time period.\\n11. The terminal according to claim 8, wherein the program code further comprises: prompt information generation code configured to cause the at least one processor to generate prompt information based on at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, the prompt information being used for prompting the terminal to stop moving.\\n12. The terminal according to claim 8, wherein the motion parameter comprises the motion frequency and the motion time.\\n13. A non-transitory computer-readable storage medium, storing a machine instruction, which, when executed by one or more processors, causes the one or more processors to perform: obtaining an image acquired by a capturing apparatus; obtaining a motion parameter of a terminal, the terminal comprising the capturing apparatus, the motion parameter comprising at least one of a motion frequency or a motion time, and two or more parameters from among an acceleration, an angular velocity, a motion amplitude, the motion frequency, and the motion time; transmitting a parameter threshold obtaining request to a data management server, the parameter threshold obtaining request comprising configuration information of the terminal; receiving corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request; comparing the two or more parameters with the corresponding preset thresholds; and controlling not to perform image processing on an acquired image based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds, wherein the acquiring comprises acquiring the image in real time, and the obtaining comprises obtaining the motion parameter of the terminal in real time, the method further comprising: in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, obtaining the motion parameter of the terminal again; and in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds, performing the image processing on the image acquired at the latest time.\\n14. The non-transitory computer-readable storage medium according to claim 13, wherein the acquired image is a face image and the image processing comprises performing face recognition.\\n15. The non-transitory computer-readable storage medium according to claim 13, wherein the obtaining the motion parameter comprises at least one of: obtaining the acceleration of the terminal by using an acceleration sensor; or obtaining the angular velocity of the terminal by using a gyro sensor.\\n16. The non-transitory computer-readable storage medium according to claim 13, wherein the motion parameter comprises the motion frequency and the motion time.', '1. A method of processing a drive-through order, the method comprising: receiving customer information detected through vision recognition; providing product information to a customer based on the customer information; and processing a product order of the customer.\\n2. The method according to claim 1, wherein the receiving of customer information comprises at least one of receiving customer information associated with vehicle information detected through vehicle recognition, or receiving customer information associated with identification information detected through face recognition.\\n3. The method according to claim 1, further comprising determining whether the customer is a pre-order customer based on the customer information, wherein when the customer is determined to be a pre-order customer: the providing of product information based on the customer information comprises providing pre-order information using at least one of audio or video, and the processing of the product order of the customer comprises: providing information for promptly guiding a vehicle to a pickup stand using at least one of audio or video, and providing information that an additional order is available.\\n4. The method according to claim 1, wherein the product information based on the customer information comprises a most recently ordered product component and a most frequently ordered product component in an order history of the customer information.\\n5. The method according to claim 1, wherein the receiving of customer information comprises receiving information about an age and gender of a passenger detected through face recognition, and the providing of product information to a customer based on the customer information comprises providing recommended menu information differentiated according to the age and gender.\\n6. The method according to claim 1, wherein the processing of a product order of the customer comprises determining a product component in a past order history or a component modified from the product component as a product order.\\n7. The method according to claim 1, wherein the processing of a product order of the customer comprises paying a product price according to biometrics-based authentication through a communication system of a vehicle or a mobile terminal.\\n8. The method according to claim 1, wherein the processing of a product order of the customer comprises: issuing a payment number for a divided payment, and performing the divided payments according to payment requests of a plurality of mobile terminals to which the payment numbers are inputted.\\n9. The method according to claim 8, wherein the processing of a product order of the customer further comprises accumulating mileage in an account corresponding to the mobile terminal undergoing a payment.\\n10. The method according to claim 1, wherein the processing of a product order of the customer further comprises suggesting a takeout packaging method according to a temperature of a product, an atmospheric temperature, weather, and a vehicle type.\\n11. An apparatus configured to process a drive-through order, the apparatus comprising: a transceiver configured to receive customer information detected through vision recognition; a digital signage configured to provide product information to a customer based on the customer information; and a processor configured to process a product order of the customer.\\n12. The apparatus according to claim 11, wherein the transceiver receives at least one of customer information associated with vehicle information detected through vehicle recognition, or customer information associated with identification information detected through face recognition.\\n13. The apparatus according to claim 11, wherein the processor is configured to: determine whether the customer is a pre-order customer based on the customer information; and when the customer is determined to be a pre-order customer, perform a control operation to provide pre-order information, and control the digital signage to output information for promptly guiding a vehicle to a pickup stand and provide information that an additional order is available.\\n14. The apparatus according to claim 11, wherein the product information based on the customer information comprises a most recently ordered product component and a most frequently ordered product component in an order history of the customer information.\\n15. The apparatus according to claim 11, wherein the transceiver is configured to receive information about an age and gender of a passenger detected through face recognition, and the processor is configured to control the digital signage to provide recommended menu information differentiated according to the age and gender.\\n16. The apparatus according to claim 11, wherein the processor is configured to determine a product component in a past order history or a component modified from the product component as the product order.\\n17. The apparatus according to claim 11, wherein the processor is configured to pay a product price according to biometrics-based authentication through a communication system of a vehicle or a mobile terminal.\\n18. The apparatus according to claim 11, wherein the processor is configured to: issue a payment number for a divided payment; and perform the divided payments according to requests of a plurality of mobile terminals to which the payment numbers are inputted.\\n19. The apparatus according to claim 18, wherein the processor is configured to accumulate mileage in an account corresponding to the mobile terminal undergoing a payment.\\n20. The apparatus according to claim 11, wherein the processor is configured to control the digital signage to suggest a takeout packaging method according to a temperature of a product, an atmospheric temperature, weather, and a vehicle type.', '1. An image information processing method performed at a computing device having one or more processors and memory storing a plurality of programs to be executed by the one or more processors, the method comprising: identifying, using face recognition, one or more faces, each face corresponding to a respective person captured in a first image; for each identified face: extracting a set of profile parameters of a corresponding person in the first image; and selecting, from a plurality of image tiles, a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile; generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles; and sharing the first image and the second image in a predefined order via a group chat session.\\n2. The method of claim 1, wherein the first image and the second image are displayed in the group chat session one image at a time such that one of the two images is replaced by the other of the two images periodically.\\n3. The method of claim 1, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model, wherein the first machine learning model is trained with the facial images and corresponding descriptive labels.\\n4. The method of claim 1, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining an identity of the corresponding person based on the identified face of the corresponding person; locating respective profile information of the first person based on the determined identity of the corresponding person; and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person.\\n5. The method of claim 1, wherein at least a first one of the first image tiles is a dynamic image tile and at least a second one of the first image tiles is a static image tile.\\n6. The method of claim 1, including: receiving a plurality of user comments from different users of the group chat session, each user comment including a descriptive term for a respective person identified in the first image; choosing a descriptive label for the respective person according to the plurality of user comments; and updating the second image by adding the descriptive label adjacent to the first image tile of the respective person.\\n7. A computing device for image information processing, comprising: one or more processors; and memory storing instructions which, when executed by the one or more processors, cause the processors to perform a plurality of operations comprising: identifying, using face recognition, one or more faces, each face corresponding to a respective person captured in a first image; for each identified face: extracting a set of profile parameters of a corresponding person in the first image; and selecting, from a plurality of image tiles, a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile; generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles; and sharing the first image and the second image in a predefined order via a group chat session.\\n8. The computing device of claim 7, wherein the first image and the second image are displayed in the group chat session one image at a time such that one of the two images is replaced by the other of the two images periodically.\\n9. The computing device of claim 7, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model, wherein the first machine learning model is trained with the facial images and corresponding descriptive labels.\\n10. The computing device of claim 7, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining an identity of the corresponding person based on the identified face of the corresponding person; locating respective profile information of the first person based on the determined identity of the corresponding person; and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person.\\n11. The computing device of claim 7, wherein at least a first one of the first image tiles is a dynamic image tile and at least a second one of the first image tiles is a static image tile.\\n12. The computing device of claim 7, wherein the plurality of operations further include: receiving a plurality of user comments from different users of the group chat session, each user comment including a descriptive term for a respective person identified in the first image; choosing a descriptive label for the respective person according to the plurality of user comments; and updating the second image by adding the descriptive label adjacent to the first image tile of the respective person.\\n13. A non-transitory computer-readable storage medium storing instructions which, when executed by a computing device having one or more processors, cause the computing device to perform a plurality of operations comprising: identifying, using face recognition, one or more faces, each face corresponding to a respective person captured in a first image; for each identified face: extracting a set of profile parameters of a corresponding person in the first image; and selecting, from a plurality of image tiles, a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile; generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles; and sharing the first image and the second image in a predefined order via a group chat session.\\n14. The non-transitory computer-readable storage medium of claim 13, wherein the first image and the second image are displayed in the group chat session one image at a time such that one of the two images is replaced by the other of the two images periodically.\\n15. The non-transitory computer-readable storage medium of claim 13, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model, wherein the first machine learning model is trained with the facial images and corresponding descriptive labels.\\n16. The non-transitory computer-readable storage medium of claim 13, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining an identity of the corresponding person based on the identified face of the corresponding person; locating respective profile information of the first person based on the determined identity of the corresponding person; and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person.\\n17. The non-transitory computer-readable storage medium of claim 13, wherein at least a first one of the first image tiles is a dynamic image tile and at least a second one of the first image tiles is a static image tile.\\n18. The non-transitory computer-readable storage medium of claim 13, wherein the plurality of operations further include: receiving a plurality of user comments from different users of the group chat session, each user comment including a descriptive term for a respective person identified in the first image; choosing a descriptive label for the respective person according to the plurality of user comments; and updating the second image by adding the descriptive label adjacent to the first image tile of the respective person.', '1. A method comprising, by a computing system: determining that a performance metric of an eye tracking system is below a first performance threshold, wherein the eye tracking system is associated with a head-mounted display worn by a user; based on the determination of the performance metric of the eye tracking system being below the first performance threshold, the computer system performing: receiving one or more first inputs associated with a body of the user; estimating a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user; determining a vergence distance of the user based at least on the one or more first inputs associated with the body of the user, the estimated region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display; and adjusting one or more configurations of the head-mounted display based on the determined vergence distance of the user.\\n2. The method of claim 1, wherein the one or more configurations of the head-mounted display comprise one or more of: a rendering image; a position of a display screen; or a position of an optics block.\\n3. The method of claim 1, further comprising: determining that the performance metric of the eye tracking system is above a second performance threshold; receiving eye tracking data from the eye tracking system; and determining the vergence distance of the user based on the eye tracking data and the one or more first inputs associated with the body of the user.\\n4. The method of claim 3, further comprising: receiving one or more second inputs associated with one or more displaying elements in the scene displayed by the head-mounted display; and determining the vergence distance of the user based at least on the eye tracking data, the one or more first inputs associated with the body of the user, and the one or more second inputs associated with the one or more displaying elements of the scene.\\n5. The method of claim 4, further comprising: feeding the one or more first inputs associated with the body of the user to a fusion algorithm, wherein the fusion algorithm assigns a weight score to each input of the one or more first inputs; determining the vergence distance of the user using the fusion algorithm based on the one or more first inputs associated with the body of the user; and determining a Z-depth of a display screen and a confidence score based on the one or more first inputs associated with the body of the user\\n6. The method of claim 5, further comprising: comparing the confidence score to a confidence level threshold; in response to a determination that the confidence score is below the confidence level threshold, feeding the one or more second inputs associated with the one or more displaying elements of the scene to the fusion algorithm; and determining the Z-depth of the display screen using the fusion algorithm based on the one or more first inputs associated with the body of the user and the one or more second inputs associated with the one or more displaying elements of the scene.\\n7. The method of claim 6, further comparing: comparing, by the fusion algorithm, confidence scores associated with a plurality of combinations of inputs; and determining, by the fusion algorithm, the Z-depth of the display screen based on a combination of inputs associated with a highest confidence score.\\n8. The method of claim 6, wherein the Z-depth and the confidence score are determined by the fusion algorithm using a piecewise comparison of the one or more first inputs and the one or more second inputs.\\n9. The method of claim 6, wherein the Z-depth and the confidence score are determined based on a correlation between two or more inputs of the one or more first inputs and the one or more second inputs.\\n10. The method of claim 5, wherein the fusion algorithm comprises a machine learning (ML) algorithm, and wherein the machine learning (ML) algorithm determines a combination of first inputs fed to the fusion algorithm.\\n11. The method of claim 4, wherein the one or more first inputs associated with the body of the user comprise one or more of: a hand position; a hand direction; a hand movement; a hand gesture; a head position; a head direction; a head movement; a head gesture; a gaze angle; rea body gesture; a body posture; a body movement; a behavior of the user; or a weighted combination of one or more related parameters.\\n12. The method of claim 11, wherein the one or more first inputs associated with the body of the user are received from one or more of: a controller; a sensor; a camera; a microphone; an accelerometer; a headset worn by the user; or a mobile device.\\n13. The method of claim 4, wherein the one or more second inputs associated with the one or more displaying elements comprise one or more of: a Z-buffer value associated with a displaying element; a displaying element marked by a developer; an image analysis result; a shape of a displaying element; a face recognition result; an object recognition result; a person identified in a displaying content; an object identified in a displaying content; a correlation of two or more displaying elements; or a weighted combination of the one or more second inputs.\\n14. The method of claim 1, further comprising: determining that the performance metric of the eye tracking system is below a second performance threshold; receiving one or more second inputs associated with one or more displaying elements in the scene displayed by the head-mounted display; and determining the vergence distance of the user based at least on the one or more first inputs associated with the body of the user and the one or more second inputs associated with the one or more displaying elements.\\n15. The method of claim 14, wherein determining that the performance metric of the eye tracking system is below the second performance threshold comprises determining that the eye tracking system does not exist or fails to provide eye tracking data.\\n16. The method of claim 1, wherein the performance metric of the eye tracking system comprises one or more of: an accuracy of a parameter from the eye tracking system; a precision of a parameter from the eye tracking system; a value of a parameter from the eye tracking system; a detectability of a pupil; a metric based on one or more parameters associated with the user; a parameter change; a parameter changing trend; a data availability; or a weighted combination of one or more performance related parameters.\\n17. The method of claim 16, wherein the one or more parameters associated with the user comprise one or more of: an eye distance of the user; a pupil position; a pupil status; a correlation of two pupils of the user; a head size of the user; a position of a headset worn by the user; an angle of the headset worn by the user; a direction of the headset worn by the user; an alignment of the eyes of the user; or a weighted combination of one or more related parameters associated with the user.\\n18. The method of claim 1, wherein the first performance threshold comprises one or more of: a pre-determined value; a pre-determined range; a state of a data; a changing speed of a data; or a trend of a data change.\\n19. One or more non-transitory computer-readable storage media embodying software that is operable when executed by a computing system to: determine that a performance metric of an eye tracking system is below a first performance threshold, wherein the eye tracking system is associated with a head-mounted display worn by a user; based on the determination of the performance metric of the eye tracking system being below the first performance threshold, the media embodying software operable when executed by the computing system to: receive one or more first inputs associated with a body of the user; estimate a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user; determine a vergence distance of the user based at least on the one or more first inputs associated with the body of the user, the estimated region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display; and adjust one or more configurations of the head-mounted display based on the determined vergence distance of the user.\\n20. A system comprising: one or more non-transitory computer-readable storage media embodying instructions; one or more processors coupled to the storage media and operable to execute the instructions to: determine that a performance metric of an eye tracking system is below a first performance threshold, wherein the eye tracking system is associated with a head-mounted display worn by a user; based on the determination of the performance metric of the eye tracking system being below the first performance threshold, the system is configured to: receive one or more first inputs associated with a body of the user; estimate a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user; determine a vergence distance of the user based at least on the one or more first inputs associated with the body of the user, the estimated region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display; and adjust one or more configurations of the head-mounted display based on the determined vergence distance of the user.', '1. A computer-implemented method for image-based, self-guided object detection, comprising: receiving, by a processor device, a set of images, each of the images having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data; training, by the processor device, a grid-based object detector using the grid level label data; determining, by the processor device, a respective bounding box for the respective object in each of the images, by applying local segmentation to each of the images; and training, by the processor device, a Region-based Convolutional Neural Network (RCNN) for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the RCNN.\\n2. The computer-implemented method of claim 1, further comprising performing an action responsive to the object localization and object classification for a respective new object in a new image to which the RCNN is applied.\\n3. The computer-implemented method of claim 2, wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object.\\n4. The computer-implemented method of claim 1, wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of images.\\n5. The computer-implemented method of claim 1, wherein the local segmentation is applied to each of the images to segment a respective target region therein.\\n6. The computer-implemented method of claim 1, wherein the Region-based Convolutional Neural Network (RCNN) forms a model during an object training stage that is to detect objects in new images during an inference stage.\\n7. The computer-implemented method of claim 1, wherein the method is performed by a system selected from the group consisting of a surveillance system, a face detection system, a face recognition system, a cancer detection system, an object tracking system, and an Advanced Driver-Assistance System.\\n8. A computer program product for image-based, self-guided object detection, the computer program product comprising a non-transitory computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computer to cause the computer to perform a method comprising: receiving, by a processor device, a set of images, each of the images having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data; training, by the processor device, a grid-based object detector using the grid level label data; determining, by the processor device, a respective bounding box for the respective object in each of the images, by applying local segmentation to each of the images; and training, by the processor device, a Region-based Convolutional Neural Network (RCNN) for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the RCNN.\\n9. The computer program product of claim 8, wherein the method further comprises performing an action responsive to the object localization and object classification for a respective new object in a new image to which the RCNN is applied.\\n10. The computer program product of claim 9, wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object.\\n11. The computer program product of claim 8, wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of images.\\n12. The computer program product of claim 8, wherein the local segmentation is applied to each of the images to segment a respective target region therein.\\n13. The computer program product of claim 8, wherein the Region-based Convolutional Neural Network (RCNN) forms a model during an object training stage that is to detect objects in new images during an inference stage.\\n14. The computer program product of claim 8, wherein the method is performed by a system selected from the group consisting of a surveillance system, a face detection system, a face recognition system, a cancer detection system, an object tracking system, and an Advanced Driver-Assistance System.\\n15. A computer processing system for image-based, self-guided object detection, comprising: a memory device for storing program code; and a processor device for running the program code to receive a set of images, each of the images having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data; train a grid-based object detector using the grid level label data; determine a respective bounding box for the respective object in each of the images, by applying local segmentation to each of the images; and train a Region-based Convolutional Neural Network (RCNN) for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the RCNN.\\n16. The computer processing system of claim 15, wherein the processor device further runs the program code to perform an action responsive to the object localization and object classification for a respective new object in a new image to which the RCNN is applied.\\n17. The computer processing system of claim 16, wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object.\\n18. The computer processing system of claim 15, wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of images.\\n19. The computer processing system of claim 15, wherein the Region-based Convolutional Neural Network (RCNN) forms a model during an object training stage that is to detect objects in new images during an inference stage.\\n20. The computer processing system of claim 15, wherein the computer processing system is comprised in a system selected from the group consisting of a surveillance system, a face detection system, a face recognition system, a cancer detection system, an object tracking system, and an Advanced Driver-Assistance System.', '1. A method of scalable, parallel, cloud-based face recognition utilizing a database of normalized stored images, comprising: capturing an image using a camera; detecting a face in the captured image; normalizing the detected facial image to match the normalized stored images; identifying facial features in the normalized detected facial image; generating a plurality of facial metrics from the facial features; calculating Euclidean distances between the facial metrics of the normalized detected facial image with corresponding facial metrics of each of the stored images; comparing each Euclidean distance against a predetermined threshold; responsive to the Euclidean distance comparison, producing a reduced candidate list of best possible image matches from the normalized stored images; comparing, in parallel, the normalized detected facial image with each of the normalized stored images of the reduced candidate list utilizing a plurality of face recognition algorithms, where each processor of a parallel processing system uses a different face recognition algorithm; responsive to the comparison, producing best match results from each parallel subset of the reduced candidate list; and selecting a final match from the best match results using a deep learning neural network face recognition algorithm trained on outputs of individual face recognition algorithms.\\n2. The method of scalable, parallel, cloud-based face recognition of claim 1, wherein detecting a face in the captured image comprises: utilizing OpenCV to detect a face in the captured image; extracting the location of the eyes and a tip of the nose in the face; determining a distance between the eyes; cropping the face from the captured image, where the width and the height of a cropped face image is a function of the distance between the eyes; and rotating the face by an angle of rotation that is a function of the distance between the eyes.\\n3. The method of scalable, parallel, cloud-based face recognition of claim 2, wherein: the width of the cropped face image is 2.5 times the distance between the eyes; the height of the cropped face image is 3.5 times the distance between the eyes; and the angle of rotation is an angle formed by a straight line joining the eyes and an x-axis of the face.\\n4. The method of scalable, parallel, cloud-based face recognition of claim 3, wherein rotating the face comprises rotating the face to provide a frontal face pattern.\\n5. The method of scalable, parallel, cloud-based face recognition of claim 4, further comprising the step of proportionally rescaling the cropped and rotated image.\\n6. The method of scalable, parallel, cloud-based face recognition of claim 5, where the proportional rescaling yields a cropped and rotated image with a size of 100=100 pixels.\\n7. The method of scalable, parallel, cloud-based face recognition of claim 1, wherein the facial features identified in the normalized detected facial image comprise a pair of eyes, a tip of a nose, a mouth, a center of the mouth, and a chin area comprising a bottom, a top left landmark, and a top right landmark.\\n8. The method of scalable, parallel, cloud-based face recognition of claim 7, wherein generating a plurality of facial metrics comprises: calculating a distance between the pair of eyes, a distance between the eyes and the tip of the nose, a distance equal to the width of the mouth, a distance between the tip of the nose and the center of mouth, a distance between the bottom of chin and the center of mouth, a distance between the top left landmark on the chin and the tip of the nose, and a distance between the top right landmark on the chin and the tip of the nose.\\n9. The method of scalable, parallel, cloud-based face recognition of claim 8, wherein performing a Euclidean distance match further comprises: partitioning the normalized stored images into a plurality of substantially equal subsets; performing a Euclidean distance match between the facial metrics of the normalized detected facial image and corresponding facial metrics of each of the stored images of the subsets of the normalized stored images with a separate processor of a parallel processing system to generate a Euclidean distance for each stored image of the subset; comparing each Euclidean distance against a predetermined threshold with the separate processors; responsive to the Euclidean distance comparison, producing a reduced candidate list of best possible image matches from the normalized stored images of each subset; and combining the reduced candidate lists from each subset to produce a single reduced candidate list.\\n10. The method of scalable, parallel, cloud-based face recognition of claim 9, wherein the plurality of face recognition algorithms utilized in comparing, in parallel, the normalized detected facial image with each of the normalized stored images of the reduced candidate list, consists of face recognition algorithms selected from a group consisting of Principle Component Analysis (PCA)-based algorithms, Linear Discriminant Analysis (LDA) algorithms, Independent Component Analysis (ICA) algorithms, kernel-based algorithms, feature-based techniques, algorithms based on neural networks, algorithms based on transforms, and model-based face recognition algorithms.\\n11. The method of scalable, parallel, cloud-based face recognition of claim 10, wherein the PCA-based algorithms include Eigenfaces for face detection/recognition, and the LDA algorithms include the Fisherfaces method of face recognition.\\n12. The method of scalable, parallel, cloud-based face recognition of claim 1, wherein comparing, in parallel, the captured image with each of the normalized stored images of the reduced candidate list further comprises: partitioning the reduced candidate list into a plurality of substantially equal subsets; processing each subset in a different processor of the parallel processing system uses a unique face recognition algorithm to produce the best match results; and using a reduce function of a MapReduce program to combine the best match results from each of the subsets to produce a single set of the best match results.\\n13. The method of scalable, parallel, cloud-based face recognition of claim 12, wherein partitioning the reduced candidate list comprises: selecting the images comprising each subset by optimizing the variance between of each of the images according to the following equation: where m and n are the number of rows and columns of the face vector image, N is the number of groups, and σij is the standard deviation of image dimension i in the group j of the face image vector.\\n14. The method of scalable, parallel, cloud-based face recognition of claim 13, wherein selecting the images comprising each subset by optimizing the variance between each of the images according to the following equation: d(μi, μj) is the Euclidean distance between the mean of the group i and the mean of group j, I is the face image vector, and L is the number of group levels.\\n15. The method of scalable, parallel, cloud-based face recognition of claim 1, where selecting a final match from the best match results utilizing a deep learning neural network face recognition algorithm comprises utilizing either an AdaBoost machine-learning algorithm or a neural networks machine-learning model.\\n16. The method of scalable, parallel, cloud-based face recognition of claim 1, where normalizing the detected facial image to match the normalized stored images includes normalizing the detected facial image to the same size, orientation, and illumination of the normalized stored images.\\n17. A non-transitory computer-readable medium containing executable program instructions for causing a computer to perform a method of face recognition, the method comprising: detecting a face in an image captured by a camera; normalizing the detected facial image to match the normalized stored images; identifying facial features in the normalized detected facial image; generating a plurality of facial metrics from the facial features; calculating Euclidean distances between the facial metrics of the normalized detected facial image with corresponding facial metrics of each of the stored images; comparing each Euclidean distance against a predetermined threshold; responsive to the Euclidean distance comparison, producing a reduced candidate list of best possible image matches from the normalized stored images; comparing, in parallel, the captured image with each of the normalized stored images of the reduced candidate list utilizing a plurality of face recognition algorithms, where each processor of a parallel processing system uses a different face recognition algorithm; responsive to the comparison, producing best match results from each parallel subset of the reduced candidate list; and selecting a final match from the best match results using a deep learning neural network face recognition algorithm trained on outputs of individual face recognition algorithms.\\n18. The non-transitory computer-readable medium containing executable program instructions of claim 17, wherein the plurality of face recognition algorithms utilized in comparing, in parallel, the normalized detected facial image with each of the normalized stored images of the reduced candidate list, consists of face recognition algorithms selected from a group consisting of Principle Component Analysis (PCA)-based algorithms, Linear Discriminant Analysis (LDA) algorithms, Independent Component Analysis (ICA) algorithms, kernel-based algorithms, feature-based techniques, algorithms based on neural networks, algorithms based on transforms, and model-based face recognition algorithms.\\n19. The non-transitory computer-readable medium containing executable program instructions of claim 18, wherein the PCA-based algorithms include Eigenfaces for face detection/recognition, and the LDA algorithms include the Fisherfaces method of face recognition.\\n20. The non-transitory computer-readable medium containing executable program instructions of claim 17, where selecting a final match from the best match results utilizing a deep learning neural network face recognition algorithm comprises utilizing either an AdaBoost machine-learning algorithm or a neural networks machine-learning model.', '1. An imaging device, comprising: a condensing lens; an image sensor configured to detect light passing through the condensing lens and comprising a pixel matrix, wherein the pixel matrix comprises a plurality of phase detection pixel pairs and a plurality of regular pixels; and a processor configured to turn on the phase detection pixel pairs for autofocusing and output autofocused pixel data after completing the autofocusing, divide the autofocused pixel data into a first subframe and a second subframe, calculate image features of at least one of the first subframe and the second subframe, wherein the image features comprise module widths of a finder pattern, and the finder pattern has a predetermined ratio, a Harr-like feature, or a Gabor feature, and determine an operating resolution of the regular pixels according to the image features calculated from at least one of the first subframe and the second subframe divided from the autofocused pixel data.\\n2. The imaging device as claimed in claim 1, wherein each of the phase detection pixel pairs comprises: a first pixel and a second pixel; a cover layer covering upon a first region of the first pixel and upon a second region of the second pixel, wherein the first region and the second region are mirror symmetrical to each other; and a microlens aligned with at least one of the first pixel and the second pixel.\\n3. The imaging device as claimed in claim 2, wherein the first region and the second region are 5% to 95% of an area of a single pixel.\\n4. The imaging device as claimed in claim 1, wherein the processor is configured to perform the autofocusing using a dual pixel autofocus technique according to pixel data of the phase detection pixel pairs before completing the autofocusing.\\n5. The imaging device as claimed in claim 1, wherein the processor is configured to divide pixel data of the phase detection pixel pairs into a third subframe and a fourth subframe before completing the autofocusing, and perform the autofocusing according to the third subframe and the fourth subframe.\\n6. The imaging device as claimed in claim 5, wherein the processor is further configured to calibrate brightness of the third subframe and the fourth subframe to be identical using a shading algorithm.\\n7. The imaging device as claimed in claim 1, wherein the operating resolution is selected as a first resolution smaller than a number of the regular pixels or as a second resolution larger than the first resolution.\\n8. The imaging device as claimed in claim 1, wherein the regular pixels are turned off in the autofocusing.\\n9. The imaging device as claimed in claim 1, wherein a number of the phase detection pixel pairs is smaller than that of the regular pixels.\\n10. An imaging device, comprising: a condensing lens; an image sensor configured to detect light passing through the condensing lens and comprising a pixel matrix, wherein the pixel matrix comprises a plurality of phase detection pixel pairs and a plurality of regular pixels; and a processor configured to turn on the phase detection pixel pairs for autofocusing and output autofocused pixel data after completing the autofocusing, divide the autofocused pixel data into a first subframe and a second subframe, calculate image features of at least one of the first subframe and the second subframe, wherein the image features comprise module widths of a finder pattern, and the finder pattern has a predetermined ratio, a Harr-like feature, or a Gabor feature, and select an image decoding or an image recognition using pixel data of the regular pixels according to the image features calculated from at least one of the first subframe and the second subframe divided from the autofocused pixel data.\\n11. The imaging device as claimed in claim 10, wherein each of the phase detection pixel pairs comprises: a first pixel and a second pixel; a cover layer covering upon a first region of the first pixel and upon a second region of the second pixel, wherein the first region and the second region are mirror symmetrical to each other; and a microlens aligned with at least one of the first pixel and the second pixel.\\n12. The imaging device as claimed in claim 10, wherein the processor is configured to perform the autofocusing using a dual pixel autofocus technique according to pixel data of the phase detection pixel pairs before completing the autofocusing.\\n13. The imaging device as claimed in claim 10, wherein the processor is configured to divide the pixel data of the phase detection pixel pairs into a third subframe and a fourth subframe before completing the autofocusing, calibrate brightness of the third subframe and the fourth subframe to be identical using a shading algorithm, and perform the autofocusing according to the third subframe and the fourth subframe.\\n14. The imaging device as claimed in claim 10, wherein the processor is configured to calculate the image features using at least one of a rule based algorithm and a machine learning algorithm.\\n15. The imaging device as claimed in claim 10, wherein the image decoding is decoding QR codes, and the image recognition is face recognition.\\n16. An operating method of an imaging device, the imaging device comprising a plurality of phase detection pixel pairs and a plurality of regular pixels, the operating method comprising: turning on the phase detection pixel pairs for autofocusing and outputting autofocused image frame after completing the autofocusing; dividing the autofocused image frame, acquired by the phase detection pixel pairs, into a first subframe and a second subframe; calculating image features of at least one of the first subframe and the second subframe, wherein the image feature comprise module widths of a finder pattern, and the finder pattern has a predetermined ratio, a Harr-like feature, or a Gabor feature; and selectively activating at least a part of the regular pixels according to the image features calculated from at least one of the first subframe and the second subframe divided from the autofocused image frame.\\n17. The operating method as claimed in claim 16, wherein the selectively activating comprises: activating a first part of the regular pixels to perform an image decoding according to pixel data of the first part of the regular pixels; or activating all the regular pixels to perform an image recognition according to pixel data of the all regular pixels.\\n18. The operating method as claimed in claim 17, wherein pixel data of the phase detection pixel pairs captured in a same frame with the pixel data of the regular pixels is also used in performing the image decoding and the image recognition.\\n19. The operating method as claimed in claim 17, wherein the image decoding is decoding QR codes, and the image recognition is face recognition.\\n20. The operating method as claimed in claim 16, wherein the phase detection pixel pairs are partially covered pixels or have a structure of dual pixel.', '1. An apparatus comprising: a first camera module configured to obtain a first image of an object with a first field of view; a second camera module configured to obtain a second image of the object with a second field of view different from the first field of view; a first depth map generator configured to generate a first depth map of the first image based on the first image and the second image; and a second depth map generator configured to generate a second depth map of the second image based on the first image, the second image, and the first depth map.\\n2. The apparatus of claim 1, wherein the first field of view is a narrow angle and the second field of view is a wider angle.\\n3. The apparatus of claim 2, wherein the second image is divided into a primary region and a residual region, and the second depth map generator comprises: a relationship estimating module configured to estimate a relationship between the primary region and the residual region based on the first image and the second image; and a depth map estimating module configured to estimate a depth map of the residual region based on the estimated relationship and the first depth map.\\n4. The apparatus of claim 3, wherein at least one of the relationship estimating module and the depth map estimating module performs an estimating operation based on a neural network module.\\n5. The apparatus of claim 1, further comprising: a depth map fusion unit configured to generate a third depth map of the second image by performing a fusion operation based on the first depth map and the second depth map.\\n6. The apparatus of claim 5, wherein the depth map fusion unit comprises: a tone mapping module configured to generate a tone-mapped second depth map to correspond to the first depth map by performing a bias removing operation on the second depth map; and a fusion module configured to generate the third depth map by fusing the tone-mapped second depth map and the first depth map.\\n7. The apparatus of claim 6, wherein the depth map fusion unit further comprises a propagating module configured to generate a propagated first depth map in the second image by iterated propagating of the first depth map based on the first depth map and the second image, and the fusion module generates the third depth map by fusing the tone-mapped second depth map and the propagated first depth map.\\n8. The apparatus of claim 6, wherein the depth map fusion unit further comprises a post-processing module configured to perform a post-processing operation on the third depth map generated by the fusion module to provide the post-processed third depth map.\\n9. The apparatus of claim 8, wherein the post-processing module performs the post-processing operation by filtering an interface generated in the third depth map in accordance with fusion of the fusion module.\\n10. The apparatus of claim 8, wherein the post-processing module removes artifacts generated in the third depth map in accordance with fusion of the fusion module.\\n11. The apparatus of claim 1, wherein the first depth map generator analyses a distance relationship between the first image and the second image, and generates a first depth map of the first image based on the distance relationship.\\n12. A method of processing an image of an electronic apparatus, the method comprising: obtaining a first image of an object using a first camera module; obtaining a second image of the object using a second camera module; generating a first depth map of the first image based on the first image and the second image; estimating a relationship between a primary region of the second image and a residual region of the second image based on the first image and the second image; and generating a second depth map of the second image based on the estimated relationship between the primary region and the residual region, and the first depth map.\\n13. The method of claim 12, wherein the electronic apparatus comprises a first camera module including a first lens having a first field of view and a second camera module including a second lens having a second field of view wider than the first field of view.\\n14. The method of claim 13, wherein the generating of the second depth map comprises: estimating a depth map of the residual region based on the estimated relationship between the primary region and the residual region, and the first depth map; and generating the second depth map based on a depth map of the residual region and the first depth map.\\n15. The method of claim 12, wherein the estimating of the relationship between a primary region of the second image is performed using a neural network model.\\n16. The method of claim 12, further comprising: performing a pre-processing operation on the second depth map; and generating a third depth map of the residual image by fusing the second depth map on which the pre-processing operation is performed and the first depth map.\\n17. The method of claim 16, wherein the performing of the pre-processing operation comprises performing a tone mapping operation between a depth map of the primary region and a depth map of the residual region based on the second depth map.\\n18. An operating method for an electronic apparatus, the electronic apparatus including; a first camera module providing a first image of an object using a first field of view and a second camera module providing a second image of the object using second field of view wider than the first field of view, and a processor generating a depth map of the second image based on a primary region of the second image and a residual region of the second image, the operating method comprising: generating a first depth map of the primary region by estimating a relationship between the first image and the second image; estimating a relationship between the primary region and the residual region based on the first image and the second image; generating a second depth map of the second image by estimating a depth map of the second region based on the estimated relationship between the primary region and the residual region; and generating a depth map of the second image by fusing the first depth map and the second depth map.\\n19. The operation method of claim 18, further comprising: executing an application that applies an image effect to the second image based on a depth map of the residual image.\\n20. The operation method of claim 19, wherein the application applies at least one image effect of auto-focusing, out-focusing, fore/background separation, face recognition, object detection within a frame, and augmented reality to the second image based on a depth map of the second image.', '1. A payment method based on a face recognition, comprising: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including: determining whether an angle of rotation in each preset direction is less than an angle threshold, wherein the head posture information includes the angle of rotation in each preset direction; determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold, wherein the gaze information includes the probability value that a user gazes at a payment screen; and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold, determining that the target user has a willingness to pay; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition.\\n2. The method as claimed in claim 1, wherein the completing a payment operation based on the face recognition comprises: triggering and performing a payment initiating operation to acquire second face image information based on the face recognition; determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay; and in response to determining that the second characteristic information indicates that the user has a willingness to pay, triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user.\\n3. The method as claimed in claim 2, wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises: determining whether a current user corresponding to the second face image information is consistent with the target user; and in response to determining that the current user is consistent with the target user, determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information.\\n4. The method as claimed in claim 1, wherein the extracting first characteristic information from the first face image information comprises: determining the head posture information of the target user using a head posture recognition model based on the first face image information; and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information.\\n5. The method as claimed in claim 4, wherein the head posture recognition model is obtained through training by: acquiring a first sample data set, wherein the first sample data set includes a plurality of pieces of first sample data, and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information; determining mean image data and variance image data of a plurality of sample face images; for each of the plurality of pieces of first sample data, preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image; setting the preprocessed sample face image and the corresponding head posture information as a first model training sample; and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model.\\n6. The method as claimed in claim 4, wherein the gaze information recognition model is obtained through training by: acquiring a second sample data set, wherein the second sample data set includes a plurality of pieces of second sample data, and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information; determining mean image data and variance image data of a plurality of sample eye images; for each of the plurality of pieces of second sample data, preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image; setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample; and performing training using a machine learning method and based on a plurality of second model training samples to obtain the gaze information recognition model.\\n7. The method as claimed in claim 1, wherein the angle of rotation in each preset direction comprises a pitch angle, a yaw angle, and a roll angle, wherein the pitch angle refers to an angle of rotation around a X-axis, the yaw angle refers to an angle of rotation around a Y-axis, and the roll angle refers to an angle of rotation around a Z-axis.\\n8. A payment device based on a face recognition, comprising: a processor, and a non-transitory computer-readable storage medium storing instructions executable by the processor to cause the device to perform operations comprising: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including: determining whether an angle of rotation in each preset direction is less than an angle threshold, wherein the head posture information includes the angle of rotation in each preset direction; determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold, wherein the gaze information includes the probability value that a user gazes at a payment screen; and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold, determining that the target user has a willingness to pay; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition.\\n9. The device as claimed in claim 8, wherein the completing a payment operation based on the face recognition comprises: triggering and performing a payment initiating operation to acquire second face image information based on the face recognition; determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay; and in response to determining that the second characteristic information indicates that the user has a willingness to pay, triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user.\\n10. The device as claimed in claim 9, wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises: determining whether a current user corresponding to the second face image information is consistent with the target user; and in response to determining that the current user is consistent with the target user, determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information.\\n11. The device as claimed in claim 8, wherein the extracting first characteristic information from the first face image information comprises: determining the head posture information of the target user using a head posture recognition model based on the first face image information; and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information.\\n12. The device as claimed in claim 11, wherein the head posture recognition model is obtained through training by: acquiring a first sample data set, wherein the first sample data set includes a plurality of pieces of first sample data, and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information; determining mean image data and variance image data of a plurality of sample face images; for each of the plurality of pieces of first sample data, preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image; setting the preprocessed sample face image and the corresponding head posture information as a first model training sample; and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model.\\n13. The device as claimed in claim 11, wherein the gaze information recognition model is obtained through training by: acquiring a second sample data set, wherein the second sample data set includes a plurality of pieces of second sample data, and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information; determining mean image data and variance image data of a plurality of sample eye images; for each of the plurality of pieces of second sample data, preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image; setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample; and performing training using a machine learning method and on a plurality of second model training samples to obtain the gaze information recognition model.\\n14. The device as claimed in claim 11, wherein the angle of rotation in each preset direction comprises a pitch angle, a yaw angle, and a roll angle, wherein the pitch angle refers to an angle of rotation around a X-axis, the yaw angle refers to an angle of rotation around a Y-axis, and the roll angle refers to an angle of rotation around a Z-axis.\\n15. A non-transitory computer-readable storage medium for a payment based on a face recognition, configured with instructions executable by one or more processors to cause the one or more processors to perform operations comprising: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user, and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including: determining whether an angle of rotation in each preset direction is less than an angle threshold, wherein the head posture information includes the angle of rotation in each preset direction; determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold, wherein the gaze information includes the probability value that a user gazes at a payment screen; and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold, determining that the target user has a willingness to pay; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition.\\n16. The storage medium as claimed in claim 15, wherein the completing a payment operation based on the face recognition comprises: triggering and performing a payment initiating operation to acquire second face image information based on the face recognition; determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay; and in response to determining that the second characteristic information indicates that the user has a willingness to pay, triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user.\\n17. The storage medium as claimed in claim 16, wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises: determining whether a current user corresponding to the second face image information is consistent with the target user; and in response to determining that the current user is consistent with the target user, determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information.\\n18. The storage medium as claimed in claim 15, wherein the extracting first characteristic information from the first face image information comprises: determining the head posture information of the target user using a head posture recognition model based on the first face image information; and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information.\\n19. The storage medium as claimed in claim 18, wherein the head posture recognition model is obtained through training by: acquiring a first sample data set, wherein the first sample data set includes a plurality of pieces of first sample data, and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information; determining mean image data and variance image data of a plurality of sample face images; for each of the plurality of pieces of first sample data, preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image; setting the preprocessed sample face image and the corresponding head posture information as a first model training sample; and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model; and wherein the gaze information recognition model is obtained through training by: acquiring a second sample data set, wherein the second sample data set includes a plurality of pieces of second sample data, and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information; determining mean image data and variance image data of a plurality of sample eye images; for each of the plurality of pieces of second sample data, preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image; setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample; and performing training using a machine learning method and based on a plurality of second model training samples to obtain the gaze information recognition model.\\n20. The storage medium as claimed in claim 18, wherein the angle of rotation in each preset direction comprises a pitch angle, a yaw angle, and a roll angle, wherein the pitch angle refers to an angle of rotation around a X-axis, the yaw angle refers to an angle of rotation around a Y-axis, and the roll angle refers to an angle of rotation around a Z-axis.', '1. A method comprising: detecting, by a motion detection module, a motion by a subject within a predetermined area of view; assigning a unique session identification number to the subject detected within a predetermined area of view; detecting a facial area of the subject detected within a predetermined area of view; generating an image of the facial area of the subject; assessing a quality of the image of the facial area of the subject; determining an identity of the subject based on the image of the facial area of the subject; identifying an intent of the subject; and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject.\\n2. The method of claim 1, further comprising: determining one or more additional subjects within the predetermined area of view; and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view.\\n3. The method of claim 1, wherein the assessing a quality of the image of the facial area of the subject comprises: assessing whether the quality of the image of the facial area of the object equates predetermined metric of quality; and upon determining that the quality of the image of the facial area of the object is inferior to the predetermined metric of quality, discarding the image of the facial area of the subject and generating a second image of the facial area of the subject.\\n4. The method of claim 1, further comprising: detecting whether the facial area of the subject is photographic image; and upon detecting that the facial area of the subject is a photographic image, generating a warning and restrict access to the point of entry.\\n5. The method of claim 1, further comprising: conducing an incremental training of the image of the facial area of the subject.\\n6. The method of claim 5, wherein conducing an incremental training of the image of the facial area of the subject comprises: capturing a first image of the facial area having facial landmarks; converting the first image of the facial area into a first numeric vector; capturing a second image of the facial area having facial landmarks; converting the second image of the facial area into a second numeric vector; calculating a weighted mean of the first numeric vector and the second numeric vector, wherein the weighted mean represents a change in a facial area; and storing the weighted mean in the database.\\n7. The method of claim 1, wherein determining an identity of the subject based on the image of the facial area of the subject comprises: comparing the image of the facial area of the subject with a plurality of images stored in a database; and authenticating the subject.\\n8. The method of claim 1, wherein identifying an intent of the subject comprises: upon detecting the facial area in a bounding box, commencing authentication of the subject; calculating a directional vector of a face of the subject; determine an intent of the subject to gain access to the point of entry based on the directional vector of the face of the subject; granting the access to the point of entry based on authentication of the subject and based on determining the intent of the subject.\\n9. A non-transitory computer readable medium having program instructions stored thereon, that in response to execution by a computing device cause the computing device to perform operations comprising: detecting a motion by a subject within a predetermined area of view; assigning a unique session identification number to the subject detected within a predetermined area of view; detecting a facial area of the subject detected within a predetermined area of view; generating an image of the facial area of the subject; assessing a quality of the image of the facial area of the subject; determining an identity of the subject based on the image of the facial area of the subject; identifying an intent of the subject; and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject.\\n10. The non-transitory computer readable medium of claim 9, further comprising: determining one or more additional subjects within the predetermined area of view; and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view.\\n11. The non-transitory computer readable medium of claim 9, wherein the assessing a quality of the image of the facial area of the subject comprises: assessing whether the quality of the image of the facial area of the object equates predetermined metric of quality; and upon determining that the quality of the image of the facial area of the object is inferior to the predetermined metric of quality, discarding the image of the facial area of the subject and generating a second image of the facial area of the subject.\\n12. The non-transitory computer readable medium of claim 9, further comprising: detecting whether the facial area of the subject is photographic image; and upon detecting that the facial area of the subject is a photographic image, generating a warning and restrict access to the access point.\\n13. The non-transitory computer readable medium of claim 9, further comprising: conducing an incremental training of the image of the facial area of the subject.\\n14. The non-transitory computer readable medium of claim 13, wherein conducing an incremental training of the image of the facial area of the subject comprises: capturing a first image of the facial area having facial landmarks; converting the first image of the facial area into a first numeric vector; capturing a second image of the facial area having facial landmarks; converting the second image of the facial area into a second numeric vector; calculating a weighted mean of the first numeric vector and the second numeric vector, wherein the weighted mean represents a change in a facial area; and storing the weighted mean in the database.\\n15. An apparatus for face recognition comprising: a processor; and a memory to store computer program instructions, the computer program instructions when executed on the processor cause the processor to perform operations comprising: detecting a motion by a subject within a predetermined area of view; assigning a unique session identification number to the subject detected within a predetermined area of view; detecting a facial area of the subject detected within a predetermined area of view; generating an image of the facial area of the subject; assessing a quality of the image of the facial area of the subject; determining an identity of the subject based on the image of the facial area of the subject; identifying an intent of the subject; and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject.\\n16. The apparatus of claim 15, further comprising: determining one or more additional subjects within the predetermined area of view; and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view.\\n17. The apparatus of claim 15, wherein the assessing a quality of the image of the facial area of the subject comprises: assessing whether the quality of the image of the facial area of the object equates predetermined metric of quality; and upon determining that the quality of the image of the facial area of the object is inferior to the predetermined metric of quality, discarding the image of the facial area of the subject and generating a second image of the facial area of the subject.\\n18. The apparatus of claim 15, further comprising: detecting whether the facial area of the subject is photographic image; and upon detecting that the facial area of the subject is a photographic image, generating a warning and restrict access to the access point.\\n19. The apparatus of claim 15, further comprising: conducing an incremental training of the image of the facial area of the subject.\\n20. The apparatus of claim 15, wherein conducing an incremental training of the image of the facial area of the subject comprises: capturing a first image of the facial area having facial landmarks; converting the first image of the facial area into a first numeric vector; capturing a second image of the facial area having facial landmarks; converting the second image of the facial area into a second numeric vector; calculating a weighted mean of the first numeric vector and the second numeric vector, wherein the weighted mean represents a change in a facial area; and storing the weighted mean in the database.', '1. A robot, comprising: a body configured to rotate and to tilt; a camera coupled to the body and configured to rotate and tilt according to the rotate and the tilt of the body, wherein the camera is configured to acquire a video of a space; a face recognition unit configured to recognize respective faces of one or more persons in the video; a tracking unit configured to track motion of each of the recognized faces of the one or more persons; and a controller configured to: calculate a respective size of each of the faces of the one or more persons; select a first person, from among the one or more persons, based on the calculated sizes of the faces; and control at least one of a direction of the rotation of the camera, an angle of the tilt of the camera and a focal distance of the camera, based on the tracked motion of the recognized face of the first person.\\n2. The robot of claim 1, wherein the controller is configured to: control the direction of the rotation of the camera and the angle of the tilt of the camera to achieve an particular orientation of the camera relative to the face of the first person; and control a focal distance of the camera by comparing respective sizes of the face of the first person before and after motion of the first person.\\n3. The robot of claim 2, wherein the particular orientation occurs when the camera faces a general direction of the face of the first person.\\n4. The robot of claim 1, wherein the controller is configured to: normalize sizes of the faces of the one or more persons based on an interocular distance; and select the first person based on the normalized sizes of the faces of the one or more persons.\\n5. The robot of claim 1, wherein the controller is configured to: select a person having a largest face size, from among the one or more persons, as the first person.\\n6. The robot of claim 1, further comprising: a microphone configured to receive a spoken audio that is present in the space; wherein the controller is further configured to select the first person further based on the received spoken audio.\\n7. The robot of claim 6, wherein the controller is further configured to: control gain of the microphone by comparing respective sizes of the face of the first person before and after motion of the first person.\\n8. The robot of claim 6, wherein the controller is configured to: calculate a position from which the spoken audio is provided; and select the first person further based on whether the one or more persons are in the position from which the voice signal is provided.\\n9. The robot of claim 8, wherein the controller is configured to: select a second person as the first person, from among the one or more persons, when the second person is located in the position from which the spoken audio is provided.\\n10. The robot of claim 8, wherein the controller is configured to: select a second person having a largest face size as the first person, from among the one or more persons, when none of the one or more persons is located in the position from which the spoken audio is provided.\\n11. The robot of claim 8, wherein the controller is configured to: select a second person having a largest face size as the first person, from among the one or more persons, when a plurality of persons from among the one or more persons are located in the position from which the spoken audio is provided.\\n12. The robot of claim 1, further comprising: a speaker, wherein the controller is configured to: control volume of the speaker by comparing respective sizes of the face of the first person before and after motion of the first person.\\n13. The robot of claim 1, wherein the body is further configured to rotate in a lateral direction, and to tilt in an vertical direction.\\n14. An electronic device, comprising: a camera coupled to the body and configured to rotate and to tilt, wherein the camera is configured to acquire a video of a space within which one or more persons are positioned; and a processor configured to: recognize respective faces of the one or more persons in the video; track motion of each of the recognized faces of the one or more persons; calculate a respective size of each of the faces of the one or more persons; select a first person, from among the one or more persons, based on the calculated sizes of the faces; and control at least one of a direction of the rotation of the camera, an angle of the tilt of the camera and a focal distance of the camera, based on the tracked motion of the recognized face of the first person.\\n15. A method, comprising: acquiring, by a camera, a video of a space within which one or more persons are positioned; recognizing respective faces of the one or more persons in the video; tracking motion of each of the recognized faces of the one or more persons; calculating a respective size of each of the faces of the one or more persons; selecting a first person, from among the one or more persons, based on the calculated sizes of the faces; and controlling at least one of a direction of rotation of the camera, an angle of tilt of the camera and a focal distance of the camera, based on the tracked motion of the recognized face of the first person.', '1. A method of inferring topics from a multimodal file, the method comprising: receiving a multimodal file;\\nextracting a set of entities from the multimodal file;\\nlinking the set of entities to produce a set of linked entities;\\nobtaining reference information for the set of entities;\\nbased at least on the reference information, generating a graph of the set of linked entities, the graph comprising nodes and edges;\\nbased at least on the nodes and edges of the graph, determining clusters in the graph;\\nbased at least on the clusters in the graph, identifying topic candidates;\\nextracting features from the clusters in the graph;\\nbased at least on the extracted features, selecting at least one TopicID from among the topic candidates to represent at least one cluster; and\\nindexing the multimodal file with the at least one TopicID.\\n2. The method of claim 1 wherein the multimodal file comprises a video portion and an audio portion and wherein extracting a set of entities from the multimodal file comprises:\\ndetecting objects in the video portion of the multimodal file; and\\ndetecting text in the audio portion of the multimodal file.\\n3. The method of claim 2 wherein detecting objects comprises performing face recognition.\\n4. The method of claim 2 wherein detecting text comprises performing a speech to text process.\\n5. The method of claim 4 further comprising:\\nidentifying a language used in the audio portion of the multimodal file, and wherein performing a speech to text process comprises performing a speech to text process in the identified language.\\n6. The method of claim 4 further comprising:\\ntranslating the detected text.\\n7. The method of claim 1 further comprising:\\ndetermining significant clusters and insignificant clusters in the determined clusters, and\\nwherein extracting features from the clusters in the graph comprises extracting features from the significant clusters in the graph.\\n8. The method of claim 1 wherein extracting features from the clusters in the graph comprises at least one process selected from the list consisting of:\\ndetermining a graph diameter and determining a Jaccard coefficient.\\n9. The method of claim 1 wherein selecting at least one TopicID to represent at least one cluster comprises:\\nbased at least on the extracted features, mapping topic candidates into a probability interval; and\\nbased at least on the mapping, ranking topic candidates within the at least one cluster, and\\nselecting the at least one TopicID based at least on the ranking.\\n10. The method of claim 1 further comprising:\\ntranslating the at least one TopicID, and\\nwherein indexing the multimodal file with the at least one TopicID comprises indexing the multimodal file with the at least one translated TopicID.\\n11. A system for inferring topics from a multimodal file, the system comprising: an entity extraction component comprising an object detection component and a speech to text component, operative to extract a set of entities from a multimodal file comprising a video portion and an audio portion;\\nan entity linking component operative to link the extracted set of entities to produce a set of linked entities;\\nan information retrieval component operative to obtain reference information for the extracted set of entities;\\na graphing and analysis component operative to:\\ngenerate a graph of the set of linked entities, the graph comprising nodes and edges;\\nbased at least on the nodes and edges of the graph, determine clusters in the graph;\\nbased at least on the clusters in the graph, identify topic candidates; and extract features from the clusters in the graph;\\na TopicID selection component operative to:\\nrank the topic candidates within at least one cluster; and\\nbased at least on the ranking, select at least one TopicID from among the topic candidates to represent at least one cluster; and a video indexer operative to index the multimodal file with the at least one TopicID.\\n12. The system of claim 11 wherein the object detection component is operative to perform face recognition.\\n13. The system of claim 11 wherein the speech to text component is operative to extract entity information in at least two different languages.\\n14. One or more computer storage devices having computer-executable instructions stored thereon for inferring topics from a multimodal file, which, on execution by a computer, cause the computer to perform operations comprising:\\nreceiving a multimodal file comprising a video portion and an audio portion; extracting a set of entities from the multimodal file, wherein extracting a set of entities from the multimodal file comprises:\\ndetecting objects in the video portion of the multimodal file with face recognition;\\ndetecting text in the audio portion of the multimodal file with a speech to text process; and\\ndisambiguating among a set of detected entity names;\\nlinking the set of entities to produce a set of linked entities;\\nobtaining reference information for the set of entities;\\nbased at least on the reference information, generating a graph of the set of linked entities, the graph comprising nodes and edges;\\nbased at least on the nodes and edges of the graph, determining clusters in the graph;\\ndetermining significant clusters and insignificant clusters in the determined clusters;\\nbased at least on the significant clusters in the graph, identifying topic candidates; extracting features from the significant clusters in the graph;\\nbased at least on the extracted features, mapping the topic candidates into a probability interval;\\nbased at least on the mapping, ranking the topic candidates within at least one significant cluster,\\nbased on the ranking, selecting at least one TopicID from among the topic candidates to represent the at least one significant cluster; and\\nindexing the multimodal file with the at least one TopicID.\\n15. The one or more computer storage devices of claim 14 wherein the operations further comprise:\\nidentifying a language used in the audio portion of the multimodal file, and detecting text in the audio portion of the multimodal file with a speech to text process comprises performing a speech to text process in the identified language.', '权利要求\\n1、 一种人脸识别方法,其特征在于,包括:\\n通过第一摄像头获取第一人脸图像;\\n提取所述第一人脸图像的第一人脸特征;\\n将所述第一人脸特征与预先存储的第二人脸特征进行对比,获得参考相似度,所述第 二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得,所述第二摄像头与所述第 一摄像头属于不同类型的摄像头;\\n根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否对应相同人。\\n2、 根据权利要求 1所述的方法,其特征在于,\\n所述第一摄像头为热成像摄像头,所述第二摄像头为可见光摄像头;\\n或者,所述第一摄像头为可见光摄像头,所述第一摄像头为热成像摄像头。\\n3、 根据权利要求 1或 2所述的方法,其特征在于,所述根据所述参考相似度确定所 述第一人脸特征与所述第二人脸特征是否对应相同人,包括:\\n根据所述参考相似度、 参考误报率以及相似度阈值确定所述第一人脸特征与所述第二 人脸特征是否对应相同人;其中,不同的误报率对应不同的相似度阈值。\\n4、 根据权利要求 1或 2所述的方法,其特征在于,所述根据所述参考相似度确定所 述第一人脸特征与所述第二人脸特征是否对应相同人,包括:\\n根据所述参考相似度以及阈值信息确定归一化后的参考相似度;\\n根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否对 应相同人。\\n5、 根据权利要求 1-4任一项所述的方法,其特征在于,所述提取所述第一人脸图像的 第_人脸特征,包括:\\n将所述第一人脸图像输入预先训练完成的神经网络,通过所述神经网络输出所述第一 人脸图像的第一人脸特征;其中,所述神经网络基于第一类型图像样本和第二类型图像样 本训练得到,所述第一类型图像样本和所述第二类型图像样本由不同类型的摄像头拍摄得 到,且所述第一类型图像样本和所述第二类型图像样本中包括人脸。\\n6、 根据权利要求 5 所述的方法,其特征在于,所述神经网络基于所述第一类型图像 样本、 所述第二类型图像样本和混合类型图像样本训练得到,所述混合类型图像样本由所 述第一类型图像样本和所述第二类型图像样本配对而得。\\n1、 根据权利要求 1-6任一项所述的方法,其特征在于,所述第一摄像头包括车载摄像 头,所述通过第一摄像头获取第一人脸图像,包括:\\n通过所述车载摄像头获取所述第一人脸图像,所述第一人脸图像包括车辆的用车人的 人脸图像。\\n8、 根据权利要求 7 所述的方法,其特征在于,所述用车人包括驾驶所述车辆的人、 乘坐所述车辆的人、 对所述车辆进行修理的人、 给所述车辆加油的人以及控制所述车辆的 人中的一项或多项。\\n9、 根据权利要求 7 所述的方法,其特征在于,所述用车人包括驾驶所述车辆的人, 所述通过所述车载摄像头获取所述第一人脸图像,包括: 在接收到触发指令的情况下,通过所述车载摄像头获取所述第一人脸图像; 或者,在所述车辆运行时,通过所述车载摄像头获取所述第一人脸图像;\\n或者,在所述车辆的运行速度达到参考速度的情况下,通过所述车载摄像头获取所述 第一人脸图像。\\n10、 根据权利要求 7-9任一项所述的方法,其特征在于,所述第二人脸图像为对所述 用车人进行人脸注册的图像,所述将所述第一人脸特征与预先存储的第二人脸特征进行对 比之前,所述方法还包括:\\n通过所述第二摄像头获取所述第二人脸图像;\\n提取所述第二人脸图像的第二人脸特征;\\n保存所述第二人脸图像的第二人脸特征。\\n11、 一种神经网络训练方法,其特征在于,包括:\\n获取第一类型图像样本和第二类型图像样本,所述第一类型图像样本和所述第二类型 图像样本由不同类型的摄像头拍摄得到,且所述第一类型图像样本和所述第二类型图像样 本中包括人脸;\\n根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。\\n12、 根据权利要求 11所述的方法,其特征在于,所述根据所述第一类型图像样本和所 述第二类型图像样本训练神经网络,包括:\\n将所述第一类型图像样本和所述第二类型图像样本配对,得到所述第一类型图像样本 和所述第二类型图像样本的混合类型图像样本;\\n根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类型图像样本,训练 所述神经网络。\\n13、 根据权利要求 12 所述的方法,其特征在于,所述根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类型图像样本,训练所述神经网络,包括:\\n通过所述神经网络获取所述第一类型图像样本的人脸预测结果、 所述第二类型图像样 本的人脸预测结果和所述混合类型图像样本的人脸预测结果;\\n根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、 所述第二类型图 像样本的人脸预测结果和人脸标注结果之间的差异、 以及所述混合类型图像样本的人脸预 测结果和人脸标注结果的差异,训练所述神经网络。\\n14、 根据权利要求 13 所述的方法,其特征在于,所述神经网络中包括第一分类器、 第二分类器和混合分类器,所述通过所述神经网络获取所述第一类型图像样本的人脸预测 结果、 所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果, 包括:\\n将所述第一类型图像样本的人脸特征输入至所述第一分类器中,得到所述第一类型图 像样本的人脸预测结果;\\n将所述第二类型图像样本的人脸特征输入至所述第二分类器中,得到所述第二类型图 像样本的人脸预测结果;\\n将所述混合类型图像样本的人脸特征输入至所述混合分类器中,得到所述混合类型图 像样本的人脸预测结果。 15、 根据权利要求 14所述的方法,其特征在于,所述方法还包括:\\n在训练完成的所述神经网络中去除所述第一分类器、 所述第二分类器和所述混合分类 器,得到用于进行人脸识别的神经网络。\\n16、 一种人脸识别装置,其特征在于,包括:\\n第一获取单元,用于通过第一摄像头获取第一人脸图像;\\n第一提取单元,用于提取所述第一人脸图像的第一人脸特征;\\n对比单元,用于将所述第一人脸特征与预先存储的第二人脸特征进行对比,获得参考 相似度,所述第二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得,所述第二 摄像头与所述第一摄像头属于不同类型的摄像头;\\n确定单元,用于根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否 对应相同人。\\n17、 根据权利要求 16所述的装置,其特征在于,\\n所述第一摄像头为热成像摄像头,所述第二摄像头为可见光摄像头;\\n或者,所述第一摄像头为可见光摄像头,所述第一摄像头为热成像摄像头。\\n18、 根据权利要求 16或 17所述的装置,其特征在于,\\n所述确定单元,具体用于根据所述参考相似度、 参考误报率以及相似度阈值确定所述 第一人脸特征与所述第二人脸特征是否对应相同人;其中,不同的误报率对应不同的相似 度阈值。\\n19、 根据权利要求 16或 17所述的装置,其特征在于,\\n所述确定单元,具体用于根据所述参考相似度以及阈值信息确定归一化后的参考相似 度;以及根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否 对应相同人。\\n20、 根据权利要求 16-19任_项所述的装置,其特征在于,\\n所述第一提取单元,具体用于将所述第一人脸图像输入预先训练完成的神经网络,通 过所述神经网络输出所述第一人脸图像的第一人脸特征;其中,所述神经网络基于第一类 型图像样本和第二类型图像样本训练得到,所述第一类型图像样本和所述第二类型图像样 本由不同类型的摄像头拍摄得到,且所述第一类型图像样本和所述第二类型图像样本中包 括人脸。\\n21、 根据权利要求 20 所述的装置,其特征在于,所述神经网络基于所述第一类型图 像样本、 所述第二类型图像样本和混合类型图像样本训练得到,所述混合类型图像样本由 所述第一类型图像样本和所述第二类型图像样本配对而得。\\n22、 根据权利要求 16-21任一项所述的装置,其特征在于,所述第一摄像头包括车载 摄像头,\\n所述第一获取单元,具体用于通过所述车载摄像头获取所述第一人脸图像,所述第一 人脸图像包括车辆的用车人的人脸图像。\\n23、 根据权利要求 22所述的装置,其特征在于,所述用车人包括驾驶所述车辆的人、 乘坐所述车辆的人、 对所述车辆进行修理的人、 给所述车辆加油的人以及控制所述车辆的 人中的一项或多项。 24、 根据权利要求 22所述的装置,其特征在于,所述用车人包括驾驶所述车辆的人, 所述第一获取单元,具体用于在接收到触发指令的情况下,通过所述车载摄像头获取所述 第一人脸图像;\\n或者,所述第一获取单元,具体用于在所述车辆运行时,通过所述车载摄像头获取所 述第 _人脸图像;\\n或者,所述第一获取单元,具体用于在所述车辆的运行速度达到参考速度的情况下, 通过所述车载摄像头获取所述第一人脸图像。\\n25、 根据权利要求 22-24任一项所述的装置,其特征在于,所述第二人脸图像为对所 述用车人进行人脸注册的图像,所述装置还包括:\\n第二获取单元,用于通过所述第二摄像头获取所述第二人脸图像;\\n第二提取单元,用于提取所述第二人脸图像的第二人脸特征;\\n保存单元,用于保存所述第二人脸图像的第二人脸特征。\\n26、 一种神经网络训练装置,其特征在于,包括:\\n获取单元,用于获取第一类型图像样本和第二类型图像样本,所述第一类型图像样本 和所述第二类型图像样本由不同类型的摄像头拍摄得到,且所述第一类型图像样本和所述 第二类型图像样本中包括人脸;\\n训练单元,用于根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。\\n27、 根据权利要求 26所述的装置,其特征在于,所述训练单元包括:\\n配对子单元,用于将所述第一类型图像样本和所述第二类型图像样本配对,得到所述 第一类型图像样本和所述第二类型图像样本的混合类型图像样本;\\n训练子单元,用于根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类 型图像样本,训练所述神经网络。\\n28、 根据权利要求 27所述的装置,其特征在于,\\n所述训练子单元,具体用于通过所述神经网络获取所述第一类型图像样本的人脸预测 结果、 所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果; 以及根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、 所述第二类型图 像样本的人脸预测结果和人脸标注结果之间的差异、 以及所述混合类型图像样本的人脸预 测结果和人脸标注结果的差异,训练所述神经网络。\\n29、 根据权利要求 28 所述的装置,其特征在于,所述神经网络中包括第一分类器、 第二分类器和混合分类器,\\n所述训练子单元,具体用于将所述第一类型图像样本的人脸特征输入至所述第一分类 器中,得到所述第一类型图像样本的人脸预测结果;以及将所述第二类型图像样本的人脸 特征输入至所述第二分类器中,得到所述第二类型图像样本的人脸预测结果;以及将所述 混合类型图像样本的人脸特征输入至所述混合分类器中,得到所述混合类型图像样本的人 脸预测结果。\\n30、 根据权利要求 29所述的装置,其特征在于,所述装置还包括:\\n神经网络应用单元,用于在训练完成的所述神经网络中去除所述第一分类器、 所述第 二分类器和所述混合分类器,得到用于进行人脸识别的神经网络。 31、 一种电子设备,其特征在于,包括处理器和存储器,所述处理器和所述存储器耦 合;其中,所述存储器用于存储程序指令,所述程序指令被所述处理器执行时,使所述处 理器执行权利要求 1-10任一项所述的方法;和/或,使所述处理器执行权利要求 11-15任一 项所述的方法。\\n32、 一种计算机可读存储介质,其特征在于,所述计算机可读存储介质中存储有计算 机程序,所述计算机程序包括程序指令,所述程序指令当被处理器执行时,使所述处理器 执行权利要求 1-10任一项所述的方法;和/或,使所述处理器执行权利要求 11-15任一项所 述的方法。', \"1. A system for alerting on vision impairment, said system comprising a processing unit configured and operable for receiving scene data being indicative of a scene of at least one consumer in an environment, identifying in the scene data a certain consumer, identifying an event being indicative of a behavioral compensation for vision impairment, and, upon identification of such an event, sending a notification relating to the vision impairment.\\n2. The system of claim 1, further comprising at least one sensing unit configured and operable for detecting the scene data.\\n3. The system of claim 2, wherein said at least one sensing unit comprises at least one of: at least one imaging unit configured and operable for capturing at least one image of at least a portion of a consumer's body, at least one motion detector configured and operable for detecting consumer data being indicative of a motion of a consumer, or at least one eye tracker configured and operable for tracking eye motion of a consumer. 4. The system of claim 3, wherein the at least one imaging unit comprises a plurality of cameras placed at different heights.\\n5. The system of any one of claims 2 to 4, wherein said sensing unit is accommodated in an optical or digital eyewear frame display.\\n6. The system of any one of claims 1 to 5, wherein said processing unit is configured and operable for identifying a consumer's condition, said consumer's condition comprising consumer data being indicative of the consumer's position and location relative to at least one object in the consumer's environment; said consumer data comprises at least one of a consumer's face, eyewear, posture, position, sound or motion.\\n7. The system of any one of claims 1 to 6, wherein said event comprises at least one position and orientation of head increase or decrease of viewing distance between the consumer and viewed object and changing the position of eyeglasses worn by the consumer.\\n8. The system of any one of claims 1 to 7, wherein said event is identified by identifying images having an image feature being indicative of behavioral compensation, performing a Bruckner test, performing a Hirschberg test, and measuring blink count/ frequency.\\n9. The system of claim 8, wherein the image feature being indicative of behavioral compensation comprises squinting, head orientation, certain distances between an object and consumer's eyes, certain position of eyeglasses on the consumer's face, strabismus, cataracts, and reflections from the eye.\\n10. The system of any one of claims 1 to 9, wherein the notification includes at least one of the data indicative of the identified event, data indicative of the identified consumer, ophthalmologic recommendations based on the identified event, or lack of events, or an appointment for a vision test.\\n11. The system of any one of claims 1 to 10, wherein said processing unit comprises a memory for storing at least one of a reference data indicative of behavioral compensation for vision impairment, data indicative of the notification, or data indicative of a follow-up of the notification.\\n12. The system of claim 11 , wherein said processing unit is configured for at least one of identifying the event upon comparison between the detected data and the reference data or determining a probability for a vision impairment of the consumer based on the comparison.\\n13. The system of any one of claims 1 to 12, wherein said processing unit comprises a communication interface being configured for sending the notification to at least one of the identified consumer or a third party.\\n14. The system of any one of claims 1 to 13, wherein said processing unit is configured for providing a frame recommendation.\\n15. The system of any one of claims 11 to 14, wherein said memory is configured for storing a database including a multiplicity of data sets related to a plurality of spectacle frame models and sizes.\\n16. The system according to claim 14 or 15, wherein said processing unit is configured and operable to correlate between frames parameters and ophthalmic prescriptions.\\n17. The system according to any of claims 14 to 16, wherein said processing unit is configured and operable to correlate between frames parameters and facial features.\\n18. The system according to any of claims 14 to 17, wherein said processing unit is configured and operable to correlate between frames parameters and eyewear preferences.\\n19. The system according to any of claims 14 to 18, comprising a server and at least one computer entity linked to the server via a network, wherein said network is configured to receive and respond to requests sent across the network; transmitting one or more modules of computer executable program instructions and displayable data to the network connected user computer platform in response to a request, wherein said modules include modules configured to: receive and transmit image information, transmitting a frame recommendation and an optical lens option recommendation based on received image information, for display by the network connected user computer platform.\\n20. A computer program instructions stored in the local storage that, when executed by a processing unit, cause the processing unit to: receive data being indicative of a scene of at least one consumer in an environment, identify in the data a certain consumer, identify an event being indicative of a behavioral compensation for vision impairment, and, upon identification of such an event, send a notification relating to the vision impairment.\\n21. A computer program product stored on a tangible computer readable medium, comprising: a library of software modules which cause a computer executing them to prompt for information pertinent to at least one of an eyeglasses recommendation and an optical lens option recommendation, to store said information or to display eyewear recommendations .\\n22. The computer program product of claim 21 , wherein said library further comprises a module for frame selection, point of sales and advertising.\\n23. A computer platform for facilitating eye glasses marketing or selection, comprising: a camera; a processor configured to execute computer program instructions to cause the processor to take an image of a consumer, identify in the image a certain consumer, identify an event being indicative of a behavioral compensation for vision impairment, and, upon identification of such an event, sending a notification relating to the vision impairment; local storage for processor executable instructions for carrying out storage of information.\\n24. A method for alerting on vision impairment; said method comprising:\\nidentifying a certain individual in scene data being indicative of a scene of at least one consumer in an environment;\\nidentifying an event being indicative of a behavioral compensation for vision impairment; and\\nupon identification of such an event, sending a notification on the vision impairment. 25. The method of claim 24, further comprising detecting data being indicative of a scene of at least one consumer in a retail environment.\\n26. The method of claim 24, wherein detecting the data being indicative of at least one consumer comprises at least one of capturing at least one image of at least one consumer, detecting data being indicative of a motion of a consumer, or tracking an eye motion of a consumer.\\n27. The method of claim 26, wherein capturing at least one image of at least one consumer comprises continuously recording a scene.\\n28. The method of any one of claims 24 to 27, further comprising identifying, in the data, the consumer' s condition including data being indicative of the consumer's position and location relative to the consumer's environment; said data comprising at least one of the consumer's face, posture, position, sound or motion.\\n29. The method of any one of claims 26 to 28, wherein said event comprises at least one of position and orientation of head, increase or decrease of viewing distance between the consumer and viewed object, or changing the position of eyeglasses worn by the consumer.\\n30. The method of any one of claims 26 to 29, wherein identifying of the event comprises identifying images having an image feature being indicative of behavioral compensation, performing a Bruckner test, performing a Hirschberg test, and measuring blink count/frequency.\\n31. The method of claim 30, wherein the image feature being indicative of behavioral compensation comprises squinting, head orientation, certain distances between an object and a consumer's eyes, certain position of eyeglasses on the consumer's face, strabismus, cataracts, and reflections from the eye.\\n32. The method of any one of claims 27 to 31, wherein identifying in the at least one image a consumer in a retail environment, comprising at least one of receiving data characterizing the retail environment, or performing face recognition.\\n33. The method of any one of claims 24 to 32, wherein sending a notification comprising sending the notification to at least one of the identified consumer or a third party.\\n34. The method of any one of claims 24 to 33, wherein the notification includes at least one of the data indicative of the identified event, data indicative of the identified consumer, ophthalmologic recommendations based on the identified event, or lack of events, and an appointment for a vision test.\\n35. The method of any one of claims 24 to 34, further comprising storing at least one of a reference data indicative of behavioral compensation for vision impairment, data indicative of the notification, or data indicative of a follow-up of the notification.\\n36. The method of claim 35, further comprising identifying the event upon comparison between the detected data and the reference data and determining a probability for a vision impairment of the consumer, based on the comparison.\\n37. A computer program intended to be stored in a memory of a processor unit of a computer system, or in a removable memory medium adapted to cooperate with a reader of the processor unit, comprising instructions for implementing the method according to any of claims 24 to 36.\"]\n",
      "CLAIM TEXT OBTAINED!\n"
     ]
    }
   ],
   "source": [
    "p2 = \"\"\n",
    "p2=re.findall(r'(?<=<claims>\\n)(?s:.+?)(?=\\n</claims>)',text)\n",
    "print(p2)\n",
    "print(\"CLAIM TEXT OBTAINED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b90597",
   "metadata": {},
   "source": [
    "### Text cleaining\n",
    "First we lower the text for both sections, then we do the whitespace and punctuation removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d89701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"an electronic apparatus including an image capturing device, a storage device and a processor and an operation method thereof are provided. the image capturing device captures an image for a user, and the storage device records a plurality of modules. the processor is coupled to the image capturing device and the storage device and is configured to: configure the image capturing device to capture a head image of a user; perform a face recognition operation to obtain a face region; detect a plurality of facial landmarks within the face region; estimate a head posture angle of the user according to the facial landmarks; calculate a gaze position where the user gazes on the screen according to the head posture angle, a plurality of rotation reference angle, and a plurality of predetermined calibration positions; and configure the screen to display a corresponding visual effect according to the gaze position.the present disclosure provides a computation method and product thereof. the computation method adopts a fusion method to perform machine learning computations. technical effects of the present disclosure include fewer computations and less power consumption.a method for detecting body information on passengers of a vehicle based on humans' status recognition is provided. the method includes steps of: a passenger body information-detecting device, (a) inputting an interior image of the vehicle into a face recognition network, to detect faces of the passengers and output passenger feature information, and inputting the interior image into a body recognition network, to detect bodies and output body-part length information; and (b) retrieving specific height mapping information by referring to a height mapping table of ratios of segment body portions of human groups to heights per the human groups, acquiring a specific height of the specific passenger, retrieving specific weight mapping information from a weight mapping table of correlations between the heights and weights per the human groups, and acquiring a weight of the specific passenger by referring to the specific height.techniques related to improved video coding based on face detection, region extraction, and tracking are discussed. such techniques may include performing a facial search of a video frame to determine candidate face regions in the video frame, testing the candidate face regions based on skin tone information to determine valid and invalid face regions, rejecting invalid face regions, and encoding the video frame based on valid face regions to generate a coded bitstream.a method for managing a smart database which stores facial images for face recognition is provided. the method includes steps of: a managing device (a) counting specific facial images corresponding to a specific person in the smart database where new facial images are continuously stored, and determining whether a first counted value, representing a count of the specific facial images, satisfies a first set value; and (b) if the first counted value satisfies the first set value, inputting the specific facial images into a neural aggregation network, to generate quality scores of the specific facial images by aggregation of the specific facial images, and, if a second counted value, representing a count of specific quality scores among the quality scores from a highest during counting thereof, satisfies a second set value, deleting part of the specific facial images, corresponding to the uncounted quality scores, from the smart database.a system capable of determining which recognition algorithms should be applied to regions of interest within digital representations is presented. a preprocessing module utilizes one or more feature identification algorithms to determine regions of interest based on feature density. the preprocessing modules leverages the feature density signature for each region to determine which of a plurality of diverse recognition modules should operate on the region of interest. a specific embodiment that focuses on structured documents is also presented. further, the disclosed approach can be enhanced by addition of an object classifier that classifies types of objects found in the regions of interest.disclosed is a mobile terminal. the mobile terminal may include a front camera obtaining a 2d face image of a user, a glance sensor tilted by a certain angle and disposed adjacent to the front camera to obtain metadata of the 2d face image, and a controller obtaining a distance between the glance sensor and the front camera, the distance enabling an area of an overlap region, where a first region representing a range photographable by the front camera overlaps a second region representing a range photographable by the glance sensor, to be the maximum.this disclosure provides systems, methods and apparatus, including computer programs encoded on computer storage media for intelligent routing of notifications related to media programming. in one aspect, a smart television (tv) can be implemented to track a user's tv watching behavior, and anticipate programming based on that behavior. in some other aspects, the smart tv can be implemented to detect a user's presence, and based on that detection, can automatically change the tv channel to media programming analyzed to be desirable to the user. in some further aspects, the smart tv can be implemented to transmit notification instructions to electronic devices within a network in an attempt to alert the user to upcoming media programming. additionally, the smart tv can be implemented to transmit detection instructions to the electronic devices within the network, whereby the electronic devices attempt to detect a user's presence through voice or facial recognition.a camera is configured to output a test depth+multi-spectral image including a plurality of pixels. each pixel corresponds to one of the plurality of sensors of a sensor array of the camera and includes at least a depth value and a spectral value for each spectral light sub-band of a plurality of spectral illuminators of the camera. a face recognition machine is previously trained with a set of labeled training depth+multi-spectral images having a same structure as the test depth+multi-spectral image. the face recognition machine is configured to output a confidence value indicating a likelihood that the test depth+multi-spectral image includes a face.embodiments of the present disclosure relate to an image processing method and apparatus, and an electronic device. the method includes: acquiring a photo album obtained from face clustering; collecting face information of respective images in the photo album, and acquiring a face parameter of each image according to the face information; selecting a cover image according to the face parameter of each image; and taking a face-region image from the cover image, and setting the face-region image as a cover of the photo album.techniques described herein provide location-based access control to secured resources. generally described, configurations disclosed herein enable a system to dynamically modify access to secured resources based on one or more location-related actions. for example, techniques disclosed herein can enable a computing system to control access to resources such as computing devices, display devices, secured locations, and secured data. in some configurations, the techniques disclosed herein can enable controlled access to secured resources based, at least in part, on an invitation associated with a location and positioning data indicating a location of a user.one embodiment provides a method comprising receiving a piece of content and salient moments data for the piece of content. the method further comprises, based on the salient moments data, determining a first path for a viewport for the piece of content. the method further comprises displaying the viewport on a display device. movement of the viewport is based on the first path during playback of the piece of content. the method further comprises generating an augmentation for a salient moment occurring in the piece of content, and presenting the augmentation in the viewport during a portion of the playback. the augmentation comprises an interactive hint for guiding the viewport to the salient moment.a computer-implemented method, system, and computer program product are provided for facial recognition. the method includes receiving, by a processor device, a plurality of images. the method also includes extracting, by the processor device with a feature extractor utilizing a convolutional neural network (cnn) with an enlarged intra-class variance of long-tail classes, feature vectors for each of the plurality of images. the method additionally includes generating, by the processor device with a feature generator, discriminative feature vectors for each of the feature vectors. the method further includes classifying, by the processor device utilizing a fully connected classifier, an identity from the discriminative feature vector. the method also includes control an operation of a processor-based machine to react in accordance with the identity.some embodiments of the invention provide efficient, expressive machine-trained networks for performing machine learning. the machine-trained (mt) networks of some embodiments use novel processing nodes with novel activation functions that allow the mt network to efficiently define with fewer processing node layers a complex mathematical expression that solves a particular problem (e.g., face recognition, speech recognition, etc.). in some embodiments, the same activation function (e.g., a cup function) is used for numerous processing nodes of the mt network, but through the machine learning, this activation function is configured differently for different processing nodes so that different nodes can emulate or implement two or more different functions (e.g., two or more boolean logical operators, such as xor and and). the activation function in some embodiments is a periodic function that can be configured to implement different functions (e.g., different sinusoidal functions).methods and systems may provide for facial recognition of at least one input image utilizing hierarchical feature learning and pair-wise classification. receptive field theory may be used on the input image to generate a pre-processed multi-channel image. channels in the pre-processed image may be activated based on the amount of feature rich details within the channels. similarly, local patches may be activated based on the discriminant features within the local patches. features may be extracted from the local patches and the most discriminant features may be selected in order to perform feature matching on pair sets. the system may utilize patch feature pooling, pair-wise matching, and large-scale training in order to quickly and accurately perform facial recognition at a low cost for both system memory and computation.a method for controlling a terminal is provided. the terminal includes a capturing apparatus and at least one processor. an image is acquired by the capturing apparatus. a motion parameter of the terminal is obtained. image processing on the acquired image is controlled to be performed based on the motion parameter being equal to or less than a preset parameter threshold, and skipped based on the motion parameter being greater than the preset parameter threshold.a drive-through order processing method and apparatus are disclosed. the drive-through order processing method includes receiving customer information detected through vision recognition, providing product information based on the customer information, and processing a product order of a customer. according to the present disclosure, it is possible to rapidly process an order using customer information based on customer recognition using an artificial intelligence (ai) model of machine learning through a 5g network.an image processing method performed at a computing device includes: identifying, using face recognition, one or more faces, each face corresponding to a respective person captured in a first image; for each identified face: extracting a set of profile parameters of a corresponding person in the first image; and selecting, from a plurality of image tiles, a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile; generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles; and sharing the first image and the second image in a predefined order via a group chat session.in one embodiment, the artificial reality system determines that a performance metric of an eye tracking system is below a first performance threshold. the eye tracking system is associated with a head-mounted display worn by a user. the artificial reality system receives first inputs associated with the body of a user and determines a region that the user is looking at within a field of view of a head-mounted display based on the received first inputs. the system determines a vergence distance of the user based at least on the first inputs associated with the body of the user, the region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display. the system adjusts one or more configurations of the head-mounted display based on the determined vergence distance of the user.a computer-implemented method is provided for image-based, self-guided object detection. the method includes receiving, by a processor device, a set of images. each of the images has a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data. the method further includes training, by the processor device, a grid-based object detector using the grid level label data. the method also includes determining, by the processor device, a respective bounding box for the respective object in each of the images, by applying local segmentation to each of the images. the method additionally includes training, by the processor device, a region-based convolutional neural network (rcnn) for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the rcnn.a system and method of face recognition comprising multiple phases implemented in a parallel architecture. the first phase is a normalization phase whereby a captured image is normalized to the same size, orientation, and illumination of stored images in a preexisting database. the second phase is a feature extraction/distance matrix phase where a distance matrix is generated for the captured image. in a coarse recognition phase, the generated distance matrix is compared with distance matrices in the database using euclidean distance matches to create candidate lists, and in a detailed recognition phase, multiple face recognition algorithms are applied to the candidate lists to produce a final result. the distance matrices in the normalized database may be broken into parallel lists for parallelization in the feature extraction/distance matrix phase, and the candidate lists may also be grouped according to a dissimilarity algorithm for parallel processing in the detailed recognition phase.an imaging device including a pixel matrix and a processor is provided. the pixel matrix includes a plurality of phase detection pixels and a plurality of regular pixels. the processor performs autofocusing according to pixel data of the phase detection pixels, and determines an operating resolution of the regular pixels according to autofocused pixel data of the phase detection pixels, wherein the phase detection pixels are always-on pixels and the regular pixels are selectively turned on after the autofocusing is accomplished.an apparatus includes a first camera module providing a first image of an object with a first field of view, a second camera module providing a second image of the object with a second field of view different from the first field of view, a first depth map generator that generates a first depth map of the first image based on the first image and the second image, and a second depth map generator that generates a second depth map of the second image based on the first image, the second image, and the first depth map.methods, systems, and apparatus, including computer programs encoded on computer storage media, for a payment based on a face recognition are provided. one of the methods includes: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including determining whether an angle of rotation in each preset direction is less than an angle threshold and whether a probability value that a user gazes at a payment screen is greater than a probability threshold; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition.a novel method and apparatus for face authentication is disclosed. the disclosed method comprises detecting a motion by a subject within a predetermined area of view, assigning a unique session identification number to the subject detected within a predetermined area of view, detecting a facial area of the subject detected within a predetermined area of view, generating an image of the facial area of the subject, assessing a quality of the image of the facial area of the subject, conducing an incremental training of the image of the facial area of the subject, determining an identity of the subject based on the image of the facial area of the subject, identifying an intent of the subject, and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject.disclosed herein is a robot and an electronic device for acquiring video, and a method for acquiring video using the robot. the robot includes a camera configured to rotate in the lateral direction and tilt in the vertical direction, and controls at least one of a direction of the rotation of the camera, an angle of the tilt of the camera, and a focal distance of the camera by recognizing and tracking users in a video acquired by the camera.systems and methods are disclosed for inferring topics from a file containing both audio and video, for example a multimodal or multimedia file, in order to facilitate video indexing. a set of entities is extracted from the file and linked to produce a graph, and reference information is also obtained for the set of entities. entities may be drawn, for example, from wikipedia categories, or other large ontological data sources. analysis of the graph, using unsupervised learning, permits determining clusters in the graph. extracting features from the clusters, possibly using supervised learning, provides for selection of topic identifiers. the topic identifiers are then used for indexing the file.a face recognition method, a neural network training method, an apparatus, and an electronic device. the method comprises: obtaining a first face image by means of a first camera (101); extracting a first face feature of the first face image (102); comparing the first face feature with a pre-stored second face feature to obtain a reference similarity, the second face feature being obtained by extracting a feature of a second face image obtained by a second camera, and the second camera and the first camera being different types of cameras (103); and determining, according to the reference similarity, whether the first face feature and the second face feature correspond to a same person (104).the present invention discloses a technique for alerting on vision impairment. the system comprises a processing unit configured and operable for receiving scene data being indicative of a scene of at least one consumer in an environment, identifying in the scene data a certain consumer, identifying an event being indicative of a behavioral compensation for vision impairment, and, upon identification of such an event, sending a notification relating to the vision impairment.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lower() is a Python function for strings\n",
    "lower_atext = \"\"\n",
    "for abstract_text in p1:\n",
    "    lower_atext += abstract_text.lower() #we pick each word and add to a variable, which will contain all the text\n",
    "lower_atext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a48cf382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. an electronic device (10), configured to make a screen (110) to display a plurality of image frames, comprising: an image capturing device (120); a storage device (130), storing a plurality of modules; and a processor (14), coupled to the image capturing device (140) and the storage device (130), configured to execute the modules in the storage device (130) to: configure the screen (110) to display a plurality of marker objects at a plurality of predetermined calibration positions; configure the image capturing device (120) to capture a plurality of first head images when a user is looking at the predetermined calibration positions; (s301) perform a plurality of first face recognition operations on the first head images to obtain a plurality of first face regions corresponding to the predetermined calibration positions; (s302) detect a plurality of first facial landmarks corresponding to the first face regions; (s303) calculate a plurality of rotation reference angles of the user looking at the predetermined calibration positions according to the first facial landmarks; configure the image capturing device (120) to capture a second head image of the user; perform a second face recognition operation on the second head image to obtain a second face region; detect a plurality of second facial landmarks within the second face region; (s304) estimate a head posture angle of the user according to the second facial landmarks; calculate a gaze position of the user on the screen (110) according to the head posture angle, the rotation reference angles, and the predetermined calibration positions; and configure the screen (110) to display a corresponding visual effect according to the gaze position.\\n2. the electronic device (10) according to claim 1, wherein the gaze position comprises a first coordinate value in a first axial direction and a second coordinate value in a second axial direction.\\n3. the electronic device (10) according to claim 2, wherein the head posture angles comprise a head pitch angle and a head yaw angle, and the rotation reference angles comprise a first pitch angle, a second pitch angle, a first yaw angle, and a second yaw angle corresponding to the predetermined calibration positions.\\n4. the electronic device (10) according to claim 3, wherein the processor (140) performs interpolation operation or extrapolation operation according to the first yaw angle, the second yaw angle, a first position corresponding to the first yaw angle among the predetermined calibration positions, a second position corresponding to the second yaw angle among the predetermined calibration positions and the head yaw angle, thereby obtaining the first coordinate value of the gaze position; and\\nthe processor (140) performs interpolation operation or extrapolation operation according to the first pitch angle, the second pitch angle, a third position corresponding to the first pitch angle among the predetermined calibration positions, a fourth position corresponding to the second pitch angle among the predetermined calibration positions and the head pitch angle, thereby obtaining the second coordinate value of the gaze position.\\n5. the electronic device (10) according to claim 1, wherein the processor (140) calculates a plurality of first viewing distances between the user and the screen (110) according to the first facial landmarks;\\nthe processor (140) estimates a second viewing distance between the user and the screen (110) according to the second facial landmarks; and\\nthe processor (140) adjusts the rotation reference angles or the gaze position according to the second viewing distance and the first viewing distances.\\n6. the electronic device (10) according to claim 1, wherein the processor (140) maps a plurality of two-dimensional position coordinates of the second facial landmarks under a plane coordinate system to a plurality of three-dimensional position coordinates under a three-dimensional coordinate system; and\\nthe processor (140) estimates the head posture angle according to the three-dimensional position coordinates of the second facial landmarks.\\n7. the electronic device (10) according to claim 1, wherein the second head image comprises a wearable device, and the second facial landmarks do not comprise a plurality of third facial landmarks of the user covered by the wearable device.\\n8. the electronic device (10) according to claim 1, wherein the second head image comprises a wearable device, and the second facial landmarks comprise one or more simulated landmarks marked by the wearable device.\\n9. an operating method, adapted for an electronic device (10) comprising an image capturing device (120) and making a screen (110) to display a plurality of image frames, the method comprising: configuring the screen (110) to display a plurality of marker objects at a plurality of predetermined calibration positions; configuring the image capturing device (120) to capture a plurality of first head images when a user is looking at the predetermined calibration positions; (s301) performing a plurality of first face recognition operations on the first head images to obtain a plurality of first face regions corresponding to the predetermined calibration positions; (s302) detecting a plurality of first facial landmarks corresponding to the first face regions; (s303) calculating a plurality of rotation reference angles of the user looking at the predetermined calibration positions according to the first facial landmarks; configuring the image capturing device (120) to capture a second head image of the user; performing a second face recognition operation on the second head image to obtain a second face region; (s304) detecting a plurality of second facial landmarks within the second face region; estimating a head posture angle of the user according to the second facial landmarks; calculating a gaze position of the user on the screen (110) according to the head posture angle, the rotation reference angles, and the predetermined calibration positions; and (s305) configuring the screen (110) to display a corresponding visual effect according to the gaze position.\\n10. the operation method according to claim 9, wherein the gaze position comprises a first coordinate value in a first axial direction and a second coordinate value in a second axial direction.\\n11. the operation method according to claim 10, wherein the head posture angles comprise a head pitch angle and a head yaw angle, and the rotation reference angles comprise a first pitch angle, a second pitch angle, a first yaw angle, and a second yaw angle corresponding to the predetermined calibration positions.\\n12. the operation method according to claim 11, wherein the step of calculating the gaze position of the user on the screen (110) according to the head posture angle, the rotation reference angles and the predetermined calibration positions comprises: performing interpolation operation or extrapolation operation according to the first yaw angle, the second yaw angle, a first position corresponding to the first yaw angle among the predetermined calibration positions, a second position corresponding to the second yaw angle among the predetermined calibration positions and the head yaw angle, thereby obtaining the first coordinate value of the gaze position; and performing interpolation operation or extrapolation operation according to the first pitch angle, the second pitch angle, a third position corresponding to the first pitch angle among the predetermined calibration positions, a fourth position corresponding to the second pitch angle among the predetermined calibration positions and the head pitch angle, thereby obtaining the second coordinate value of the gaze position.\\n13. the operation method according to claim 9, wherein the method further comprises: calculating a plurality of first viewing distances between the user and the screen (110) according to the first facial landmarks; estimating a second viewing distance between the user and the screen (110) according to the second facial landmarks; and adjusting the rotation reference angles or the gaze position according to the second viewing distance and the first viewing distances.\\n14. the operation method according to claim 9, wherein the method further comprises: mapping a plurality of two-dimensional position coordinates of the second facial landmarks under a plane coordinate system to a plurality of three-dimensional position coordinates under a three-dimensional coordinate system; and estimating the head posture angle according to the three-dimensional position coordinates of the second facial landmarks.\\n15. the operation method according to claim 9, wherein the second head image comprises a wearable device, and the second facial landmarks do not comprise a plurality of third facial landmarks of the user covered by the wearable device.\\n16. the operation method according to claim 9, wherein the second head image comprises a wearable device, and the second facial landmarks comprise one or more simulated landmarks marked by the wearable device.1. a computation method applied to a computing system, wherein the computing system comprises: a control unit, a computation group, and a general storage unit, wherein the control unit comprises: a first memory, a decoding logic, and a controller, wherein the computation group comprises: a group controller and a plurality of computing units; the general storage unit is configured to store data; and the computation method comprises: receiving, by the controller, a first level instruction sequence, and partitioning, by the decoding logic, the first level instruction sequence into a plurality of second level instruction sequences, creating, by the controller, m threads for the plurality of second level instruction sequences, and allocating, by the controller, an independent register as well as configuring an independent addressing function for each thread of the m threads, wherein m is an integer greater than or equal to 1; and obtaining, by the group controller, a plurality of computation types of the plurality of second level instruction sequences, obtaining a corresponding fusion computation manner of the computation types according to the plurality of computation types, and adopting, by the plurality of computing units, the fusion computation manner to call the m threads for performing computations on the plurality of second level instruction sequences to obtain a final result.\\n2. the method of claim 1, wherein, the obtaining, by the group controller, a plurality of computation types of the plurality of second level instruction sequences, obtaining a corresponding fusion computation manner of the computation types according to the plurality of computation types, and adopting, by the plurality of computing units, the fusion computation manner to call the m threads for performing computations on the plurality of second instruction sequences to obtain a final result:\\nif the computation types represent computation operations of the same type, the group controller calls a combined computation manner in which single instruction multiple data of the same type is in combination with single instruction multiple threads, and uses the m threads to perform the combined computation manner to obtain a final result, which includes:\\npartitioning, by the decoding logic, the m threads into n wraps for allocating to the the plurality of computing units, converting, by the group controller, the plurality of second instruction sequences into a plurality of second control signals and sending the second control signals to the plurality of computing units, calling, by the plurality of computing units, wraps that are allocated to the computing units and the second control signals to fetch corresponding data according to the independent addressing function, performing, by the plurality of computing units, computations on the data to obtain a plurality of intermediate results, and splicing the plurality of intermediate results to obtain a final result.\\n3. the method of claim 1, wherein, the obtaining, by the group controller, a plurality of computation types of the plurality of second level instruction sequences, obtaining a corresponding fusion computation manner of the computation types according to the plurality of computation types, and adopting, by the plurality of computing units, the fusion computation manner to call the m threads for performing computations on the plurality of second instruction sequences to obtain a final result:\\nif the computation types represent computation operations of different types, the group controller calls simultaneous multi-threading and the m threads to perform computations to obtain a final result, which includes:\\npartitioning, by the decoding logic, the m threads into n wraps, converting the plurality of second instruction sequences into a plurality of second control signals, obtaining, by the group controller, computation types supported by the plurality of computing units, allocating, by the controller, the n wraps and the plurality of second control signals to corresponding computing units that support computation types of the wraps and the second control signals, calling, by the plurality of computing units, wraps that are allocated to the computing units and the second control signals, fetching, by the plurality of computing units, corresponding data, performing, by the plurality of computing units, computations on the data to obtain a plurality of intermediate results, and splicing all the intermediate results to obtain a final result.\\n4. the method of claim 2 or 3, further comprising:\\nif a wrap a in the plurality of wraps is blocked, adding the wrap a to a waiting queue, and if data of the wrap a are already fetched, adding the wrap a to a preparation queue, wherein the preparation queue is a queue where a wrap to be scheduled for executing is located when a computing resource is idle.\\n5. the method of claim 1, wherein\\nthe first level instruction sequence includes a very long instruction, and the second level instruction sequence includes an instruction sequence.\\n6. the method of claim 1, wherein the computing system further includes: a tree module, wherein the tree module includes: a root port and a plurality of branch ports, wherein the root port of the tree module is connected to the group controller, and the plurality of branch ports of the tree module are connected to a computing unit of the plurality of computing units respectively; and\\nthe tree module is configured to forward data blocks, wraps, or instruction sequences between the group controller and the plurality of computing units.\\n7. the method of claim 6, wherein the tree module is an n-ary tree, wherein n is an integer greater than or equal to 2.\\n8. the method of claim 1, wherein the computing system further includes a branch processing circuit,\\nwherein the branch processing circuit is connected between the group controller and the plurality of computing units; and\\nthe branch processing circuit is configured to forward data, wraps, or instruction sequences between the group controller and the plurality of computing units.\\n9. a computing system, comprising: a control unit, a computation group, and a general storage unit, wherein the control unit includes: a first memory, a decoding logic, and a controller, the computation group includes: a group controller and a plurality of computing units; the general storage unit is configured to store data;\\nthe controller is configured to receive a first level instruction sequence and control the first memory and the decoding logic;\\nthe decoding logic is configured to partition the first level instruction sequence into a plurality of second level instruction sequences;\\nthe the controller is further configured to create m threads for the plurality of second level instruction sequences, and allocate an independent register and configure an independent addressing function for each thread of the m threads; m is an integer greater than or equal to 1; and the controller is further configured to convert the plurality of second instruction sequences into a plurality of control signals for sending to the group controller;\\nthe group controller is configured to receive the plurality of control signals, obtain a plurality of computational types if the plurality of control signals, divide the m threads into n wraps, and allocate the n wraps and the plurality of control signals to the plurality of computing units according to the plurality of computational types;\\nthe plurality of computing units are configured to fetch data from the general storage unit through allocated wraps and control signals, and perform computations to obtain an intermediate result; and\\nthe group controller is configured to splice all intermediate results to obtain a final computation result.\\n10. the computing system of claim 9, wherein\\nthe plurality of computing units includes: an addition computing unit, a multiplication computing unit, an activation computing unit, or a dedicated computing unit.\\n11. the computing system of claim 9, wherein\\nthe dedicated computing unit includes: a face recognition computing unit, a graphics computing unit, a fingerprint computing unit, or a neural network computing unit.\\n12. the computing system of claim 11, wherein\\nthe group controller is configured to, if computation types of a plurality of control signals are graphics computations, fingerprint identification, face recognition, or neural network operations, allocate the plurality of control signals to the face recognition computing unit, the graphics computing unit, the fingerprint computing unit, or the neural network computing unit respectively.\\n13. the computing system of claim 9, wherein\\nthe first level instruction sequence includes a very long instruction, and the second level instruction sequence includes an instruction sequence.\\n14. the computing system of claim 9, further comprising a tree module, wherein the tree module includes: a root port and a plurality of branch ports, wherein the root port of the tree module is connected to the group controller, and the plurality of branch ports of the tree module are connected to a computing unit of the plurality of computing units respectively; and\\nthe tree module is configured to forward data blocks, wraps, or instruction sequences between the group controller and the plurality of computing units.\\n15. the computing system of claim 14, wherein the tree module is an n-ary tree, wherein n is an integer greater than or equal to 2.\\n16. the computing system of claim 9, wherein the computing system includes a branch processing circuit,\\nthe branch processing circuit is connected between the group controller and the plurality of computing units; and\\nthe branch processing circuit is configured to forward data, wraps, or instruction sequences between the group controller and the plurality of computing units.\\n17. a computer program product, comprising a non-instant computer readable storage medium, wherein a computer program is stored in the non-instant computer readable storage medium, and the computer program is capable of causing a computer to perform the method of any of claims 1-8 through operations.1. a method for detecting body information on one or more passengers of a vehicle based on humans\\' status recognition, comprising steps of: (a) if at least one interior image of an interior of the vehicle is acquired, a passenger body information-detecting device performing (i) a process of inputting the interior image into a face recognition network, to thereby allow the face recognition network to detect each of faces of each of the passengers from the interior image, and thus to output multiple pieces of passenger feature information corresponding to each of the detected faces, and (ii) a process of inputting the interior image into a body recognition network, to thereby allow the body recognition network to detect each of bodies of each of the passengers from the interior image, and thus to output body-part length information of each of the detected bodies; and (b) the passenger body information-detecting device performing a process of retrieving specific height mapping information corresponding to specific passenger feature information on a specific passenger from a height mapping table which stores height mapping information representing respective one or more predetermined ratios of one or more segment body portions of each of human groups to each of heights per each of the human groups, a process of acquiring a specific height of the specific passenger from the specific height mapping information by referring to specific body-part length information of the specific passenger, a process of retrieving specific weight mapping information corresponding to the specific passenger feature information from a weight mapping table which stores multiple pieces of weight mapping information representing predetermined correlations between each of the heights and each of weights per each of the human groups, and a process of acquiring a weight of the specific passenger from the specific weight mapping information by referring to the specific height of the specific passenger.\\n2. the method of claim 1, wherein, at the step of (a), the passenger body information-detecting device performs a process of inputting the interior image into the body recognition network, to thereby allow the body recognition network to (i) output one or more feature tensors with one or more channels corresponding to the interior image via a feature extraction network, (ii) generate at least one keypoint heatmap and at least one part affinity field with one or more channels corresponding to each of the feature tensors via a keypoint heatmap & part affinity field extractor, and (iii) extract keypoints from the keypoint heatmap via a keypoint detector, to group the extracted keypoints by referring to the part affinity field, and thus to generate body parts per the passengers, and as a result, allow the body recognition network to output multiple pieces of body-part length information on each of the passengers by referring to the body parts per the passengers.\\n3. the method of claim 2, wherein the feature extraction network includes at least one convolutional layer and applies at least one convolution operation to the interior image, to thereby output the feature tensors.\\n4. the method of claim 2, wherein the keypoint heatmap & part affinity field extractor includes one of a fully convolutional network and a 1×1 convolutional layer, and applies a fully-convolution operation or 1×1 convolution operation to the feature tensors, to thereby generate the keypoint heatmap and the part affinity field.\\n5. the method of claim 2, wherein the keypoint detector connects, by referring to the part affinity field, pairs respectively having highest mutual connection probabilities of being connected among the extracted keypoints, to thereby group the extracted keypoints.\\n6. the method of claim 2, wherein the feature extraction network and the keypoint heatmap & part affinity field extractor have been learned by a learning device performing (i) a process of inputting at least one training image including one or more objects for training into the feature extraction network, to thereby allow the feature extraction network to generate one or more feature tensors for training having one or more channels by applying at least one convolutional operation to the training image, (ii) a process of inputting the feature tensors for training into the keypoint heatmap & part affinity field extractor, to thereby allow the keypoint heatmap & part affinity field extractor to generate one or more keypoint heatmaps for training and one or more part affinity fields for training having one or more channels for each of the feature tensors for training, (iii) a process of inputting the keypoint heatmaps for training and the part affinity fields for training into the keypoint detector, to thereby allow the keypoint detector to extract keypoints for training from each of the keypoint heatmaps for training and a process of grouping the extracted keypoints for training by referring to each of the part affinity fields for training, to thereby detect keypoints per each of the objects for training, and (iv) a process of allowing a loss layer to calculate one or more losses by referring to the keypoints per each of the objects for training and their corresponding ground truths, to thereby adjust one or more parameters of the feature extraction network and the keypoint heatmap & part affinity field extractor such that the losses are minimized by backpropagation using the losses.\\n7. the method of claim 1, wherein, at the step of (a), the passenger body information-detecting device performs a process of inputting the interior image into the face recognition network, to thereby allow the face recognition network to detect each of the faces of each of the passengers located in the interior image via a face detector, and to output multiple pieces of the passenger feature information on each of the facial images via a facial feature classifier.\\n8. the method of claim 1, wherein, at the step of (a), the passenger body information-detecting device performs a process of inputting the interior image into the face recognition network, to thereby allow the face recognition network to (i) apply at least one convolution operation to the interior image and thus to output at least one feature map corresponding to the interior image via at least one convolutional layer, (ii) output one or more proposal boxes, where the passengers are estimated as located, on the feature map, via a region proposal network, (iii) apply pooling operation to one or more regions, corresponding to the proposal boxes, on the feature map and thus to output at least one feature vector via a pooling layer, and (iv) apply fully-connected operation to the feature vector, and thus to output the multiple pieces of the passenger feature information corresponding to each of the faces of each of the passengers corresponding to each of the proposal boxes via a fully connected layer.\\n9. the method of claim 1, wherein the multiple pieces of the passenger feature information include each of ages, each of genders and each of races corresponding to each of the passengers.\\n10. a passenger body information-detecting device for detecting body information on one or more passengers of a vehicle based on humans\\' status recognition, comprising: at least one memory that stores instructions; and at least one processor configured to execute the instructions to perform or support another device to perform: (i) if at least one interior image of an interior of the vehicle is acquired, (i) a process of inputting the interior image into a face recognition network, to thereby allow the face recognition network to detect each of faces of each of the passengers from the interior image, and thus to output multiple pieces of passenger feature information corresponding to each of the detected faces, and (ii) a process of inputting the interior image into a body recognition network, to thereby allow the body recognition network to detect each of bodies of each of the passengers from the interior image, and thus to output body-part length information of each of the detected bodies, and (ii) a process of retrieving specific height mapping information corresponding to specific passenger feature information on a specific passenger from a height mapping table which stores height mapping information representing respective one or more predetermined ratios of one or more segment body portions of each of human groups to each of heights per each of the human groups, a process of acquiring a specific height of the specific passenger from the specific height mapping information by referring to specific body-part length information of the specific passenger, a process of retrieving specific weight mapping information corresponding to the specific passenger feature information from a weight mapping table which stores multiple pieces of weight mapping information representing predetermined correlations between each of the heights and each of weights per each of the human groups, and a process of acquiring a weight of the specific passenger from the specific weight mapping information by referring to the specific height of the specific passenger.\\n11. the passenger body information-detecting device of claim 10, wherein, at the process of (i), the processor performs a process of inputting the interior image into the body recognition network, to thereby allow the body recognition network to (i) output one or more feature tensors with one or more channels corresponding to the interior image via a feature extraction network, (ii) generate at least one keypoint heatmap and at least one part affinity field with one or more channels corresponding to each of the feature tensors via a keypoint heatmap & part affinity field extractor, and (iii) extract keypoints from the keypoint heatmap via a keypoint detector, to group the extracted keypoints by referring to the part affinity field, and thus to generate body parts per the passengers, and as a result, allow the body recognition network to output multiple pieces of body-part length information on each of the passengers by referring to the body parts per the passengers.\\n12. the passenger body information-detecting device of claim 11, wherein the keypoint heatmap & part affinity field extractor includes one of a fully convolutional network and a 1×1 convolutional layer, and applies a fully-convolution operation or 1×1 convolution operation to the feature tensors, to thereby generate the keypoint heatmap and the part affinity field.\\n13. the passenger body information-detecting device of claim 11, wherein the keypoint detector connects, by referring to the part affinity field, pairs respectively having highest mutual connection probabilities of being connected among the extracted keypoints, to thereby group the extracted keypoints.\\n14. the passenger body information-detecting device of claim 11, wherein the feature extraction network and the keypoint heatmap & part affinity field extractor have been learned by a learning device performing (i) a process of inputting at least one training image including one or more objects for training into the feature extraction network, to thereby allow the feature extraction network to generate one or more feature tensors for training having one or more channels by applying at least one convolutional operation to the training image, (ii) a process of inputting the feature tensors for training into the keypoint heatmap & part affinity field extractor, to thereby allow the keypoint heatmap & part affinity field extractor to generate one or more keypoint heatmaps for training and one or more part affinity fields for training having one or more channels for each of the feature tensors for training, (iii) a process of inputting the keypoint heatmaps for training and the part affinity fields for training into the keypoint detector, to thereby allow the keypoint detector to extract keypoints for training from each of the keypoint heatmaps for training and a process of grouping the extracted keypoints for training by referring to each of the part affinity fields for training, to thereby detect keypoints per each of the objects for training, and (iv) a process of allowing a loss layer to calculate one or more losses by referring to the keypoints per each of the objects for training and their corresponding ground truths, to thereby adjust one or more parameters of the feature extraction network and the keypoint heatmap & part affinity field extractor such that the losses are minimized by backpropagation using the losses.\\n15. the passenger body information-detecting device of claim 10, wherein, at the process of (i), the processor performs a process of inputting the interior image into the face recognition network, to thereby allow the face recognition network to (i) apply at least one convolution operation to the interior image and thus to output at least one feature map corresponding to the interior image via at least one convolutional layer, (ii) output one or more proposal boxes, where the passengers are estimated as located, on the feature map, via a region proposal network, (iii) apply pooling operation to one or more regions, corresponding to the proposal boxes, on the feature map and thus to output at least one feature vector via a pooling layer, and (iv) apply fully-connected operation to the feature vector, and thus to output the multiple pieces of the passenger feature information corresponding to each of the faces of each of the passengers corresponding to each of the proposal boxes via a fully connected layer.1. a computer implemented method for performing video coding based on face detection comprising: receiving a video frame comprising one of a plurality of video frames of a video sequence; determining the video frame is a key frame of the video sequence; performing, in response to the video frame being a key frame of the video sequence, a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame; testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region; rejecting the second candidate face region and outputting the first candidate face region; and encoding the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream.\\n2. the method of claim 1, wherein the skin tone information comprises a skin probability map.\\n3. the method of claim 1, wherein said testing the first and second candidate face regions based on skin tone information is performed in response to the video frame being a key frame of the video sequence.\\n4. the method of claim 1, wherein the first candidate face region comprises a rectangular region, the method further comprising: determining a free form shape face region corresponding to the first candidate face region, wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy.\\n5. the method of claim 4, wherein determining the free form shape face region comprises: generating an enhanced skip probability map corresponding to the first candidate face region; binarizing the enhanced skip probability map; and overlaying the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region.\\n6. the method of claim 4, wherein a second video frame comprises a non-key frame of the video sequence, the method further comprising performing face detection in the second video frame of the video sequence based on the free form shape face region.\\n7. the method of claim 6, further comprising: tracking a second free form shape face region in the second video frame based on the free form shape face region in the video frame.\\n8. the method of claim 7, wherein tracking the second free form shape face region comprises determining a location of a second valid face region in the second video frame based on a displacement offset with respect to the first candidate face region.\\n9. the method of claim 8, further comprising: determining the displacement offset based on an offset between a centroid of a bounding box around a skin enhanced region corresponding to the first candidate face region and a second centroid of a second bounding box around a second skin enhanced region in the second video frame.\\n10. the method of claim 1, wherein encoding the video frame based at least in part on the first candidate face region being a valid face region comprises at least one of reducing a quantization parameter corresponding to the first candidate face region, adjusting a lambda value for the first candidate face region, or disabling skip coding for the first candidate face region.\\n11. the method of claim 1, wherein the bitstream comprises at least one of an h.264/advanced video coding (avc) compliant bitstream, an h.265/high efficiency video coding (hevc) compliant bitstream, a vp9 compliant bitstream, a vp10 compliant bitstream, or an alliance for open media (aom) av1 compliant bitstream.\\n12. a computer implemented method for performing face detection comprising: receiving a video frame of a sequence of video frames; performing a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame; testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region; rejecting the second candidate face region and outputting the first candidate face region as a valid face region for further processing; and providing an index indicative of a person being present in the video frame based on the valid face region.\\n13. the method of claim 12, wherein the sequence of video frames comprises a sequence of surveillance video frames, the method further comprising: performing face recognition in the surveillance video frames based on the valid face region.\\n14. the method of claim 12, wherein the sequence of video frames comprises a sequence of decoded video frames, the method further comprising: adding a marker corresponding to the received video frame to perform face recognition on the received video frame based on the valid face region.\\n15. the method of claim 12, wherein the sequence of video frames is received during a device login attempt, the method further comprising: performing face recognition based on the valid face region; and allowing access to the device if a secured face is recognized.\\n16. the method of claim 12, wherein the sequence of video frames comprises a sequence of videoconferencing frames, the method further comprising: encoding the video frame based at least in part on the valid face region to generate a coded bitstream.\\n17. the method of claim 16, wherein encoding the video frame comprises not encoding a background region of the video frame into the bitstream.\\n18. the method of claim 12, further comprising: encoding the video frame based at least in part on the valid face region to generate a coded bitstream, wherein encoding the video frame comprises including metadata corresponding to the valid face region in the bitstream.\\n19. the method of claim 18, further comprising: decoding the coded bitstream to generate a decoded video frame and to determine the metadata corresponding to the valid face region in the bitstream.\\n20. the method of claim 19, further comprising at least one of replacing the valid face region based on the decoded metadata, cropping and displaying image data corresponding only to the valid face region based on the decoded metadata, or indexing the decoded video frame based on the decoded metadata.\\n21. a system for performing video coding based on face detection comprising: a memory configured to store a video frame comprising one of a plurality of video frames of a video sequence; and a processor coupled to the memory, the processor to receive the video frame, to determine the video frame is a key frame of the video sequence; to perform, in response to the video frame being a key frame of the video sequence, a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame, to test the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region, to reject the second candidate face region and outputting the first candidate face region, and to encode the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream.\\n22. the system of claim 21, wherein the skin tone information comprises a skin probability map.\\n23. the system of claim 21, wherein the first candidate face region comprises a rectangular region, the processor further to determine a free form shape face region corresponding to the first candidate face region, wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy.\\n24. the system of claim 23, wherein the processor to determine the free form shape face region comprises the processor to generate an enhanced skip probability map corresponding to the first candidate face region, to binarize the enhanced skip probability map, and to overlay the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region.\\n25. the system of claim 23, wherein a second video frame comprises a non-key frame of the video sequence, and the processor is further to perform face detection in the second video frame of the video sequence based on the free form shape face region.\\n26. the system of claim 25, wherein the processor is further to track a second free form shape face region in the second video frame based on the free form shape face region in the video frame.\\n27. the system of claim 21, wherein to encode the video frame based at least in part on the first candidate face region being a valid face region comprises the processor to reduce a quantization parameter corresponding to the first candidate face region, adjust a lambda value for the first candidate face region, or disable skip coding for the first candidate face region.\\n28. at least one non-transitory machine readable medium comprising a plurality of instructions that, in response to being executed on a device, cause the device to perform video coding based on face detection by: receiving a video frame comprising one of a plurality of video frames of a video sequence; determining the video frame is a key frame of the video sequence; performing, in response to the video frame being a key frame of the video sequence, a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame; testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region; rejecting the second candidate face region and outputting the first candidate face region; and encoding the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream.\\n29. the non-transitory machine readable medium of claim 28, wherein the skin tone information comprises a skin probability map.\\n30. the non-transitory machine readable medium of claim 28, wherein the first candidate face region comprises a rectangular region, the machine readable medium comprising further instructions that, in response to being executed on the device, cause the device to perform video coding based on face detection by: determining a free form shape face region corresponding to the first candidate face region, wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy.\\n31. the non-transitory machine readable medium of claim 30, wherein determining the free form shape face region comprises: generating an enhanced skip probability map corresponding to the first candidate face region; binarizing the enhanced skip probability map; and overlaying the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region.\\n32. the non-transitory machine readable medium of claim 30, wherein a second video frame comprises a non-key frame of the video sequence, the machine readable medium comprising further instructions that, in response to being executed on the device, cause the device to perform video coding based on face detection by performing face detection in the second video frame of the video sequence based on the free form shape face region.\\n33. the non-transitory machine readable medium of claim 32, the machine readable medium comprising further instructions that, in response to being executed on the device, cause the device to perform video coding based on face detection by: tracking a second free form shape face region in the second video frame based on the free form shape face region in the video frame.\\n34. the non-transitory machine readable medium of claim 28, wherein encoding the video frame based at least in part on the first candidate face region being a valid face region comprises at least one of reducing a quantization parameter corresponding to the first candidate face region, adjusting a lambda value for the first candidate face region, or disabling skip coding for the first candidate face region.1. a method for managing a smart database which stores facial images for face recognition, comprising steps of: (a) a managing device performing a process of counting one or more specific facial images corresponding to at least one specific person stored in the smart database where new facial images for the face recognition are continuously stored, and a process of determining whether a first counted value representing a count of the specific facial images satisfies a preset first set value; and (b) if the first counted value is determined as satisfying the first set value, the managing device performing a process of inputting the specific facial images into a neural aggregation network, to thereby allow the neural aggregation network to generate each of quality scores of each of the specific facial images by aggregation of the specific facial images, and a process of sorting the quality scores corresponding to the specific facial images in a descending order of the quality scores, a process of counting the sorted specific facial images in the descending order until a second counted value which represents the number of a counted part of the specific facial images becomes equal to a preset second set value, and a process of deleting an uncounted part of the specific facial images from the smart database.\\n2. the method of claim 1, further comprising a step of: (c) the managing device performing a process of generating at least one optimal feature by weighted summation of one or more features of the specific facial images using the counted part of the quality scores and a process of setting the optimal feature as a representative face corresponding to the specific person.\\n3. the method of claim 1, wherein, at the step of (b), the managing device performs a process of inputting the specific facial images into a cnn of the neural aggregation network, to thereby allow the cnn to generate one or more features corresponding to each of the specific facial images, and a process of inputting at least one feature vector, where the features are embedded, into an aggregation module including at least two attention blocks, to thereby allow the aggregation module to generate each of the quality scores of each of the features.\\n4. the method of claim 1, wherein, at the step of (b), the managing device performs a process of matching (i) (i-1) one or more features corresponding to each of the specific facial images stored in the smart database and (i-2) the quality scores with (ii) the specific person, and a process of storing the matched features and the matched quality scores in the smart database.\\n5. the method of claim 1, further comprising a step of: (d) the managing device performing one of (i) a process of learning a face recognition system by using the specific facial images corresponding to the specific person stored in the smart database and (ii) a process of transmitting the specific facial images, corresponding to the specific person, to a learning device corresponding to the face recognition system, to thereby allow the learning device to learn the face recognition system using the specific facial images.\\n6. the method of claim 1, wherein the neural aggregation network has been learned by a learning device repeating more than once (i) a process of inputting multiple facial images for training corresponding to an image set of a single face or a video of the single face into a cnn of the neural aggregation network, to thereby allow the cnn to generate one or more features for training by applying at least one convolution operation to the facial images for training, (ii) a process of inputting at least one feature vector for training, where the features for training are embedded, into an aggregation module, including at least two attention blocks, of the neural aggregation network, to thereby allow the aggregation module to generate each of quality scores for training of each of the features for training by aggregation of the features for training using one or more attention parameters learned in a previous iteration, (iii) a process of outputting at least one optimal feature for training by weighted summation of the features for training using the quality scores for training, and (iv) a process of updating the attention parameters learned in the previous iteration of the at least two attention blocks such that one or more losses are minimized which are outputted from a loss layer by referring to the optimal feature for training and its corresponding ground truth.\\n7. a managing device for managing a smart database which stores facial images for face recognition, comprising: at least one memory that stores instructions; and at least one processor configured to execute the instructions to perform or support another device to perform: (i) a process of counting one or more specific facial images corresponding to at least one specific person stored in the smart database where new facial images for the face recognition are continuously stored, and a process of determining whether a first counted value representing a count of the specific facial images satisfies a preset first set value, and (ii) if the first counted value is determined as satisfying the first set value, a process of inputting the specific facial images into a neural aggregation network, to thereby allow the neural aggregation network to generate each of quality scores of each of the specific facial images by aggregation of the specific facial images, and a process of sorting the quality scores corresponding to the specific facial images in a descending order of the quality scores, a process of counting the sorted specific facial images in the descending order until a second counted value which represents the number of a counted part of the specific facial images becomes equal to a preset second set value, and a process of deleting an uncounted part of the specific facial images from the smart database.\\n8. the managing device of claim 7, wherein the processor further performs: (iii) a process of generating at least one optimal feature by weighted summation of one or more features of the specific facial images using the counted part of the quality scores and a process of setting the optimal feature as a representative face corresponding to the specific person.\\n9. the managing device of claim 7, wherein, at the process of (ii), the processor performs a process of inputting the specific facial images into a cnn of the neural aggregation network, to thereby allow the cnn to generate one or more features corresponding to each of the specific facial images, and a process of inputting at least one feature vector, where the features are embedded, into an aggregation module including at least two attention blocks, to thereby allow the aggregation module to generate each of the quality scores of each of the features.\\n10. the managing device of claim 7, wherein, at the process of (ii), the processor performs a process of matching (i) (i-1) one or more features corresponding to each of the specific facial images stored in the smart database and (i-2) the quality scores with (ii) the specific person, and a process of storing the matched features and the matched quality scores in the smart database.\\n11. the managing device of claim 7, wherein the processor further performs: (iv) one of (i) a process of learning a face recognition system by using the specific facial images corresponding to the specific person stored in the smart database and (ii) a process of transmitting the specific facial images, corresponding to the specific person, to a learning device corresponding to the face recognition system, to thereby allow the learning device to learn the face recognition system using the specific facial images.\\n12. the managing device of claim 7, wherein the neural aggregation network has been learned by a learning device repeating more than once (i) a process of inputting multiple facial images for training corresponding to an image set of a single face or a video of the single face into a cnn of the neural aggregation network, to thereby allow the cnn to generate one or more features for training by applying at least one convolution operation to the facial images for training, (ii) a process of inputting at least one feature vector for training, where the features for training are embedded, into an aggregation module, including at least two attention blocks, of the neural aggregation network, to thereby allow the aggregation module to generate each of quality scores for training of each of the features for training by aggregation of the features for training using one or more attention parameters learned in a previous iteration, (iii) a process of outputting at least one optimal feature for training by weighted summation of the features for training using the quality scores for training, and (iv) a process of updating the attention parameters learned in the previous iteration of the at least two attention blocks such that one or more losses are minimized which are outputted from a loss layer by referring to the optimal feature for training and its corresponding ground truth.1. an object data processing system comprising: at least one processor configured to execute: at least one implementation of a plurality of recognition algorithms stored on at least one non-transitory computer-readable storage medium, each recognition algorithm having feature density selection criteria; and data preprocessing code executed by at least one processor, the data preprocessing code comprising an invariant feature identification algorithm and configured to: obtain a digital representation of a scene, the scene comprising one or more textual media; generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation; cluster the set of invariant features into regions of interest in the digital representation of the scene, each region of interest having a region feature density; classify, by region classifier code, at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation, wherein the at least one of the classified regions of interest corresponds to text; and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type, wherein the another of the regions of interest corresponds to a region of interest for images.\\n2. the system of claim 1, wherein preprocessing code, based on the feature density selection criteria, determines that an ocr algorithm is applicable to the text, and that other recognition algorithms are applicable to aspects of the photographs and to logos.\\n3. the system of claim 1, wherein a user creates a user profile for a camera-equipped smartphone that includes the information that the user is visually impaired, which causes prioritized execution of the ocr algorithm such that a text reader program begins reading the text to the user as quickly as possible.\\n4. the system of claim 3, further comprising an audio or tactile feedback mechanism that helps the user to position the smart phone relative to the text.\\n5. the system of claim 4, further comprising a \"hold still\" audio feedback signal that is sent to the user when the text is at the center of the captured scene.\\n6. the system of claim 1, wherein the digital representation comprises at least one of the following types of digital data: image data, video data, and audio data.\\n7. the system of claim 1, wherein invariant feature identification algorithm comprises at least one of the following feature identification algorithms: fast, sift, freak, brisk, harris, daisy, and mser.\\n8. the system of claim 1, wherein the invariant feature identification algorithm includes at least one of the following: edge detection algorithm, corner detection algorithm, saliency map algorithm, curve detection algorithm, a texton identification algorithm, and wavelets algorithm.\\n9. the system of claim 1, wherein at least one region of interest represents at least one physical object in the scene.\\n10. the system of claim 1, wherein at least one region of interest represents at least one textual media in the scene.\\n11. the system of claim 10, wherein the region of interest represents a document as the textual media.\\n12. the system of claim 11, wherein the region of interest represents a financial document.\\n13. the system of claim 11, wherein the region of interest represents a structured document.\\n14. the system of claim 1, wherein at least one implementation of a plurality of recognition algorithms includes at least one of the following: a template driven algorithm, a face recognition algorithm, an optical character recognition algorithm, a speech recognition algorithm, and an object recognition algorithm.\\n15. the system of claim 1, wherein data preprocessing code is further configured to assign each region of interest at least one recognition algorithm as a function of a scene context derived from the digital representation.\\n16. the system of claim 15, wherein the scene context includes at least one of the following types of data: a location, a position, a time, a user identity, a news event, a medical event, and a promotion.\\n17. the system of claim 1, further comprising a mobile device comprising at least one implementation of a plurality of recognition algorithms and data preprocessing code.\\n18. the system of claim 17, wherein the mobile device comprises at least one of the following: a smart phone, a tablet, wearable glass, a toy, a vehicle, a computer, and a phablet.\\n19. the system of claim 1, further comprising a network-accessible server device comprising at least one implementation of a plurality of recognition algorithms and data preprocessing code.\\n20. the system of claim 1, wherein the object type includes at least one of the following: a face, an animal, a vehicle, a document, a plant, a building, an appliance, clothing, a body part, and a toy.\\n21. an object data processing system comprising: at least one processor configured to execute: at least one implementation of a plurality of recognition algorithms stored on at least one non-transitory computer-readable storage medium, each recognition algorithm having feature density selection criteria; and data preprocessing code executed by at least one processor, the data preprocessing code comprising an invariant feature identification algorithm and configured to: obtain a digital representation of a scene, the scene comprising one or more textual media; generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation; cluster the set of invariant features into regions of interest in the digital representation of the scene, each region of interest having a region feature density; classify, by region classifier code, at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation; wherein the at least one of the classified regions of interest corresponds to text; and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type, wherein the another of the regions of interest corresponds to a region of interest for images; assign each region of interest at least one recognition algorithm from at least one implementation of a plurality of diverse recognition algorithms as a function of the region feature density of each region of interest and the feature density selection criteria of the at least one implementation of a plurality of diverse recognition algorithms; and configure the assigned recognition algorithms to process their respective regions of interest, wherein preprocessing code, based on the feature density selection criteria, determines that an ocr algorithm is applicable to the text, and that other recognition algorithms are applicable to aspects of the photographs and to logos.\\n22. a device comprising: at least one processor configured to execute: at least one implementation of a plurality of recognition algorithms stored on at least one non-transitory computer-readable storage medium, each recognition algorithm having feature density selection criteria; and data preprocessing code executed by at least one processor, the data preprocessing code comprising an invariant feature identification algorithm and configured to: obtain a digital representation of a scene, the scene comprising one or more textual media; generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation; cluster the set of invariant features into regions of interest in the digital representation of the scene, each region of interest having a region feature density; and classify, by region classifier code, at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation, wherein the at least one of the classified regions of interest corresponds to text; and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type, wherein the another of the regions of interest corresponds to a region of interest for images.1. a mobile terminal comprising: a front camera configured to obtain a two-dimensional (2d) face image of a user; a glance sensor tilted by a certain angle and disposed adjacent to the front camera to obtain metadata of the 2d face image; and a controller obtaining a distance between the glance sensor and the front camera, the distance enabling an area of an overlap region, where a first region representing a range photographable by the front camera overlaps a second region representing a range photographable by the glance sensor, to be the maximum.\\n2. the mobile terminal of claim 1, wherein the controller is configured to obtain the distance, enabling the area of the overlap region to be the maximum, between the glance sensor and the front camera by varying a tilting angle of the glance sensor.\\n3. the mobile terminal of claim 2, wherein the controller is configured to set the distance, enabling the area of the overlap region to be the maximum, between the glance sensor and the front camera and the tilting angle of the glance sensor as an optimal disposition location of the glance sensor.\\n4. the mobile terminal of claim 3, wherein the controller is configured to set a disposition location of the front camera as an original point and calculates coordinates of a first triangle representing the first region, based on a field of view of the front camera and a maximum photographing distance of the front camera.\\n5. the mobile terminal of claim 4, wherein the controller is configured to calculate coordinates of a second triangle representing the second region, based on a field of view of the glance sensor, a maximum photographing distance of the glance sensor, a distance between the front camera and the glance sensor, and a tilting angle of the glance sensor.\\n6. the mobile terminal of claim 5, wherein before the glance sensor is tilted, the controller is configured to calculate coordinates of a third triangle representing a third region photographable by the glance sensor, and the controller is configured to rotation-convert the coordinates of the third triangle, based on the tilting angle of the glance sensor and calculate the coordinates of the second triangle.\\n7. the mobile terminal of claim 6, wherein the controller is configured to calculate coordinates of the overlap region, based on the coordinates of the first triangle and the coordinates of the second triangle and calculates the area of the overlap region, based on the coordinates of the overlap region.\\n8. the mobile terminal of claim 1, wherein the controller is configured to generate three-dimensional (3d) face information, based on the 2d face image obtained by the front camera and metadata obtained by the glance sensor.\\n9. the mobile terminal of claim 8, wherein the metadata comprises one or more of an angle of a face of the user, a size of the face, and a location of the face.\\n10. the mobile terminal of claim 9, wherein the angle of the face comprises an angle by which the face is rotated about one or more of a pitch axis, a roll axis, and a yaw axis.\\n11. the mobile terminal of claim 8, further comprising a memory storing the generated 3d face information, wherein the controller is configured to performs a user authentication process by comparing the stored 3d face information with 3d face information obtained for user authentication.\\n12. the mobile terminal of claim 1, wherein the glance sensor is controlled to be permanently activated with a low power to obtain a front image and metadata of the front image.\\n13. the mobile terminal of claim 1, wherein the front camera and the glance sensor are disposed on the same line in an upper end of the mobile terminal.\\n14. the mobile terminal of claim 1, wherein the glance sensor is tilted in one direction of an up direction, a down direction, a left direction, and a right direction.\\n15. the mobile terminal of claim 1, wherein the metadata is data which is changed when the mobile terminal is tilted by an external physical force.1. a method, comprising: receiving, by a smart television (tv), an indication of upcoming media programming, wherein the upcoming media programming is based on a user profile; identifying one or more devices in communication with the smart tv, each of the one or more devices including at least one of a microphone or a camera; instructing at least one identified device to detect audio signals using its respective microphone, or to detect visual signals using its respective camera; selecting at least one device of the one or more devices based on the detected audio signal or detected visual signal; and providing instructions to the selected device to output a notification related to the upcoming media programming.\\n2. the method of claim 1, wherein the upcoming media programming is one of a live television program, a recorded television program, a broadcast television program, or an application-provided program.\\n3. the method of claim 1, wherein selecting the first device based on the detected audio signal includes recognizing a voice.\\n4. the method of claim 3, further comprising determining a distance to the recognized voice, and wherein selecting the first device is further based on the determined distance.\\n5. the method of claim 1, wherein selecting the first device based on the detected visual signals includes recognizing a face.\\n6. the method of claim 5, wherein recognizing the face includes a face recognition technique.\\n7. the method of claim 1, further comprising presenting, on the smart tv, the upcoming media programming in a favorite channel list.\\n8. the method of claim 7, further comprising: obtaining media programming viewing data, wherein the media programming viewing data includes at least one of a historical time and a historical date that one or more media programs were viewed; obtaining at least one of a current time and a current date; processing the media programming viewing data to determine a probability of the one or more media programs being viewed based on at least one of the current time and the current date; and presenting the favorite channel list based on the determined probability of the one or more media programs being viewed.\\n9. the method of claim 8, wherein processing the media programming viewing data includes employing a neural network model.\\n10. the method of claim 9, wherein employing the neural network model comprises: determining a duration that the one or more media programs were viewed for each of the at least one of the historical time and the historical date; setting a threshold time duration; comparing the determined duration to the threshold time duration; and filtering out the one or more media programs viewed below the threshold time duration.\\n11. a smart television (tv), comprising: a network interface; a non-transitory computer-readable medium; and a processor in communication with the network interface, and the non-transitory computer-readable medium, and capable of executing processor-executable program code stored in the non-transitory computer-readable medium, to cause the smart tv to: receive an indication of upcoming media programming, wherein the upcoming media programming is based on a user profile; identify one or more devices in communication with the smart tv, each of the one or more devices including at least one of a microphone or a camera; instruct at least one identified device to detect audio signals using its respective microphone, or to detect visual signals using its respective camera; select at least one device of the one or more devices based on the detected audio signal or detected visual signal; and provide instructions to the selected device to output a notification related to the upcoming media programming.\\n12. the smart tv of claim 11, wherein selecting the first device based on the detected audio signal includes recognizing a voice.\\n13. the smart tv of claim 12, wherein the processor is further capable of executing processor-executable program code to: determine a distance to the recognized voice, and wherein selecting the first device is further based on the determined distance.\\n14. the smart tv of claim 11, wherein selecting the first device based on the detected visual signals includes detecting the presence of a user.\\n15. the smart tv of claim 14, wherein detecting the presence of the user includes employing one or more of a camera, a microphone, or a fingerprint sensor associated with at least one of the smart tv a mobile device, a smartphone, a laptop computer, a tablet device, a wearable device, an internet of things (iot) device, an internet of everything (ioe) device, an iot hub, or an ioe hub.\\n16. a smart television (tv), comprising: means for receiving an indication of upcoming media programming, wherein the upcoming media programming is based on a user profile; means for identifying one or more devices in communication with the smart tv, each of the one or more devices including at least one of a microphone or a camera; means for instructing at least one identified device to detect audio signals using its respective microphone, or to detect visual signals using its respective camera; means for selecting at least one device of the one or more devices based on the detected audio signal or detected visual signal; and means for providing instructions to the selected device to output a notification related to the upcoming media programming.\\n17. the smart tv of claim 16, wherein the one or more devices includes at least one of a mobile device, a smartphone, a laptop computer, a tablet device, a wearable device, an internet of things (iot) device, an internet of everything (ioe) device, an iot hub, an ioe hub, or another smart tv.\\n18. the smart tv of claim 16, wherein the upcoming media programming is one of a live television program, a recorded television program, a broadcast television program, or an application-provided program.\\n19. the smart tv of claim 16, wherein the notification includes at least one of a push message, a sms message, a way2sms message, an audio alert, an audio message, or an email message.\\n20. the smart tv of claim 16, further comprising presenting the upcoming media programming in a favorite channel list.\\n21. the smart tv of claim 20, further comprising: means for obtaining media programming viewing data, wherein the media programming viewing data includes at least one of a historical time and a historical date that one or more media programs were viewed on the smart tv; means for obtaining at least one of a current time and a current date; means for processing the media programming viewing data to determine a probability of the one or more media programs being viewed on the smart tv based on at least one of the current time and the current date; and means for presenting the favorite channel list based on the determined probability of the one or more media programs being viewed.\\n22. the smart tv of claim 21, wherein the means for processing the media programming viewing data includes employing a neural network model.\\n23. the smart tv of claim 22, wherein employing the neural network model comprises: determining a duration that the one or more media programs were viewed on the smart tv for each of the at least one of the historical time and the historical date; setting a threshold time duration; comparing the determined duration to the threshold time duration; and filtering out the one or more media programs viewed below the threshold time duration.\\n24. the smart tv of claim 21, further comprising: means for adjusting at least one of a volume or a brightness of the smart tv, wherein the adjusting is based on at least one of the historical time and the historical date.\\n25. the smart tv of claim 21, further comprising means for restricting access to one or more media programs.\\n26. a non-transitory computer-readable medium comprising processor-executable program code configured to cause a processor of a smart television (tv) to: receive an indication of upcoming media programming, wherein the upcoming media programming is based on a user profile; identify one or more devices in communication with the smart tv, each of the one or more devices including at least one of a microphone or a camera; instruct at least one identified device to detect audio signals using its respective microphone, or to detect visual signals using its respective camera; select at least one device of the one or more devices based on the detected audio signal or detected visual signal; and provide instructions to the selected device to output a notification related to the upcoming media programming.\\n27. the non-transitory computer-readable medium of claim 26, wherein selecting the first device based on the detected audio signal includes recognizing a voice.\\n28. the non-transitory computer-readable medium of claim 27, wherein the processor is further capable of executing processor-executable program code to: determine a distance to the recognized voice, and wherein selecting the first device is further based on the determined distance.\\n29. the non-transitory computer-readable medium of claim 26, wherein selecting the first device based on the detected visual signals includes recognizing a face.\\n30. the non-transitory computer-readable medium of claim 29, wherein recognizing the face includes a face recognition technique.1. a camera comprising: a sensor array including a plurality of sensors; an infrared (ir) illuminator configured to emit active ir light in an ir light sub-band; a plurality of spectral illuminators, each spectral illuminator configured to emit active spectral light in a different spectral light sub-band; a depth controller machine configured to determine a depth value for each of the plurality of sensors based on the active ir light, a spectral controller machine configured to, for each of the plurality of sensors, determine a spectral value for each spectral light sub-band of the plurality of spectral illuminators; and an output machine configured to output a test depth+multi-spectral image including a plurality of pixels, each pixel corresponding to one of the plurality of sensors of the sensor array and including at least: a depth value, and a spectral value for each spectral light sub-band of the plurality of spectral illuminators; a face recognition machine previously trained with a set of labeled training depth+multi-spectral images having a same structure as the test depth+multi-spectral image, the face recognition machine configured to output a confidence value indicating a likelihood that the test depth+multi-spectral image includes a face.\\n2. the camera of claim 1, wherein each spectral value is calculated based on the depth value determined for the sensor that corresponds to the pixel.\\n3. the camera of claim 1, wherein the face recognition machine is configured to use a convolutional neural network to determine the confidence value.\\n4. the camera of claim 3, wherein the face recognition machine includes a plurality of input nodes, wherein each input node is configured to receive a pixel value array corresponding to a different pixel of the plurality of pixels of the test depth+multi-spectral image, and wherein the pixel value array includes the depth value and the plurality of multi-spectral values for the pixel.\\n5. the camera of claim 4, wherein the plurality of multi-spectral values for the pixel include more than three spectral values.\\n6. the camera of claim 4, wherein the output machine is configured to output a surface normal for each pixel of the test depth+multi-spectral image, and wherein the pixel value array includes the surface normal.\\n7. the camera of claim 4, wherein the output machine is configured to output a curvature for each pixel of the test depth+multi-spectral image, and wherein the pixel value array includes the curvature.\\n8. the camera of claim 3, wherein the face recognition machine is configured to use a plurality of models to determine the confidence value, wherein the plurality of models includes a plurality of channel-specific models, wherein each channel-specific model is configured to process a different pixel parameter for the plurality of pixels of the test depth+multi-spectral image, wherein each channel-specific model includes a plurality of input nodes, and wherein, for each channel-specific model, each input node is configured to receive a pixel parameter value for a different pixel of the plurality of pixels of the test depth+multi-spectral image.\\n9. the camera of claim 1, wherein the face recognition machine is configured to use a statistical model to determine the confidence value.\\n10. the camera of claim 9, wherein the statistical model includes a nearest neighbor algorithm.\\n11. the camera of claim 9, wherein the statistical model includes a support vector machine.\\n12. the camera of claim 1, wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of a bounding box around a recognized face.\\n13. the camera of claim 1, wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of an identified two-dimensional (2d) facial feature of a recognized face.\\n14. the camera of claim 1, wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of an identified three-dimensional (3d) facial feature of a recognized face.\\n15. the camera of claim 1, wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of an identified spectral feature on a recognized face.\\n16. the camera of claim 1, wherein the face recognition machine is further configured to output, for each pixel of the test depth+multi-spectral image, a confidence value indicating a likelihood that the pixel is included in a face.\\n17. the camera of claim 1, wherein the face recognition machine is further configured to output an identity of a face recognized in the test depth+multi-spectral image.\\n18. the camera of claim 1, wherein the plurality of sensors of the sensor array are differential sensors, and wherein each spectral value is determined based on a depth value and a differential measurement for that differential sensor.\\n19. a camera comprising: a sensor array including a plurality of sensors; an infrared (ir) illuminator configured to emit active ir light in an ir light sub-band; a plurality of spectral illuminators, each spectral illuminator configured to emit active spectral light in a different spectral light sub-band; a depth controller machine configured to determine a depth value for each of the plurality of sensors based on the active ir light, a spectral controller machine configured to, for each of the plurality of sensors, determine a spectral value for each spectral light sub-band of the plurality of spectral illuminators, wherein each spectral value is calculated based on the depth value determined for the sensor that corresponds to the pixel; and an output machine configured to output a test depth+multi-spectral image including a plurality of pixels, each pixel corresponding to one of the plurality of sensors of the sensor array and including at least: a depth value, and a spectral value for each spectral light sub-band of the plurality of spectral illuminators; and a face recognition machine including a convolutional neural network previously trained with a set of labeled training depth+multi-spectral images having a same structure as the test depth+multi-spectral image, the face recognition machine configured to output a confidence value indicating a likelihood that the test depth+multi-spectral image includes a face.1. an image processing method, comprising: acquiring a photo album obtained from face clustering; collecting face information of respective images in the photo album, and acquiring a face parameter of each image according to the face information; selecting a cover image according to the face parameter of each image; and taking a face-region image from the cover image, and setting the face-region image as a cover of the photo album; wherein selecting the cover image according to the face parameter of each image comprises: performing calculation on the face parameter of each image in a preset way, to obtain a cover score of each image; selecting the image with a highest cover score as the cover image; wherein selecting the image with the highest cover score as the cover image comprises: acquiring a source of each image; and selecting the image with the highest cover score in images coming from a preset source as the cover image.\\n2. the method according to claim 1, wherein selecting the image with the highest cover score as the cover image comprises: acquiring the number of faces contained in each image; determining single-person images according to the number of faces; and selecting the single-person image with the highest cover score as the cover image.\\n3. the method according to claim 2, wherein selecting the image with the highest cover score as the cover image further comprises: when there is no single-person image in the photo album, determining images including two faces from the photo album; and selecting the image with the highest cover score from the images including two faces as the cover image.\\n4. the method according to claim 1, wherein the face information comprises face feature points, and the face parameter comprises a face turning angle; acquiring the face parameter of each image according to the face information comprises: acquiring coordinate values of the face feature points; determining distances and angles between the face feature points; and determining the face turning angle according to the distances and the angles.\\n5. the method according to claim 1, wherein the face parameter comprises a face ratio; acquiring the face parameter of each image according to the face information comprises: determining a face region of the image according to the face information; and calculating a ratio of an area of the face region to an area of the image to obtain the face ratio.\\n6. the method according to claim 5, wherein calculating the face ratio comprises: when there is more than one face in the image, subtracting an area occupied faces other than a face corresponding to the photo album from the face region to obtain a remaining area; and calculating a ratio of the remaining area to the area of the image to obtain the face ratio.\\n7. the method according to claim 1, wherein collecting face information of respective images in the photo album comprises: acquiring image identifications of images in the photo album; extracting face information corresponding to the image identifications from a face database, the face database being stored with face recognition results of images, the face recognition results including the face information.\\n8. an image processing apparatus, comprising: a processor; and a memory, configured to store instructions executable by the processor, wherein the processor is configured to run a program corresponding to the instructions by reading the instructions stored in the memory, so as to perform: acquiring a photo album obtained from face clustering; collecting face information of each image in the photo album; acquiring a face parameter of each image according to the face information; selecting a cover image according to the face parameter of each image; taking a face-region image from the cover image, and setting the face-region image as a cover of the photo album; wherein the processor is configured to: perform calculation on the face parameter of each image in a preset way, to obtain a cover score of each image; and select the image with a highest cover score as the cover image; and wherein the processor is configured to: acquire a source of each image; and select the image with the highest cover score in images coming from a preset source as the cover image.\\n9. the apparatus according to claim 8, wherein the processor is configured to: acquire the number of faces contained in each image; determine single-person images according to the number of faces; and select the single-person image with the highest cover score as the cover image.\\n10. the apparatus according to claim 9, wherein the processor is further configured to: when there is no single-person image in the photo album, determine images including two faces from the photo album; and select the image with the highest cover score from the images including two faces as the cover image.\\n11. the apparatus according to claim 8, wherein the face information comprises face feature points, and the face parameter comprises a face turning angle; the processor is configured to: acquire coordinate values of the face feature points; determine distances and angles between the face feature points; and determine the face turning angle according to the distances and the angles.\\n12. the apparatus according to claim 8, wherein the face parameter comprises a face ratio; the processor is configured to: determine a face region of the image according to the face information; and calculate a ratio of an area of the face region to an area of the image to obtain the face ratio.\\n13. the apparatus according to claim 12, wherein the processor is configured to: when there is more than one face in the image, subtract an area occupied faces other than a face corresponding to the photo album from the face region to obtain a remaining area; and calculate a ratio of the remaining area to the area of the image to obtain the face ratio.\\n14. the apparatus according to claim 8, wherein the processor is configured to: acquire image identifications of images in the photo album; extract face information corresponding to the image identifications from a face database, the face database being stored with face recognition results of images, the face recognition results including the face information.\\n15. an electronic device, comprising a processor, a memory, a display screen and an input device connected via a system bus, wherein the memory is stored with computer programs that, when executed by the processor, cause the processor to implement an image processing method, the image processing method comprising: acquiring a photo album obtained from face clustering; collecting face information of respective images in the photo album, and acquiring a face parameter of each image according to the face information; selecting a cover image according to the face parameter of each image; and taking a face-region image from the cover image, and setting the face-region image as a cover of the photo album; wherein selecting the cover image according to the face parameter of each image comprises: performing calculation on the face parameter of each image in a preset way, to obtain a cover score of each image; and selecting the image with a highest cover score as the cover image; and wherein selecting the image with the highest cover score as the cover image comprises: acquiring a source of each image; and selecting the image with the highest cover score in images coming from a preset source as the cover image.\\n16. the electronic device according to claim 15, wherein the electronic device comprises at least one of a mobile phone, a tablet computer, a personal digital assistant and a wearable device.1. a computer-implemented method, comprising: receiving, at a computing device, a meeting invitation identifying a location and at least one invitee, the meeting invitation configured to provide the at least one invitee with physical access to the location, wherein the meeting invitation causes a system to control a pathway allowing physical access to the location; providing, based on the meeting invitation, the at least one invitee with physical access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location wherein the positioning data is based in part on a face recognition camera system identifying the at least one invitee; receiving the positioning data from the face recognition camera system identifying the at least one invitee, wherein the positioning data indicates a pattern of movement of the at least one invitee; determining that the pattern of movement indicates that the at least one invitee has exited the location; and revoking physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway, in response to determining that the pattern of movement indicates that the at least one invitee has exited the location.\\n2. the computer-implemented method of claim 1, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location in a predetermined direction.\\n3. the computer-implemented method of claim 1, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has moved through an area in a predetermined direction.\\n4. the computer-implemented method of claim 1, wherein the positioning data indicates a second pattern of movement of the at least one invitee and, wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement.\\n5. the computer-implemented method of claim 1, further comprising: collating secured data and public data to generate resource data; and communicating the resource data to a client computing device associated with the at least one invitee when access of the location is provided.\\n6. the computer-implemented method of claim 1, wherein the positioning data indicates that the at least one invitee is at the predetermined location when the at least one invitee passes through the predetermined location.\\n7. the computer-implemented method of claim 1, wherein the positioning data indicates that the at least one invitee is at the predetermined location when the at least one invitee passes through the predetermined location near the location in a predetermined direction.\\n8. a system, comprising: a processor; and a memory in communication with the processor, the memory having computer-readable instructions stored thereupon that, when executed by the processor, cause the processor to: receive a meeting invitation indicating a location and an identity, the meeting invitation configured to provide at least one invitee with physical access to the location, wherein the meeting invitation causes the system to control a pathway allowing physical access to the location; provide the at least one invitee associated with the identity access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location, wherein the positioning data is based in part on a face recognition camera system identifying the at least one invitee; receive the positioning data from the face recognition camera system identifying the at least one invitee, wherein the positioning data indicates a pattern of movement of the at least one invitee; determine that the pattern of movement indicates that the at least one invitee has exited the location; and revoke physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway, in response to determining that the pattern of movement indicates that the at least one invitee has exited the location.\\n9. the system of claim 8, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location.\\n10. the system of claim 8, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has moved through an area in a predetermined direction.\\n11. the system of claim 8, wherein the positioning data indicates a second pattern of movement of the at least one invitee and wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement.\\n12. the system of claim 8, wherein the instructions further cause the processor to: collate secured data and public data to generate resource data; and communicate the resource data to a client computing device associated with the at least one invitee when access of the location is provided.\\n13. a non-transitory computer-readable storage medium having computer-executable instructions stored thereupon which, when executed by one or more processors of a computing device, cause the one or more processors of the computing device to: receive a meeting invitation indicating a location and an identity, the meeting invitation configured to provide at least one invitee with physical access to the location, wherein the meeting invitation causes a system to control a pathway allowing physical access to the location; provide the at least one invitee associated with the identity access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location, wherein the positioning data is based in part on a face recognition camera system identifying the at least one invitee; receive the positioning data from the face recognition camera system identifying the at least one invitee, wherein the positioning data indicates a pattern of movement of the at least one invitee; determine that the pattern of movement indicates that the at least one invitee has exited the location; and revoke physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway, in response to determining that the pattern of movement indicates that the at least one invitee has exited the location.\\n14. the non-transitory computer-readable storage medium of claim 13, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location.\\n15. the non-transitory computer-readable storage medium of claim 13, wherein the positioning data indicates a second pattern of movement of the at least one invitee and wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement.\\n16. the non-transitory computer-readable storage medium of claim 13, wherein the instructions further cause the one or more processors to: collate secured data and public data to generate resource data; and communicate the resource data to a client computing device associated with the at least one invitee when access of the location is provided.1. a method, comprising: receiving a piece of content and salient data for the piece of content; based on the salient data, determining a first path for a viewport for the piece of content, wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content; providing the viewport on a display device, wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback; detecting an additional salient event in the piece of content that is not included in the first path for the viewport; and providing an indication for the additional salient event in the viewport during the playback.\\n2. the method of claim 1, wherein the salient data identifies each salient event in the piece of content, and the salient data indicates, for each salient event in the piece of content, a corresponding point location of the salient event in the piece of content and a corresponding time at which the salient event occurs during the playback.\\n3. the method of claim 2, wherein the salient data further indicates, for each salient event in the piece of content, a corresponding type of the salient event and a corresponding strength value of the salient event.\\n4. the method of claim 1, wherein the first path for the viewport controls the movement of the viewport to put the different salient events in a view of the viewport at the different times during the playback.\\n5. the method of claim 1, further comprising: detecting one or more salient events in the piece of content based on at least one of the following: visual data of the piece of content, audio data of the piece of content, or content consumption experience data for the piece of content; wherein the salient data is indicative of each salient event detected.\\n6. the method of claim 1, further comprising: detecting one or more salient events in the piece of content based on at least one of the following: face recognition, facial emotion recognition, object recognition, motion recognition, or metadata of the piece of content; wherein the salient data is indicative of each salient event detected.\\n7. the method of claim 1, further comprising: detecting user interaction with the indication, wherein the indication comprises an interactive hint; and in response to detecting the user interaction: adapting the first path for the viewport to a second path for the viewport based on the user interaction, wherein the second path for the viewport includes the additional salient event; and providing an updated viewport for the piece of content on the display device, wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback, and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport.\\n8. the method of claim 7, further comprising: changing a weight assigned to the additional salient event and one or more other salient events in the piece of content having the same type as the additional salient event.\\n9. the method of claim 7, wherein the second path for the viewport includes one or more other salient events in the piece of content having the same type as the additional salient event.\\n10. a system, comprising: at least one processor; and a non-transitory processor-readable memory device storing instructions that when executed by the at least one processor causes the at least one processor to perform operations including: receiving a piece of content and salient data for the piece of content; based on the salient data, determining a first path for a viewport for the piece of content, wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content; providing the viewport on a display device, wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback; detecting an additional salient event in the piece of content that is not included in the first path for the viewport; and providing an indication for the additional salient event in the viewport during the playback.\\n11. the system of claim 10, wherein the salient data identifies each salient event in the piece of content, and the salient data indicates, for each salient event in the piece of content, a corresponding point location of the salient event in the piece of content and a corresponding time at which the salient event occurs during the playback.\\n12. the system of claim 11, wherein the salient data further indicates, for each salient event in the piece of content, a corresponding type of the salient event and a corresponding strength value of the salient event.\\n13. the system of claim 10, wherein the salient data is generated offline on a server.\\n14. the system of claim 10, the operations further comprising: detecting one or more salient events in the piece of content based on at least one of the following: visual data of the piece of content, audio data of the piece of content, or content consumption experience data for the piece of content; wherein the salient data is indicative of each salient event detected.\\n15. the system of claim 10, the operations further comprising: detecting one or more salient events in the piece of content based on at least one of the following: face recognition, facial emotion recognition, object recognition, motion recognition, or metadata of the piece of content; wherein the salient data is indicative of each salient event detected.\\n16. the system of claim 10, the operations further comprising: detecting user interaction with the indication, wherein the indication comprises an interactive hint; and in response to detecting the user interaction: adapting the first path for the viewport to a second path for the viewport based on the user interaction, wherein the second path for the viewport includes the additional salient event; and providing an updated viewport for the piece of content on the display device, wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback, and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport.\\n17. the system of claim 16, the operations further comprising: changing a weight assigned to the additional salient event and one or more other salient events in the piece of content having the same type as the additional salient event.\\n18. the system of claim 16, wherein the second path for the viewport includes one or more other salient events in the piece of content having the same type as the additional salient event.\\n19. a non-transitory computer readable storage medium including instructions to perform a method comprising: receiving a piece of content and salient data for the piece of content; based on the salient data, determining a first path for a viewport for the piece of content, wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content; providing the viewport on a display device, wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback; detecting an additional salient event in the piece of content that is not included in the first path for the viewport; and providing an indication for the additional salient event in the viewport during the playback.\\n20. the computer readable storage medium of claim 19, the method further comprising: detecting user interaction with the indication, wherein the indication comprises an interactive hint; and in response to detecting the user interaction: adapting the first path for the viewport to a second path for the viewport based on the user interaction, wherein the second path for the viewport includes the additional salient event; and providing an updated viewport for the piece of content on the display device, wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback, and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport.1. a mobile device with facial recognition, the mobile device comprising: one or more cameras; a processor device and memory coupled to the processor device, the processing system programmed to: receive a plurality of images from the one or more cameras; extract, with a feature extractor utilizing a convolutional neural network (cnn) with an enlarged intra-class variance of long-tail classes, feature vectors from each of the plurality of images; generate, with a feature generator, discriminative feature vectors for each of the feature vectors; classify, with a fully connected classifier, an identity from the discriminative feature vectors; and control an operation of the mobile device to react in accordance with the identity.\\n2. the mobile device as recited in claim 1, further includes a communication system.\\n3. the mobile device as recited in claim 1, wherein the operation tags the video with the identity and uploads the video to social media.\\n4. the mobile device as recited in claim 1, wherein the operation tags the video with the identity and sends the video to a user.\\n5. the mobile device as recited in claim 1, wherein the mobile device is a smart phone.\\n6. the mobile device as recited in claim 1, wherein the mobile device is a body cam.\\n7. the mobile device as recited in claim 1, further programmed to train the feature extractor, the feature generator, and the fully connected classifier with an alternative bi-stage strategy.\\n8. the mobile device as recited in claim 1, wherein the feature extractor shares covariance matrices across all classes to transfer intra-class variance from regular classes to the long-tail classes.\\n9. the mobile device as recited in claim 1, wherein the feature generator optimizes a softmax loss by joint regularization of weights and features through a magnitude of an inner product of the weights and features.\\n10. the mobile device as recited in claim 1, wherein the feature extractor averages the feature vector with a flipped feature vector, the flipped feature vector being generated from a horizontally flipped frame from one of the plurality of images.\\n11. the mobile device as recited in claim 1, wherein each of the plurality of images is selected from the group consisting of an image, a video, and a frame from the video.\\n12. the mobile device as recited in claim 2, wherein the communication system connects to a remote server that includes a facial recognition network.\\n13. the mobile device as recited in claim 7, wherein one stage of the alternative bi-stage strategy fixes the feature extractor and applies the feature generator to generate new transferred features that are more diverse and violate a decision boundary.\\n14. the mobile device as recited in claim 7, wherein one stage of the alternative bi-stage strategy fixes the fully connected classifier and updates the feature extractor and the feature generator.\\n15. a computer program product for a mobile device with facial recognition, the computer program product comprising a non-transitory computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computer to cause the computer to perform a method comprising: receiving, by a processor device, a plurality of images; extracting, by the processor device with a feature extractor utilizing a convolutional neural network (cnn) with an enlarged intra-class variance of long-tail classes, feature vectors for each of the plurality of images; generating, by the processor device with a feature generator, discriminative feature vectors for each of the feature vectors; classifying, by the processor device utilizing a fully connected classifier, an identity from the discriminative feature vector; and controlling an operation of the mobile device to react in accordance with the identity.\\n16. a computer-implemented method for facial recognition in a mobile device, the method comprising: receiving, by a processor device, a plurality of images; extracting, by the processor device with a feature extractor utilizing a convolutional neural network (cnn) with an enlarged intra-class variance of long-tail classes, feature vectors for each of the plurality of images; generating, by the processor device with a feature generator, discriminative feature vectors for each of the feature vectors; classifying, by the processor device utilizing a fully connected classifier, an identity from the discriminative feature vector; and controlling an operation of the mobile device to react in accordance with the identity.\\n17. the computer-implemented method as recited in claim 16, wherein controlling includes tagging the video with the identity and uploading the video to social media.\\n18. the computer-implemented method as recited in claim 16, wherein controlling includes tagging the video with the identity and sending the video to a user.\\n19. the computer-implemented method as recited in claim 16, wherein extracting includes sharing covariance matrices across all classes to transfer intra-class variance from regular classes to the long-tail classes.1. a computing device comprising: a non-transitory machine readable medium storing a machine trained (mt) network comprising a plurality of layers of processing nodes, each processing node configured to: compute a first output value by combining a set of output values from a set of processing nodes, and use a piecewise linear cup function to compute a second output value from the first output value of the processing node, wherein the piecewise linear cup function prior to training of the mt network comprises at least (i) a first linear section with a first slope, followed by (ii) a second linear section with a negative second slope, followed by (iii) a third linear section with a negative third slope that is different from the second slope, followed by (iv) a fourth linear section with a positive fourth slope, followed by (v) a fifth linear section with a positive fifth slope that is different from the fourth slope, followed by (vi) a sixth linear section with a sixth slope, wherein the piecewise linear cup function is symmetric about a vertical axis between the third and fourth linear sections prior to training of the mt network; a content capturing circuit for capturing content for processing by the mt network; and a set of processing units for executing the processing nodes to process content captured by the content capturing circuit, wherein by training a set of parameters that define the piecewise linear cup function of each node in first and second pluralities of processing nodes, (i) each processing node in the first plurality of processing nodes is configured to emulate a boolean and operator such that an output value of the processing node is in a range associated with a \"1\" value only when a set of inputs to the processing node have a set of values in a range associated with \"1\" and (ii) each processing node in the second plurality of processing nodes is configured to emulate a boolean xnor operator such that an output value of the processing node is in the range associated with \"1\" only when (a) a set of inputs to the node have a set of values in a range associated with \"1\" or (b) the set of inputs to the node have a set of values in a range associated with a \"0\" value.\\n2. the computing device of claim 1, wherein the third linear section of the piecewise linear cup function of a first processing node in the mt network has a different slope from the third linear section of a second processing node in the mt network.\\n3. the computing device of claim 1, wherein the length of the third section of a piecewise linear cup function of a first processing node in the mt network is different from the length of the third section of a piecewise linear cup function of a second processing node in the mt network.\\n4. the computing device of claim 1, wherein the sets of parameters are trained in part by a back propagating module for back propagating errors in output values of later layers of processing nodes to earlier layers of processing nodes by adjusting the set of parameters that define the piecewise linear cup functions of the earlier layers of processing nodes.\\n5. the computing device of claim 4, wherein each processing node uses a linear function that is defined by a set of parameters to compute the first output value of the processing node, wherein the back propagating module back propagates errors in output values of later layers of processing nodes to earlier layers of processing nodes by adjusting the set of parameters that define the linear functions of the earlier layers of processing nodes.\\n6. the computing device of claim 1, wherein the first plurality of processing nodes that emulate the boolean and operator and the second plurality of processing nodes that emulate the boolean xnor operator enable the mt network to implement mathematical problems.\\n7. the computing device of claim 1, wherein each of a plurality of processing node layers has a plurality of processing nodes that receive as input values the output values from a plurality of processing nodes in a set of prior layers.\\n8. the computing device of claim 7, wherein each processing node uses a linear function to compute the first output value of the processing node, wherein each processing node\\'s piecewise linear cup function is defined along first and second axes, the first axis defining a range of output values from the processing node\\'s linear function, and the second axis defining a range of output values produced by the piecewise linear cup function for the range of output values from the processing node\\'s linear function.\\n9. the computing device of claim 1, further comprising: a content output circuit for presenting an output based on the processing of the content by the mt network.\\n10. the computing device of claim 9, wherein the captured content is one of an image and an audio segment, and wherein the presented output is an output display on a display screen of the computing device or an audio presentation output on a speaker of the computing device.\\n11. the computing device of claim 10, wherein the computing device is a mobile device.\\n12. the computing device of claim 1, wherein the mt network is a mt neural network and the processing nodes are mt neurons.\\n13. the computing device of claim 1, wherein the set of parameters configured through training for a plurality of the processing nodes comprise at least one of the negative second and third slopes for the second and third linear sections, the positive fourth and fifth slopes for the fourth and fifth linear sections, a first intercept for the second linear section, a second intercept for the fifth linear section, and a set of lengths for at least the second, third, fourth, and fifth sections.\\n14. the computing device of claim 1, wherein the trained set of parameters that define the piecewise linear cup function of each node comprise a plurality of output values.\\n15. the computing device of claim 1, wherein the first and sixth slopes are zero.we claim:\\n1. a system comprising: a memory device to store an input image; a processor including, an image input interface to receive the input image, a pre-processor to model the input image to yield a multi-channel image, a feature extractor to extract a set of features based on the multi-channel image, a feature selector to select one or more features from the set of features of the multi-channel image, wherein the one or more features are selected based on an ability to differentiate features, a feature matcher to match the one or more features to a learned feature set, and a similarity detector to determine whether the one or more features meet a pre-defined similarity threshold.\\n2. the system of claim 1, wherein the pre-processor further is to activate one or more channels of the multi-channel image to yield one or more activated channels.\\n3. the system of claim 2, wherein the one or more activated channels are to be determined based on their ability to differentiate features.\\n4. the system of claim 2, wherein the pre-processor further is to activate one or more local patches of the one or more activated channels.\\n5. the system of claim 4, wherein the one or more local patches are to be determined based on their ability to differentiate features.\\n6. the system of claim 1, wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching.\\n7. an apparatus comprising: an image input interface to receive an input image; a pre-processor to model the input image to yield a multi-channel image; a feature extractor to extract a set of features based on the multi-channel image; a feature selector to select one or more features from the set of features of the multi-channel image, wherein the one or more features are selected based on an ability to differentiate features; a feature matcher to match the one or more features to a learned feature set; and a similarity detector to determine whether the one or more features meet a pre-defined similarity threshold.\\n8. the apparatus of claim 7, wherein the pre-processor further is to activate one or more channels of the multi-channel image to yield one or more activated channels.\\n9. the apparatus of claim 8, wherein the one or more activated channels are to be determined based on their ability to differentiate features.\\n10. the apparatus of claim 8, wherein the pre-processor further is to activate one or more local patches of the one or more activated channels.\\n11. the apparatus of claim 10, wherein the one or more local patches are to be determined based on their ability to differentiate features.\\n12. the apparatus of claim 7, wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching.\\n13. a method comprising: modeling an input image to yield a multi-channel image; extracting a set of features based on the multi-channel image; selecting one or more features from the set of features of the multi-channel image, wherein the one or more features are selected based on an ability to differentiate features; matching the one or more features to a learned feature set; and determining whether the one or more features meet a pre-defined similarity threshold.\\n14. the method of claim 13, wherein modeling the input image further is to include activating one or more channels of the multi-channel image to yield one or more activated channels.\\n15. the method of claim 14, wherein the one or more activated channels are to be determined based on their ability to differentiate features.\\n16. the method of claim 13, wherein extracting features of the input image further is to include activating one or more local patches of the one or more activated channels.\\n17. the method of claim 16, wherein the one or more local patches are to be determined based on their ability to differentiate features.\\n18. the method of claim 13, wherein the feature matcher utilizes a large-scale data learning process to perform the feature matching.\\n19. at least one non-transitory computer readable storage medium comprising a set of instructions which, when executed by a computing device, cause the computing device to: model an input image to yield a multi-channel image, extract a set of features based on the multi-channel image, select one or more features from the set of features of the multi-channel image, wherein the features are selected based on an ability to differentiate features, match the one or more features to a learned feature set, and determine whether the one or more features meet a pre-defined similarity threshold.\\n20. the at least one non-transitory computer readable storage medium of claim 19, wherein the instructions, when executed, cause a computing device to activate one or more channels of the multi-channel image to yield one or more activated channels.\\n21. the at least one non-transitory computer readable storage medium of claim 20, wherein the instructions, when executed, cause a computing device to determine the one or more activated channels based on their ability to differentiate features.\\n22. the at least one non-transitory computer readable storage medium of claim 20, wherein extracting features of the input image is to further include activating one or more local patches of the one or more activated channels.\\n23. the at least one non-transitory computer readable storage medium of claim 22, wherein the one or more local patches are to be determined based on their ability to differentiate features.\\n24. the at least one non-transitory computer readable storage medium of claim 19, wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching.\\n25. an apparatus comprising: means for modeling an input image to yield a multi-channel image, means for extracting a set of features based on the multi-channel image, means for selecting one or more features from the set of features of the multi-channel image, wherein the one or more features are selected based on an ability to differentiate features, means for matching the one or more features to a learned feature set, and means for determining whether the one or more features meet a pre-defined similarity threshold.1. a method for controlling a terminal, the terminal comprising a capturing apparatus and at least one processor, the method comprising: acquiring, by the capturing apparatus, an image; obtaining, by the at least one processor, a motion parameter of the terminal, the motion parameter comprising at least one of a motion frequency or a motion time, and two or more parameters from among an acceleration, an angular velocity, a motion amplitude, the motion frequency, and the motion time; transmitting, by the at least one processor, a parameter threshold obtaining request to a data management server, the parameter threshold obtaining request comprising configuration information of the terminal; receiving corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request; comparing the two or more parameters with the corresponding preset thresholds; and controlling, by the at least one processor, not to perform image processing on the acquired image based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds, wherein the acquiring comprises acquiring the image in real time, and the obtaining comprises obtaining the motion parameter of the terminal in real time, the method further comprising: in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, obtaining the motion parameter of the terminal again; and in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds, performing the image processing on the image acquired at the latest time.\\n2. the method according to claim 1, wherein the acquiring comprises: controlling, by the at least one processor, to turn on the capturing apparatus based on a face recognition instruction; and acquiring, by the capturing apparatus, a face image when the capturing apparatus is turned on.\\n3. the method according to claim 2, wherein the controlling not to perform the image processing comprises: skipping performing face recognition on the acquired face image based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds.\\n4. the method according to claim 1, wherein the obtaining comprises at least one of: obtaining the acceleration of the terminal by using an acceleration sensor; or obtaining the angular velocity of the terminal by using a gyro sensor.\\n5. the method according to claim 1, wherein the transmitting comprises: transmitting the parameter threshold obtaining request to the data management server according to a preset time period.\\n6. the method according to claim 1, further comprising: generating prompt information based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, the prompt information being used for prompting the terminal to stop moving.\\n7. the method according to claim 1, wherein the motion parameter comprises the motion frequency and the motion time.\\n8. a terminal comprising: a capturing apparatus; at least one memory configured to store program code; and at least one processor configured to access the at least one memory and operate according to the program code, the program code comprising: motion parameter obtaining code configured to cause the at least one processor to acquire an image by using the capturing apparatus and obtain a motion parameter of the terminal, the motion parameter comprising at least one of a motion frequency or a motion time, and two or more parameters from among an acceleration, an angular velocity, a motion amplitude, the motion frequency, and the motion time; request transmitting code configured to cause the at least one processor to transmit a parameter threshold obtaining request to a data management server, the parameter threshold obtaining request comprising configuration information of the terminal; parameter threshold receiving code configured to cause the at least one processor to receive corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request; comparing code configured to cause the at least one processor to compare the two or more parameters with the corresponding preset thresholds; and control code configured to cause the at least one processor not to perform image processing on the acquired image based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds, wherein the motion parameter obtaining code causes the at least one processor to: acquire the image in real time and obtain the motion parameter of the terminal in real time, and in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, obtain the motion parameter of the terminal again, and wherein the control code causes the at least one processor to, in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds, perform the image processing on the image acquired at the latest time.\\n9. the terminal according to claim 8, wherein the program code further comprises face instruction receiving code configured to cause the at least one processor to receive a face recognition instruction, wherein the motion parameter obtaining code causes the at least one processor to control, according to the face recognition instruction, the capturing apparatus to turn on, and acquire a face image by using the capturing apparatus when the capturing apparatus is turned on; and wherein the control code causes the at least one processor to skip performing face recognition on the acquired face image based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds.\\n10. the terminal according to claim 8, wherein the request transmitting code causes the at least one processor to transmit the parameter threshold obtaining request to the data management server according to a preset time period.\\n11. the terminal according to claim 8, wherein the program code further comprises: prompt information generation code configured to cause the at least one processor to generate prompt information based on at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, the prompt information being used for prompting the terminal to stop moving.\\n12. the terminal according to claim 8, wherein the motion parameter comprises the motion frequency and the motion time.\\n13. a non-transitory computer-readable storage medium, storing a machine instruction, which, when executed by one or more processors, causes the one or more processors to perform: obtaining an image acquired by a capturing apparatus; obtaining a motion parameter of a terminal, the terminal comprising the capturing apparatus, the motion parameter comprising at least one of a motion frequency or a motion time, and two or more parameters from among an acceleration, an angular velocity, a motion amplitude, the motion frequency, and the motion time; transmitting a parameter threshold obtaining request to a data management server, the parameter threshold obtaining request comprising configuration information of the terminal; receiving corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request; comparing the two or more parameters with the corresponding preset thresholds; and controlling not to perform image processing on an acquired image based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds, wherein the acquiring comprises acquiring the image in real time, and the obtaining comprises obtaining the motion parameter of the terminal in real time, the method further comprising: in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, obtaining the motion parameter of the terminal again; and in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds, performing the image processing on the image acquired at the latest time.\\n14. the non-transitory computer-readable storage medium according to claim 13, wherein the acquired image is a face image and the image processing comprises performing face recognition.\\n15. the non-transitory computer-readable storage medium according to claim 13, wherein the obtaining the motion parameter comprises at least one of: obtaining the acceleration of the terminal by using an acceleration sensor; or obtaining the angular velocity of the terminal by using a gyro sensor.\\n16. the non-transitory computer-readable storage medium according to claim 13, wherein the motion parameter comprises the motion frequency and the motion time.1. a method of processing a drive-through order, the method comprising: receiving customer information detected through vision recognition; providing product information to a customer based on the customer information; and processing a product order of the customer.\\n2. the method according to claim 1, wherein the receiving of customer information comprises at least one of receiving customer information associated with vehicle information detected through vehicle recognition, or receiving customer information associated with identification information detected through face recognition.\\n3. the method according to claim 1, further comprising determining whether the customer is a pre-order customer based on the customer information, wherein when the customer is determined to be a pre-order customer: the providing of product information based on the customer information comprises providing pre-order information using at least one of audio or video, and the processing of the product order of the customer comprises: providing information for promptly guiding a vehicle to a pickup stand using at least one of audio or video, and providing information that an additional order is available.\\n4. the method according to claim 1, wherein the product information based on the customer information comprises a most recently ordered product component and a most frequently ordered product component in an order history of the customer information.\\n5. the method according to claim 1, wherein the receiving of customer information comprises receiving information about an age and gender of a passenger detected through face recognition, and the providing of product information to a customer based on the customer information comprises providing recommended menu information differentiated according to the age and gender.\\n6. the method according to claim 1, wherein the processing of a product order of the customer comprises determining a product component in a past order history or a component modified from the product component as a product order.\\n7. the method according to claim 1, wherein the processing of a product order of the customer comprises paying a product price according to biometrics-based authentication through a communication system of a vehicle or a mobile terminal.\\n8. the method according to claim 1, wherein the processing of a product order of the customer comprises: issuing a payment number for a divided payment, and performing the divided payments according to payment requests of a plurality of mobile terminals to which the payment numbers are inputted.\\n9. the method according to claim 8, wherein the processing of a product order of the customer further comprises accumulating mileage in an account corresponding to the mobile terminal undergoing a payment.\\n10. the method according to claim 1, wherein the processing of a product order of the customer further comprises suggesting a takeout packaging method according to a temperature of a product, an atmospheric temperature, weather, and a vehicle type.\\n11. an apparatus configured to process a drive-through order, the apparatus comprising: a transceiver configured to receive customer information detected through vision recognition; a digital signage configured to provide product information to a customer based on the customer information; and a processor configured to process a product order of the customer.\\n12. the apparatus according to claim 11, wherein the transceiver receives at least one of customer information associated with vehicle information detected through vehicle recognition, or customer information associated with identification information detected through face recognition.\\n13. the apparatus according to claim 11, wherein the processor is configured to: determine whether the customer is a pre-order customer based on the customer information; and when the customer is determined to be a pre-order customer, perform a control operation to provide pre-order information, and control the digital signage to output information for promptly guiding a vehicle to a pickup stand and provide information that an additional order is available.\\n14. the apparatus according to claim 11, wherein the product information based on the customer information comprises a most recently ordered product component and a most frequently ordered product component in an order history of the customer information.\\n15. the apparatus according to claim 11, wherein the transceiver is configured to receive information about an age and gender of a passenger detected through face recognition, and the processor is configured to control the digital signage to provide recommended menu information differentiated according to the age and gender.\\n16. the apparatus according to claim 11, wherein the processor is configured to determine a product component in a past order history or a component modified from the product component as the product order.\\n17. the apparatus according to claim 11, wherein the processor is configured to pay a product price according to biometrics-based authentication through a communication system of a vehicle or a mobile terminal.\\n18. the apparatus according to claim 11, wherein the processor is configured to: issue a payment number for a divided payment; and perform the divided payments according to requests of a plurality of mobile terminals to which the payment numbers are inputted.\\n19. the apparatus according to claim 18, wherein the processor is configured to accumulate mileage in an account corresponding to the mobile terminal undergoing a payment.\\n20. the apparatus according to claim 11, wherein the processor is configured to control the digital signage to suggest a takeout packaging method according to a temperature of a product, an atmospheric temperature, weather, and a vehicle type.1. an image information processing method performed at a computing device having one or more processors and memory storing a plurality of programs to be executed by the one or more processors, the method comprising: identifying, using face recognition, one or more faces, each face corresponding to a respective person captured in a first image; for each identified face: extracting a set of profile parameters of a corresponding person in the first image; and selecting, from a plurality of image tiles, a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile; generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles; and sharing the first image and the second image in a predefined order via a group chat session.\\n2. the method of claim 1, wherein the first image and the second image are displayed in the group chat session one image at a time such that one of the two images is replaced by the other of the two images periodically.\\n3. the method of claim 1, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model, wherein the first machine learning model is trained with the facial images and corresponding descriptive labels.\\n4. the method of claim 1, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining an identity of the corresponding person based on the identified face of the corresponding person; locating respective profile information of the first person based on the determined identity of the corresponding person; and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person.\\n5. the method of claim 1, wherein at least a first one of the first image tiles is a dynamic image tile and at least a second one of the first image tiles is a static image tile.\\n6. the method of claim 1, including: receiving a plurality of user comments from different users of the group chat session, each user comment including a descriptive term for a respective person identified in the first image; choosing a descriptive label for the respective person according to the plurality of user comments; and updating the second image by adding the descriptive label adjacent to the first image tile of the respective person.\\n7. a computing device for image information processing, comprising: one or more processors; and memory storing instructions which, when executed by the one or more processors, cause the processors to perform a plurality of operations comprising: identifying, using face recognition, one or more faces, each face corresponding to a respective person captured in a first image; for each identified face: extracting a set of profile parameters of a corresponding person in the first image; and selecting, from a plurality of image tiles, a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile; generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles; and sharing the first image and the second image in a predefined order via a group chat session.\\n8. the computing device of claim 7, wherein the first image and the second image are displayed in the group chat session one image at a time such that one of the two images is replaced by the other of the two images periodically.\\n9. the computing device of claim 7, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model, wherein the first machine learning model is trained with the facial images and corresponding descriptive labels.\\n10. the computing device of claim 7, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining an identity of the corresponding person based on the identified face of the corresponding person; locating respective profile information of the first person based on the determined identity of the corresponding person; and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person.\\n11. the computing device of claim 7, wherein at least a first one of the first image tiles is a dynamic image tile and at least a second one of the first image tiles is a static image tile.\\n12. the computing device of claim 7, wherein the plurality of operations further include: receiving a plurality of user comments from different users of the group chat session, each user comment including a descriptive term for a respective person identified in the first image; choosing a descriptive label for the respective person according to the plurality of user comments; and updating the second image by adding the descriptive label adjacent to the first image tile of the respective person.\\n13. a non-transitory computer-readable storage medium storing instructions which, when executed by a computing device having one or more processors, cause the computing device to perform a plurality of operations comprising: identifying, using face recognition, one or more faces, each face corresponding to a respective person captured in a first image; for each identified face: extracting a set of profile parameters of a corresponding person in the first image; and selecting, from a plurality of image tiles, a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile; generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles; and sharing the first image and the second image in a predefined order via a group chat session.\\n14. the non-transitory computer-readable storage medium of claim 13, wherein the first image and the second image are displayed in the group chat session one image at a time such that one of the two images is replaced by the other of the two images periodically.\\n15. the non-transitory computer-readable storage medium of claim 13, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model, wherein the first machine learning model is trained with the facial images and corresponding descriptive labels.\\n16. the non-transitory computer-readable storage medium of claim 13, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining an identity of the corresponding person based on the identified face of the corresponding person; locating respective profile information of the first person based on the determined identity of the corresponding person; and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person.\\n17. the non-transitory computer-readable storage medium of claim 13, wherein at least a first one of the first image tiles is a dynamic image tile and at least a second one of the first image tiles is a static image tile.\\n18. the non-transitory computer-readable storage medium of claim 13, wherein the plurality of operations further include: receiving a plurality of user comments from different users of the group chat session, each user comment including a descriptive term for a respective person identified in the first image; choosing a descriptive label for the respective person according to the plurality of user comments; and updating the second image by adding the descriptive label adjacent to the first image tile of the respective person.1. a method comprising, by a computing system: determining that a performance metric of an eye tracking system is below a first performance threshold, wherein the eye tracking system is associated with a head-mounted display worn by a user; based on the determination of the performance metric of the eye tracking system being below the first performance threshold, the computer system performing: receiving one or more first inputs associated with a body of the user; estimating a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user; determining a vergence distance of the user based at least on the one or more first inputs associated with the body of the user, the estimated region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display; and adjusting one or more configurations of the head-mounted display based on the determined vergence distance of the user.\\n2. the method of claim 1, wherein the one or more configurations of the head-mounted display comprise one or more of: a rendering image; a position of a display screen; or a position of an optics block.\\n3. the method of claim 1, further comprising: determining that the performance metric of the eye tracking system is above a second performance threshold; receiving eye tracking data from the eye tracking system; and determining the vergence distance of the user based on the eye tracking data and the one or more first inputs associated with the body of the user.\\n4. the method of claim 3, further comprising: receiving one or more second inputs associated with one or more displaying elements in the scene displayed by the head-mounted display; and determining the vergence distance of the user based at least on the eye tracking data, the one or more first inputs associated with the body of the user, and the one or more second inputs associated with the one or more displaying elements of the scene.\\n5. the method of claim 4, further comprising: feeding the one or more first inputs associated with the body of the user to a fusion algorithm, wherein the fusion algorithm assigns a weight score to each input of the one or more first inputs; determining the vergence distance of the user using the fusion algorithm based on the one or more first inputs associated with the body of the user; and determining a z-depth of a display screen and a confidence score based on the one or more first inputs associated with the body of the user\\n6. the method of claim 5, further comprising: comparing the confidence score to a confidence level threshold; in response to a determination that the confidence score is below the confidence level threshold, feeding the one or more second inputs associated with the one or more displaying elements of the scene to the fusion algorithm; and determining the z-depth of the display screen using the fusion algorithm based on the one or more first inputs associated with the body of the user and the one or more second inputs associated with the one or more displaying elements of the scene.\\n7. the method of claim 6, further comparing: comparing, by the fusion algorithm, confidence scores associated with a plurality of combinations of inputs; and determining, by the fusion algorithm, the z-depth of the display screen based on a combination of inputs associated with a highest confidence score.\\n8. the method of claim 6, wherein the z-depth and the confidence score are determined by the fusion algorithm using a piecewise comparison of the one or more first inputs and the one or more second inputs.\\n9. the method of claim 6, wherein the z-depth and the confidence score are determined based on a correlation between two or more inputs of the one or more first inputs and the one or more second inputs.\\n10. the method of claim 5, wherein the fusion algorithm comprises a machine learning (ml) algorithm, and wherein the machine learning (ml) algorithm determines a combination of first inputs fed to the fusion algorithm.\\n11. the method of claim 4, wherein the one or more first inputs associated with the body of the user comprise one or more of: a hand position; a hand direction; a hand movement; a hand gesture; a head position; a head direction; a head movement; a head gesture; a gaze angle; rea body gesture; a body posture; a body movement; a behavior of the user; or a weighted combination of one or more related parameters.\\n12. the method of claim 11, wherein the one or more first inputs associated with the body of the user are received from one or more of: a controller; a sensor; a camera; a microphone; an accelerometer; a headset worn by the user; or a mobile device.\\n13. the method of claim 4, wherein the one or more second inputs associated with the one or more displaying elements comprise one or more of: a z-buffer value associated with a displaying element; a displaying element marked by a developer; an image analysis result; a shape of a displaying element; a face recognition result; an object recognition result; a person identified in a displaying content; an object identified in a displaying content; a correlation of two or more displaying elements; or a weighted combination of the one or more second inputs.\\n14. the method of claim 1, further comprising: determining that the performance metric of the eye tracking system is below a second performance threshold; receiving one or more second inputs associated with one or more displaying elements in the scene displayed by the head-mounted display; and determining the vergence distance of the user based at least on the one or more first inputs associated with the body of the user and the one or more second inputs associated with the one or more displaying elements.\\n15. the method of claim 14, wherein determining that the performance metric of the eye tracking system is below the second performance threshold comprises determining that the eye tracking system does not exist or fails to provide eye tracking data.\\n16. the method of claim 1, wherein the performance metric of the eye tracking system comprises one or more of: an accuracy of a parameter from the eye tracking system; a precision of a parameter from the eye tracking system; a value of a parameter from the eye tracking system; a detectability of a pupil; a metric based on one or more parameters associated with the user; a parameter change; a parameter changing trend; a data availability; or a weighted combination of one or more performance related parameters.\\n17. the method of claim 16, wherein the one or more parameters associated with the user comprise one or more of: an eye distance of the user; a pupil position; a pupil status; a correlation of two pupils of the user; a head size of the user; a position of a headset worn by the user; an angle of the headset worn by the user; a direction of the headset worn by the user; an alignment of the eyes of the user; or a weighted combination of one or more related parameters associated with the user.\\n18. the method of claim 1, wherein the first performance threshold comprises one or more of: a pre-determined value; a pre-determined range; a state of a data; a changing speed of a data; or a trend of a data change.\\n19. one or more non-transitory computer-readable storage media embodying software that is operable when executed by a computing system to: determine that a performance metric of an eye tracking system is below a first performance threshold, wherein the eye tracking system is associated with a head-mounted display worn by a user; based on the determination of the performance metric of the eye tracking system being below the first performance threshold, the media embodying software operable when executed by the computing system to: receive one or more first inputs associated with a body of the user; estimate a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user; determine a vergence distance of the user based at least on the one or more first inputs associated with the body of the user, the estimated region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display; and adjust one or more configurations of the head-mounted display based on the determined vergence distance of the user.\\n20. a system comprising: one or more non-transitory computer-readable storage media embodying instructions; one or more processors coupled to the storage media and operable to execute the instructions to: determine that a performance metric of an eye tracking system is below a first performance threshold, wherein the eye tracking system is associated with a head-mounted display worn by a user; based on the determination of the performance metric of the eye tracking system being below the first performance threshold, the system is configured to: receive one or more first inputs associated with a body of the user; estimate a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user; determine a vergence distance of the user based at least on the one or more first inputs associated with the body of the user, the estimated region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display; and adjust one or more configurations of the head-mounted display based on the determined vergence distance of the user.1. a computer-implemented method for image-based, self-guided object detection, comprising: receiving, by a processor device, a set of images, each of the images having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data; training, by the processor device, a grid-based object detector using the grid level label data; determining, by the processor device, a respective bounding box for the respective object in each of the images, by applying local segmentation to each of the images; and training, by the processor device, a region-based convolutional neural network (rcnn) for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the rcnn.\\n2. the computer-implemented method of claim 1, further comprising performing an action responsive to the object localization and object classification for a respective new object in a new image to which the rcnn is applied.\\n3. the computer-implemented method of claim 2, wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object.\\n4. the computer-implemented method of claim 1, wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of images.\\n5. the computer-implemented method of claim 1, wherein the local segmentation is applied to each of the images to segment a respective target region therein.\\n6. the computer-implemented method of claim 1, wherein the region-based convolutional neural network (rcnn) forms a model during an object training stage that is to detect objects in new images during an inference stage.\\n7. the computer-implemented method of claim 1, wherein the method is performed by a system selected from the group consisting of a surveillance system, a face detection system, a face recognition system, a cancer detection system, an object tracking system, and an advanced driver-assistance system.\\n8. a computer program product for image-based, self-guided object detection, the computer program product comprising a non-transitory computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computer to cause the computer to perform a method comprising: receiving, by a processor device, a set of images, each of the images having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data; training, by the processor device, a grid-based object detector using the grid level label data; determining, by the processor device, a respective bounding box for the respective object in each of the images, by applying local segmentation to each of the images; and training, by the processor device, a region-based convolutional neural network (rcnn) for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the rcnn.\\n9. the computer program product of claim 8, wherein the method further comprises performing an action responsive to the object localization and object classification for a respective new object in a new image to which the rcnn is applied.\\n10. the computer program product of claim 9, wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object.\\n11. the computer program product of claim 8, wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of images.\\n12. the computer program product of claim 8, wherein the local segmentation is applied to each of the images to segment a respective target region therein.\\n13. the computer program product of claim 8, wherein the region-based convolutional neural network (rcnn) forms a model during an object training stage that is to detect objects in new images during an inference stage.\\n14. the computer program product of claim 8, wherein the method is performed by a system selected from the group consisting of a surveillance system, a face detection system, a face recognition system, a cancer detection system, an object tracking system, and an advanced driver-assistance system.\\n15. a computer processing system for image-based, self-guided object detection, comprising: a memory device for storing program code; and a processor device for running the program code to receive a set of images, each of the images having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data; train a grid-based object detector using the grid level label data; determine a respective bounding box for the respective object in each of the images, by applying local segmentation to each of the images; and train a region-based convolutional neural network (rcnn) for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the rcnn.\\n16. the computer processing system of claim 15, wherein the processor device further runs the program code to perform an action responsive to the object localization and object classification for a respective new object in a new image to which the rcnn is applied.\\n17. the computer processing system of claim 16, wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object.\\n18. the computer processing system of claim 15, wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of images.\\n19. the computer processing system of claim 15, wherein the region-based convolutional neural network (rcnn) forms a model during an object training stage that is to detect objects in new images during an inference stage.\\n20. the computer processing system of claim 15, wherein the computer processing system is comprised in a system selected from the group consisting of a surveillance system, a face detection system, a face recognition system, a cancer detection system, an object tracking system, and an advanced driver-assistance system.1. a method of scalable, parallel, cloud-based face recognition utilizing a database of normalized stored images, comprising: capturing an image using a camera; detecting a face in the captured image; normalizing the detected facial image to match the normalized stored images; identifying facial features in the normalized detected facial image; generating a plurality of facial metrics from the facial features; calculating euclidean distances between the facial metrics of the normalized detected facial image with corresponding facial metrics of each of the stored images; comparing each euclidean distance against a predetermined threshold; responsive to the euclidean distance comparison, producing a reduced candidate list of best possible image matches from the normalized stored images; comparing, in parallel, the normalized detected facial image with each of the normalized stored images of the reduced candidate list utilizing a plurality of face recognition algorithms, where each processor of a parallel processing system uses a different face recognition algorithm; responsive to the comparison, producing best match results from each parallel subset of the reduced candidate list; and selecting a final match from the best match results using a deep learning neural network face recognition algorithm trained on outputs of individual face recognition algorithms.\\n2. the method of scalable, parallel, cloud-based face recognition of claim 1, wherein detecting a face in the captured image comprises: utilizing opencv to detect a face in the captured image; extracting the location of the eyes and a tip of the nose in the face; determining a distance between the eyes; cropping the face from the captured image, where the width and the height of a cropped face image is a function of the distance between the eyes; and rotating the face by an angle of rotation that is a function of the distance between the eyes.\\n3. the method of scalable, parallel, cloud-based face recognition of claim 2, wherein: the width of the cropped face image is 2.5 times the distance between the eyes; the height of the cropped face image is 3.5 times the distance between the eyes; and the angle of rotation is an angle formed by a straight line joining the eyes and an x-axis of the face.\\n4. the method of scalable, parallel, cloud-based face recognition of claim 3, wherein rotating the face comprises rotating the face to provide a frontal face pattern.\\n5. the method of scalable, parallel, cloud-based face recognition of claim 4, further comprising the step of proportionally rescaling the cropped and rotated image.\\n6. the method of scalable, parallel, cloud-based face recognition of claim 5, where the proportional rescaling yields a cropped and rotated image with a size of 100=100 pixels.\\n7. the method of scalable, parallel, cloud-based face recognition of claim 1, wherein the facial features identified in the normalized detected facial image comprise a pair of eyes, a tip of a nose, a mouth, a center of the mouth, and a chin area comprising a bottom, a top left landmark, and a top right landmark.\\n8. the method of scalable, parallel, cloud-based face recognition of claim 7, wherein generating a plurality of facial metrics comprises: calculating a distance between the pair of eyes, a distance between the eyes and the tip of the nose, a distance equal to the width of the mouth, a distance between the tip of the nose and the center of mouth, a distance between the bottom of chin and the center of mouth, a distance between the top left landmark on the chin and the tip of the nose, and a distance between the top right landmark on the chin and the tip of the nose.\\n9. the method of scalable, parallel, cloud-based face recognition of claim 8, wherein performing a euclidean distance match further comprises: partitioning the normalized stored images into a plurality of substantially equal subsets; performing a euclidean distance match between the facial metrics of the normalized detected facial image and corresponding facial metrics of each of the stored images of the subsets of the normalized stored images with a separate processor of a parallel processing system to generate a euclidean distance for each stored image of the subset; comparing each euclidean distance against a predetermined threshold with the separate processors; responsive to the euclidean distance comparison, producing a reduced candidate list of best possible image matches from the normalized stored images of each subset; and combining the reduced candidate lists from each subset to produce a single reduced candidate list.\\n10. the method of scalable, parallel, cloud-based face recognition of claim 9, wherein the plurality of face recognition algorithms utilized in comparing, in parallel, the normalized detected facial image with each of the normalized stored images of the reduced candidate list, consists of face recognition algorithms selected from a group consisting of principle component analysis (pca)-based algorithms, linear discriminant analysis (lda) algorithms, independent component analysis (ica) algorithms, kernel-based algorithms, feature-based techniques, algorithms based on neural networks, algorithms based on transforms, and model-based face recognition algorithms.\\n11. the method of scalable, parallel, cloud-based face recognition of claim 10, wherein the pca-based algorithms include eigenfaces for face detection/recognition, and the lda algorithms include the fisherfaces method of face recognition.\\n12. the method of scalable, parallel, cloud-based face recognition of claim 1, wherein comparing, in parallel, the captured image with each of the normalized stored images of the reduced candidate list further comprises: partitioning the reduced candidate list into a plurality of substantially equal subsets; processing each subset in a different processor of the parallel processing system uses a unique face recognition algorithm to produce the best match results; and using a reduce function of a mapreduce program to combine the best match results from each of the subsets to produce a single set of the best match results.\\n13. the method of scalable, parallel, cloud-based face recognition of claim 12, wherein partitioning the reduced candidate list comprises: selecting the images comprising each subset by optimizing the variance between of each of the images according to the following equation: where m and n are the number of rows and columns of the face vector image, n is the number of groups, and σij is the standard deviation of image dimension i in the group j of the face image vector.\\n14. the method of scalable, parallel, cloud-based face recognition of claim 13, wherein selecting the images comprising each subset by optimizing the variance between each of the images according to the following equation: d(μi, μj) is the euclidean distance between the mean of the group i and the mean of group j, i is the face image vector, and l is the number of group levels.\\n15. the method of scalable, parallel, cloud-based face recognition of claim 1, where selecting a final match from the best match results utilizing a deep learning neural network face recognition algorithm comprises utilizing either an adaboost machine-learning algorithm or a neural networks machine-learning model.\\n16. the method of scalable, parallel, cloud-based face recognition of claim 1, where normalizing the detected facial image to match the normalized stored images includes normalizing the detected facial image to the same size, orientation, and illumination of the normalized stored images.\\n17. a non-transitory computer-readable medium containing executable program instructions for causing a computer to perform a method of face recognition, the method comprising: detecting a face in an image captured by a camera; normalizing the detected facial image to match the normalized stored images; identifying facial features in the normalized detected facial image; generating a plurality of facial metrics from the facial features; calculating euclidean distances between the facial metrics of the normalized detected facial image with corresponding facial metrics of each of the stored images; comparing each euclidean distance against a predetermined threshold; responsive to the euclidean distance comparison, producing a reduced candidate list of best possible image matches from the normalized stored images; comparing, in parallel, the captured image with each of the normalized stored images of the reduced candidate list utilizing a plurality of face recognition algorithms, where each processor of a parallel processing system uses a different face recognition algorithm; responsive to the comparison, producing best match results from each parallel subset of the reduced candidate list; and selecting a final match from the best match results using a deep learning neural network face recognition algorithm trained on outputs of individual face recognition algorithms.\\n18. the non-transitory computer-readable medium containing executable program instructions of claim 17, wherein the plurality of face recognition algorithms utilized in comparing, in parallel, the normalized detected facial image with each of the normalized stored images of the reduced candidate list, consists of face recognition algorithms selected from a group consisting of principle component analysis (pca)-based algorithms, linear discriminant analysis (lda) algorithms, independent component analysis (ica) algorithms, kernel-based algorithms, feature-based techniques, algorithms based on neural networks, algorithms based on transforms, and model-based face recognition algorithms.\\n19. the non-transitory computer-readable medium containing executable program instructions of claim 18, wherein the pca-based algorithms include eigenfaces for face detection/recognition, and the lda algorithms include the fisherfaces method of face recognition.\\n20. the non-transitory computer-readable medium containing executable program instructions of claim 17, where selecting a final match from the best match results utilizing a deep learning neural network face recognition algorithm comprises utilizing either an adaboost machine-learning algorithm or a neural networks machine-learning model.1. an imaging device, comprising: a condensing lens; an image sensor configured to detect light passing through the condensing lens and comprising a pixel matrix, wherein the pixel matrix comprises a plurality of phase detection pixel pairs and a plurality of regular pixels; and a processor configured to turn on the phase detection pixel pairs for autofocusing and output autofocused pixel data after completing the autofocusing, divide the autofocused pixel data into a first subframe and a second subframe, calculate image features of at least one of the first subframe and the second subframe, wherein the image features comprise module widths of a finder pattern, and the finder pattern has a predetermined ratio, a harr-like feature, or a gabor feature, and determine an operating resolution of the regular pixels according to the image features calculated from at least one of the first subframe and the second subframe divided from the autofocused pixel data.\\n2. the imaging device as claimed in claim 1, wherein each of the phase detection pixel pairs comprises: a first pixel and a second pixel; a cover layer covering upon a first region of the first pixel and upon a second region of the second pixel, wherein the first region and the second region are mirror symmetrical to each other; and a microlens aligned with at least one of the first pixel and the second pixel.\\n3. the imaging device as claimed in claim 2, wherein the first region and the second region are 5% to 95% of an area of a single pixel.\\n4. the imaging device as claimed in claim 1, wherein the processor is configured to perform the autofocusing using a dual pixel autofocus technique according to pixel data of the phase detection pixel pairs before completing the autofocusing.\\n5. the imaging device as claimed in claim 1, wherein the processor is configured to divide pixel data of the phase detection pixel pairs into a third subframe and a fourth subframe before completing the autofocusing, and perform the autofocusing according to the third subframe and the fourth subframe.\\n6. the imaging device as claimed in claim 5, wherein the processor is further configured to calibrate brightness of the third subframe and the fourth subframe to be identical using a shading algorithm.\\n7. the imaging device as claimed in claim 1, wherein the operating resolution is selected as a first resolution smaller than a number of the regular pixels or as a second resolution larger than the first resolution.\\n8. the imaging device as claimed in claim 1, wherein the regular pixels are turned off in the autofocusing.\\n9. the imaging device as claimed in claim 1, wherein a number of the phase detection pixel pairs is smaller than that of the regular pixels.\\n10. an imaging device, comprising: a condensing lens; an image sensor configured to detect light passing through the condensing lens and comprising a pixel matrix, wherein the pixel matrix comprises a plurality of phase detection pixel pairs and a plurality of regular pixels; and a processor configured to turn on the phase detection pixel pairs for autofocusing and output autofocused pixel data after completing the autofocusing, divide the autofocused pixel data into a first subframe and a second subframe, calculate image features of at least one of the first subframe and the second subframe, wherein the image features comprise module widths of a finder pattern, and the finder pattern has a predetermined ratio, a harr-like feature, or a gabor feature, and select an image decoding or an image recognition using pixel data of the regular pixels according to the image features calculated from at least one of the first subframe and the second subframe divided from the autofocused pixel data.\\n11. the imaging device as claimed in claim 10, wherein each of the phase detection pixel pairs comprises: a first pixel and a second pixel; a cover layer covering upon a first region of the first pixel and upon a second region of the second pixel, wherein the first region and the second region are mirror symmetrical to each other; and a microlens aligned with at least one of the first pixel and the second pixel.\\n12. the imaging device as claimed in claim 10, wherein the processor is configured to perform the autofocusing using a dual pixel autofocus technique according to pixel data of the phase detection pixel pairs before completing the autofocusing.\\n13. the imaging device as claimed in claim 10, wherein the processor is configured to divide the pixel data of the phase detection pixel pairs into a third subframe and a fourth subframe before completing the autofocusing, calibrate brightness of the third subframe and the fourth subframe to be identical using a shading algorithm, and perform the autofocusing according to the third subframe and the fourth subframe.\\n14. the imaging device as claimed in claim 10, wherein the processor is configured to calculate the image features using at least one of a rule based algorithm and a machine learning algorithm.\\n15. the imaging device as claimed in claim 10, wherein the image decoding is decoding qr codes, and the image recognition is face recognition.\\n16. an operating method of an imaging device, the imaging device comprising a plurality of phase detection pixel pairs and a plurality of regular pixels, the operating method comprising: turning on the phase detection pixel pairs for autofocusing and outputting autofocused image frame after completing the autofocusing; dividing the autofocused image frame, acquired by the phase detection pixel pairs, into a first subframe and a second subframe; calculating image features of at least one of the first subframe and the second subframe, wherein the image feature comprise module widths of a finder pattern, and the finder pattern has a predetermined ratio, a harr-like feature, or a gabor feature; and selectively activating at least a part of the regular pixels according to the image features calculated from at least one of the first subframe and the second subframe divided from the autofocused image frame.\\n17. the operating method as claimed in claim 16, wherein the selectively activating comprises: activating a first part of the regular pixels to perform an image decoding according to pixel data of the first part of the regular pixels; or activating all the regular pixels to perform an image recognition according to pixel data of the all regular pixels.\\n18. the operating method as claimed in claim 17, wherein pixel data of the phase detection pixel pairs captured in a same frame with the pixel data of the regular pixels is also used in performing the image decoding and the image recognition.\\n19. the operating method as claimed in claim 17, wherein the image decoding is decoding qr codes, and the image recognition is face recognition.\\n20. the operating method as claimed in claim 16, wherein the phase detection pixel pairs are partially covered pixels or have a structure of dual pixel.1. an apparatus comprising: a first camera module configured to obtain a first image of an object with a first field of view; a second camera module configured to obtain a second image of the object with a second field of view different from the first field of view; a first depth map generator configured to generate a first depth map of the first image based on the first image and the second image; and a second depth map generator configured to generate a second depth map of the second image based on the first image, the second image, and the first depth map.\\n2. the apparatus of claim 1, wherein the first field of view is a narrow angle and the second field of view is a wider angle.\\n3. the apparatus of claim 2, wherein the second image is divided into a primary region and a residual region, and the second depth map generator comprises: a relationship estimating module configured to estimate a relationship between the primary region and the residual region based on the first image and the second image; and a depth map estimating module configured to estimate a depth map of the residual region based on the estimated relationship and the first depth map.\\n4. the apparatus of claim 3, wherein at least one of the relationship estimating module and the depth map estimating module performs an estimating operation based on a neural network module.\\n5. the apparatus of claim 1, further comprising: a depth map fusion unit configured to generate a third depth map of the second image by performing a fusion operation based on the first depth map and the second depth map.\\n6. the apparatus of claim 5, wherein the depth map fusion unit comprises: a tone mapping module configured to generate a tone-mapped second depth map to correspond to the first depth map by performing a bias removing operation on the second depth map; and a fusion module configured to generate the third depth map by fusing the tone-mapped second depth map and the first depth map.\\n7. the apparatus of claim 6, wherein the depth map fusion unit further comprises a propagating module configured to generate a propagated first depth map in the second image by iterated propagating of the first depth map based on the first depth map and the second image, and the fusion module generates the third depth map by fusing the tone-mapped second depth map and the propagated first depth map.\\n8. the apparatus of claim 6, wherein the depth map fusion unit further comprises a post-processing module configured to perform a post-processing operation on the third depth map generated by the fusion module to provide the post-processed third depth map.\\n9. the apparatus of claim 8, wherein the post-processing module performs the post-processing operation by filtering an interface generated in the third depth map in accordance with fusion of the fusion module.\\n10. the apparatus of claim 8, wherein the post-processing module removes artifacts generated in the third depth map in accordance with fusion of the fusion module.\\n11. the apparatus of claim 1, wherein the first depth map generator analyses a distance relationship between the first image and the second image, and generates a first depth map of the first image based on the distance relationship.\\n12. a method of processing an image of an electronic apparatus, the method comprising: obtaining a first image of an object using a first camera module; obtaining a second image of the object using a second camera module; generating a first depth map of the first image based on the first image and the second image; estimating a relationship between a primary region of the second image and a residual region of the second image based on the first image and the second image; and generating a second depth map of the second image based on the estimated relationship between the primary region and the residual region, and the first depth map.\\n13. the method of claim 12, wherein the electronic apparatus comprises a first camera module including a first lens having a first field of view and a second camera module including a second lens having a second field of view wider than the first field of view.\\n14. the method of claim 13, wherein the generating of the second depth map comprises: estimating a depth map of the residual region based on the estimated relationship between the primary region and the residual region, and the first depth map; and generating the second depth map based on a depth map of the residual region and the first depth map.\\n15. the method of claim 12, wherein the estimating of the relationship between a primary region of the second image is performed using a neural network model.\\n16. the method of claim 12, further comprising: performing a pre-processing operation on the second depth map; and generating a third depth map of the residual image by fusing the second depth map on which the pre-processing operation is performed and the first depth map.\\n17. the method of claim 16, wherein the performing of the pre-processing operation comprises performing a tone mapping operation between a depth map of the primary region and a depth map of the residual region based on the second depth map.\\n18. an operating method for an electronic apparatus, the electronic apparatus including; a first camera module providing a first image of an object using a first field of view and a second camera module providing a second image of the object using second field of view wider than the first field of view, and a processor generating a depth map of the second image based on a primary region of the second image and a residual region of the second image, the operating method comprising: generating a first depth map of the primary region by estimating a relationship between the first image and the second image; estimating a relationship between the primary region and the residual region based on the first image and the second image; generating a second depth map of the second image by estimating a depth map of the second region based on the estimated relationship between the primary region and the residual region; and generating a depth map of the second image by fusing the first depth map and the second depth map.\\n19. the operation method of claim 18, further comprising: executing an application that applies an image effect to the second image based on a depth map of the residual image.\\n20. the operation method of claim 19, wherein the application applies at least one image effect of auto-focusing, out-focusing, fore/background separation, face recognition, object detection within a frame, and augmented reality to the second image based on a depth map of the second image.1. a payment method based on a face recognition, comprising: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including: determining whether an angle of rotation in each preset direction is less than an angle threshold, wherein the head posture information includes the angle of rotation in each preset direction; determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold, wherein the gaze information includes the probability value that a user gazes at a payment screen; and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold, determining that the target user has a willingness to pay; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition.\\n2. the method as claimed in claim 1, wherein the completing a payment operation based on the face recognition comprises: triggering and performing a payment initiating operation to acquire second face image information based on the face recognition; determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay; and in response to determining that the second characteristic information indicates that the user has a willingness to pay, triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user.\\n3. the method as claimed in claim 2, wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises: determining whether a current user corresponding to the second face image information is consistent with the target user; and in response to determining that the current user is consistent with the target user, determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information.\\n4. the method as claimed in claim 1, wherein the extracting first characteristic information from the first face image information comprises: determining the head posture information of the target user using a head posture recognition model based on the first face image information; and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information.\\n5. the method as claimed in claim 4, wherein the head posture recognition model is obtained through training by: acquiring a first sample data set, wherein the first sample data set includes a plurality of pieces of first sample data, and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information; determining mean image data and variance image data of a plurality of sample face images; for each of the plurality of pieces of first sample data, preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image; setting the preprocessed sample face image and the corresponding head posture information as a first model training sample; and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model.\\n6. the method as claimed in claim 4, wherein the gaze information recognition model is obtained through training by: acquiring a second sample data set, wherein the second sample data set includes a plurality of pieces of second sample data, and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information; determining mean image data and variance image data of a plurality of sample eye images; for each of the plurality of pieces of second sample data, preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image; setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample; and performing training using a machine learning method and based on a plurality of second model training samples to obtain the gaze information recognition model.\\n7. the method as claimed in claim 1, wherein the angle of rotation in each preset direction comprises a pitch angle, a yaw angle, and a roll angle, wherein the pitch angle refers to an angle of rotation around a x-axis, the yaw angle refers to an angle of rotation around a y-axis, and the roll angle refers to an angle of rotation around a z-axis.\\n8. a payment device based on a face recognition, comprising: a processor, and a non-transitory computer-readable storage medium storing instructions executable by the processor to cause the device to perform operations comprising: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including: determining whether an angle of rotation in each preset direction is less than an angle threshold, wherein the head posture information includes the angle of rotation in each preset direction; determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold, wherein the gaze information includes the probability value that a user gazes at a payment screen; and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold, determining that the target user has a willingness to pay; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition.\\n9. the device as claimed in claim 8, wherein the completing a payment operation based on the face recognition comprises: triggering and performing a payment initiating operation to acquire second face image information based on the face recognition; determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay; and in response to determining that the second characteristic information indicates that the user has a willingness to pay, triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user.\\n10. the device as claimed in claim 9, wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises: determining whether a current user corresponding to the second face image information is consistent with the target user; and in response to determining that the current user is consistent with the target user, determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information.\\n11. the device as claimed in claim 8, wherein the extracting first characteristic information from the first face image information comprises: determining the head posture information of the target user using a head posture recognition model based on the first face image information; and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information.\\n12. the device as claimed in claim 11, wherein the head posture recognition model is obtained through training by: acquiring a first sample data set, wherein the first sample data set includes a plurality of pieces of first sample data, and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information; determining mean image data and variance image data of a plurality of sample face images; for each of the plurality of pieces of first sample data, preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image; setting the preprocessed sample face image and the corresponding head posture information as a first model training sample; and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model.\\n13. the device as claimed in claim 11, wherein the gaze information recognition model is obtained through training by: acquiring a second sample data set, wherein the second sample data set includes a plurality of pieces of second sample data, and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information; determining mean image data and variance image data of a plurality of sample eye images; for each of the plurality of pieces of second sample data, preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image; setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample; and performing training using a machine learning method and on a plurality of second model training samples to obtain the gaze information recognition model.\\n14. the device as claimed in claim 11, wherein the angle of rotation in each preset direction comprises a pitch angle, a yaw angle, and a roll angle, wherein the pitch angle refers to an angle of rotation around a x-axis, the yaw angle refers to an angle of rotation around a y-axis, and the roll angle refers to an angle of rotation around a z-axis.\\n15. a non-transitory computer-readable storage medium for a payment based on a face recognition, configured with instructions executable by one or more processors to cause the one or more processors to perform operations comprising: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user, and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including: determining whether an angle of rotation in each preset direction is less than an angle threshold, wherein the head posture information includes the angle of rotation in each preset direction; determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold, wherein the gaze information includes the probability value that a user gazes at a payment screen; and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold, determining that the target user has a willingness to pay; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition.\\n16. the storage medium as claimed in claim 15, wherein the completing a payment operation based on the face recognition comprises: triggering and performing a payment initiating operation to acquire second face image information based on the face recognition; determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay; and in response to determining that the second characteristic information indicates that the user has a willingness to pay, triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user.\\n17. the storage medium as claimed in claim 16, wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises: determining whether a current user corresponding to the second face image information is consistent with the target user; and in response to determining that the current user is consistent with the target user, determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information.\\n18. the storage medium as claimed in claim 15, wherein the extracting first characteristic information from the first face image information comprises: determining the head posture information of the target user using a head posture recognition model based on the first face image information; and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information.\\n19. the storage medium as claimed in claim 18, wherein the head posture recognition model is obtained through training by: acquiring a first sample data set, wherein the first sample data set includes a plurality of pieces of first sample data, and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information; determining mean image data and variance image data of a plurality of sample face images; for each of the plurality of pieces of first sample data, preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image; setting the preprocessed sample face image and the corresponding head posture information as a first model training sample; and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model; and wherein the gaze information recognition model is obtained through training by: acquiring a second sample data set, wherein the second sample data set includes a plurality of pieces of second sample data, and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information; determining mean image data and variance image data of a plurality of sample eye images; for each of the plurality of pieces of second sample data, preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image; setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample; and performing training using a machine learning method and based on a plurality of second model training samples to obtain the gaze information recognition model.\\n20. the storage medium as claimed in claim 18, wherein the angle of rotation in each preset direction comprises a pitch angle, a yaw angle, and a roll angle, wherein the pitch angle refers to an angle of rotation around a x-axis, the yaw angle refers to an angle of rotation around a y-axis, and the roll angle refers to an angle of rotation around a z-axis.1. a method comprising: detecting, by a motion detection module, a motion by a subject within a predetermined area of view; assigning a unique session identification number to the subject detected within a predetermined area of view; detecting a facial area of the subject detected within a predetermined area of view; generating an image of the facial area of the subject; assessing a quality of the image of the facial area of the subject; determining an identity of the subject based on the image of the facial area of the subject; identifying an intent of the subject; and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject.\\n2. the method of claim 1, further comprising: determining one or more additional subjects within the predetermined area of view; and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view.\\n3. the method of claim 1, wherein the assessing a quality of the image of the facial area of the subject comprises: assessing whether the quality of the image of the facial area of the object equates predetermined metric of quality; and upon determining that the quality of the image of the facial area of the object is inferior to the predetermined metric of quality, discarding the image of the facial area of the subject and generating a second image of the facial area of the subject.\\n4. the method of claim 1, further comprising: detecting whether the facial area of the subject is photographic image; and upon detecting that the facial area of the subject is a photographic image, generating a warning and restrict access to the point of entry.\\n5. the method of claim 1, further comprising: conducing an incremental training of the image of the facial area of the subject.\\n6. the method of claim 5, wherein conducing an incremental training of the image of the facial area of the subject comprises: capturing a first image of the facial area having facial landmarks; converting the first image of the facial area into a first numeric vector; capturing a second image of the facial area having facial landmarks; converting the second image of the facial area into a second numeric vector; calculating a weighted mean of the first numeric vector and the second numeric vector, wherein the weighted mean represents a change in a facial area; and storing the weighted mean in the database.\\n7. the method of claim 1, wherein determining an identity of the subject based on the image of the facial area of the subject comprises: comparing the image of the facial area of the subject with a plurality of images stored in a database; and authenticating the subject.\\n8. the method of claim 1, wherein identifying an intent of the subject comprises: upon detecting the facial area in a bounding box, commencing authentication of the subject; calculating a directional vector of a face of the subject; determine an intent of the subject to gain access to the point of entry based on the directional vector of the face of the subject; granting the access to the point of entry based on authentication of the subject and based on determining the intent of the subject.\\n9. a non-transitory computer readable medium having program instructions stored thereon, that in response to execution by a computing device cause the computing device to perform operations comprising: detecting a motion by a subject within a predetermined area of view; assigning a unique session identification number to the subject detected within a predetermined area of view; detecting a facial area of the subject detected within a predetermined area of view; generating an image of the facial area of the subject; assessing a quality of the image of the facial area of the subject; determining an identity of the subject based on the image of the facial area of the subject; identifying an intent of the subject; and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject.\\n10. the non-transitory computer readable medium of claim 9, further comprising: determining one or more additional subjects within the predetermined area of view; and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view.\\n11. the non-transitory computer readable medium of claim 9, wherein the assessing a quality of the image of the facial area of the subject comprises: assessing whether the quality of the image of the facial area of the object equates predetermined metric of quality; and upon determining that the quality of the image of the facial area of the object is inferior to the predetermined metric of quality, discarding the image of the facial area of the subject and generating a second image of the facial area of the subject.\\n12. the non-transitory computer readable medium of claim 9, further comprising: detecting whether the facial area of the subject is photographic image; and upon detecting that the facial area of the subject is a photographic image, generating a warning and restrict access to the access point.\\n13. the non-transitory computer readable medium of claim 9, further comprising: conducing an incremental training of the image of the facial area of the subject.\\n14. the non-transitory computer readable medium of claim 13, wherein conducing an incremental training of the image of the facial area of the subject comprises: capturing a first image of the facial area having facial landmarks; converting the first image of the facial area into a first numeric vector; capturing a second image of the facial area having facial landmarks; converting the second image of the facial area into a second numeric vector; calculating a weighted mean of the first numeric vector and the second numeric vector, wherein the weighted mean represents a change in a facial area; and storing the weighted mean in the database.\\n15. an apparatus for face recognition comprising: a processor; and a memory to store computer program instructions, the computer program instructions when executed on the processor cause the processor to perform operations comprising: detecting a motion by a subject within a predetermined area of view; assigning a unique session identification number to the subject detected within a predetermined area of view; detecting a facial area of the subject detected within a predetermined area of view; generating an image of the facial area of the subject; assessing a quality of the image of the facial area of the subject; determining an identity of the subject based on the image of the facial area of the subject; identifying an intent of the subject; and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject.\\n16. the apparatus of claim 15, further comprising: determining one or more additional subjects within the predetermined area of view; and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view.\\n17. the apparatus of claim 15, wherein the assessing a quality of the image of the facial area of the subject comprises: assessing whether the quality of the image of the facial area of the object equates predetermined metric of quality; and upon determining that the quality of the image of the facial area of the object is inferior to the predetermined metric of quality, discarding the image of the facial area of the subject and generating a second image of the facial area of the subject.\\n18. the apparatus of claim 15, further comprising: detecting whether the facial area of the subject is photographic image; and upon detecting that the facial area of the subject is a photographic image, generating a warning and restrict access to the access point.\\n19. the apparatus of claim 15, further comprising: conducing an incremental training of the image of the facial area of the subject.\\n20. the apparatus of claim 15, wherein conducing an incremental training of the image of the facial area of the subject comprises: capturing a first image of the facial area having facial landmarks; converting the first image of the facial area into a first numeric vector; capturing a second image of the facial area having facial landmarks; converting the second image of the facial area into a second numeric vector; calculating a weighted mean of the first numeric vector and the second numeric vector, wherein the weighted mean represents a change in a facial area; and storing the weighted mean in the database.1. a robot, comprising: a body configured to rotate and to tilt; a camera coupled to the body and configured to rotate and tilt according to the rotate and the tilt of the body, wherein the camera is configured to acquire a video of a space; a face recognition unit configured to recognize respective faces of one or more persons in the video; a tracking unit configured to track motion of each of the recognized faces of the one or more persons; and a controller configured to: calculate a respective size of each of the faces of the one or more persons; select a first person, from among the one or more persons, based on the calculated sizes of the faces; and control at least one of a direction of the rotation of the camera, an angle of the tilt of the camera and a focal distance of the camera, based on the tracked motion of the recognized face of the first person.\\n2. the robot of claim 1, wherein the controller is configured to: control the direction of the rotation of the camera and the angle of the tilt of the camera to achieve an particular orientation of the camera relative to the face of the first person; and control a focal distance of the camera by comparing respective sizes of the face of the first person before and after motion of the first person.\\n3. the robot of claim 2, wherein the particular orientation occurs when the camera faces a general direction of the face of the first person.\\n4. the robot of claim 1, wherein the controller is configured to: normalize sizes of the faces of the one or more persons based on an interocular distance; and select the first person based on the normalized sizes of the faces of the one or more persons.\\n5. the robot of claim 1, wherein the controller is configured to: select a person having a largest face size, from among the one or more persons, as the first person.\\n6. the robot of claim 1, further comprising: a microphone configured to receive a spoken audio that is present in the space; wherein the controller is further configured to select the first person further based on the received spoken audio.\\n7. the robot of claim 6, wherein the controller is further configured to: control gain of the microphone by comparing respective sizes of the face of the first person before and after motion of the first person.\\n8. the robot of claim 6, wherein the controller is configured to: calculate a position from which the spoken audio is provided; and select the first person further based on whether the one or more persons are in the position from which the voice signal is provided.\\n9. the robot of claim 8, wherein the controller is configured to: select a second person as the first person, from among the one or more persons, when the second person is located in the position from which the spoken audio is provided.\\n10. the robot of claim 8, wherein the controller is configured to: select a second person having a largest face size as the first person, from among the one or more persons, when none of the one or more persons is located in the position from which the spoken audio is provided.\\n11. the robot of claim 8, wherein the controller is configured to: select a second person having a largest face size as the first person, from among the one or more persons, when a plurality of persons from among the one or more persons are located in the position from which the spoken audio is provided.\\n12. the robot of claim 1, further comprising: a speaker, wherein the controller is configured to: control volume of the speaker by comparing respective sizes of the face of the first person before and after motion of the first person.\\n13. the robot of claim 1, wherein the body is further configured to rotate in a lateral direction, and to tilt in an vertical direction.\\n14. an electronic device, comprising: a camera coupled to the body and configured to rotate and to tilt, wherein the camera is configured to acquire a video of a space within which one or more persons are positioned; and a processor configured to: recognize respective faces of the one or more persons in the video; track motion of each of the recognized faces of the one or more persons; calculate a respective size of each of the faces of the one or more persons; select a first person, from among the one or more persons, based on the calculated sizes of the faces; and control at least one of a direction of the rotation of the camera, an angle of the tilt of the camera and a focal distance of the camera, based on the tracked motion of the recognized face of the first person.\\n15. a method, comprising: acquiring, by a camera, a video of a space within which one or more persons are positioned; recognizing respective faces of the one or more persons in the video; tracking motion of each of the recognized faces of the one or more persons; calculating a respective size of each of the faces of the one or more persons; selecting a first person, from among the one or more persons, based on the calculated sizes of the faces; and controlling at least one of a direction of rotation of the camera, an angle of tilt of the camera and a focal distance of the camera, based on the tracked motion of the recognized face of the first person.1. a method of inferring topics from a multimodal file, the method comprising: receiving a multimodal file;\\nextracting a set of entities from the multimodal file;\\nlinking the set of entities to produce a set of linked entities;\\nobtaining reference information for the set of entities;\\nbased at least on the reference information, generating a graph of the set of linked entities, the graph comprising nodes and edges;\\nbased at least on the nodes and edges of the graph, determining clusters in the graph;\\nbased at least on the clusters in the graph, identifying topic candidates;\\nextracting features from the clusters in the graph;\\nbased at least on the extracted features, selecting at least one topicid from among the topic candidates to represent at least one cluster; and\\nindexing the multimodal file with the at least one topicid.\\n2. the method of claim 1 wherein the multimodal file comprises a video portion and an audio portion and wherein extracting a set of entities from the multimodal file comprises:\\ndetecting objects in the video portion of the multimodal file; and\\ndetecting text in the audio portion of the multimodal file.\\n3. the method of claim 2 wherein detecting objects comprises performing face recognition.\\n4. the method of claim 2 wherein detecting text comprises performing a speech to text process.\\n5. the method of claim 4 further comprising:\\nidentifying a language used in the audio portion of the multimodal file, and wherein performing a speech to text process comprises performing a speech to text process in the identified language.\\n6. the method of claim 4 further comprising:\\ntranslating the detected text.\\n7. the method of claim 1 further comprising:\\ndetermining significant clusters and insignificant clusters in the determined clusters, and\\nwherein extracting features from the clusters in the graph comprises extracting features from the significant clusters in the graph.\\n8. the method of claim 1 wherein extracting features from the clusters in the graph comprises at least one process selected from the list consisting of:\\ndetermining a graph diameter and determining a jaccard coefficient.\\n9. the method of claim 1 wherein selecting at least one topicid to represent at least one cluster comprises:\\nbased at least on the extracted features, mapping topic candidates into a probability interval; and\\nbased at least on the mapping, ranking topic candidates within the at least one cluster, and\\nselecting the at least one topicid based at least on the ranking.\\n10. the method of claim 1 further comprising:\\ntranslating the at least one topicid, and\\nwherein indexing the multimodal file with the at least one topicid comprises indexing the multimodal file with the at least one translated topicid.\\n11. a system for inferring topics from a multimodal file, the system comprising: an entity extraction component comprising an object detection component and a speech to text component, operative to extract a set of entities from a multimodal file comprising a video portion and an audio portion;\\nan entity linking component operative to link the extracted set of entities to produce a set of linked entities;\\nan information retrieval component operative to obtain reference information for the extracted set of entities;\\na graphing and analysis component operative to:\\ngenerate a graph of the set of linked entities, the graph comprising nodes and edges;\\nbased at least on the nodes and edges of the graph, determine clusters in the graph;\\nbased at least on the clusters in the graph, identify topic candidates; and extract features from the clusters in the graph;\\na topicid selection component operative to:\\nrank the topic candidates within at least one cluster; and\\nbased at least on the ranking, select at least one topicid from among the topic candidates to represent at least one cluster; and a video indexer operative to index the multimodal file with the at least one topicid.\\n12. the system of claim 11 wherein the object detection component is operative to perform face recognition.\\n13. the system of claim 11 wherein the speech to text component is operative to extract entity information in at least two different languages.\\n14. one or more computer storage devices having computer-executable instructions stored thereon for inferring topics from a multimodal file, which, on execution by a computer, cause the computer to perform operations comprising:\\nreceiving a multimodal file comprising a video portion and an audio portion; extracting a set of entities from the multimodal file, wherein extracting a set of entities from the multimodal file comprises:\\ndetecting objects in the video portion of the multimodal file with face recognition;\\ndetecting text in the audio portion of the multimodal file with a speech to text process; and\\ndisambiguating among a set of detected entity names;\\nlinking the set of entities to produce a set of linked entities;\\nobtaining reference information for the set of entities;\\nbased at least on the reference information, generating a graph of the set of linked entities, the graph comprising nodes and edges;\\nbased at least on the nodes and edges of the graph, determining clusters in the graph;\\ndetermining significant clusters and insignificant clusters in the determined clusters;\\nbased at least on the significant clusters in the graph, identifying topic candidates; extracting features from the significant clusters in the graph;\\nbased at least on the extracted features, mapping the topic candidates into a probability interval;\\nbased at least on the mapping, ranking the topic candidates within at least one significant cluster,\\nbased on the ranking, selecting at least one topicid from among the topic candidates to represent the at least one significant cluster; and\\nindexing the multimodal file with the at least one topicid.\\n15. the one or more computer storage devices of claim 14 wherein the operations further comprise:\\nidentifying a language used in the audio portion of the multimodal file, and detecting text in the audio portion of the multimodal file with a speech to text process comprises performing a speech to text process in the identified language.权利要求\\n1、 一种人脸识别方法,其特征在于,包括:\\n通过第一摄像头获取第一人脸图像;\\n提取所述第一人脸图像的第一人脸特征;\\n将所述第一人脸特征与预先存储的第二人脸特征进行对比,获得参考相似度,所述第 二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得,所述第二摄像头与所述第 一摄像头属于不同类型的摄像头;\\n根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否对应相同人。\\n2、 根据权利要求 1所述的方法,其特征在于,\\n所述第一摄像头为热成像摄像头,所述第二摄像头为可见光摄像头;\\n或者,所述第一摄像头为可见光摄像头,所述第一摄像头为热成像摄像头。\\n3、 根据权利要求 1或 2所述的方法,其特征在于,所述根据所述参考相似度确定所 述第一人脸特征与所述第二人脸特征是否对应相同人,包括:\\n根据所述参考相似度、 参考误报率以及相似度阈值确定所述第一人脸特征与所述第二 人脸特征是否对应相同人;其中,不同的误报率对应不同的相似度阈值。\\n4、 根据权利要求 1或 2所述的方法,其特征在于,所述根据所述参考相似度确定所 述第一人脸特征与所述第二人脸特征是否对应相同人,包括:\\n根据所述参考相似度以及阈值信息确定归一化后的参考相似度;\\n根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否对 应相同人。\\n5、 根据权利要求 1-4任一项所述的方法,其特征在于,所述提取所述第一人脸图像的 第_人脸特征,包括:\\n将所述第一人脸图像输入预先训练完成的神经网络,通过所述神经网络输出所述第一 人脸图像的第一人脸特征;其中,所述神经网络基于第一类型图像样本和第二类型图像样 本训练得到,所述第一类型图像样本和所述第二类型图像样本由不同类型的摄像头拍摄得 到,且所述第一类型图像样本和所述第二类型图像样本中包括人脸。\\n6、 根据权利要求 5 所述的方法,其特征在于,所述神经网络基于所述第一类型图像 样本、 所述第二类型图像样本和混合类型图像样本训练得到,所述混合类型图像样本由所 述第一类型图像样本和所述第二类型图像样本配对而得。\\n1、 根据权利要求 1-6任一项所述的方法,其特征在于,所述第一摄像头包括车载摄像 头,所述通过第一摄像头获取第一人脸图像,包括:\\n通过所述车载摄像头获取所述第一人脸图像,所述第一人脸图像包括车辆的用车人的 人脸图像。\\n8、 根据权利要求 7 所述的方法,其特征在于,所述用车人包括驾驶所述车辆的人、 乘坐所述车辆的人、 对所述车辆进行修理的人、 给所述车辆加油的人以及控制所述车辆的 人中的一项或多项。\\n9、 根据权利要求 7 所述的方法,其特征在于,所述用车人包括驾驶所述车辆的人, 所述通过所述车载摄像头获取所述第一人脸图像,包括: 在接收到触发指令的情况下,通过所述车载摄像头获取所述第一人脸图像; 或者,在所述车辆运行时,通过所述车载摄像头获取所述第一人脸图像;\\n或者,在所述车辆的运行速度达到参考速度的情况下,通过所述车载摄像头获取所述 第一人脸图像。\\n10、 根据权利要求 7-9任一项所述的方法,其特征在于,所述第二人脸图像为对所述 用车人进行人脸注册的图像,所述将所述第一人脸特征与预先存储的第二人脸特征进行对 比之前,所述方法还包括:\\n通过所述第二摄像头获取所述第二人脸图像;\\n提取所述第二人脸图像的第二人脸特征;\\n保存所述第二人脸图像的第二人脸特征。\\n11、 一种神经网络训练方法,其特征在于,包括:\\n获取第一类型图像样本和第二类型图像样本,所述第一类型图像样本和所述第二类型 图像样本由不同类型的摄像头拍摄得到,且所述第一类型图像样本和所述第二类型图像样 本中包括人脸;\\n根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。\\n12、 根据权利要求 11所述的方法,其特征在于,所述根据所述第一类型图像样本和所 述第二类型图像样本训练神经网络,包括:\\n将所述第一类型图像样本和所述第二类型图像样本配对,得到所述第一类型图像样本 和所述第二类型图像样本的混合类型图像样本;\\n根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类型图像样本,训练 所述神经网络。\\n13、 根据权利要求 12 所述的方法,其特征在于,所述根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类型图像样本,训练所述神经网络,包括:\\n通过所述神经网络获取所述第一类型图像样本的人脸预测结果、 所述第二类型图像样 本的人脸预测结果和所述混合类型图像样本的人脸预测结果;\\n根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、 所述第二类型图 像样本的人脸预测结果和人脸标注结果之间的差异、 以及所述混合类型图像样本的人脸预 测结果和人脸标注结果的差异,训练所述神经网络。\\n14、 根据权利要求 13 所述的方法,其特征在于,所述神经网络中包括第一分类器、 第二分类器和混合分类器,所述通过所述神经网络获取所述第一类型图像样本的人脸预测 结果、 所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果, 包括:\\n将所述第一类型图像样本的人脸特征输入至所述第一分类器中,得到所述第一类型图 像样本的人脸预测结果;\\n将所述第二类型图像样本的人脸特征输入至所述第二分类器中,得到所述第二类型图 像样本的人脸预测结果;\\n将所述混合类型图像样本的人脸特征输入至所述混合分类器中,得到所述混合类型图 像样本的人脸预测结果。 15、 根据权利要求 14所述的方法,其特征在于,所述方法还包括:\\n在训练完成的所述神经网络中去除所述第一分类器、 所述第二分类器和所述混合分类 器,得到用于进行人脸识别的神经网络。\\n16、 一种人脸识别装置,其特征在于,包括:\\n第一获取单元,用于通过第一摄像头获取第一人脸图像;\\n第一提取单元,用于提取所述第一人脸图像的第一人脸特征;\\n对比单元,用于将所述第一人脸特征与预先存储的第二人脸特征进行对比,获得参考 相似度,所述第二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得,所述第二 摄像头与所述第一摄像头属于不同类型的摄像头;\\n确定单元,用于根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否 对应相同人。\\n17、 根据权利要求 16所述的装置,其特征在于,\\n所述第一摄像头为热成像摄像头,所述第二摄像头为可见光摄像头;\\n或者,所述第一摄像头为可见光摄像头,所述第一摄像头为热成像摄像头。\\n18、 根据权利要求 16或 17所述的装置,其特征在于,\\n所述确定单元,具体用于根据所述参考相似度、 参考误报率以及相似度阈值确定所述 第一人脸特征与所述第二人脸特征是否对应相同人;其中,不同的误报率对应不同的相似 度阈值。\\n19、 根据权利要求 16或 17所述的装置,其特征在于,\\n所述确定单元,具体用于根据所述参考相似度以及阈值信息确定归一化后的参考相似 度;以及根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否 对应相同人。\\n20、 根据权利要求 16-19任_项所述的装置,其特征在于,\\n所述第一提取单元,具体用于将所述第一人脸图像输入预先训练完成的神经网络,通 过所述神经网络输出所述第一人脸图像的第一人脸特征;其中,所述神经网络基于第一类 型图像样本和第二类型图像样本训练得到,所述第一类型图像样本和所述第二类型图像样 本由不同类型的摄像头拍摄得到,且所述第一类型图像样本和所述第二类型图像样本中包 括人脸。\\n21、 根据权利要求 20 所述的装置,其特征在于,所述神经网络基于所述第一类型图 像样本、 所述第二类型图像样本和混合类型图像样本训练得到,所述混合类型图像样本由 所述第一类型图像样本和所述第二类型图像样本配对而得。\\n22、 根据权利要求 16-21任一项所述的装置,其特征在于,所述第一摄像头包括车载 摄像头,\\n所述第一获取单元,具体用于通过所述车载摄像头获取所述第一人脸图像,所述第一 人脸图像包括车辆的用车人的人脸图像。\\n23、 根据权利要求 22所述的装置,其特征在于,所述用车人包括驾驶所述车辆的人、 乘坐所述车辆的人、 对所述车辆进行修理的人、 给所述车辆加油的人以及控制所述车辆的 人中的一项或多项。 24、 根据权利要求 22所述的装置,其特征在于,所述用车人包括驾驶所述车辆的人, 所述第一获取单元,具体用于在接收到触发指令的情况下,通过所述车载摄像头获取所述 第一人脸图像;\\n或者,所述第一获取单元,具体用于在所述车辆运行时,通过所述车载摄像头获取所 述第 _人脸图像;\\n或者,所述第一获取单元,具体用于在所述车辆的运行速度达到参考速度的情况下, 通过所述车载摄像头获取所述第一人脸图像。\\n25、 根据权利要求 22-24任一项所述的装置,其特征在于,所述第二人脸图像为对所 述用车人进行人脸注册的图像,所述装置还包括:\\n第二获取单元,用于通过所述第二摄像头获取所述第二人脸图像;\\n第二提取单元,用于提取所述第二人脸图像的第二人脸特征;\\n保存单元,用于保存所述第二人脸图像的第二人脸特征。\\n26、 一种神经网络训练装置,其特征在于,包括:\\n获取单元,用于获取第一类型图像样本和第二类型图像样本,所述第一类型图像样本 和所述第二类型图像样本由不同类型的摄像头拍摄得到,且所述第一类型图像样本和所述 第二类型图像样本中包括人脸;\\n训练单元,用于根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。\\n27、 根据权利要求 26所述的装置,其特征在于,所述训练单元包括:\\n配对子单元,用于将所述第一类型图像样本和所述第二类型图像样本配对,得到所述 第一类型图像样本和所述第二类型图像样本的混合类型图像样本;\\n训练子单元,用于根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类 型图像样本,训练所述神经网络。\\n28、 根据权利要求 27所述的装置,其特征在于,\\n所述训练子单元,具体用于通过所述神经网络获取所述第一类型图像样本的人脸预测 结果、 所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果; 以及根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、 所述第二类型图 像样本的人脸预测结果和人脸标注结果之间的差异、 以及所述混合类型图像样本的人脸预 测结果和人脸标注结果的差异,训练所述神经网络。\\n29、 根据权利要求 28 所述的装置,其特征在于,所述神经网络中包括第一分类器、 第二分类器和混合分类器,\\n所述训练子单元,具体用于将所述第一类型图像样本的人脸特征输入至所述第一分类 器中,得到所述第一类型图像样本的人脸预测结果;以及将所述第二类型图像样本的人脸 特征输入至所述第二分类器中,得到所述第二类型图像样本的人脸预测结果;以及将所述 混合类型图像样本的人脸特征输入至所述混合分类器中,得到所述混合类型图像样本的人 脸预测结果。\\n30、 根据权利要求 29所述的装置,其特征在于,所述装置还包括:\\n神经网络应用单元,用于在训练完成的所述神经网络中去除所述第一分类器、 所述第 二分类器和所述混合分类器,得到用于进行人脸识别的神经网络。 31、 一种电子设备,其特征在于,包括处理器和存储器,所述处理器和所述存储器耦 合;其中,所述存储器用于存储程序指令,所述程序指令被所述处理器执行时,使所述处 理器执行权利要求 1-10任一项所述的方法;和/或,使所述处理器执行权利要求 11-15任一 项所述的方法。\\n32、 一种计算机可读存储介质,其特征在于,所述计算机可读存储介质中存储有计算 机程序,所述计算机程序包括程序指令,所述程序指令当被处理器执行时,使所述处理器 执行权利要求 1-10任一项所述的方法;和/或,使所述处理器执行权利要求 11-15任一项所 述的方法。1. a system for alerting on vision impairment, said system comprising a processing unit configured and operable for receiving scene data being indicative of a scene of at least one consumer in an environment, identifying in the scene data a certain consumer, identifying an event being indicative of a behavioral compensation for vision impairment, and, upon identification of such an event, sending a notification relating to the vision impairment.\\n2. the system of claim 1, further comprising at least one sensing unit configured and operable for detecting the scene data.\\n3. the system of claim 2, wherein said at least one sensing unit comprises at least one of: at least one imaging unit configured and operable for capturing at least one image of at least a portion of a consumer\\'s body, at least one motion detector configured and operable for detecting consumer data being indicative of a motion of a consumer, or at least one eye tracker configured and operable for tracking eye motion of a consumer. 4. the system of claim 3, wherein the at least one imaging unit comprises a plurality of cameras placed at different heights.\\n5. the system of any one of claims 2 to 4, wherein said sensing unit is accommodated in an optical or digital eyewear frame display.\\n6. the system of any one of claims 1 to 5, wherein said processing unit is configured and operable for identifying a consumer\\'s condition, said consumer\\'s condition comprising consumer data being indicative of the consumer\\'s position and location relative to at least one object in the consumer\\'s environment; said consumer data comprises at least one of a consumer\\'s face, eyewear, posture, position, sound or motion.\\n7. the system of any one of claims 1 to 6, wherein said event comprises at least one position and orientation of head increase or decrease of viewing distance between the consumer and viewed object and changing the position of eyeglasses worn by the consumer.\\n8. the system of any one of claims 1 to 7, wherein said event is identified by identifying images having an image feature being indicative of behavioral compensation, performing a bruckner test, performing a hirschberg test, and measuring blink count/ frequency.\\n9. the system of claim 8, wherein the image feature being indicative of behavioral compensation comprises squinting, head orientation, certain distances between an object and consumer\\'s eyes, certain position of eyeglasses on the consumer\\'s face, strabismus, cataracts, and reflections from the eye.\\n10. the system of any one of claims 1 to 9, wherein the notification includes at least one of the data indicative of the identified event, data indicative of the identified consumer, ophthalmologic recommendations based on the identified event, or lack of events, or an appointment for a vision test.\\n11. the system of any one of claims 1 to 10, wherein said processing unit comprises a memory for storing at least one of a reference data indicative of behavioral compensation for vision impairment, data indicative of the notification, or data indicative of a follow-up of the notification.\\n12. the system of claim 11 , wherein said processing unit is configured for at least one of identifying the event upon comparison between the detected data and the reference data or determining a probability for a vision impairment of the consumer based on the comparison.\\n13. the system of any one of claims 1 to 12, wherein said processing unit comprises a communication interface being configured for sending the notification to at least one of the identified consumer or a third party.\\n14. the system of any one of claims 1 to 13, wherein said processing unit is configured for providing a frame recommendation.\\n15. the system of any one of claims 11 to 14, wherein said memory is configured for storing a database including a multiplicity of data sets related to a plurality of spectacle frame models and sizes.\\n16. the system according to claim 14 or 15, wherein said processing unit is configured and operable to correlate between frames parameters and ophthalmic prescriptions.\\n17. the system according to any of claims 14 to 16, wherein said processing unit is configured and operable to correlate between frames parameters and facial features.\\n18. the system according to any of claims 14 to 17, wherein said processing unit is configured and operable to correlate between frames parameters and eyewear preferences.\\n19. the system according to any of claims 14 to 18, comprising a server and at least one computer entity linked to the server via a network, wherein said network is configured to receive and respond to requests sent across the network; transmitting one or more modules of computer executable program instructions and displayable data to the network connected user computer platform in response to a request, wherein said modules include modules configured to: receive and transmit image information, transmitting a frame recommendation and an optical lens option recommendation based on received image information, for display by the network connected user computer platform.\\n20. a computer program instructions stored in the local storage that, when executed by a processing unit, cause the processing unit to: receive data being indicative of a scene of at least one consumer in an environment, identify in the data a certain consumer, identify an event being indicative of a behavioral compensation for vision impairment, and, upon identification of such an event, send a notification relating to the vision impairment.\\n21. a computer program product stored on a tangible computer readable medium, comprising: a library of software modules which cause a computer executing them to prompt for information pertinent to at least one of an eyeglasses recommendation and an optical lens option recommendation, to store said information or to display eyewear recommendations .\\n22. the computer program product of claim 21 , wherein said library further comprises a module for frame selection, point of sales and advertising.\\n23. a computer platform for facilitating eye glasses marketing or selection, comprising: a camera; a processor configured to execute computer program instructions to cause the processor to take an image of a consumer, identify in the image a certain consumer, identify an event being indicative of a behavioral compensation for vision impairment, and, upon identification of such an event, sending a notification relating to the vision impairment; local storage for processor executable instructions for carrying out storage of information.\\n24. a method for alerting on vision impairment; said method comprising:\\nidentifying a certain individual in scene data being indicative of a scene of at least one consumer in an environment;\\nidentifying an event being indicative of a behavioral compensation for vision impairment; and\\nupon identification of such an event, sending a notification on the vision impairment. 25. the method of claim 24, further comprising detecting data being indicative of a scene of at least one consumer in a retail environment.\\n26. the method of claim 24, wherein detecting the data being indicative of at least one consumer comprises at least one of capturing at least one image of at least one consumer, detecting data being indicative of a motion of a consumer, or tracking an eye motion of a consumer.\\n27. the method of claim 26, wherein capturing at least one image of at least one consumer comprises continuously recording a scene.\\n28. the method of any one of claims 24 to 27, further comprising identifying, in the data, the consumer\\' s condition including data being indicative of the consumer\\'s position and location relative to the consumer\\'s environment; said data comprising at least one of the consumer\\'s face, posture, position, sound or motion.\\n29. the method of any one of claims 26 to 28, wherein said event comprises at least one of position and orientation of head, increase or decrease of viewing distance between the consumer and viewed object, or changing the position of eyeglasses worn by the consumer.\\n30. the method of any one of claims 26 to 29, wherein identifying of the event comprises identifying images having an image feature being indicative of behavioral compensation, performing a bruckner test, performing a hirschberg test, and measuring blink count/frequency.\\n31. the method of claim 30, wherein the image feature being indicative of behavioral compensation comprises squinting, head orientation, certain distances between an object and a consumer\\'s eyes, certain position of eyeglasses on the consumer\\'s face, strabismus, cataracts, and reflections from the eye.\\n32. the method of any one of claims 27 to 31, wherein identifying in the at least one image a consumer in a retail environment, comprising at least one of receiving data characterizing the retail environment, or performing face recognition.\\n33. the method of any one of claims 24 to 32, wherein sending a notification comprising sending the notification to at least one of the identified consumer or a third party.\\n34. the method of any one of claims 24 to 33, wherein the notification includes at least one of the data indicative of the identified event, data indicative of the identified consumer, ophthalmologic recommendations based on the identified event, or lack of events, and an appointment for a vision test.\\n35. the method of any one of claims 24 to 34, further comprising storing at least one of a reference data indicative of behavioral compensation for vision impairment, data indicative of the notification, or data indicative of a follow-up of the notification.\\n36. the method of claim 35, further comprising identifying the event upon comparison between the detected data and the reference data and determining a probability for a vision impairment of the consumer, based on the comparison.\\n37. a computer program intended to be stored in a memory of a processor unit of a computer system, or in a removable memory medium adapted to cooperate with a reader of the processor unit, comprising instructions for implementing the method according to any of claims 24 to 36.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_ctext = \"\"\n",
    "for claim_text in p2:\n",
    "    lower_ctext += claim_text.lower() #we pick each word and add to a variable, which will contain all the text\n",
    "lower_ctext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be41e13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. an electronic device (10), configured to make a screen (110) to display a plurality of image frames, comprising: an image capturing device (120); a storage device (130), storing a plurality of modules; and a processor (14), coupled to the image capturing device (140) and the storage device (130), configured to execute the modules in the storage device (130) to: configure the screen (110) to display a plurality of marker objects at a plurality of predetermined calibration positions; configure the image capturing device (120) to capture a plurality of first head images when a user is looking at the predetermined calibration positions; (s301) perform a plurality of first face recognition operations on the first head images to obtain a plurality of first face regions corresponding to the predetermined calibration positions; (s302) detect a plurality of first facial landmarks corresponding to the first face regions; (s303) calculate a plurality of rotation reference angles of the user looking at the predetermined calibration positions according to the first facial landmarks; configure the image capturing device (120) to capture a second head image of the user; perform a second face recognition operation on the second head image to obtain a second face region; detect a plurality of second facial landmarks within the second face region; (s304) estimate a head posture angle of the user according to the second facial landmarks; calculate a gaze position of the user on the screen (110) according to the head posture angle, the rotation reference angles, and the predetermined calibration positions; and configure the screen (110) to display a corresponding visual effect according to the gaze position. 2. the electronic device (10) according to claim 1, wherein the gaze position comprises a first coordinate value in a first axial direction and a second coordinate value in a second axial direction. 3. the electronic device (10) according to claim 2, wherein the head posture angles comprise a head pitch angle and a head yaw angle, and the rotation reference angles comprise a first pitch angle, a second pitch angle, a first yaw angle, and a second yaw angle corresponding to the predetermined calibration positions. 4. the electronic device (10) according to claim 3, wherein the processor (140) performs interpolation operation or extrapolation operation according to the first yaw angle, the second yaw angle, a first position corresponding to the first yaw angle among the predetermined calibration positions, a second position corresponding to the second yaw angle among the predetermined calibration positions and the head yaw angle, thereby obtaining the first coordinate value of the gaze position; and the processor (140) performs interpolation operation or extrapolation operation according to the first pitch angle, the second pitch angle, a third position corresponding to the first pitch angle among the predetermined calibration positions, a fourth position corresponding to the second pitch angle among the predetermined calibration positions and the head pitch angle, thereby obtaining the second coordinate value of the gaze position. 5. the electronic device (10) according to claim 1, wherein the processor (140) calculates a plurality of first viewing distances between the user and the screen (110) according to the first facial landmarks; the processor (140) estimates a second viewing distance between the user and the screen (110) according to the second facial landmarks; and the processor (140) adjusts the rotation reference angles or the gaze position according to the second viewing distance and the first viewing distances. 6. the electronic device (10) according to claim 1, wherein the processor (140) maps a plurality of two-dimensional position coordinates of the second facial landmarks under a plane coordinate system to a plurality of three-dimensional position coordinates under a three-dimensional coordinate system; and the processor (140) estimates the head posture angle according to the three-dimensional position coordinates of the second facial landmarks. 7. the electronic device (10) according to claim 1, wherein the second head image comprises a wearable device, and the second facial landmarks do not comprise a plurality of third facial landmarks of the user covered by the wearable device. 8. the electronic device (10) according to claim 1, wherein the second head image comprises a wearable device, and the second facial landmarks comprise one or more simulated landmarks marked by the wearable device. 9. an operating method, adapted for an electronic device (10) comprising an image capturing device (120) and making a screen (110) to display a plurality of image frames, the method comprising: configuring the screen (110) to display a plurality of marker objects at a plurality of predetermined calibration positions; configuring the image capturing device (120) to capture a plurality of first head images when a user is looking at the predetermined calibration positions; (s301) performing a plurality of first face recognition operations on the first head images to obtain a plurality of first face regions corresponding to the predetermined calibration positions; (s302) detecting a plurality of first facial landmarks corresponding to the first face regions; (s303) calculating a plurality of rotation reference angles of the user looking at the predetermined calibration positions according to the first facial landmarks; configuring the image capturing device (120) to capture a second head image of the user; performing a second face recognition operation on the second head image to obtain a second face region; (s304) detecting a plurality of second facial landmarks within the second face region; estimating a head posture angle of the user according to the second facial landmarks; calculating a gaze position of the user on the screen (110) according to the head posture angle, the rotation reference angles, and the predetermined calibration positions; and (s305) configuring the screen (110) to display a corresponding visual effect according to the gaze position. 10. the operation method according to claim 9, wherein the gaze position comprises a first coordinate value in a first axial direction and a second coordinate value in a second axial direction. 11. the operation method according to claim 10, wherein the head posture angles comprise a head pitch angle and a head yaw angle, and the rotation reference angles comprise a first pitch angle, a second pitch angle, a first yaw angle, and a second yaw angle corresponding to the predetermined calibration positions. 12. the operation method according to claim 11, wherein the step of calculating the gaze position of the user on the screen (110) according to the head posture angle, the rotation reference angles and the predetermined calibration positions comprises: performing interpolation operation or extrapolation operation according to the first yaw angle, the second yaw angle, a first position corresponding to the first yaw angle among the predetermined calibration positions, a second position corresponding to the second yaw angle among the predetermined calibration positions and the head yaw angle, thereby obtaining the first coordinate value of the gaze position; and performing interpolation operation or extrapolation operation according to the first pitch angle, the second pitch angle, a third position corresponding to the first pitch angle among the predetermined calibration positions, a fourth position corresponding to the second pitch angle among the predetermined calibration positions and the head pitch angle, thereby obtaining the second coordinate value of the gaze position. 13. the operation method according to claim 9, wherein the method further comprises: calculating a plurality of first viewing distances between the user and the screen (110) according to the first facial landmarks; estimating a second viewing distance between the user and the screen (110) according to the second facial landmarks; and adjusting the rotation reference angles or the gaze position according to the second viewing distance and the first viewing distances. 14. the operation method according to claim 9, wherein the method further comprises: mapping a plurality of two-dimensional position coordinates of the second facial landmarks under a plane coordinate system to a plurality of three-dimensional position coordinates under a three-dimensional coordinate system; and estimating the head posture angle according to the three-dimensional position coordinates of the second facial landmarks. 15. the operation method according to claim 9, wherein the second head image comprises a wearable device, and the second facial landmarks do not comprise a plurality of third facial landmarks of the user covered by the wearable device. 16. the operation method according to claim 9, wherein the second head image comprises a wearable device, and the second facial landmarks comprise one or more simulated landmarks marked by the wearable device.1. a computation method applied to a computing system, wherein the computing system comprises: a control unit, a computation group, and a general storage unit, wherein the control unit comprises: a first memory, a decoding logic, and a controller, wherein the computation group comprises: a group controller and a plurality of computing units; the general storage unit is configured to store data; and the computation method comprises: receiving, by the controller, a first level instruction sequence, and partitioning, by the decoding logic, the first level instruction sequence into a plurality of second level instruction sequences, creating, by the controller, m threads for the plurality of second level instruction sequences, and allocating, by the controller, an independent register as well as configuring an independent addressing function for each thread of the m threads, wherein m is an integer greater than or equal to 1; and obtaining, by the group controller, a plurality of computation types of the plurality of second level instruction sequences, obtaining a corresponding fusion computation manner of the computation types according to the plurality of computation types, and adopting, by the plurality of computing units, the fusion computation manner to call the m threads for performing computations on the plurality of second level instruction sequences to obtain a final result. 2. the method of claim 1, wherein, the obtaining, by the group controller, a plurality of computation types of the plurality of second level instruction sequences, obtaining a corresponding fusion computation manner of the computation types according to the plurality of computation types, and adopting, by the plurality of computing units, the fusion computation manner to call the m threads for performing computations on the plurality of second instruction sequences to obtain a final result: if the computation types represent computation operations of the same type, the group controller calls a combined computation manner in which single instruction multiple data of the same type is in combination with single instruction multiple threads, and uses the m threads to perform the combined computation manner to obtain a final result, which includes: partitioning, by the decoding logic, the m threads into n wraps for allocating to the the plurality of computing units, converting, by the group controller, the plurality of second instruction sequences into a plurality of second control signals and sending the second control signals to the plurality of computing units, calling, by the plurality of computing units, wraps that are allocated to the computing units and the second control signals to fetch corresponding data according to the independent addressing function, performing, by the plurality of computing units, computations on the data to obtain a plurality of intermediate results, and splicing the plurality of intermediate results to obtain a final result. 3. the method of claim 1, wherein, the obtaining, by the group controller, a plurality of computation types of the plurality of second level instruction sequences, obtaining a corresponding fusion computation manner of the computation types according to the plurality of computation types, and adopting, by the plurality of computing units, the fusion computation manner to call the m threads for performing computations on the plurality of second instruction sequences to obtain a final result: if the computation types represent computation operations of different types, the group controller calls simultaneous multi-threading and the m threads to perform computations to obtain a final result, which includes: partitioning, by the decoding logic, the m threads into n wraps, converting the plurality of second instruction sequences into a plurality of second control signals, obtaining, by the group controller, computation types supported by the plurality of computing units, allocating, by the controller, the n wraps and the plurality of second control signals to corresponding computing units that support computation types of the wraps and the second control signals, calling, by the plurality of computing units, wraps that are allocated to the computing units and the second control signals, fetching, by the plurality of computing units, corresponding data, performing, by the plurality of computing units, computations on the data to obtain a plurality of intermediate results, and splicing all the intermediate results to obtain a final result. 4. the method of claim 2 or 3, further comprising: if a wrap a in the plurality of wraps is blocked, adding the wrap a to a waiting queue, and if data of the wrap a are already fetched, adding the wrap a to a preparation queue, wherein the preparation queue is a queue where a wrap to be scheduled for executing is located when a computing resource is idle. 5. the method of claim 1, wherein the first level instruction sequence includes a very long instruction, and the second level instruction sequence includes an instruction sequence. 6. the method of claim 1, wherein the computing system further includes: a tree module, wherein the tree module includes: a root port and a plurality of branch ports, wherein the root port of the tree module is connected to the group controller, and the plurality of branch ports of the tree module are connected to a computing unit of the plurality of computing units respectively; and the tree module is configured to forward data blocks, wraps, or instruction sequences between the group controller and the plurality of computing units. 7. the method of claim 6, wherein the tree module is an n-ary tree, wherein n is an integer greater than or equal to 2. 8. the method of claim 1, wherein the computing system further includes a branch processing circuit, wherein the branch processing circuit is connected between the group controller and the plurality of computing units; and the branch processing circuit is configured to forward data, wraps, or instruction sequences between the group controller and the plurality of computing units. 9. a computing system, comprising: a control unit, a computation group, and a general storage unit, wherein the control unit includes: a first memory, a decoding logic, and a controller, the computation group includes: a group controller and a plurality of computing units; the general storage unit is configured to store data; the controller is configured to receive a first level instruction sequence and control the first memory and the decoding logic; the decoding logic is configured to partition the first level instruction sequence into a plurality of second level instruction sequences; the the controller is further configured to create m threads for the plurality of second level instruction sequences, and allocate an independent register and configure an independent addressing function for each thread of the m threads; m is an integer greater than or equal to 1; and the controller is further configured to convert the plurality of second instruction sequences into a plurality of control signals for sending to the group controller; the group controller is configured to receive the plurality of control signals, obtain a plurality of computational types if the plurality of control signals, divide the m threads into n wraps, and allocate the n wraps and the plurality of control signals to the plurality of computing units according to the plurality of computational types; the plurality of computing units are configured to fetch data from the general storage unit through allocated wraps and control signals, and perform computations to obtain an intermediate result; and the group controller is configured to splice all intermediate results to obtain a final computation result. 10. the computing system of claim 9, wherein the plurality of computing units includes: an addition computing unit, a multiplication computing unit, an activation computing unit, or a dedicated computing unit. 11. the computing system of claim 9, wherein the dedicated computing unit includes: a face recognition computing unit, a graphics computing unit, a fingerprint computing unit, or a neural network computing unit. 12. the computing system of claim 11, wherein the group controller is configured to, if computation types of a plurality of control signals are graphics computations, fingerprint identification, face recognition, or neural network operations, allocate the plurality of control signals to the face recognition computing unit, the graphics computing unit, the fingerprint computing unit, or the neural network computing unit respectively. 13. the computing system of claim 9, wherein the first level instruction sequence includes a very long instruction, and the second level instruction sequence includes an instruction sequence. 14. the computing system of claim 9, further comprising a tree module, wherein the tree module includes: a root port and a plurality of branch ports, wherein the root port of the tree module is connected to the group controller, and the plurality of branch ports of the tree module are connected to a computing unit of the plurality of computing units respectively; and the tree module is configured to forward data blocks, wraps, or instruction sequences between the group controller and the plurality of computing units. 15. the computing system of claim 14, wherein the tree module is an n-ary tree, wherein n is an integer greater than or equal to 2. 16. the computing system of claim 9, wherein the computing system includes a branch processing circuit, the branch processing circuit is connected between the group controller and the plurality of computing units; and the branch processing circuit is configured to forward data, wraps, or instruction sequences between the group controller and the plurality of computing units. 17. a computer program product, comprising a non-instant computer readable storage medium, wherein a computer program is stored in the non-instant computer readable storage medium, and the computer program is capable of causing a computer to perform the method of any of claims 1-8 through operations.1. a method for detecting body information on one or more passengers of a vehicle based on humans\\' status recognition, comprising steps of: (a) if at least one interior image of an interior of the vehicle is acquired, a passenger body information-detecting device performing (i) a process of inputting the interior image into a face recognition network, to thereby allow the face recognition network to detect each of faces of each of the passengers from the interior image, and thus to output multiple pieces of passenger feature information corresponding to each of the detected faces, and (ii) a process of inputting the interior image into a body recognition network, to thereby allow the body recognition network to detect each of bodies of each of the passengers from the interior image, and thus to output body-part length information of each of the detected bodies; and (b) the passenger body information-detecting device performing a process of retrieving specific height mapping information corresponding to specific passenger feature information on a specific passenger from a height mapping table which stores height mapping information representing respective one or more predetermined ratios of one or more segment body portions of each of human groups to each of heights per each of the human groups, a process of acquiring a specific height of the specific passenger from the specific height mapping information by referring to specific body-part length information of the specific passenger, a process of retrieving specific weight mapping information corresponding to the specific passenger feature information from a weight mapping table which stores multiple pieces of weight mapping information representing predetermined correlations between each of the heights and each of weights per each of the human groups, and a process of acquiring a weight of the specific passenger from the specific weight mapping information by referring to the specific height of the specific passenger. 2. the method of claim 1, wherein, at the step of (a), the passenger body information-detecting device performs a process of inputting the interior image into the body recognition network, to thereby allow the body recognition network to (i) output one or more feature tensors with one or more channels corresponding to the interior image via a feature extraction network, (ii) generate at least one keypoint heatmap and at least one part affinity field with one or more channels corresponding to each of the feature tensors via a keypoint heatmap & part affinity field extractor, and (iii) extract keypoints from the keypoint heatmap via a keypoint detector, to group the extracted keypoints by referring to the part affinity field, and thus to generate body parts per the passengers, and as a result, allow the body recognition network to output multiple pieces of body-part length information on each of the passengers by referring to the body parts per the passengers. 3. the method of claim 2, wherein the feature extraction network includes at least one convolutional layer and applies at least one convolution operation to the interior image, to thereby output the feature tensors. 4. the method of claim 2, wherein the keypoint heatmap & part affinity field extractor includes one of a fully convolutional network and a 1×1 convolutional layer, and applies a fully-convolution operation or 1×1 convolution operation to the feature tensors, to thereby generate the keypoint heatmap and the part affinity field. 5. the method of claim 2, wherein the keypoint detector connects, by referring to the part affinity field, pairs respectively having highest mutual connection probabilities of being connected among the extracted keypoints, to thereby group the extracted keypoints. 6. the method of claim 2, wherein the feature extraction network and the keypoint heatmap & part affinity field extractor have been learned by a learning device performing (i) a process of inputting at least one training image including one or more objects for training into the feature extraction network, to thereby allow the feature extraction network to generate one or more feature tensors for training having one or more channels by applying at least one convolutional operation to the training image, (ii) a process of inputting the feature tensors for training into the keypoint heatmap & part affinity field extractor, to thereby allow the keypoint heatmap & part affinity field extractor to generate one or more keypoint heatmaps for training and one or more part affinity fields for training having one or more channels for each of the feature tensors for training, (iii) a process of inputting the keypoint heatmaps for training and the part affinity fields for training into the keypoint detector, to thereby allow the keypoint detector to extract keypoints for training from each of the keypoint heatmaps for training and a process of grouping the extracted keypoints for training by referring to each of the part affinity fields for training, to thereby detect keypoints per each of the objects for training, and (iv) a process of allowing a loss layer to calculate one or more losses by referring to the keypoints per each of the objects for training and their corresponding ground truths, to thereby adjust one or more parameters of the feature extraction network and the keypoint heatmap & part affinity field extractor such that the losses are minimized by backpropagation using the losses. 7. the method of claim 1, wherein, at the step of (a), the passenger body information-detecting device performs a process of inputting the interior image into the face recognition network, to thereby allow the face recognition network to detect each of the faces of each of the passengers located in the interior image via a face detector, and to output multiple pieces of the passenger feature information on each of the facial images via a facial feature classifier. 8. the method of claim 1, wherein, at the step of (a), the passenger body information-detecting device performs a process of inputting the interior image into the face recognition network, to thereby allow the face recognition network to (i) apply at least one convolution operation to the interior image and thus to output at least one feature map corresponding to the interior image via at least one convolutional layer, (ii) output one or more proposal boxes, where the passengers are estimated as located, on the feature map, via a region proposal network, (iii) apply pooling operation to one or more regions, corresponding to the proposal boxes, on the feature map and thus to output at least one feature vector via a pooling layer, and (iv) apply fully-connected operation to the feature vector, and thus to output the multiple pieces of the passenger feature information corresponding to each of the faces of each of the passengers corresponding to each of the proposal boxes via a fully connected layer. 9. the method of claim 1, wherein the multiple pieces of the passenger feature information include each of ages, each of genders and each of races corresponding to each of the passengers. 10. a passenger body information-detecting device for detecting body information on one or more passengers of a vehicle based on humans\\' status recognition, comprising: at least one memory that stores instructions; and at least one processor configured to execute the instructions to perform or support another device to perform: (i) if at least one interior image of an interior of the vehicle is acquired, (i) a process of inputting the interior image into a face recognition network, to thereby allow the face recognition network to detect each of faces of each of the passengers from the interior image, and thus to output multiple pieces of passenger feature information corresponding to each of the detected faces, and (ii) a process of inputting the interior image into a body recognition network, to thereby allow the body recognition network to detect each of bodies of each of the passengers from the interior image, and thus to output body-part length information of each of the detected bodies, and (ii) a process of retrieving specific height mapping information corresponding to specific passenger feature information on a specific passenger from a height mapping table which stores height mapping information representing respective one or more predetermined ratios of one or more segment body portions of each of human groups to each of heights per each of the human groups, a process of acquiring a specific height of the specific passenger from the specific height mapping information by referring to specific body-part length information of the specific passenger, a process of retrieving specific weight mapping information corresponding to the specific passenger feature information from a weight mapping table which stores multiple pieces of weight mapping information representing predetermined correlations between each of the heights and each of weights per each of the human groups, and a process of acquiring a weight of the specific passenger from the specific weight mapping information by referring to the specific height of the specific passenger. 11. the passenger body information-detecting device of claim 10, wherein, at the process of (i), the processor performs a process of inputting the interior image into the body recognition network, to thereby allow the body recognition network to (i) output one or more feature tensors with one or more channels corresponding to the interior image via a feature extraction network, (ii) generate at least one keypoint heatmap and at least one part affinity field with one or more channels corresponding to each of the feature tensors via a keypoint heatmap & part affinity field extractor, and (iii) extract keypoints from the keypoint heatmap via a keypoint detector, to group the extracted keypoints by referring to the part affinity field, and thus to generate body parts per the passengers, and as a result, allow the body recognition network to output multiple pieces of body-part length information on each of the passengers by referring to the body parts per the passengers. 12. the passenger body information-detecting device of claim 11, wherein the keypoint heatmap & part affinity field extractor includes one of a fully convolutional network and a 1×1 convolutional layer, and applies a fully-convolution operation or 1×1 convolution operation to the feature tensors, to thereby generate the keypoint heatmap and the part affinity field. 13. the passenger body information-detecting device of claim 11, wherein the keypoint detector connects, by referring to the part affinity field, pairs respectively having highest mutual connection probabilities of being connected among the extracted keypoints, to thereby group the extracted keypoints. 14. the passenger body information-detecting device of claim 11, wherein the feature extraction network and the keypoint heatmap & part affinity field extractor have been learned by a learning device performing (i) a process of inputting at least one training image including one or more objects for training into the feature extraction network, to thereby allow the feature extraction network to generate one or more feature tensors for training having one or more channels by applying at least one convolutional operation to the training image, (ii) a process of inputting the feature tensors for training into the keypoint heatmap & part affinity field extractor, to thereby allow the keypoint heatmap & part affinity field extractor to generate one or more keypoint heatmaps for training and one or more part affinity fields for training having one or more channels for each of the feature tensors for training, (iii) a process of inputting the keypoint heatmaps for training and the part affinity fields for training into the keypoint detector, to thereby allow the keypoint detector to extract keypoints for training from each of the keypoint heatmaps for training and a process of grouping the extracted keypoints for training by referring to each of the part affinity fields for training, to thereby detect keypoints per each of the objects for training, and (iv) a process of allowing a loss layer to calculate one or more losses by referring to the keypoints per each of the objects for training and their corresponding ground truths, to thereby adjust one or more parameters of the feature extraction network and the keypoint heatmap & part affinity field extractor such that the losses are minimized by backpropagation using the losses. 15. the passenger body information-detecting device of claim 10, wherein, at the process of (i), the processor performs a process of inputting the interior image into the face recognition network, to thereby allow the face recognition network to (i) apply at least one convolution operation to the interior image and thus to output at least one feature map corresponding to the interior image via at least one convolutional layer, (ii) output one or more proposal boxes, where the passengers are estimated as located, on the feature map, via a region proposal network, (iii) apply pooling operation to one or more regions, corresponding to the proposal boxes, on the feature map and thus to output at least one feature vector via a pooling layer, and (iv) apply fully-connected operation to the feature vector, and thus to output the multiple pieces of the passenger feature information corresponding to each of the faces of each of the passengers corresponding to each of the proposal boxes via a fully connected layer.1. a computer implemented method for performing video coding based on face detection comprising: receiving a video frame comprising one of a plurality of video frames of a video sequence; determining the video frame is a key frame of the video sequence; performing, in response to the video frame being a key frame of the video sequence, a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame; testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region; rejecting the second candidate face region and outputting the first candidate face region; and encoding the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream. 2. the method of claim 1, wherein the skin tone information comprises a skin probability map. 3. the method of claim 1, wherein said testing the first and second candidate face regions based on skin tone information is performed in response to the video frame being a key frame of the video sequence. 4. the method of claim 1, wherein the first candidate face region comprises a rectangular region, the method further comprising: determining a free form shape face region corresponding to the first candidate face region, wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy. 5. the method of claim 4, wherein determining the free form shape face region comprises: generating an enhanced skip probability map corresponding to the first candidate face region; binarizing the enhanced skip probability map; and overlaying the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region. 6. the method of claim 4, wherein a second video frame comprises a non-key frame of the video sequence, the method further comprising performing face detection in the second video frame of the video sequence based on the free form shape face region. 7. the method of claim 6, further comprising: tracking a second free form shape face region in the second video frame based on the free form shape face region in the video frame. 8. the method of claim 7, wherein tracking the second free form shape face region comprises determining a location of a second valid face region in the second video frame based on a displacement offset with respect to the first candidate face region. 9. the method of claim 8, further comprising: determining the displacement offset based on an offset between a centroid of a bounding box around a skin enhanced region corresponding to the first candidate face region and a second centroid of a second bounding box around a second skin enhanced region in the second video frame. 10. the method of claim 1, wherein encoding the video frame based at least in part on the first candidate face region being a valid face region comprises at least one of reducing a quantization parameter corresponding to the first candidate face region, adjusting a lambda value for the first candidate face region, or disabling skip coding for the first candidate face region. 11. the method of claim 1, wherein the bitstream comprises at least one of an h.264/advanced video coding (avc) compliant bitstream, an h.265/high efficiency video coding (hevc) compliant bitstream, a vp9 compliant bitstream, a vp10 compliant bitstream, or an alliance for open media (aom) av1 compliant bitstream. 12. a computer implemented method for performing face detection comprising: receiving a video frame of a sequence of video frames; performing a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame; testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region; rejecting the second candidate face region and outputting the first candidate face region as a valid face region for further processing; and providing an index indicative of a person being present in the video frame based on the valid face region. 13. the method of claim 12, wherein the sequence of video frames comprises a sequence of surveillance video frames, the method further comprising: performing face recognition in the surveillance video frames based on the valid face region. 14. the method of claim 12, wherein the sequence of video frames comprises a sequence of decoded video frames, the method further comprising: adding a marker corresponding to the received video frame to perform face recognition on the received video frame based on the valid face region. 15. the method of claim 12, wherein the sequence of video frames is received during a device login attempt, the method further comprising: performing face recognition based on the valid face region; and allowing access to the device if a secured face is recognized. 16. the method of claim 12, wherein the sequence of video frames comprises a sequence of videoconferencing frames, the method further comprising: encoding the video frame based at least in part on the valid face region to generate a coded bitstream. 17. the method of claim 16, wherein encoding the video frame comprises not encoding a background region of the video frame into the bitstream. 18. the method of claim 12, further comprising: encoding the video frame based at least in part on the valid face region to generate a coded bitstream, wherein encoding the video frame comprises including metadata corresponding to the valid face region in the bitstream. 19. the method of claim 18, further comprising: decoding the coded bitstream to generate a decoded video frame and to determine the metadata corresponding to the valid face region in the bitstream. 20. the method of claim 19, further comprising at least one of replacing the valid face region based on the decoded metadata, cropping and displaying image data corresponding only to the valid face region based on the decoded metadata, or indexing the decoded video frame based on the decoded metadata. 21. a system for performing video coding based on face detection comprising: a memory configured to store a video frame comprising one of a plurality of video frames of a video sequence; and a processor coupled to the memory, the processor to receive the video frame, to determine the video frame is a key frame of the video sequence; to perform, in response to the video frame being a key frame of the video sequence, a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame, to test the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region, to reject the second candidate face region and outputting the first candidate face region, and to encode the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream. 22. the system of claim 21, wherein the skin tone information comprises a skin probability map. 23. the system of claim 21, wherein the first candidate face region comprises a rectangular region, the processor further to determine a free form shape face region corresponding to the first candidate face region, wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy. 24. the system of claim 23, wherein the processor to determine the free form shape face region comprises the processor to generate an enhanced skip probability map corresponding to the first candidate face region, to binarize the enhanced skip probability map, and to overlay the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region. 25. the system of claim 23, wherein a second video frame comprises a non-key frame of the video sequence, and the processor is further to perform face detection in the second video frame of the video sequence based on the free form shape face region. 26. the system of claim 25, wherein the processor is further to track a second free form shape face region in the second video frame based on the free form shape face region in the video frame. 27. the system of claim 21, wherein to encode the video frame based at least in part on the first candidate face region being a valid face region comprises the processor to reduce a quantization parameter corresponding to the first candidate face region, adjust a lambda value for the first candidate face region, or disable skip coding for the first candidate face region. 28. at least one non-transitory machine readable medium comprising a plurality of instructions that, in response to being executed on a device, cause the device to perform video coding based on face detection by: receiving a video frame comprising one of a plurality of video frames of a video sequence; determining the video frame is a key frame of the video sequence; performing, in response to the video frame being a key frame of the video sequence, a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame; testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region; rejecting the second candidate face region and outputting the first candidate face region; and encoding the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream. 29. the non-transitory machine readable medium of claim 28, wherein the skin tone information comprises a skin probability map. 30. the non-transitory machine readable medium of claim 28, wherein the first candidate face region comprises a rectangular region, the machine readable medium comprising further instructions that, in response to being executed on the device, cause the device to perform video coding based on face detection by: determining a free form shape face region corresponding to the first candidate face region, wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy. 31. the non-transitory machine readable medium of claim 30, wherein determining the free form shape face region comprises: generating an enhanced skip probability map corresponding to the first candidate face region; binarizing the enhanced skip probability map; and overlaying the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region. 32. the non-transitory machine readable medium of claim 30, wherein a second video frame comprises a non-key frame of the video sequence, the machine readable medium comprising further instructions that, in response to being executed on the device, cause the device to perform video coding based on face detection by performing face detection in the second video frame of the video sequence based on the free form shape face region. 33. the non-transitory machine readable medium of claim 32, the machine readable medium comprising further instructions that, in response to being executed on the device, cause the device to perform video coding based on face detection by: tracking a second free form shape face region in the second video frame based on the free form shape face region in the video frame. 34. the non-transitory machine readable medium of claim 28, wherein encoding the video frame based at least in part on the first candidate face region being a valid face region comprises at least one of reducing a quantization parameter corresponding to the first candidate face region, adjusting a lambda value for the first candidate face region, or disabling skip coding for the first candidate face region.1. a method for managing a smart database which stores facial images for face recognition, comprising steps of: (a) a managing device performing a process of counting one or more specific facial images corresponding to at least one specific person stored in the smart database where new facial images for the face recognition are continuously stored, and a process of determining whether a first counted value representing a count of the specific facial images satisfies a preset first set value; and (b) if the first counted value is determined as satisfying the first set value, the managing device performing a process of inputting the specific facial images into a neural aggregation network, to thereby allow the neural aggregation network to generate each of quality scores of each of the specific facial images by aggregation of the specific facial images, and a process of sorting the quality scores corresponding to the specific facial images in a descending order of the quality scores, a process of counting the sorted specific facial images in the descending order until a second counted value which represents the number of a counted part of the specific facial images becomes equal to a preset second set value, and a process of deleting an uncounted part of the specific facial images from the smart database. 2. the method of claim 1, further comprising a step of: (c) the managing device performing a process of generating at least one optimal feature by weighted summation of one or more features of the specific facial images using the counted part of the quality scores and a process of setting the optimal feature as a representative face corresponding to the specific person. 3. the method of claim 1, wherein, at the step of (b), the managing device performs a process of inputting the specific facial images into a cnn of the neural aggregation network, to thereby allow the cnn to generate one or more features corresponding to each of the specific facial images, and a process of inputting at least one feature vector, where the features are embedded, into an aggregation module including at least two attention blocks, to thereby allow the aggregation module to generate each of the quality scores of each of the features. 4. the method of claim 1, wherein, at the step of (b), the managing device performs a process of matching (i) (i-1) one or more features corresponding to each of the specific facial images stored in the smart database and (i-2) the quality scores with (ii) the specific person, and a process of storing the matched features and the matched quality scores in the smart database. 5. the method of claim 1, further comprising a step of: (d) the managing device performing one of (i) a process of learning a face recognition system by using the specific facial images corresponding to the specific person stored in the smart database and (ii) a process of transmitting the specific facial images, corresponding to the specific person, to a learning device corresponding to the face recognition system, to thereby allow the learning device to learn the face recognition system using the specific facial images. 6. the method of claim 1, wherein the neural aggregation network has been learned by a learning device repeating more than once (i) a process of inputting multiple facial images for training corresponding to an image set of a single face or a video of the single face into a cnn of the neural aggregation network, to thereby allow the cnn to generate one or more features for training by applying at least one convolution operation to the facial images for training, (ii) a process of inputting at least one feature vector for training, where the features for training are embedded, into an aggregation module, including at least two attention blocks, of the neural aggregation network, to thereby allow the aggregation module to generate each of quality scores for training of each of the features for training by aggregation of the features for training using one or more attention parameters learned in a previous iteration, (iii) a process of outputting at least one optimal feature for training by weighted summation of the features for training using the quality scores for training, and (iv) a process of updating the attention parameters learned in the previous iteration of the at least two attention blocks such that one or more losses are minimized which are outputted from a loss layer by referring to the optimal feature for training and its corresponding ground truth. 7. a managing device for managing a smart database which stores facial images for face recognition, comprising: at least one memory that stores instructions; and at least one processor configured to execute the instructions to perform or support another device to perform: (i) a process of counting one or more specific facial images corresponding to at least one specific person stored in the smart database where new facial images for the face recognition are continuously stored, and a process of determining whether a first counted value representing a count of the specific facial images satisfies a preset first set value, and (ii) if the first counted value is determined as satisfying the first set value, a process of inputting the specific facial images into a neural aggregation network, to thereby allow the neural aggregation network to generate each of quality scores of each of the specific facial images by aggregation of the specific facial images, and a process of sorting the quality scores corresponding to the specific facial images in a descending order of the quality scores, a process of counting the sorted specific facial images in the descending order until a second counted value which represents the number of a counted part of the specific facial images becomes equal to a preset second set value, and a process of deleting an uncounted part of the specific facial images from the smart database. 8. the managing device of claim 7, wherein the processor further performs: (iii) a process of generating at least one optimal feature by weighted summation of one or more features of the specific facial images using the counted part of the quality scores and a process of setting the optimal feature as a representative face corresponding to the specific person. 9. the managing device of claim 7, wherein, at the process of (ii), the processor performs a process of inputting the specific facial images into a cnn of the neural aggregation network, to thereby allow the cnn to generate one or more features corresponding to each of the specific facial images, and a process of inputting at least one feature vector, where the features are embedded, into an aggregation module including at least two attention blocks, to thereby allow the aggregation module to generate each of the quality scores of each of the features. 10. the managing device of claim 7, wherein, at the process of (ii), the processor performs a process of matching (i) (i-1) one or more features corresponding to each of the specific facial images stored in the smart database and (i-2) the quality scores with (ii) the specific person, and a process of storing the matched features and the matched quality scores in the smart database. 11. the managing device of claim 7, wherein the processor further performs: (iv) one of (i) a process of learning a face recognition system by using the specific facial images corresponding to the specific person stored in the smart database and (ii) a process of transmitting the specific facial images, corresponding to the specific person, to a learning device corresponding to the face recognition system, to thereby allow the learning device to learn the face recognition system using the specific facial images. 12. the managing device of claim 7, wherein the neural aggregation network has been learned by a learning device repeating more than once (i) a process of inputting multiple facial images for training corresponding to an image set of a single face or a video of the single face into a cnn of the neural aggregation network, to thereby allow the cnn to generate one or more features for training by applying at least one convolution operation to the facial images for training, (ii) a process of inputting at least one feature vector for training, where the features for training are embedded, into an aggregation module, including at least two attention blocks, of the neural aggregation network, to thereby allow the aggregation module to generate each of quality scores for training of each of the features for training by aggregation of the features for training using one or more attention parameters learned in a previous iteration, (iii) a process of outputting at least one optimal feature for training by weighted summation of the features for training using the quality scores for training, and (iv) a process of updating the attention parameters learned in the previous iteration of the at least two attention blocks such that one or more losses are minimized which are outputted from a loss layer by referring to the optimal feature for training and its corresponding ground truth.1. an object data processing system comprising: at least one processor configured to execute: at least one implementation of a plurality of recognition algorithms stored on at least one non-transitory computer-readable storage medium, each recognition algorithm having feature density selection criteria; and data preprocessing code executed by at least one processor, the data preprocessing code comprising an invariant feature identification algorithm and configured to: obtain a digital representation of a scene, the scene comprising one or more textual media; generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation; cluster the set of invariant features into regions of interest in the digital representation of the scene, each region of interest having a region feature density; classify, by region classifier code, at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation, wherein the at least one of the classified regions of interest corresponds to text; and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type, wherein the another of the regions of interest corresponds to a region of interest for images. 2. the system of claim 1, wherein preprocessing code, based on the feature density selection criteria, determines that an ocr algorithm is applicable to the text, and that other recognition algorithms are applicable to aspects of the photographs and to logos. 3. the system of claim 1, wherein a user creates a user profile for a camera-equipped smartphone that includes the information that the user is visually impaired, which causes prioritized execution of the ocr algorithm such that a text reader program begins reading the text to the user as quickly as possible. 4. the system of claim 3, further comprising an audio or tactile feedback mechanism that helps the user to position the smart phone relative to the text. 5. the system of claim 4, further comprising a \"hold still\" audio feedback signal that is sent to the user when the text is at the center of the captured scene. 6. the system of claim 1, wherein the digital representation comprises at least one of the following types of digital data: image data, video data, and audio data. 7. the system of claim 1, wherein invariant feature identification algorithm comprises at least one of the following feature identification algorithms: fast, sift, freak, brisk, harris, daisy, and mser. 8. the system of claim 1, wherein the invariant feature identification algorithm includes at least one of the following: edge detection algorithm, corner detection algorithm, saliency map algorithm, curve detection algorithm, a texton identification algorithm, and wavelets algorithm. 9. the system of claim 1, wherein at least one region of interest represents at least one physical object in the scene. 10. the system of claim 1, wherein at least one region of interest represents at least one textual media in the scene. 11. the system of claim 10, wherein the region of interest represents a document as the textual media. 12. the system of claim 11, wherein the region of interest represents a financial document. 13. the system of claim 11, wherein the region of interest represents a structured document. 14. the system of claim 1, wherein at least one implementation of a plurality of recognition algorithms includes at least one of the following: a template driven algorithm, a face recognition algorithm, an optical character recognition algorithm, a speech recognition algorithm, and an object recognition algorithm. 15. the system of claim 1, wherein data preprocessing code is further configured to assign each region of interest at least one recognition algorithm as a function of a scene context derived from the digital representation. 16. the system of claim 15, wherein the scene context includes at least one of the following types of data: a location, a position, a time, a user identity, a news event, a medical event, and a promotion. 17. the system of claim 1, further comprising a mobile device comprising at least one implementation of a plurality of recognition algorithms and data preprocessing code. 18. the system of claim 17, wherein the mobile device comprises at least one of the following: a smart phone, a tablet, wearable glass, a toy, a vehicle, a computer, and a phablet. 19. the system of claim 1, further comprising a network-accessible server device comprising at least one implementation of a plurality of recognition algorithms and data preprocessing code. 20. the system of claim 1, wherein the object type includes at least one of the following: a face, an animal, a vehicle, a document, a plant, a building, an appliance, clothing, a body part, and a toy. 21. an object data processing system comprising: at least one processor configured to execute: at least one implementation of a plurality of recognition algorithms stored on at least one non-transitory computer-readable storage medium, each recognition algorithm having feature density selection criteria; and data preprocessing code executed by at least one processor, the data preprocessing code comprising an invariant feature identification algorithm and configured to: obtain a digital representation of a scene, the scene comprising one or more textual media; generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation; cluster the set of invariant features into regions of interest in the digital representation of the scene, each region of interest having a region feature density; classify, by region classifier code, at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation; wherein the at least one of the classified regions of interest corresponds to text; and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type, wherein the another of the regions of interest corresponds to a region of interest for images; assign each region of interest at least one recognition algorithm from at least one implementation of a plurality of diverse recognition algorithms as a function of the region feature density of each region of interest and the feature density selection criteria of the at least one implementation of a plurality of diverse recognition algorithms; and configure the assigned recognition algorithms to process their respective regions of interest, wherein preprocessing code, based on the feature density selection criteria, determines that an ocr algorithm is applicable to the text, and that other recognition algorithms are applicable to aspects of the photographs and to logos. 22. a device comprising: at least one processor configured to execute: at least one implementation of a plurality of recognition algorithms stored on at least one non-transitory computer-readable storage medium, each recognition algorithm having feature density selection criteria; and data preprocessing code executed by at least one processor, the data preprocessing code comprising an invariant feature identification algorithm and configured to: obtain a digital representation of a scene, the scene comprising one or more textual media; generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation; cluster the set of invariant features into regions of interest in the digital representation of the scene, each region of interest having a region feature density; and classify, by region classifier code, at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation, wherein the at least one of the classified regions of interest corresponds to text; and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type, wherein the another of the regions of interest corresponds to a region of interest for images.1. a mobile terminal comprising: a front camera configured to obtain a two-dimensional (2d) face image of a user; a glance sensor tilted by a certain angle and disposed adjacent to the front camera to obtain metadata of the 2d face image; and a controller obtaining a distance between the glance sensor and the front camera, the distance enabling an area of an overlap region, where a first region representing a range photographable by the front camera overlaps a second region representing a range photographable by the glance sensor, to be the maximum. 2. the mobile terminal of claim 1, wherein the controller is configured to obtain the distance, enabling the area of the overlap region to be the maximum, between the glance sensor and the front camera by varying a tilting angle of the glance sensor. 3. the mobile terminal of claim 2, wherein the controller is configured to set the distance, enabling the area of the overlap region to be the maximum, between the glance sensor and the front camera and the tilting angle of the glance sensor as an optimal disposition location of the glance sensor. 4. the mobile terminal of claim 3, wherein the controller is configured to set a disposition location of the front camera as an original point and calculates coordinates of a first triangle representing the first region, based on a field of view of the front camera and a maximum photographing distance of the front camera. 5. the mobile terminal of claim 4, wherein the controller is configured to calculate coordinates of a second triangle representing the second region, based on a field of view of the glance sensor, a maximum photographing distance of the glance sensor, a distance between the front camera and the glance sensor, and a tilting angle of the glance sensor. 6. the mobile terminal of claim 5, wherein before the glance sensor is tilted, the controller is configured to calculate coordinates of a third triangle representing a third region photographable by the glance sensor, and the controller is configured to rotation-convert the coordinates of the third triangle, based on the tilting angle of the glance sensor and calculate the coordinates of the second triangle. 7. the mobile terminal of claim 6, wherein the controller is configured to calculate coordinates of the overlap region, based on the coordinates of the first triangle and the coordinates of the second triangle and calculates the area of the overlap region, based on the coordinates of the overlap region. 8. the mobile terminal of claim 1, wherein the controller is configured to generate three-dimensional (3d) face information, based on the 2d face image obtained by the front camera and metadata obtained by the glance sensor. 9. the mobile terminal of claim 8, wherein the metadata comprises one or more of an angle of a face of the user, a size of the face, and a location of the face. 10. the mobile terminal of claim 9, wherein the angle of the face comprises an angle by which the face is rotated about one or more of a pitch axis, a roll axis, and a yaw axis. 11. the mobile terminal of claim 8, further comprising a memory storing the generated 3d face information, wherein the controller is configured to performs a user authentication process by comparing the stored 3d face information with 3d face information obtained for user authentication. 12. the mobile terminal of claim 1, wherein the glance sensor is controlled to be permanently activated with a low power to obtain a front image and metadata of the front image. 13. the mobile terminal of claim 1, wherein the front camera and the glance sensor are disposed on the same line in an upper end of the mobile terminal. 14. the mobile terminal of claim 1, wherein the glance sensor is tilted in one direction of an up direction, a down direction, a left direction, and a right direction. 15. the mobile terminal of claim 1, wherein the metadata is data which is changed when the mobile terminal is tilted by an external physical force.1. a method, comprising: receiving, by a smart television (tv), an indication of upcoming media programming, wherein the upcoming media programming is based on a user profile; identifying one or more devices in communication with the smart tv, each of the one or more devices including at least one of a microphone or a camera; instructing at least one identified device to detect audio signals using its respective microphone, or to detect visual signals using its respective camera; selecting at least one device of the one or more devices based on the detected audio signal or detected visual signal; and providing instructions to the selected device to output a notification related to the upcoming media programming. 2. the method of claim 1, wherein the upcoming media programming is one of a live television program, a recorded television program, a broadcast television program, or an application-provided program. 3. the method of claim 1, wherein selecting the first device based on the detected audio signal includes recognizing a voice. 4. the method of claim 3, further comprising determining a distance to the recognized voice, and wherein selecting the first device is further based on the determined distance. 5. the method of claim 1, wherein selecting the first device based on the detected visual signals includes recognizing a face. 6. the method of claim 5, wherein recognizing the face includes a face recognition technique. 7. the method of claim 1, further comprising presenting, on the smart tv, the upcoming media programming in a favorite channel list. 8. the method of claim 7, further comprising: obtaining media programming viewing data, wherein the media programming viewing data includes at least one of a historical time and a historical date that one or more media programs were viewed; obtaining at least one of a current time and a current date; processing the media programming viewing data to determine a probability of the one or more media programs being viewed based on at least one of the current time and the current date; and presenting the favorite channel list based on the determined probability of the one or more media programs being viewed. 9. the method of claim 8, wherein processing the media programming viewing data includes employing a neural network model. 10. the method of claim 9, wherein employing the neural network model comprises: determining a duration that the one or more media programs were viewed for each of the at least one of the historical time and the historical date; setting a threshold time duration; comparing the determined duration to the threshold time duration; and filtering out the one or more media programs viewed below the threshold time duration. 11. a smart television (tv), comprising: a network interface; a non-transitory computer-readable medium; and a processor in communication with the network interface, and the non-transitory computer-readable medium, and capable of executing processor-executable program code stored in the non-transitory computer-readable medium, to cause the smart tv to: receive an indication of upcoming media programming, wherein the upcoming media programming is based on a user profile; identify one or more devices in communication with the smart tv, each of the one or more devices including at least one of a microphone or a camera; instruct at least one identified device to detect audio signals using its respective microphone, or to detect visual signals using its respective camera; select at least one device of the one or more devices based on the detected audio signal or detected visual signal; and provide instructions to the selected device to output a notification related to the upcoming media programming. 12. the smart tv of claim 11, wherein selecting the first device based on the detected audio signal includes recognizing a voice. 13. the smart tv of claim 12, wherein the processor is further capable of executing processor-executable program code to: determine a distance to the recognized voice, and wherein selecting the first device is further based on the determined distance. 14. the smart tv of claim 11, wherein selecting the first device based on the detected visual signals includes detecting the presence of a user. 15. the smart tv of claim 14, wherein detecting the presence of the user includes employing one or more of a camera, a microphone, or a fingerprint sensor associated with at least one of the smart tv a mobile device, a smartphone, a laptop computer, a tablet device, a wearable device, an internet of things (iot) device, an internet of everything (ioe) device, an iot hub, or an ioe hub. 16. a smart television (tv), comprising: means for receiving an indication of upcoming media programming, wherein the upcoming media programming is based on a user profile; means for identifying one or more devices in communication with the smart tv, each of the one or more devices including at least one of a microphone or a camera; means for instructing at least one identified device to detect audio signals using its respective microphone, or to detect visual signals using its respective camera; means for selecting at least one device of the one or more devices based on the detected audio signal or detected visual signal; and means for providing instructions to the selected device to output a notification related to the upcoming media programming. 17. the smart tv of claim 16, wherein the one or more devices includes at least one of a mobile device, a smartphone, a laptop computer, a tablet device, a wearable device, an internet of things (iot) device, an internet of everything (ioe) device, an iot hub, an ioe hub, or another smart tv. 18. the smart tv of claim 16, wherein the upcoming media programming is one of a live television program, a recorded television program, a broadcast television program, or an application-provided program. 19. the smart tv of claim 16, wherein the notification includes at least one of a push message, a sms message, a way2sms message, an audio alert, an audio message, or an email message. 20. the smart tv of claim 16, further comprising presenting the upcoming media programming in a favorite channel list. 21. the smart tv of claim 20, further comprising: means for obtaining media programming viewing data, wherein the media programming viewing data includes at least one of a historical time and a historical date that one or more media programs were viewed on the smart tv; means for obtaining at least one of a current time and a current date; means for processing the media programming viewing data to determine a probability of the one or more media programs being viewed on the smart tv based on at least one of the current time and the current date; and means for presenting the favorite channel list based on the determined probability of the one or more media programs being viewed. 22. the smart tv of claim 21, wherein the means for processing the media programming viewing data includes employing a neural network model. 23. the smart tv of claim 22, wherein employing the neural network model comprises: determining a duration that the one or more media programs were viewed on the smart tv for each of the at least one of the historical time and the historical date; setting a threshold time duration; comparing the determined duration to the threshold time duration; and filtering out the one or more media programs viewed below the threshold time duration. 24. the smart tv of claim 21, further comprising: means for adjusting at least one of a volume or a brightness of the smart tv, wherein the adjusting is based on at least one of the historical time and the historical date. 25. the smart tv of claim 21, further comprising means for restricting access to one or more media programs. 26. a non-transitory computer-readable medium comprising processor-executable program code configured to cause a processor of a smart television (tv) to: receive an indication of upcoming media programming, wherein the upcoming media programming is based on a user profile; identify one or more devices in communication with the smart tv, each of the one or more devices including at least one of a microphone or a camera; instruct at least one identified device to detect audio signals using its respective microphone, or to detect visual signals using its respective camera; select at least one device of the one or more devices based on the detected audio signal or detected visual signal; and provide instructions to the selected device to output a notification related to the upcoming media programming. 27. the non-transitory computer-readable medium of claim 26, wherein selecting the first device based on the detected audio signal includes recognizing a voice. 28. the non-transitory computer-readable medium of claim 27, wherein the processor is further capable of executing processor-executable program code to: determine a distance to the recognized voice, and wherein selecting the first device is further based on the determined distance. 29. the non-transitory computer-readable medium of claim 26, wherein selecting the first device based on the detected visual signals includes recognizing a face. 30. the non-transitory computer-readable medium of claim 29, wherein recognizing the face includes a face recognition technique.1. a camera comprising: a sensor array including a plurality of sensors; an infrared (ir) illuminator configured to emit active ir light in an ir light sub-band; a plurality of spectral illuminators, each spectral illuminator configured to emit active spectral light in a different spectral light sub-band; a depth controller machine configured to determine a depth value for each of the plurality of sensors based on the active ir light, a spectral controller machine configured to, for each of the plurality of sensors, determine a spectral value for each spectral light sub-band of the plurality of spectral illuminators; and an output machine configured to output a test depth+multi-spectral image including a plurality of pixels, each pixel corresponding to one of the plurality of sensors of the sensor array and including at least: a depth value, and a spectral value for each spectral light sub-band of the plurality of spectral illuminators; a face recognition machine previously trained with a set of labeled training depth+multi-spectral images having a same structure as the test depth+multi-spectral image, the face recognition machine configured to output a confidence value indicating a likelihood that the test depth+multi-spectral image includes a face. 2. the camera of claim 1, wherein each spectral value is calculated based on the depth value determined for the sensor that corresponds to the pixel. 3. the camera of claim 1, wherein the face recognition machine is configured to use a convolutional neural network to determine the confidence value. 4. the camera of claim 3, wherein the face recognition machine includes a plurality of input nodes, wherein each input node is configured to receive a pixel value array corresponding to a different pixel of the plurality of pixels of the test depth+multi-spectral image, and wherein the pixel value array includes the depth value and the plurality of multi-spectral values for the pixel. 5. the camera of claim 4, wherein the plurality of multi-spectral values for the pixel include more than three spectral values. 6. the camera of claim 4, wherein the output machine is configured to output a surface normal for each pixel of the test depth+multi-spectral image, and wherein the pixel value array includes the surface normal. 7. the camera of claim 4, wherein the output machine is configured to output a curvature for each pixel of the test depth+multi-spectral image, and wherein the pixel value array includes the curvature. 8. the camera of claim 3, wherein the face recognition machine is configured to use a plurality of models to determine the confidence value, wherein the plurality of models includes a plurality of channel-specific models, wherein each channel-specific model is configured to process a different pixel parameter for the plurality of pixels of the test depth+multi-spectral image, wherein each channel-specific model includes a plurality of input nodes, and wherein, for each channel-specific model, each input node is configured to receive a pixel parameter value for a different pixel of the plurality of pixels of the test depth+multi-spectral image. 9. the camera of claim 1, wherein the face recognition machine is configured to use a statistical model to determine the confidence value. 10. the camera of claim 9, wherein the statistical model includes a nearest neighbor algorithm. 11. the camera of claim 9, wherein the statistical model includes a support vector machine. 12. the camera of claim 1, wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of a bounding box around a recognized face. 13. the camera of claim 1, wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of an identified two-dimensional (2d) facial feature of a recognized face. 14. the camera of claim 1, wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of an identified three-dimensional (3d) facial feature of a recognized face. 15. the camera of claim 1, wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of an identified spectral feature on a recognized face. 16. the camera of claim 1, wherein the face recognition machine is further configured to output, for each pixel of the test depth+multi-spectral image, a confidence value indicating a likelihood that the pixel is included in a face. 17. the camera of claim 1, wherein the face recognition machine is further configured to output an identity of a face recognized in the test depth+multi-spectral image. 18. the camera of claim 1, wherein the plurality of sensors of the sensor array are differential sensors, and wherein each spectral value is determined based on a depth value and a differential measurement for that differential sensor. 19. a camera comprising: a sensor array including a plurality of sensors; an infrared (ir) illuminator configured to emit active ir light in an ir light sub-band; a plurality of spectral illuminators, each spectral illuminator configured to emit active spectral light in a different spectral light sub-band; a depth controller machine configured to determine a depth value for each of the plurality of sensors based on the active ir light, a spectral controller machine configured to, for each of the plurality of sensors, determine a spectral value for each spectral light sub-band of the plurality of spectral illuminators, wherein each spectral value is calculated based on the depth value determined for the sensor that corresponds to the pixel; and an output machine configured to output a test depth+multi-spectral image including a plurality of pixels, each pixel corresponding to one of the plurality of sensors of the sensor array and including at least: a depth value, and a spectral value for each spectral light sub-band of the plurality of spectral illuminators; and a face recognition machine including a convolutional neural network previously trained with a set of labeled training depth+multi-spectral images having a same structure as the test depth+multi-spectral image, the face recognition machine configured to output a confidence value indicating a likelihood that the test depth+multi-spectral image includes a face.1. an image processing method, comprising: acquiring a photo album obtained from face clustering; collecting face information of respective images in the photo album, and acquiring a face parameter of each image according to the face information; selecting a cover image according to the face parameter of each image; and taking a face-region image from the cover image, and setting the face-region image as a cover of the photo album; wherein selecting the cover image according to the face parameter of each image comprises: performing calculation on the face parameter of each image in a preset way, to obtain a cover score of each image; selecting the image with a highest cover score as the cover image; wherein selecting the image with the highest cover score as the cover image comprises: acquiring a source of each image; and selecting the image with the highest cover score in images coming from a preset source as the cover image. 2. the method according to claim 1, wherein selecting the image with the highest cover score as the cover image comprises: acquiring the number of faces contained in each image; determining single-person images according to the number of faces; and selecting the single-person image with the highest cover score as the cover image. 3. the method according to claim 2, wherein selecting the image with the highest cover score as the cover image further comprises: when there is no single-person image in the photo album, determining images including two faces from the photo album; and selecting the image with the highest cover score from the images including two faces as the cover image. 4. the method according to claim 1, wherein the face information comprises face feature points, and the face parameter comprises a face turning angle; acquiring the face parameter of each image according to the face information comprises: acquiring coordinate values of the face feature points; determining distances and angles between the face feature points; and determining the face turning angle according to the distances and the angles. 5. the method according to claim 1, wherein the face parameter comprises a face ratio; acquiring the face parameter of each image according to the face information comprises: determining a face region of the image according to the face information; and calculating a ratio of an area of the face region to an area of the image to obtain the face ratio. 6. the method according to claim 5, wherein calculating the face ratio comprises: when there is more than one face in the image, subtracting an area occupied faces other than a face corresponding to the photo album from the face region to obtain a remaining area; and calculating a ratio of the remaining area to the area of the image to obtain the face ratio. 7. the method according to claim 1, wherein collecting face information of respective images in the photo album comprises: acquiring image identifications of images in the photo album; extracting face information corresponding to the image identifications from a face database, the face database being stored with face recognition results of images, the face recognition results including the face information. 8. an image processing apparatus, comprising: a processor; and a memory, configured to store instructions executable by the processor, wherein the processor is configured to run a program corresponding to the instructions by reading the instructions stored in the memory, so as to perform: acquiring a photo album obtained from face clustering; collecting face information of each image in the photo album; acquiring a face parameter of each image according to the face information; selecting a cover image according to the face parameter of each image; taking a face-region image from the cover image, and setting the face-region image as a cover of the photo album; wherein the processor is configured to: perform calculation on the face parameter of each image in a preset way, to obtain a cover score of each image; and select the image with a highest cover score as the cover image; and wherein the processor is configured to: acquire a source of each image; and select the image with the highest cover score in images coming from a preset source as the cover image. 9. the apparatus according to claim 8, wherein the processor is configured to: acquire the number of faces contained in each image; determine single-person images according to the number of faces; and select the single-person image with the highest cover score as the cover image. 10. the apparatus according to claim 9, wherein the processor is further configured to: when there is no single-person image in the photo album, determine images including two faces from the photo album; and select the image with the highest cover score from the images including two faces as the cover image. 11. the apparatus according to claim 8, wherein the face information comprises face feature points, and the face parameter comprises a face turning angle; the processor is configured to: acquire coordinate values of the face feature points; determine distances and angles between the face feature points; and determine the face turning angle according to the distances and the angles. 12. the apparatus according to claim 8, wherein the face parameter comprises a face ratio; the processor is configured to: determine a face region of the image according to the face information; and calculate a ratio of an area of the face region to an area of the image to obtain the face ratio. 13. the apparatus according to claim 12, wherein the processor is configured to: when there is more than one face in the image, subtract an area occupied faces other than a face corresponding to the photo album from the face region to obtain a remaining area; and calculate a ratio of the remaining area to the area of the image to obtain the face ratio. 14. the apparatus according to claim 8, wherein the processor is configured to: acquire image identifications of images in the photo album; extract face information corresponding to the image identifications from a face database, the face database being stored with face recognition results of images, the face recognition results including the face information. 15. an electronic device, comprising a processor, a memory, a display screen and an input device connected via a system bus, wherein the memory is stored with computer programs that, when executed by the processor, cause the processor to implement an image processing method, the image processing method comprising: acquiring a photo album obtained from face clustering; collecting face information of respective images in the photo album, and acquiring a face parameter of each image according to the face information; selecting a cover image according to the face parameter of each image; and taking a face-region image from the cover image, and setting the face-region image as a cover of the photo album; wherein selecting the cover image according to the face parameter of each image comprises: performing calculation on the face parameter of each image in a preset way, to obtain a cover score of each image; and selecting the image with a highest cover score as the cover image; and wherein selecting the image with the highest cover score as the cover image comprises: acquiring a source of each image; and selecting the image with the highest cover score in images coming from a preset source as the cover image. 16. the electronic device according to claim 15, wherein the electronic device comprises at least one of a mobile phone, a tablet computer, a personal digital assistant and a wearable device.1. a computer-implemented method, comprising: receiving, at a computing device, a meeting invitation identifying a location and at least one invitee, the meeting invitation configured to provide the at least one invitee with physical access to the location, wherein the meeting invitation causes a system to control a pathway allowing physical access to the location; providing, based on the meeting invitation, the at least one invitee with physical access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location wherein the positioning data is based in part on a face recognition camera system identifying the at least one invitee; receiving the positioning data from the face recognition camera system identifying the at least one invitee, wherein the positioning data indicates a pattern of movement of the at least one invitee; determining that the pattern of movement indicates that the at least one invitee has exited the location; and revoking physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway, in response to determining that the pattern of movement indicates that the at least one invitee has exited the location. 2. the computer-implemented method of claim 1, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location in a predetermined direction. 3. the computer-implemented method of claim 1, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has moved through an area in a predetermined direction. 4. the computer-implemented method of claim 1, wherein the positioning data indicates a second pattern of movement of the at least one invitee and, wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement. 5. the computer-implemented method of claim 1, further comprising: collating secured data and public data to generate resource data; and communicating the resource data to a client computing device associated with the at least one invitee when access of the location is provided. 6. the computer-implemented method of claim 1, wherein the positioning data indicates that the at least one invitee is at the predetermined location when the at least one invitee passes through the predetermined location. 7. the computer-implemented method of claim 1, wherein the positioning data indicates that the at least one invitee is at the predetermined location when the at least one invitee passes through the predetermined location near the location in a predetermined direction. 8. a system, comprising: a processor; and a memory in communication with the processor, the memory having computer-readable instructions stored thereupon that, when executed by the processor, cause the processor to: receive a meeting invitation indicating a location and an identity, the meeting invitation configured to provide at least one invitee with physical access to the location, wherein the meeting invitation causes the system to control a pathway allowing physical access to the location; provide the at least one invitee associated with the identity access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location, wherein the positioning data is based in part on a face recognition camera system identifying the at least one invitee; receive the positioning data from the face recognition camera system identifying the at least one invitee, wherein the positioning data indicates a pattern of movement of the at least one invitee; determine that the pattern of movement indicates that the at least one invitee has exited the location; and revoke physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway, in response to determining that the pattern of movement indicates that the at least one invitee has exited the location. 9. the system of claim 8, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location. 10. the system of claim 8, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has moved through an area in a predetermined direction. 11. the system of claim 8, wherein the positioning data indicates a second pattern of movement of the at least one invitee and wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement. 12. the system of claim 8, wherein the instructions further cause the processor to: collate secured data and public data to generate resource data; and communicate the resource data to a client computing device associated with the at least one invitee when access of the location is provided. 13. a non-transitory computer-readable storage medium having computer-executable instructions stored thereupon which, when executed by one or more processors of a computing device, cause the one or more processors of the computing device to: receive a meeting invitation indicating a location and an identity, the meeting invitation configured to provide at least one invitee with physical access to the location, wherein the meeting invitation causes a system to control a pathway allowing physical access to the location; provide the at least one invitee associated with the identity access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location, wherein the positioning data is based in part on a face recognition camera system identifying the at least one invitee; receive the positioning data from the face recognition camera system identifying the at least one invitee, wherein the positioning data indicates a pattern of movement of the at least one invitee; determine that the pattern of movement indicates that the at least one invitee has exited the location; and revoke physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway, in response to determining that the pattern of movement indicates that the at least one invitee has exited the location. 14. the non-transitory computer-readable storage medium of claim 13, wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location. 15. the non-transitory computer-readable storage medium of claim 13, wherein the positioning data indicates a second pattern of movement of the at least one invitee and wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement. 16. the non-transitory computer-readable storage medium of claim 13, wherein the instructions further cause the one or more processors to: collate secured data and public data to generate resource data; and communicate the resource data to a client computing device associated with the at least one invitee when access of the location is provided.1. a method, comprising: receiving a piece of content and salient data for the piece of content; based on the salient data, determining a first path for a viewport for the piece of content, wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content; providing the viewport on a display device, wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback; detecting an additional salient event in the piece of content that is not included in the first path for the viewport; and providing an indication for the additional salient event in the viewport during the playback. 2. the method of claim 1, wherein the salient data identifies each salient event in the piece of content, and the salient data indicates, for each salient event in the piece of content, a corresponding point location of the salient event in the piece of content and a corresponding time at which the salient event occurs during the playback. 3. the method of claim 2, wherein the salient data further indicates, for each salient event in the piece of content, a corresponding type of the salient event and a corresponding strength value of the salient event. 4. the method of claim 1, wherein the first path for the viewport controls the movement of the viewport to put the different salient events in a view of the viewport at the different times during the playback. 5. the method of claim 1, further comprising: detecting one or more salient events in the piece of content based on at least one of the following: visual data of the piece of content, audio data of the piece of content, or content consumption experience data for the piece of content; wherein the salient data is indicative of each salient event detected. 6. the method of claim 1, further comprising: detecting one or more salient events in the piece of content based on at least one of the following: face recognition, facial emotion recognition, object recognition, motion recognition, or metadata of the piece of content; wherein the salient data is indicative of each salient event detected. 7. the method of claim 1, further comprising: detecting user interaction with the indication, wherein the indication comprises an interactive hint; and in response to detecting the user interaction: adapting the first path for the viewport to a second path for the viewport based on the user interaction, wherein the second path for the viewport includes the additional salient event; and providing an updated viewport for the piece of content on the display device, wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback, and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport. 8. the method of claim 7, further comprising: changing a weight assigned to the additional salient event and one or more other salient events in the piece of content having the same type as the additional salient event. 9. the method of claim 7, wherein the second path for the viewport includes one or more other salient events in the piece of content having the same type as the additional salient event. 10. a system, comprising: at least one processor; and a non-transitory processor-readable memory device storing instructions that when executed by the at least one processor causes the at least one processor to perform operations including: receiving a piece of content and salient data for the piece of content; based on the salient data, determining a first path for a viewport for the piece of content, wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content; providing the viewport on a display device, wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback; detecting an additional salient event in the piece of content that is not included in the first path for the viewport; and providing an indication for the additional salient event in the viewport during the playback. 11. the system of claim 10, wherein the salient data identifies each salient event in the piece of content, and the salient data indicates, for each salient event in the piece of content, a corresponding point location of the salient event in the piece of content and a corresponding time at which the salient event occurs during the playback. 12. the system of claim 11, wherein the salient data further indicates, for each salient event in the piece of content, a corresponding type of the salient event and a corresponding strength value of the salient event. 13. the system of claim 10, wherein the salient data is generated offline on a server. 14. the system of claim 10, the operations further comprising: detecting one or more salient events in the piece of content based on at least one of the following: visual data of the piece of content, audio data of the piece of content, or content consumption experience data for the piece of content; wherein the salient data is indicative of each salient event detected. 15. the system of claim 10, the operations further comprising: detecting one or more salient events in the piece of content based on at least one of the following: face recognition, facial emotion recognition, object recognition, motion recognition, or metadata of the piece of content; wherein the salient data is indicative of each salient event detected. 16. the system of claim 10, the operations further comprising: detecting user interaction with the indication, wherein the indication comprises an interactive hint; and in response to detecting the user interaction: adapting the first path for the viewport to a second path for the viewport based on the user interaction, wherein the second path for the viewport includes the additional salient event; and providing an updated viewport for the piece of content on the display device, wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback, and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport. 17. the system of claim 16, the operations further comprising: changing a weight assigned to the additional salient event and one or more other salient events in the piece of content having the same type as the additional salient event. 18. the system of claim 16, wherein the second path for the viewport includes one or more other salient events in the piece of content having the same type as the additional salient event. 19. a non-transitory computer readable storage medium including instructions to perform a method comprising: receiving a piece of content and salient data for the piece of content; based on the salient data, determining a first path for a viewport for the piece of content, wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content; providing the viewport on a display device, wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback; detecting an additional salient event in the piece of content that is not included in the first path for the viewport; and providing an indication for the additional salient event in the viewport during the playback. 20. the computer readable storage medium of claim 19, the method further comprising: detecting user interaction with the indication, wherein the indication comprises an interactive hint; and in response to detecting the user interaction: adapting the first path for the viewport to a second path for the viewport based on the user interaction, wherein the second path for the viewport includes the additional salient event; and providing an updated viewport for the piece of content on the display device, wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback, and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport.1. a mobile device with facial recognition, the mobile device comprising: one or more cameras; a processor device and memory coupled to the processor device, the processing system programmed to: receive a plurality of images from the one or more cameras; extract, with a feature extractor utilizing a convolutional neural network (cnn) with an enlarged intra-class variance of long-tail classes, feature vectors from each of the plurality of images; generate, with a feature generator, discriminative feature vectors for each of the feature vectors; classify, with a fully connected classifier, an identity from the discriminative feature vectors; and control an operation of the mobile device to react in accordance with the identity. 2. the mobile device as recited in claim 1, further includes a communication system. 3. the mobile device as recited in claim 1, wherein the operation tags the video with the identity and uploads the video to social media. 4. the mobile device as recited in claim 1, wherein the operation tags the video with the identity and sends the video to a user. 5. the mobile device as recited in claim 1, wherein the mobile device is a smart phone. 6. the mobile device as recited in claim 1, wherein the mobile device is a body cam. 7. the mobile device as recited in claim 1, further programmed to train the feature extractor, the feature generator, and the fully connected classifier with an alternative bi-stage strategy. 8. the mobile device as recited in claim 1, wherein the feature extractor shares covariance matrices across all classes to transfer intra-class variance from regular classes to the long-tail classes. 9. the mobile device as recited in claim 1, wherein the feature generator optimizes a softmax loss by joint regularization of weights and features through a magnitude of an inner product of the weights and features. 10. the mobile device as recited in claim 1, wherein the feature extractor averages the feature vector with a flipped feature vector, the flipped feature vector being generated from a horizontally flipped frame from one of the plurality of images. 11. the mobile device as recited in claim 1, wherein each of the plurality of images is selected from the group consisting of an image, a video, and a frame from the video. 12. the mobile device as recited in claim 2, wherein the communication system connects to a remote server that includes a facial recognition network. 13. the mobile device as recited in claim 7, wherein one stage of the alternative bi-stage strategy fixes the feature extractor and applies the feature generator to generate new transferred features that are more diverse and violate a decision boundary. 14. the mobile device as recited in claim 7, wherein one stage of the alternative bi-stage strategy fixes the fully connected classifier and updates the feature extractor and the feature generator. 15. a computer program product for a mobile device with facial recognition, the computer program product comprising a non-transitory computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computer to cause the computer to perform a method comprising: receiving, by a processor device, a plurality of images; extracting, by the processor device with a feature extractor utilizing a convolutional neural network (cnn) with an enlarged intra-class variance of long-tail classes, feature vectors for each of the plurality of images; generating, by the processor device with a feature generator, discriminative feature vectors for each of the feature vectors; classifying, by the processor device utilizing a fully connected classifier, an identity from the discriminative feature vector; and controlling an operation of the mobile device to react in accordance with the identity. 16. a computer-implemented method for facial recognition in a mobile device, the method comprising: receiving, by a processor device, a plurality of images; extracting, by the processor device with a feature extractor utilizing a convolutional neural network (cnn) with an enlarged intra-class variance of long-tail classes, feature vectors for each of the plurality of images; generating, by the processor device with a feature generator, discriminative feature vectors for each of the feature vectors; classifying, by the processor device utilizing a fully connected classifier, an identity from the discriminative feature vector; and controlling an operation of the mobile device to react in accordance with the identity. 17. the computer-implemented method as recited in claim 16, wherein controlling includes tagging the video with the identity and uploading the video to social media. 18. the computer-implemented method as recited in claim 16, wherein controlling includes tagging the video with the identity and sending the video to a user. 19. the computer-implemented method as recited in claim 16, wherein extracting includes sharing covariance matrices across all classes to transfer intra-class variance from regular classes to the long-tail classes.1. a computing device comprising: a non-transitory machine readable medium storing a machine trained (mt) network comprising a plurality of layers of processing nodes, each processing node configured to: compute a first output value by combining a set of output values from a set of processing nodes, and use a piecewise linear cup function to compute a second output value from the first output value of the processing node, wherein the piecewise linear cup function prior to training of the mt network comprises at least (i) a first linear section with a first slope, followed by (ii) a second linear section with a negative second slope, followed by (iii) a third linear section with a negative third slope that is different from the second slope, followed by (iv) a fourth linear section with a positive fourth slope, followed by (v) a fifth linear section with a positive fifth slope that is different from the fourth slope, followed by (vi) a sixth linear section with a sixth slope, wherein the piecewise linear cup function is symmetric about a vertical axis between the third and fourth linear sections prior to training of the mt network; a content capturing circuit for capturing content for processing by the mt network; and a set of processing units for executing the processing nodes to process content captured by the content capturing circuit, wherein by training a set of parameters that define the piecewise linear cup function of each node in first and second pluralities of processing nodes, (i) each processing node in the first plurality of processing nodes is configured to emulate a boolean and operator such that an output value of the processing node is in a range associated with a \"1\" value only when a set of inputs to the processing node have a set of values in a range associated with \"1\" and (ii) each processing node in the second plurality of processing nodes is configured to emulate a boolean xnor operator such that an output value of the processing node is in the range associated with \"1\" only when (a) a set of inputs to the node have a set of values in a range associated with \"1\" or (b) the set of inputs to the node have a set of values in a range associated with a \"0\" value. 2. the computing device of claim 1, wherein the third linear section of the piecewise linear cup function of a first processing node in the mt network has a different slope from the third linear section of a second processing node in the mt network. 3. the computing device of claim 1, wherein the length of the third section of a piecewise linear cup function of a first processing node in the mt network is different from the length of the third section of a piecewise linear cup function of a second processing node in the mt network. 4. the computing device of claim 1, wherein the sets of parameters are trained in part by a back propagating module for back propagating errors in output values of later layers of processing nodes to earlier layers of processing nodes by adjusting the set of parameters that define the piecewise linear cup functions of the earlier layers of processing nodes. 5. the computing device of claim 4, wherein each processing node uses a linear function that is defined by a set of parameters to compute the first output value of the processing node, wherein the back propagating module back propagates errors in output values of later layers of processing nodes to earlier layers of processing nodes by adjusting the set of parameters that define the linear functions of the earlier layers of processing nodes. 6. the computing device of claim 1, wherein the first plurality of processing nodes that emulate the boolean and operator and the second plurality of processing nodes that emulate the boolean xnor operator enable the mt network to implement mathematical problems. 7. the computing device of claim 1, wherein each of a plurality of processing node layers has a plurality of processing nodes that receive as input values the output values from a plurality of processing nodes in a set of prior layers. 8. the computing device of claim 7, wherein each processing node uses a linear function to compute the first output value of the processing node, wherein each processing node\\'s piecewise linear cup function is defined along first and second axes, the first axis defining a range of output values from the processing node\\'s linear function, and the second axis defining a range of output values produced by the piecewise linear cup function for the range of output values from the processing node\\'s linear function. 9. the computing device of claim 1, further comprising: a content output circuit for presenting an output based on the processing of the content by the mt network. 10. the computing device of claim 9, wherein the captured content is one of an image and an audio segment, and wherein the presented output is an output display on a display screen of the computing device or an audio presentation output on a speaker of the computing device. 11. the computing device of claim 10, wherein the computing device is a mobile device. 12. the computing device of claim 1, wherein the mt network is a mt neural network and the processing nodes are mt neurons. 13. the computing device of claim 1, wherein the set of parameters configured through training for a plurality of the processing nodes comprise at least one of the negative second and third slopes for the second and third linear sections, the positive fourth and fifth slopes for the fourth and fifth linear sections, a first intercept for the second linear section, a second intercept for the fifth linear section, and a set of lengths for at least the second, third, fourth, and fifth sections. 14. the computing device of claim 1, wherein the trained set of parameters that define the piecewise linear cup function of each node comprise a plurality of output values. 15. the computing device of claim 1, wherein the first and sixth slopes are zero.we claim: 1. a system comprising: a memory device to store an input image; a processor including, an image input interface to receive the input image, a pre-processor to model the input image to yield a multi-channel image, a feature extractor to extract a set of features based on the multi-channel image, a feature selector to select one or more features from the set of features of the multi-channel image, wherein the one or more features are selected based on an ability to differentiate features, a feature matcher to match the one or more features to a learned feature set, and a similarity detector to determine whether the one or more features meet a pre-defined similarity threshold. 2. the system of claim 1, wherein the pre-processor further is to activate one or more channels of the multi-channel image to yield one or more activated channels. 3. the system of claim 2, wherein the one or more activated channels are to be determined based on their ability to differentiate features. 4. the system of claim 2, wherein the pre-processor further is to activate one or more local patches of the one or more activated channels. 5. the system of claim 4, wherein the one or more local patches are to be determined based on their ability to differentiate features. 6. the system of claim 1, wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching. 7. an apparatus comprising: an image input interface to receive an input image; a pre-processor to model the input image to yield a multi-channel image; a feature extractor to extract a set of features based on the multi-channel image; a feature selector to select one or more features from the set of features of the multi-channel image, wherein the one or more features are selected based on an ability to differentiate features; a feature matcher to match the one or more features to a learned feature set; and a similarity detector to determine whether the one or more features meet a pre-defined similarity threshold. 8. the apparatus of claim 7, wherein the pre-processor further is to activate one or more channels of the multi-channel image to yield one or more activated channels. 9. the apparatus of claim 8, wherein the one or more activated channels are to be determined based on their ability to differentiate features. 10. the apparatus of claim 8, wherein the pre-processor further is to activate one or more local patches of the one or more activated channels. 11. the apparatus of claim 10, wherein the one or more local patches are to be determined based on their ability to differentiate features. 12. the apparatus of claim 7, wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching. 13. a method comprising: modeling an input image to yield a multi-channel image; extracting a set of features based on the multi-channel image; selecting one or more features from the set of features of the multi-channel image, wherein the one or more features are selected based on an ability to differentiate features; matching the one or more features to a learned feature set; and determining whether the one or more features meet a pre-defined similarity threshold. 14. the method of claim 13, wherein modeling the input image further is to include activating one or more channels of the multi-channel image to yield one or more activated channels. 15. the method of claim 14, wherein the one or more activated channels are to be determined based on their ability to differentiate features. 16. the method of claim 13, wherein extracting features of the input image further is to include activating one or more local patches of the one or more activated channels. 17. the method of claim 16, wherein the one or more local patches are to be determined based on their ability to differentiate features. 18. the method of claim 13, wherein the feature matcher utilizes a large-scale data learning process to perform the feature matching. 19. at least one non-transitory computer readable storage medium comprising a set of instructions which, when executed by a computing device, cause the computing device to: model an input image to yield a multi-channel image, extract a set of features based on the multi-channel image, select one or more features from the set of features of the multi-channel image, wherein the features are selected based on an ability to differentiate features, match the one or more features to a learned feature set, and determine whether the one or more features meet a pre-defined similarity threshold. 20. the at least one non-transitory computer readable storage medium of claim 19, wherein the instructions, when executed, cause a computing device to activate one or more channels of the multi-channel image to yield one or more activated channels. 21. the at least one non-transitory computer readable storage medium of claim 20, wherein the instructions, when executed, cause a computing device to determine the one or more activated channels based on their ability to differentiate features. 22. the at least one non-transitory computer readable storage medium of claim 20, wherein extracting features of the input image is to further include activating one or more local patches of the one or more activated channels. 23. the at least one non-transitory computer readable storage medium of claim 22, wherein the one or more local patches are to be determined based on their ability to differentiate features. 24. the at least one non-transitory computer readable storage medium of claim 19, wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching. 25. an apparatus comprising: means for modeling an input image to yield a multi-channel image, means for extracting a set of features based on the multi-channel image, means for selecting one or more features from the set of features of the multi-channel image, wherein the one or more features are selected based on an ability to differentiate features, means for matching the one or more features to a learned feature set, and means for determining whether the one or more features meet a pre-defined similarity threshold.1. a method for controlling a terminal, the terminal comprising a capturing apparatus and at least one processor, the method comprising: acquiring, by the capturing apparatus, an image; obtaining, by the at least one processor, a motion parameter of the terminal, the motion parameter comprising at least one of a motion frequency or a motion time, and two or more parameters from among an acceleration, an angular velocity, a motion amplitude, the motion frequency, and the motion time; transmitting, by the at least one processor, a parameter threshold obtaining request to a data management server, the parameter threshold obtaining request comprising configuration information of the terminal; receiving corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request; comparing the two or more parameters with the corresponding preset thresholds; and controlling, by the at least one processor, not to perform image processing on the acquired image based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds, wherein the acquiring comprises acquiring the image in real time, and the obtaining comprises obtaining the motion parameter of the terminal in real time, the method further comprising: in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, obtaining the motion parameter of the terminal again; and in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds, performing the image processing on the image acquired at the latest time. 2. the method according to claim 1, wherein the acquiring comprises: controlling, by the at least one processor, to turn on the capturing apparatus based on a face recognition instruction; and acquiring, by the capturing apparatus, a face image when the capturing apparatus is turned on. 3. the method according to claim 2, wherein the controlling not to perform the image processing comprises: skipping performing face recognition on the acquired face image based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds. 4. the method according to claim 1, wherein the obtaining comprises at least one of: obtaining the acceleration of the terminal by using an acceleration sensor; or obtaining the angular velocity of the terminal by using a gyro sensor. 5. the method according to claim 1, wherein the transmitting comprises: transmitting the parameter threshold obtaining request to the data management server according to a preset time period. 6. the method according to claim 1, further comprising: generating prompt information based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, the prompt information being used for prompting the terminal to stop moving. 7. the method according to claim 1, wherein the motion parameter comprises the motion frequency and the motion time. 8. a terminal comprising: a capturing apparatus; at least one memory configured to store program code; and at least one processor configured to access the at least one memory and operate according to the program code, the program code comprising: motion parameter obtaining code configured to cause the at least one processor to acquire an image by using the capturing apparatus and obtain a motion parameter of the terminal, the motion parameter comprising at least one of a motion frequency or a motion time, and two or more parameters from among an acceleration, an angular velocity, a motion amplitude, the motion frequency, and the motion time; request transmitting code configured to cause the at least one processor to transmit a parameter threshold obtaining request to a data management server, the parameter threshold obtaining request comprising configuration information of the terminal; parameter threshold receiving code configured to cause the at least one processor to receive corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request; comparing code configured to cause the at least one processor to compare the two or more parameters with the corresponding preset thresholds; and control code configured to cause the at least one processor not to perform image processing on the acquired image based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds, wherein the motion parameter obtaining code causes the at least one processor to: acquire the image in real time and obtain the motion parameter of the terminal in real time, and in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, obtain the motion parameter of the terminal again, and wherein the control code causes the at least one processor to, in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds, perform the image processing on the image acquired at the latest time. 9. the terminal according to claim 8, wherein the program code further comprises face instruction receiving code configured to cause the at least one processor to receive a face recognition instruction, wherein the motion parameter obtaining code causes the at least one processor to control, according to the face recognition instruction, the capturing apparatus to turn on, and acquire a face image by using the capturing apparatus when the capturing apparatus is turned on; and wherein the control code causes the at least one processor to skip performing face recognition on the acquired face image based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds. 10. the terminal according to claim 8, wherein the request transmitting code causes the at least one processor to transmit the parameter threshold obtaining request to the data management server according to a preset time period. 11. the terminal according to claim 8, wherein the program code further comprises: prompt information generation code configured to cause the at least one processor to generate prompt information based on at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, the prompt information being used for prompting the terminal to stop moving. 12. the terminal according to claim 8, wherein the motion parameter comprises the motion frequency and the motion time. 13. a non-transitory computer-readable storage medium, storing a machine instruction, which, when executed by one or more processors, causes the one or more processors to perform: obtaining an image acquired by a capturing apparatus; obtaining a motion parameter of a terminal, the terminal comprising the capturing apparatus, the motion parameter comprising at least one of a motion frequency or a motion time, and two or more parameters from among an acceleration, an angular velocity, a motion amplitude, the motion frequency, and the motion time; transmitting a parameter threshold obtaining request to a data management server, the parameter threshold obtaining request comprising configuration information of the terminal; receiving corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request; comparing the two or more parameters with the corresponding preset thresholds; and controlling not to perform image processing on an acquired image based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds, wherein the acquiring comprises acquiring the image in real time, and the obtaining comprises obtaining the motion parameter of the terminal in real time, the method further comprising: in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold, obtaining the motion parameter of the terminal again; and in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds, performing the image processing on the image acquired at the latest time. 14. the non-transitory computer-readable storage medium according to claim 13, wherein the acquired image is a face image and the image processing comprises performing face recognition. 15. the non-transitory computer-readable storage medium according to claim 13, wherein the obtaining the motion parameter comprises at least one of: obtaining the acceleration of the terminal by using an acceleration sensor; or obtaining the angular velocity of the terminal by using a gyro sensor. 16. the non-transitory computer-readable storage medium according to claim 13, wherein the motion parameter comprises the motion frequency and the motion time.1. a method of processing a drive-through order, the method comprising: receiving customer information detected through vision recognition; providing product information to a customer based on the customer information; and processing a product order of the customer. 2. the method according to claim 1, wherein the receiving of customer information comprises at least one of receiving customer information associated with vehicle information detected through vehicle recognition, or receiving customer information associated with identification information detected through face recognition. 3. the method according to claim 1, further comprising determining whether the customer is a pre-order customer based on the customer information, wherein when the customer is determined to be a pre-order customer: the providing of product information based on the customer information comprises providing pre-order information using at least one of audio or video, and the processing of the product order of the customer comprises: providing information for promptly guiding a vehicle to a pickup stand using at least one of audio or video, and providing information that an additional order is available. 4. the method according to claim 1, wherein the product information based on the customer information comprises a most recently ordered product component and a most frequently ordered product component in an order history of the customer information. 5. the method according to claim 1, wherein the receiving of customer information comprises receiving information about an age and gender of a passenger detected through face recognition, and the providing of product information to a customer based on the customer information comprises providing recommended menu information differentiated according to the age and gender. 6. the method according to claim 1, wherein the processing of a product order of the customer comprises determining a product component in a past order history or a component modified from the product component as a product order. 7. the method according to claim 1, wherein the processing of a product order of the customer comprises paying a product price according to biometrics-based authentication through a communication system of a vehicle or a mobile terminal. 8. the method according to claim 1, wherein the processing of a product order of the customer comprises: issuing a payment number for a divided payment, and performing the divided payments according to payment requests of a plurality of mobile terminals to which the payment numbers are inputted. 9. the method according to claim 8, wherein the processing of a product order of the customer further comprises accumulating mileage in an account corresponding to the mobile terminal undergoing a payment. 10. the method according to claim 1, wherein the processing of a product order of the customer further comprises suggesting a takeout packaging method according to a temperature of a product, an atmospheric temperature, weather, and a vehicle type. 11. an apparatus configured to process a drive-through order, the apparatus comprising: a transceiver configured to receive customer information detected through vision recognition; a digital signage configured to provide product information to a customer based on the customer information; and a processor configured to process a product order of the customer. 12. the apparatus according to claim 11, wherein the transceiver receives at least one of customer information associated with vehicle information detected through vehicle recognition, or customer information associated with identification information detected through face recognition. 13. the apparatus according to claim 11, wherein the processor is configured to: determine whether the customer is a pre-order customer based on the customer information; and when the customer is determined to be a pre-order customer, perform a control operation to provide pre-order information, and control the digital signage to output information for promptly guiding a vehicle to a pickup stand and provide information that an additional order is available. 14. the apparatus according to claim 11, wherein the product information based on the customer information comprises a most recently ordered product component and a most frequently ordered product component in an order history of the customer information. 15. the apparatus according to claim 11, wherein the transceiver is configured to receive information about an age and gender of a passenger detected through face recognition, and the processor is configured to control the digital signage to provide recommended menu information differentiated according to the age and gender. 16. the apparatus according to claim 11, wherein the processor is configured to determine a product component in a past order history or a component modified from the product component as the product order. 17. the apparatus according to claim 11, wherein the processor is configured to pay a product price according to biometrics-based authentication through a communication system of a vehicle or a mobile terminal. 18. the apparatus according to claim 11, wherein the processor is configured to: issue a payment number for a divided payment; and perform the divided payments according to requests of a plurality of mobile terminals to which the payment numbers are inputted. 19. the apparatus according to claim 18, wherein the processor is configured to accumulate mileage in an account corresponding to the mobile terminal undergoing a payment. 20. the apparatus according to claim 11, wherein the processor is configured to control the digital signage to suggest a takeout packaging method according to a temperature of a product, an atmospheric temperature, weather, and a vehicle type.1. an image information processing method performed at a computing device having one or more processors and memory storing a plurality of programs to be executed by the one or more processors, the method comprising: identifying, using face recognition, one or more faces, each face corresponding to a respective person captured in a first image; for each identified face: extracting a set of profile parameters of a corresponding person in the first image; and selecting, from a plurality of image tiles, a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile; generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles; and sharing the first image and the second image in a predefined order via a group chat session. 2. the method of claim 1, wherein the first image and the second image are displayed in the group chat session one image at a time such that one of the two images is replaced by the other of the two images periodically. 3. the method of claim 1, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model, wherein the first machine learning model is trained with the facial images and corresponding descriptive labels. 4. the method of claim 1, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining an identity of the corresponding person based on the identified face of the corresponding person; locating respective profile information of the first person based on the determined identity of the corresponding person; and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person. 5. the method of claim 1, wherein at least a first one of the first image tiles is a dynamic image tile and at least a second one of the first image tiles is a static image tile. 6. the method of claim 1, including: receiving a plurality of user comments from different users of the group chat session, each user comment including a descriptive term for a respective person identified in the first image; choosing a descriptive label for the respective person according to the plurality of user comments; and updating the second image by adding the descriptive label adjacent to the first image tile of the respective person. 7. a computing device for image information processing, comprising: one or more processors; and memory storing instructions which, when executed by the one or more processors, cause the processors to perform a plurality of operations comprising: identifying, using face recognition, one or more faces, each face corresponding to a respective person captured in a first image; for each identified face: extracting a set of profile parameters of a corresponding person in the first image; and selecting, from a plurality of image tiles, a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile; generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles; and sharing the first image and the second image in a predefined order via a group chat session. 8. the computing device of claim 7, wherein the first image and the second image are displayed in the group chat session one image at a time such that one of the two images is replaced by the other of the two images periodically. 9. the computing device of claim 7, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model, wherein the first machine learning model is trained with the facial images and corresponding descriptive labels. 10. the computing device of claim 7, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining an identity of the corresponding person based on the identified face of the corresponding person; locating respective profile information of the first person based on the determined identity of the corresponding person; and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person. 11. the computing device of claim 7, wherein at least a first one of the first image tiles is a dynamic image tile and at least a second one of the first image tiles is a static image tile. 12. the computing device of claim 7, wherein the plurality of operations further include: receiving a plurality of user comments from different users of the group chat session, each user comment including a descriptive term for a respective person identified in the first image; choosing a descriptive label for the respective person according to the plurality of user comments; and updating the second image by adding the descriptive label adjacent to the first image tile of the respective person. 13. a non-transitory computer-readable storage medium storing instructions which, when executed by a computing device having one or more processors, cause the computing device to perform a plurality of operations comprising: identifying, using face recognition, one or more faces, each face corresponding to a respective person captured in a first image; for each identified face: extracting a set of profile parameters of a corresponding person in the first image; and selecting, from a plurality of image tiles, a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile; generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles; and sharing the first image and the second image in a predefined order via a group chat session. 14. the non-transitory computer-readable storage medium of claim 13, wherein the first image and the second image are displayed in the group chat session one image at a time such that one of the two images is replaced by the other of the two images periodically. 15. the non-transitory computer-readable storage medium of claim 13, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model, wherein the first machine learning model is trained with the facial images and corresponding descriptive labels. 16. the non-transitory computer-readable storage medium of claim 13, wherein extracting a set of profile parameters of a corresponding person in the first image includes: determining an identity of the corresponding person based on the identified face of the corresponding person; locating respective profile information of the first person based on the determined identity of the corresponding person; and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person. 17. the non-transitory computer-readable storage medium of claim 13, wherein at least a first one of the first image tiles is a dynamic image tile and at least a second one of the first image tiles is a static image tile. 18. the non-transitory computer-readable storage medium of claim 13, wherein the plurality of operations further include: receiving a plurality of user comments from different users of the group chat session, each user comment including a descriptive term for a respective person identified in the first image; choosing a descriptive label for the respective person according to the plurality of user comments; and updating the second image by adding the descriptive label adjacent to the first image tile of the respective person.1. a method comprising, by a computing system: determining that a performance metric of an eye tracking system is below a first performance threshold, wherein the eye tracking system is associated with a head-mounted display worn by a user; based on the determination of the performance metric of the eye tracking system being below the first performance threshold, the computer system performing: receiving one or more first inputs associated with a body of the user; estimating a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user; determining a vergence distance of the user based at least on the one or more first inputs associated with the body of the user, the estimated region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display; and adjusting one or more configurations of the head-mounted display based on the determined vergence distance of the user. 2. the method of claim 1, wherein the one or more configurations of the head-mounted display comprise one or more of: a rendering image; a position of a display screen; or a position of an optics block. 3. the method of claim 1, further comprising: determining that the performance metric of the eye tracking system is above a second performance threshold; receiving eye tracking data from the eye tracking system; and determining the vergence distance of the user based on the eye tracking data and the one or more first inputs associated with the body of the user. 4. the method of claim 3, further comprising: receiving one or more second inputs associated with one or more displaying elements in the scene displayed by the head-mounted display; and determining the vergence distance of the user based at least on the eye tracking data, the one or more first inputs associated with the body of the user, and the one or more second inputs associated with the one or more displaying elements of the scene. 5. the method of claim 4, further comprising: feeding the one or more first inputs associated with the body of the user to a fusion algorithm, wherein the fusion algorithm assigns a weight score to each input of the one or more first inputs; determining the vergence distance of the user using the fusion algorithm based on the one or more first inputs associated with the body of the user; and determining a z-depth of a display screen and a confidence score based on the one or more first inputs associated with the body of the user 6. the method of claim 5, further comprising: comparing the confidence score to a confidence level threshold; in response to a determination that the confidence score is below the confidence level threshold, feeding the one or more second inputs associated with the one or more displaying elements of the scene to the fusion algorithm; and determining the z-depth of the display screen using the fusion algorithm based on the one or more first inputs associated with the body of the user and the one or more second inputs associated with the one or more displaying elements of the scene. 7. the method of claim 6, further comparing: comparing, by the fusion algorithm, confidence scores associated with a plurality of combinations of inputs; and determining, by the fusion algorithm, the z-depth of the display screen based on a combination of inputs associated with a highest confidence score. 8. the method of claim 6, wherein the z-depth and the confidence score are determined by the fusion algorithm using a piecewise comparison of the one or more first inputs and the one or more second inputs. 9. the method of claim 6, wherein the z-depth and the confidence score are determined based on a correlation between two or more inputs of the one or more first inputs and the one or more second inputs. 10. the method of claim 5, wherein the fusion algorithm comprises a machine learning (ml) algorithm, and wherein the machine learning (ml) algorithm determines a combination of first inputs fed to the fusion algorithm. 11. the method of claim 4, wherein the one or more first inputs associated with the body of the user comprise one or more of: a hand position; a hand direction; a hand movement; a hand gesture; a head position; a head direction; a head movement; a head gesture; a gaze angle; rea body gesture; a body posture; a body movement; a behavior of the user; or a weighted combination of one or more related parameters. 12. the method of claim 11, wherein the one or more first inputs associated with the body of the user are received from one or more of: a controller; a sensor; a camera; a microphone; an accelerometer; a headset worn by the user; or a mobile device. 13. the method of claim 4, wherein the one or more second inputs associated with the one or more displaying elements comprise one or more of: a z-buffer value associated with a displaying element; a displaying element marked by a developer; an image analysis result; a shape of a displaying element; a face recognition result; an object recognition result; a person identified in a displaying content; an object identified in a displaying content; a correlation of two or more displaying elements; or a weighted combination of the one or more second inputs. 14. the method of claim 1, further comprising: determining that the performance metric of the eye tracking system is below a second performance threshold; receiving one or more second inputs associated with one or more displaying elements in the scene displayed by the head-mounted display; and determining the vergence distance of the user based at least on the one or more first inputs associated with the body of the user and the one or more second inputs associated with the one or more displaying elements. 15. the method of claim 14, wherein determining that the performance metric of the eye tracking system is below the second performance threshold comprises determining that the eye tracking system does not exist or fails to provide eye tracking data. 16. the method of claim 1, wherein the performance metric of the eye tracking system comprises one or more of: an accuracy of a parameter from the eye tracking system; a precision of a parameter from the eye tracking system; a value of a parameter from the eye tracking system; a detectability of a pupil; a metric based on one or more parameters associated with the user; a parameter change; a parameter changing trend; a data availability; or a weighted combination of one or more performance related parameters. 17. the method of claim 16, wherein the one or more parameters associated with the user comprise one or more of: an eye distance of the user; a pupil position; a pupil status; a correlation of two pupils of the user; a head size of the user; a position of a headset worn by the user; an angle of the headset worn by the user; a direction of the headset worn by the user; an alignment of the eyes of the user; or a weighted combination of one or more related parameters associated with the user. 18. the method of claim 1, wherein the first performance threshold comprises one or more of: a pre-determined value; a pre-determined range; a state of a data; a changing speed of a data; or a trend of a data change. 19. one or more non-transitory computer-readable storage media embodying software that is operable when executed by a computing system to: determine that a performance metric of an eye tracking system is below a first performance threshold, wherein the eye tracking system is associated with a head-mounted display worn by a user; based on the determination of the performance metric of the eye tracking system being below the first performance threshold, the media embodying software operable when executed by the computing system to: receive one or more first inputs associated with a body of the user; estimate a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user; determine a vergence distance of the user based at least on the one or more first inputs associated with the body of the user, the estimated region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display; and adjust one or more configurations of the head-mounted display based on the determined vergence distance of the user. 20. a system comprising: one or more non-transitory computer-readable storage media embodying instructions; one or more processors coupled to the storage media and operable to execute the instructions to: determine that a performance metric of an eye tracking system is below a first performance threshold, wherein the eye tracking system is associated with a head-mounted display worn by a user; based on the determination of the performance metric of the eye tracking system being below the first performance threshold, the system is configured to: receive one or more first inputs associated with a body of the user; estimate a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user; determine a vergence distance of the user based at least on the one or more first inputs associated with the body of the user, the estimated region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display; and adjust one or more configurations of the head-mounted display based on the determined vergence distance of the user.1. a computer-implemented method for image-based, self-guided object detection, comprising: receiving, by a processor device, a set of images, each of the images having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data; training, by the processor device, a grid-based object detector using the grid level label data; determining, by the processor device, a respective bounding box for the respective object in each of the images, by applying local segmentation to each of the images; and training, by the processor device, a region-based convolutional neural network (rcnn) for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the rcnn. 2. the computer-implemented method of claim 1, further comprising performing an action responsive to the object localization and object classification for a respective new object in a new image to which the rcnn is applied. 3. the computer-implemented method of claim 2, wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object. 4. the computer-implemented method of claim 1, wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of images. 5. the computer-implemented method of claim 1, wherein the local segmentation is applied to each of the images to segment a respective target region therein. 6. the computer-implemented method of claim 1, wherein the region-based convolutional neural network (rcnn) forms a model during an object training stage that is to detect objects in new images during an inference stage. 7. the computer-implemented method of claim 1, wherein the method is performed by a system selected from the group consisting of a surveillance system, a face detection system, a face recognition system, a cancer detection system, an object tracking system, and an advanced driver-assistance system. 8. a computer program product for image-based, self-guided object detection, the computer program product comprising a non-transitory computer readable storage medium having program instructions embodied therewith, the program instructions executable by a computer to cause the computer to perform a method comprising: receiving, by a processor device, a set of images, each of the images having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data; training, by the processor device, a grid-based object detector using the grid level label data; determining, by the processor device, a respective bounding box for the respective object in each of the images, by applying local segmentation to each of the images; and training, by the processor device, a region-based convolutional neural network (rcnn) for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the rcnn. 9. the computer program product of claim 8, wherein the method further comprises performing an action responsive to the object localization and object classification for a respective new object in a new image to which the rcnn is applied. 10. the computer program product of claim 9, wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object. 11. the computer program product of claim 8, wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of images. 12. the computer program product of claim 8, wherein the local segmentation is applied to each of the images to segment a respective target region therein. 13. the computer program product of claim 8, wherein the region-based convolutional neural network (rcnn) forms a model during an object training stage that is to detect objects in new images during an inference stage. 14. the computer program product of claim 8, wherein the method is performed by a system selected from the group consisting of a surveillance system, a face detection system, a face recognition system, a cancer detection system, an object tracking system, and an advanced driver-assistance system. 15. a computer processing system for image-based, self-guided object detection, comprising: a memory device for storing program code; and a processor device for running the program code to receive a set of images, each of the images having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data; train a grid-based object detector using the grid level label data; determine a respective bounding box for the respective object in each of the images, by applying local segmentation to each of the images; and train a region-based convolutional neural network (rcnn) for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the rcnn. 16. the computer processing system of claim 15, wherein the processor device further runs the program code to perform an action responsive to the object localization and object classification for a respective new object in a new image to which the rcnn is applied. 17. the computer processing system of claim 16, wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object. 18. the computer processing system of claim 15, wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of images. 19. the computer processing system of claim 15, wherein the region-based convolutional neural network (rcnn) forms a model during an object training stage that is to detect objects in new images during an inference stage. 20. the computer processing system of claim 15, wherein the computer processing system is comprised in a system selected from the group consisting of a surveillance system, a face detection system, a face recognition system, a cancer detection system, an object tracking system, and an advanced driver-assistance system.1. a method of scalable, parallel, cloud-based face recognition utilizing a database of normalized stored images, comprising: capturing an image using a camera; detecting a face in the captured image; normalizing the detected facial image to match the normalized stored images; identifying facial features in the normalized detected facial image; generating a plurality of facial metrics from the facial features; calculating euclidean distances between the facial metrics of the normalized detected facial image with corresponding facial metrics of each of the stored images; comparing each euclidean distance against a predetermined threshold; responsive to the euclidean distance comparison, producing a reduced candidate list of best possible image matches from the normalized stored images; comparing, in parallel, the normalized detected facial image with each of the normalized stored images of the reduced candidate list utilizing a plurality of face recognition algorithms, where each processor of a parallel processing system uses a different face recognition algorithm; responsive to the comparison, producing best match results from each parallel subset of the reduced candidate list; and selecting a final match from the best match results using a deep learning neural network face recognition algorithm trained on outputs of individual face recognition algorithms. 2. the method of scalable, parallel, cloud-based face recognition of claim 1, wherein detecting a face in the captured image comprises: utilizing opencv to detect a face in the captured image; extracting the location of the eyes and a tip of the nose in the face; determining a distance between the eyes; cropping the face from the captured image, where the width and the height of a cropped face image is a function of the distance between the eyes; and rotating the face by an angle of rotation that is a function of the distance between the eyes. 3. the method of scalable, parallel, cloud-based face recognition of claim 2, wherein: the width of the cropped face image is 2.5 times the distance between the eyes; the height of the cropped face image is 3.5 times the distance between the eyes; and the angle of rotation is an angle formed by a straight line joining the eyes and an x-axis of the face. 4. the method of scalable, parallel, cloud-based face recognition of claim 3, wherein rotating the face comprises rotating the face to provide a frontal face pattern. 5. the method of scalable, parallel, cloud-based face recognition of claim 4, further comprising the step of proportionally rescaling the cropped and rotated image. 6. the method of scalable, parallel, cloud-based face recognition of claim 5, where the proportional rescaling yields a cropped and rotated image with a size of 100=100 pixels. 7. the method of scalable, parallel, cloud-based face recognition of claim 1, wherein the facial features identified in the normalized detected facial image comprise a pair of eyes, a tip of a nose, a mouth, a center of the mouth, and a chin area comprising a bottom, a top left landmark, and a top right landmark. 8. the method of scalable, parallel, cloud-based face recognition of claim 7, wherein generating a plurality of facial metrics comprises: calculating a distance between the pair of eyes, a distance between the eyes and the tip of the nose, a distance equal to the width of the mouth, a distance between the tip of the nose and the center of mouth, a distance between the bottom of chin and the center of mouth, a distance between the top left landmark on the chin and the tip of the nose, and a distance between the top right landmark on the chin and the tip of the nose. 9. the method of scalable, parallel, cloud-based face recognition of claim 8, wherein performing a euclidean distance match further comprises: partitioning the normalized stored images into a plurality of substantially equal subsets; performing a euclidean distance match between the facial metrics of the normalized detected facial image and corresponding facial metrics of each of the stored images of the subsets of the normalized stored images with a separate processor of a parallel processing system to generate a euclidean distance for each stored image of the subset; comparing each euclidean distance against a predetermined threshold with the separate processors; responsive to the euclidean distance comparison, producing a reduced candidate list of best possible image matches from the normalized stored images of each subset; and combining the reduced candidate lists from each subset to produce a single reduced candidate list. 10. the method of scalable, parallel, cloud-based face recognition of claim 9, wherein the plurality of face recognition algorithms utilized in comparing, in parallel, the normalized detected facial image with each of the normalized stored images of the reduced candidate list, consists of face recognition algorithms selected from a group consisting of principle component analysis (pca)-based algorithms, linear discriminant analysis (lda) algorithms, independent component analysis (ica) algorithms, kernel-based algorithms, feature-based techniques, algorithms based on neural networks, algorithms based on transforms, and model-based face recognition algorithms. 11. the method of scalable, parallel, cloud-based face recognition of claim 10, wherein the pca-based algorithms include eigenfaces for face detection/recognition, and the lda algorithms include the fisherfaces method of face recognition. 12. the method of scalable, parallel, cloud-based face recognition of claim 1, wherein comparing, in parallel, the captured image with each of the normalized stored images of the reduced candidate list further comprises: partitioning the reduced candidate list into a plurality of substantially equal subsets; processing each subset in a different processor of the parallel processing system uses a unique face recognition algorithm to produce the best match results; and using a reduce function of a mapreduce program to combine the best match results from each of the subsets to produce a single set of the best match results. 13. the method of scalable, parallel, cloud-based face recognition of claim 12, wherein partitioning the reduced candidate list comprises: selecting the images comprising each subset by optimizing the variance between of each of the images according to the following equation: where m and n are the number of rows and columns of the face vector image, n is the number of groups, and σij is the standard deviation of image dimension i in the group j of the face image vector. 14. the method of scalable, parallel, cloud-based face recognition of claim 13, wherein selecting the images comprising each subset by optimizing the variance between each of the images according to the following equation: d(μi, μj) is the euclidean distance between the mean of the group i and the mean of group j, i is the face image vector, and l is the number of group levels. 15. the method of scalable, parallel, cloud-based face recognition of claim 1, where selecting a final match from the best match results utilizing a deep learning neural network face recognition algorithm comprises utilizing either an adaboost machine-learning algorithm or a neural networks machine-learning model. 16. the method of scalable, parallel, cloud-based face recognition of claim 1, where normalizing the detected facial image to match the normalized stored images includes normalizing the detected facial image to the same size, orientation, and illumination of the normalized stored images. 17. a non-transitory computer-readable medium containing executable program instructions for causing a computer to perform a method of face recognition, the method comprising: detecting a face in an image captured by a camera; normalizing the detected facial image to match the normalized stored images; identifying facial features in the normalized detected facial image; generating a plurality of facial metrics from the facial features; calculating euclidean distances between the facial metrics of the normalized detected facial image with corresponding facial metrics of each of the stored images; comparing each euclidean distance against a predetermined threshold; responsive to the euclidean distance comparison, producing a reduced candidate list of best possible image matches from the normalized stored images; comparing, in parallel, the captured image with each of the normalized stored images of the reduced candidate list utilizing a plurality of face recognition algorithms, where each processor of a parallel processing system uses a different face recognition algorithm; responsive to the comparison, producing best match results from each parallel subset of the reduced candidate list; and selecting a final match from the best match results using a deep learning neural network face recognition algorithm trained on outputs of individual face recognition algorithms. 18. the non-transitory computer-readable medium containing executable program instructions of claim 17, wherein the plurality of face recognition algorithms utilized in comparing, in parallel, the normalized detected facial image with each of the normalized stored images of the reduced candidate list, consists of face recognition algorithms selected from a group consisting of principle component analysis (pca)-based algorithms, linear discriminant analysis (lda) algorithms, independent component analysis (ica) algorithms, kernel-based algorithms, feature-based techniques, algorithms based on neural networks, algorithms based on transforms, and model-based face recognition algorithms. 19. the non-transitory computer-readable medium containing executable program instructions of claim 18, wherein the pca-based algorithms include eigenfaces for face detection/recognition, and the lda algorithms include the fisherfaces method of face recognition. 20. the non-transitory computer-readable medium containing executable program instructions of claim 17, where selecting a final match from the best match results utilizing a deep learning neural network face recognition algorithm comprises utilizing either an adaboost machine-learning algorithm or a neural networks machine-learning model.1. an imaging device, comprising: a condensing lens; an image sensor configured to detect light passing through the condensing lens and comprising a pixel matrix, wherein the pixel matrix comprises a plurality of phase detection pixel pairs and a plurality of regular pixels; and a processor configured to turn on the phase detection pixel pairs for autofocusing and output autofocused pixel data after completing the autofocusing, divide the autofocused pixel data into a first subframe and a second subframe, calculate image features of at least one of the first subframe and the second subframe, wherein the image features comprise module widths of a finder pattern, and the finder pattern has a predetermined ratio, a harr-like feature, or a gabor feature, and determine an operating resolution of the regular pixels according to the image features calculated from at least one of the first subframe and the second subframe divided from the autofocused pixel data. 2. the imaging device as claimed in claim 1, wherein each of the phase detection pixel pairs comprises: a first pixel and a second pixel; a cover layer covering upon a first region of the first pixel and upon a second region of the second pixel, wherein the first region and the second region are mirror symmetrical to each other; and a microlens aligned with at least one of the first pixel and the second pixel. 3. the imaging device as claimed in claim 2, wherein the first region and the second region are 5% to 95% of an area of a single pixel. 4. the imaging device as claimed in claim 1, wherein the processor is configured to perform the autofocusing using a dual pixel autofocus technique according to pixel data of the phase detection pixel pairs before completing the autofocusing. 5. the imaging device as claimed in claim 1, wherein the processor is configured to divide pixel data of the phase detection pixel pairs into a third subframe and a fourth subframe before completing the autofocusing, and perform the autofocusing according to the third subframe and the fourth subframe. 6. the imaging device as claimed in claim 5, wherein the processor is further configured to calibrate brightness of the third subframe and the fourth subframe to be identical using a shading algorithm. 7. the imaging device as claimed in claim 1, wherein the operating resolution is selected as a first resolution smaller than a number of the regular pixels or as a second resolution larger than the first resolution. 8. the imaging device as claimed in claim 1, wherein the regular pixels are turned off in the autofocusing. 9. the imaging device as claimed in claim 1, wherein a number of the phase detection pixel pairs is smaller than that of the regular pixels. 10. an imaging device, comprising: a condensing lens; an image sensor configured to detect light passing through the condensing lens and comprising a pixel matrix, wherein the pixel matrix comprises a plurality of phase detection pixel pairs and a plurality of regular pixels; and a processor configured to turn on the phase detection pixel pairs for autofocusing and output autofocused pixel data after completing the autofocusing, divide the autofocused pixel data into a first subframe and a second subframe, calculate image features of at least one of the first subframe and the second subframe, wherein the image features comprise module widths of a finder pattern, and the finder pattern has a predetermined ratio, a harr-like feature, or a gabor feature, and select an image decoding or an image recognition using pixel data of the regular pixels according to the image features calculated from at least one of the first subframe and the second subframe divided from the autofocused pixel data. 11. the imaging device as claimed in claim 10, wherein each of the phase detection pixel pairs comprises: a first pixel and a second pixel; a cover layer covering upon a first region of the first pixel and upon a second region of the second pixel, wherein the first region and the second region are mirror symmetrical to each other; and a microlens aligned with at least one of the first pixel and the second pixel. 12. the imaging device as claimed in claim 10, wherein the processor is configured to perform the autofocusing using a dual pixel autofocus technique according to pixel data of the phase detection pixel pairs before completing the autofocusing. 13. the imaging device as claimed in claim 10, wherein the processor is configured to divide the pixel data of the phase detection pixel pairs into a third subframe and a fourth subframe before completing the autofocusing, calibrate brightness of the third subframe and the fourth subframe to be identical using a shading algorithm, and perform the autofocusing according to the third subframe and the fourth subframe. 14. the imaging device as claimed in claim 10, wherein the processor is configured to calculate the image features using at least one of a rule based algorithm and a machine learning algorithm. 15. the imaging device as claimed in claim 10, wherein the image decoding is decoding qr codes, and the image recognition is face recognition. 16. an operating method of an imaging device, the imaging device comprising a plurality of phase detection pixel pairs and a plurality of regular pixels, the operating method comprising: turning on the phase detection pixel pairs for autofocusing and outputting autofocused image frame after completing the autofocusing; dividing the autofocused image frame, acquired by the phase detection pixel pairs, into a first subframe and a second subframe; calculating image features of at least one of the first subframe and the second subframe, wherein the image feature comprise module widths of a finder pattern, and the finder pattern has a predetermined ratio, a harr-like feature, or a gabor feature; and selectively activating at least a part of the regular pixels according to the image features calculated from at least one of the first subframe and the second subframe divided from the autofocused image frame. 17. the operating method as claimed in claim 16, wherein the selectively activating comprises: activating a first part of the regular pixels to perform an image decoding according to pixel data of the first part of the regular pixels; or activating all the regular pixels to perform an image recognition according to pixel data of the all regular pixels. 18. the operating method as claimed in claim 17, wherein pixel data of the phase detection pixel pairs captured in a same frame with the pixel data of the regular pixels is also used in performing the image decoding and the image recognition. 19. the operating method as claimed in claim 17, wherein the image decoding is decoding qr codes, and the image recognition is face recognition. 20. the operating method as claimed in claim 16, wherein the phase detection pixel pairs are partially covered pixels or have a structure of dual pixel.1. an apparatus comprising: a first camera module configured to obtain a first image of an object with a first field of view; a second camera module configured to obtain a second image of the object with a second field of view different from the first field of view; a first depth map generator configured to generate a first depth map of the first image based on the first image and the second image; and a second depth map generator configured to generate a second depth map of the second image based on the first image, the second image, and the first depth map. 2. the apparatus of claim 1, wherein the first field of view is a narrow angle and the second field of view is a wider angle. 3. the apparatus of claim 2, wherein the second image is divided into a primary region and a residual region, and the second depth map generator comprises: a relationship estimating module configured to estimate a relationship between the primary region and the residual region based on the first image and the second image; and a depth map estimating module configured to estimate a depth map of the residual region based on the estimated relationship and the first depth map. 4. the apparatus of claim 3, wherein at least one of the relationship estimating module and the depth map estimating module performs an estimating operation based on a neural network module. 5. the apparatus of claim 1, further comprising: a depth map fusion unit configured to generate a third depth map of the second image by performing a fusion operation based on the first depth map and the second depth map. 6. the apparatus of claim 5, wherein the depth map fusion unit comprises: a tone mapping module configured to generate a tone-mapped second depth map to correspond to the first depth map by performing a bias removing operation on the second depth map; and a fusion module configured to generate the third depth map by fusing the tone-mapped second depth map and the first depth map. 7. the apparatus of claim 6, wherein the depth map fusion unit further comprises a propagating module configured to generate a propagated first depth map in the second image by iterated propagating of the first depth map based on the first depth map and the second image, and the fusion module generates the third depth map by fusing the tone-mapped second depth map and the propagated first depth map. 8. the apparatus of claim 6, wherein the depth map fusion unit further comprises a post-processing module configured to perform a post-processing operation on the third depth map generated by the fusion module to provide the post-processed third depth map. 9. the apparatus of claim 8, wherein the post-processing module performs the post-processing operation by filtering an interface generated in the third depth map in accordance with fusion of the fusion module. 10. the apparatus of claim 8, wherein the post-processing module removes artifacts generated in the third depth map in accordance with fusion of the fusion module. 11. the apparatus of claim 1, wherein the first depth map generator analyses a distance relationship between the first image and the second image, and generates a first depth map of the first image based on the distance relationship. 12. a method of processing an image of an electronic apparatus, the method comprising: obtaining a first image of an object using a first camera module; obtaining a second image of the object using a second camera module; generating a first depth map of the first image based on the first image and the second image; estimating a relationship between a primary region of the second image and a residual region of the second image based on the first image and the second image; and generating a second depth map of the second image based on the estimated relationship between the primary region and the residual region, and the first depth map. 13. the method of claim 12, wherein the electronic apparatus comprises a first camera module including a first lens having a first field of view and a second camera module including a second lens having a second field of view wider than the first field of view. 14. the method of claim 13, wherein the generating of the second depth map comprises: estimating a depth map of the residual region based on the estimated relationship between the primary region and the residual region, and the first depth map; and generating the second depth map based on a depth map of the residual region and the first depth map. 15. the method of claim 12, wherein the estimating of the relationship between a primary region of the second image is performed using a neural network model. 16. the method of claim 12, further comprising: performing a pre-processing operation on the second depth map; and generating a third depth map of the residual image by fusing the second depth map on which the pre-processing operation is performed and the first depth map. 17. the method of claim 16, wherein the performing of the pre-processing operation comprises performing a tone mapping operation between a depth map of the primary region and a depth map of the residual region based on the second depth map. 18. an operating method for an electronic apparatus, the electronic apparatus including; a first camera module providing a first image of an object using a first field of view and a second camera module providing a second image of the object using second field of view wider than the first field of view, and a processor generating a depth map of the second image based on a primary region of the second image and a residual region of the second image, the operating method comprising: generating a first depth map of the primary region by estimating a relationship between the first image and the second image; estimating a relationship between the primary region and the residual region based on the first image and the second image; generating a second depth map of the second image by estimating a depth map of the second region based on the estimated relationship between the primary region and the residual region; and generating a depth map of the second image by fusing the first depth map and the second depth map. 19. the operation method of claim 18, further comprising: executing an application that applies an image effect to the second image based on a depth map of the residual image. 20. the operation method of claim 19, wherein the application applies at least one image effect of auto-focusing, out-focusing, fore/background separation, face recognition, object detection within a frame, and augmented reality to the second image based on a depth map of the second image.1. a payment method based on a face recognition, comprising: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including: determining whether an angle of rotation in each preset direction is less than an angle threshold, wherein the head posture information includes the angle of rotation in each preset direction; determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold, wherein the gaze information includes the probability value that a user gazes at a payment screen; and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold, determining that the target user has a willingness to pay; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition. 2. the method as claimed in claim 1, wherein the completing a payment operation based on the face recognition comprises: triggering and performing a payment initiating operation to acquire second face image information based on the face recognition; determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay; and in response to determining that the second characteristic information indicates that the user has a willingness to pay, triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user. 3. the method as claimed in claim 2, wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises: determining whether a current user corresponding to the second face image information is consistent with the target user; and in response to determining that the current user is consistent with the target user, determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information. 4. the method as claimed in claim 1, wherein the extracting first characteristic information from the first face image information comprises: determining the head posture information of the target user using a head posture recognition model based on the first face image information; and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information. 5. the method as claimed in claim 4, wherein the head posture recognition model is obtained through training by: acquiring a first sample data set, wherein the first sample data set includes a plurality of pieces of first sample data, and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information; determining mean image data and variance image data of a plurality of sample face images; for each of the plurality of pieces of first sample data, preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image; setting the preprocessed sample face image and the corresponding head posture information as a first model training sample; and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model. 6. the method as claimed in claim 4, wherein the gaze information recognition model is obtained through training by: acquiring a second sample data set, wherein the second sample data set includes a plurality of pieces of second sample data, and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information; determining mean image data and variance image data of a plurality of sample eye images; for each of the plurality of pieces of second sample data, preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image; setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample; and performing training using a machine learning method and based on a plurality of second model training samples to obtain the gaze information recognition model. 7. the method as claimed in claim 1, wherein the angle of rotation in each preset direction comprises a pitch angle, a yaw angle, and a roll angle, wherein the pitch angle refers to an angle of rotation around a x-axis, the yaw angle refers to an angle of rotation around a y-axis, and the roll angle refers to an angle of rotation around a z-axis. 8. a payment device based on a face recognition, comprising: a processor, and a non-transitory computer-readable storage medium storing instructions executable by the processor to cause the device to perform operations comprising: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including: determining whether an angle of rotation in each preset direction is less than an angle threshold, wherein the head posture information includes the angle of rotation in each preset direction; determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold, wherein the gaze information includes the probability value that a user gazes at a payment screen; and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold, determining that the target user has a willingness to pay; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition. 9. the device as claimed in claim 8, wherein the completing a payment operation based on the face recognition comprises: triggering and performing a payment initiating operation to acquire second face image information based on the face recognition; determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay; and in response to determining that the second characteristic information indicates that the user has a willingness to pay, triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user. 10. the device as claimed in claim 9, wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises: determining whether a current user corresponding to the second face image information is consistent with the target user; and in response to determining that the current user is consistent with the target user, determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information. 11. the device as claimed in claim 8, wherein the extracting first characteristic information from the first face image information comprises: determining the head posture information of the target user using a head posture recognition model based on the first face image information; and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information. 12. the device as claimed in claim 11, wherein the head posture recognition model is obtained through training by: acquiring a first sample data set, wherein the first sample data set includes a plurality of pieces of first sample data, and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information; determining mean image data and variance image data of a plurality of sample face images; for each of the plurality of pieces of first sample data, preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image; setting the preprocessed sample face image and the corresponding head posture information as a first model training sample; and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model. 13. the device as claimed in claim 11, wherein the gaze information recognition model is obtained through training by: acquiring a second sample data set, wherein the second sample data set includes a plurality of pieces of second sample data, and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information; determining mean image data and variance image data of a plurality of sample eye images; for each of the plurality of pieces of second sample data, preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image; setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample; and performing training using a machine learning method and on a plurality of second model training samples to obtain the gaze information recognition model. 14. the device as claimed in claim 11, wherein the angle of rotation in each preset direction comprises a pitch angle, a yaw angle, and a roll angle, wherein the pitch angle refers to an angle of rotation around a x-axis, the yaw angle refers to an angle of rotation around a y-axis, and the roll angle refers to an angle of rotation around a z-axis. 15. a non-transitory computer-readable storage medium for a payment based on a face recognition, configured with instructions executable by one or more processors to cause the one or more processors to perform operations comprising: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user, and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including: determining whether an angle of rotation in each preset direction is less than an angle threshold, wherein the head posture information includes the angle of rotation in each preset direction; determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold, wherein the gaze information includes the probability value that a user gazes at a payment screen; and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold, determining that the target user has a willingness to pay; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition. 16. the storage medium as claimed in claim 15, wherein the completing a payment operation based on the face recognition comprises: triggering and performing a payment initiating operation to acquire second face image information based on the face recognition; determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay; and in response to determining that the second characteristic information indicates that the user has a willingness to pay, triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user. 17. the storage medium as claimed in claim 16, wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises: determining whether a current user corresponding to the second face image information is consistent with the target user; and in response to determining that the current user is consistent with the target user, determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information. 18. the storage medium as claimed in claim 15, wherein the extracting first characteristic information from the first face image information comprises: determining the head posture information of the target user using a head posture recognition model based on the first face image information; and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information. 19. the storage medium as claimed in claim 18, wherein the head posture recognition model is obtained through training by: acquiring a first sample data set, wherein the first sample data set includes a plurality of pieces of first sample data, and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information; determining mean image data and variance image data of a plurality of sample face images; for each of the plurality of pieces of first sample data, preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image; setting the preprocessed sample face image and the corresponding head posture information as a first model training sample; and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model; and wherein the gaze information recognition model is obtained through training by: acquiring a second sample data set, wherein the second sample data set includes a plurality of pieces of second sample data, and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information; determining mean image data and variance image data of a plurality of sample eye images; for each of the plurality of pieces of second sample data, preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image; setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample; and performing training using a machine learning method and based on a plurality of second model training samples to obtain the gaze information recognition model. 20. the storage medium as claimed in claim 18, wherein the angle of rotation in each preset direction comprises a pitch angle, a yaw angle, and a roll angle, wherein the pitch angle refers to an angle of rotation around a x-axis, the yaw angle refers to an angle of rotation around a y-axis, and the roll angle refers to an angle of rotation around a z-axis.1. a method comprising: detecting, by a motion detection module, a motion by a subject within a predetermined area of view; assigning a unique session identification number to the subject detected within a predetermined area of view; detecting a facial area of the subject detected within a predetermined area of view; generating an image of the facial area of the subject; assessing a quality of the image of the facial area of the subject; determining an identity of the subject based on the image of the facial area of the subject; identifying an intent of the subject; and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject. 2. the method of claim 1, further comprising: determining one or more additional subjects within the predetermined area of view; and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view. 3. the method of claim 1, wherein the assessing a quality of the image of the facial area of the subject comprises: assessing whether the quality of the image of the facial area of the object equates predetermined metric of quality; and upon determining that the quality of the image of the facial area of the object is inferior to the predetermined metric of quality, discarding the image of the facial area of the subject and generating a second image of the facial area of the subject. 4. the method of claim 1, further comprising: detecting whether the facial area of the subject is photographic image; and upon detecting that the facial area of the subject is a photographic image, generating a warning and restrict access to the point of entry. 5. the method of claim 1, further comprising: conducing an incremental training of the image of the facial area of the subject. 6. the method of claim 5, wherein conducing an incremental training of the image of the facial area of the subject comprises: capturing a first image of the facial area having facial landmarks; converting the first image of the facial area into a first numeric vector; capturing a second image of the facial area having facial landmarks; converting the second image of the facial area into a second numeric vector; calculating a weighted mean of the first numeric vector and the second numeric vector, wherein the weighted mean represents a change in a facial area; and storing the weighted mean in the database. 7. the method of claim 1, wherein determining an identity of the subject based on the image of the facial area of the subject comprises: comparing the image of the facial area of the subject with a plurality of images stored in a database; and authenticating the subject. 8. the method of claim 1, wherein identifying an intent of the subject comprises: upon detecting the facial area in a bounding box, commencing authentication of the subject; calculating a directional vector of a face of the subject; determine an intent of the subject to gain access to the point of entry based on the directional vector of the face of the subject; granting the access to the point of entry based on authentication of the subject and based on determining the intent of the subject. 9. a non-transitory computer readable medium having program instructions stored thereon, that in response to execution by a computing device cause the computing device to perform operations comprising: detecting a motion by a subject within a predetermined area of view; assigning a unique session identification number to the subject detected within a predetermined area of view; detecting a facial area of the subject detected within a predetermined area of view; generating an image of the facial area of the subject; assessing a quality of the image of the facial area of the subject; determining an identity of the subject based on the image of the facial area of the subject; identifying an intent of the subject; and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject. 10. the non-transitory computer readable medium of claim 9, further comprising: determining one or more additional subjects within the predetermined area of view; and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view. 11. the non-transitory computer readable medium of claim 9, wherein the assessing a quality of the image of the facial area of the subject comprises: assessing whether the quality of the image of the facial area of the object equates predetermined metric of quality; and upon determining that the quality of the image of the facial area of the object is inferior to the predetermined metric of quality, discarding the image of the facial area of the subject and generating a second image of the facial area of the subject. 12. the non-transitory computer readable medium of claim 9, further comprising: detecting whether the facial area of the subject is photographic image; and upon detecting that the facial area of the subject is a photographic image, generating a warning and restrict access to the access point. 13. the non-transitory computer readable medium of claim 9, further comprising: conducing an incremental training of the image of the facial area of the subject. 14. the non-transitory computer readable medium of claim 13, wherein conducing an incremental training of the image of the facial area of the subject comprises: capturing a first image of the facial area having facial landmarks; converting the first image of the facial area into a first numeric vector; capturing a second image of the facial area having facial landmarks; converting the second image of the facial area into a second numeric vector; calculating a weighted mean of the first numeric vector and the second numeric vector, wherein the weighted mean represents a change in a facial area; and storing the weighted mean in the database. 15. an apparatus for face recognition comprising: a processor; and a memory to store computer program instructions, the computer program instructions when executed on the processor cause the processor to perform operations comprising: detecting a motion by a subject within a predetermined area of view; assigning a unique session identification number to the subject detected within a predetermined area of view; detecting a facial area of the subject detected within a predetermined area of view; generating an image of the facial area of the subject; assessing a quality of the image of the facial area of the subject; determining an identity of the subject based on the image of the facial area of the subject; identifying an intent of the subject; and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject. 16. the apparatus of claim 15, further comprising: determining one or more additional subjects within the predetermined area of view; and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view. 17. the apparatus of claim 15, wherein the assessing a quality of the image of the facial area of the subject comprises: assessing whether the quality of the image of the facial area of the object equates predetermined metric of quality; and upon determining that the quality of the image of the facial area of the object is inferior to the predetermined metric of quality, discarding the image of the facial area of the subject and generating a second image of the facial area of the subject. 18. the apparatus of claim 15, further comprising: detecting whether the facial area of the subject is photographic image; and upon detecting that the facial area of the subject is a photographic image, generating a warning and restrict access to the access point. 19. the apparatus of claim 15, further comprising: conducing an incremental training of the image of the facial area of the subject. 20. the apparatus of claim 15, wherein conducing an incremental training of the image of the facial area of the subject comprises: capturing a first image of the facial area having facial landmarks; converting the first image of the facial area into a first numeric vector; capturing a second image of the facial area having facial landmarks; converting the second image of the facial area into a second numeric vector; calculating a weighted mean of the first numeric vector and the second numeric vector, wherein the weighted mean represents a change in a facial area; and storing the weighted mean in the database.1. a robot, comprising: a body configured to rotate and to tilt; a camera coupled to the body and configured to rotate and tilt according to the rotate and the tilt of the body, wherein the camera is configured to acquire a video of a space; a face recognition unit configured to recognize respective faces of one or more persons in the video; a tracking unit configured to track motion of each of the recognized faces of the one or more persons; and a controller configured to: calculate a respective size of each of the faces of the one or more persons; select a first person, from among the one or more persons, based on the calculated sizes of the faces; and control at least one of a direction of the rotation of the camera, an angle of the tilt of the camera and a focal distance of the camera, based on the tracked motion of the recognized face of the first person. 2. the robot of claim 1, wherein the controller is configured to: control the direction of the rotation of the camera and the angle of the tilt of the camera to achieve an particular orientation of the camera relative to the face of the first person; and control a focal distance of the camera by comparing respective sizes of the face of the first person before and after motion of the first person. 3. the robot of claim 2, wherein the particular orientation occurs when the camera faces a general direction of the face of the first person. 4. the robot of claim 1, wherein the controller is configured to: normalize sizes of the faces of the one or more persons based on an interocular distance; and select the first person based on the normalized sizes of the faces of the one or more persons. 5. the robot of claim 1, wherein the controller is configured to: select a person having a largest face size, from among the one or more persons, as the first person. 6. the robot of claim 1, further comprising: a microphone configured to receive a spoken audio that is present in the space; wherein the controller is further configured to select the first person further based on the received spoken audio. 7. the robot of claim 6, wherein the controller is further configured to: control gain of the microphone by comparing respective sizes of the face of the first person before and after motion of the first person. 8. the robot of claim 6, wherein the controller is configured to: calculate a position from which the spoken audio is provided; and select the first person further based on whether the one or more persons are in the position from which the voice signal is provided. 9. the robot of claim 8, wherein the controller is configured to: select a second person as the first person, from among the one or more persons, when the second person is located in the position from which the spoken audio is provided. 10. the robot of claim 8, wherein the controller is configured to: select a second person having a largest face size as the first person, from among the one or more persons, when none of the one or more persons is located in the position from which the spoken audio is provided. 11. the robot of claim 8, wherein the controller is configured to: select a second person having a largest face size as the first person, from among the one or more persons, when a plurality of persons from among the one or more persons are located in the position from which the spoken audio is provided. 12. the robot of claim 1, further comprising: a speaker, wherein the controller is configured to: control volume of the speaker by comparing respective sizes of the face of the first person before and after motion of the first person. 13. the robot of claim 1, wherein the body is further configured to rotate in a lateral direction, and to tilt in an vertical direction. 14. an electronic device, comprising: a camera coupled to the body and configured to rotate and to tilt, wherein the camera is configured to acquire a video of a space within which one or more persons are positioned; and a processor configured to: recognize respective faces of the one or more persons in the video; track motion of each of the recognized faces of the one or more persons; calculate a respective size of each of the faces of the one or more persons; select a first person, from among the one or more persons, based on the calculated sizes of the faces; and control at least one of a direction of the rotation of the camera, an angle of the tilt of the camera and a focal distance of the camera, based on the tracked motion of the recognized face of the first person. 15. a method, comprising: acquiring, by a camera, a video of a space within which one or more persons are positioned; recognizing respective faces of the one or more persons in the video; tracking motion of each of the recognized faces of the one or more persons; calculating a respective size of each of the faces of the one or more persons; selecting a first person, from among the one or more persons, based on the calculated sizes of the faces; and controlling at least one of a direction of rotation of the camera, an angle of tilt of the camera and a focal distance of the camera, based on the tracked motion of the recognized face of the first person.1. a method of inferring topics from a multimodal file, the method comprising: receiving a multimodal file; extracting a set of entities from the multimodal file; linking the set of entities to produce a set of linked entities; obtaining reference information for the set of entities; based at least on the reference information, generating a graph of the set of linked entities, the graph comprising nodes and edges; based at least on the nodes and edges of the graph, determining clusters in the graph; based at least on the clusters in the graph, identifying topic candidates; extracting features from the clusters in the graph; based at least on the extracted features, selecting at least one topicid from among the topic candidates to represent at least one cluster; and indexing the multimodal file with the at least one topicid. 2. the method of claim 1 wherein the multimodal file comprises a video portion and an audio portion and wherein extracting a set of entities from the multimodal file comprises: detecting objects in the video portion of the multimodal file; and detecting text in the audio portion of the multimodal file. 3. the method of claim 2 wherein detecting objects comprises performing face recognition. 4. the method of claim 2 wherein detecting text comprises performing a speech to text process. 5. the method of claim 4 further comprising: identifying a language used in the audio portion of the multimodal file, and wherein performing a speech to text process comprises performing a speech to text process in the identified language. 6. the method of claim 4 further comprising: translating the detected text. 7. the method of claim 1 further comprising: determining significant clusters and insignificant clusters in the determined clusters, and wherein extracting features from the clusters in the graph comprises extracting features from the significant clusters in the graph. 8. the method of claim 1 wherein extracting features from the clusters in the graph comprises at least one process selected from the list consisting of: determining a graph diameter and determining a jaccard coefficient. 9. the method of claim 1 wherein selecting at least one topicid to represent at least one cluster comprises: based at least on the extracted features, mapping topic candidates into a probability interval; and based at least on the mapping, ranking topic candidates within the at least one cluster, and selecting the at least one topicid based at least on the ranking. 10. the method of claim 1 further comprising: translating the at least one topicid, and wherein indexing the multimodal file with the at least one topicid comprises indexing the multimodal file with the at least one translated topicid. 11. a system for inferring topics from a multimodal file, the system comprising: an entity extraction component comprising an object detection component and a speech to text component, operative to extract a set of entities from a multimodal file comprising a video portion and an audio portion; an entity linking component operative to link the extracted set of entities to produce a set of linked entities; an information retrieval component operative to obtain reference information for the extracted set of entities; a graphing and analysis component operative to: generate a graph of the set of linked entities, the graph comprising nodes and edges; based at least on the nodes and edges of the graph, determine clusters in the graph; based at least on the clusters in the graph, identify topic candidates; and extract features from the clusters in the graph; a topicid selection component operative to: rank the topic candidates within at least one cluster; and based at least on the ranking, select at least one topicid from among the topic candidates to represent at least one cluster; and a video indexer operative to index the multimodal file with the at least one topicid. 12. the system of claim 11 wherein the object detection component is operative to perform face recognition. 13. the system of claim 11 wherein the speech to text component is operative to extract entity information in at least two different languages. 14. one or more computer storage devices having computer-executable instructions stored thereon for inferring topics from a multimodal file, which, on execution by a computer, cause the computer to perform operations comprising: receiving a multimodal file comprising a video portion and an audio portion; extracting a set of entities from the multimodal file, wherein extracting a set of entities from the multimodal file comprises: detecting objects in the video portion of the multimodal file with face recognition; detecting text in the audio portion of the multimodal file with a speech to text process; and disambiguating among a set of detected entity names; linking the set of entities to produce a set of linked entities; obtaining reference information for the set of entities; based at least on the reference information, generating a graph of the set of linked entities, the graph comprising nodes and edges; based at least on the nodes and edges of the graph, determining clusters in the graph; determining significant clusters and insignificant clusters in the determined clusters; based at least on the significant clusters in the graph, identifying topic candidates; extracting features from the significant clusters in the graph; based at least on the extracted features, mapping the topic candidates into a probability interval; based at least on the mapping, ranking the topic candidates within at least one significant cluster, based on the ranking, selecting at least one topicid from among the topic candidates to represent the at least one significant cluster; and indexing the multimodal file with the at least one topicid. 15. the one or more computer storage devices of claim 14 wherein the operations further comprise: identifying a language used in the audio portion of the multimodal file, and detecting text in the audio portion of the multimodal file with a speech to text process comprises performing a speech to text process in the identified language.权利要求 1、 一种人脸识别方法,其特征在于,包括: 通过第一摄像头获取第一人脸图像; 提取所述第一人脸图像的第一人脸特征; 将所述第一人脸特征与预先存储的第二人脸特征进行对比,获得参考相似度,所述第 二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得,所述第二摄像头与所述第 一摄像头属于不同类型的摄像头; 根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否对应相同人。 2、 根据权利要求 1所述的方法,其特征在于, 所述第一摄像头为热成像摄像头,所述第二摄像头为可见光摄像头; 或者,所述第一摄像头为可见光摄像头,所述第一摄像头为热成像摄像头。 3、 根据权利要求 1或 2所述的方法,其特征在于,所述根据所述参考相似度确定所 述第一人脸特征与所述第二人脸特征是否对应相同人,包括: 根据所述参考相似度、 参考误报率以及相似度阈值确定所述第一人脸特征与所述第二 人脸特征是否对应相同人;其中,不同的误报率对应不同的相似度阈值。 4、 根据权利要求 1或 2所述的方法,其特征在于,所述根据所述参考相似度确定所 述第一人脸特征与所述第二人脸特征是否对应相同人,包括: 根据所述参考相似度以及阈值信息确定归一化后的参考相似度; 根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否对 应相同人。 5、 根据权利要求 1-4任一项所述的方法,其特征在于,所述提取所述第一人脸图像的 第_人脸特征,包括: 将所述第一人脸图像输入预先训练完成的神经网络,通过所述神经网络输出所述第一 人脸图像的第一人脸特征;其中,所述神经网络基于第一类型图像样本和第二类型图像样 本训练得到,所述第一类型图像样本和所述第二类型图像样本由不同类型的摄像头拍摄得 到,且所述第一类型图像样本和所述第二类型图像样本中包括人脸。 6、 根据权利要求 5 所述的方法,其特征在于,所述神经网络基于所述第一类型图像 样本、 所述第二类型图像样本和混合类型图像样本训练得到,所述混合类型图像样本由所 述第一类型图像样本和所述第二类型图像样本配对而得。 1、 根据权利要求 1-6任一项所述的方法,其特征在于,所述第一摄像头包括车载摄像 头,所述通过第一摄像头获取第一人脸图像,包括: 通过所述车载摄像头获取所述第一人脸图像,所述第一人脸图像包括车辆的用车人的 人脸图像。 8、 根据权利要求 7 所述的方法,其特征在于,所述用车人包括驾驶所述车辆的人、 乘坐所述车辆的人、 对所述车辆进行修理的人、 给所述车辆加油的人以及控制所述车辆的 人中的一项或多项。 9、 根据权利要求 7 所述的方法,其特征在于,所述用车人包括驾驶所述车辆的人, 所述通过所述车载摄像头获取所述第一人脸图像,包括: 在接收到触发指令的情况下,通过所述车载摄像头获取所述第一人脸图像; 或者,在所述车辆运行时,通过所述车载摄像头获取所述第一人脸图像; 或者,在所述车辆的运行速度达到参考速度的情况下,通过所述车载摄像头获取所述 第一人脸图像。 10、 根据权利要求 7-9任一项所述的方法,其特征在于,所述第二人脸图像为对所述 用车人进行人脸注册的图像,所述将所述第一人脸特征与预先存储的第二人脸特征进行对 比之前,所述方法还包括: 通过所述第二摄像头获取所述第二人脸图像; 提取所述第二人脸图像的第二人脸特征; 保存所述第二人脸图像的第二人脸特征。 11、 一种神经网络训练方法,其特征在于,包括: 获取第一类型图像样本和第二类型图像样本,所述第一类型图像样本和所述第二类型 图像样本由不同类型的摄像头拍摄得到,且所述第一类型图像样本和所述第二类型图像样 本中包括人脸; 根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。 12、 根据权利要求 11所述的方法,其特征在于,所述根据所述第一类型图像样本和所 述第二类型图像样本训练神经网络,包括: 将所述第一类型图像样本和所述第二类型图像样本配对,得到所述第一类型图像样本 和所述第二类型图像样本的混合类型图像样本; 根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类型图像样本,训练 所述神经网络。 13、 根据权利要求 12 所述的方法,其特征在于,所述根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类型图像样本,训练所述神经网络,包括: 通过所述神经网络获取所述第一类型图像样本的人脸预测结果、 所述第二类型图像样 本的人脸预测结果和所述混合类型图像样本的人脸预测结果; 根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、 所述第二类型图 像样本的人脸预测结果和人脸标注结果之间的差异、 以及所述混合类型图像样本的人脸预 测结果和人脸标注结果的差异,训练所述神经网络。 14、 根据权利要求 13 所述的方法,其特征在于,所述神经网络中包括第一分类器、 第二分类器和混合分类器,所述通过所述神经网络获取所述第一类型图像样本的人脸预测 结果、 所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果, 包括: 将所述第一类型图像样本的人脸特征输入至所述第一分类器中,得到所述第一类型图 像样本的人脸预测结果; 将所述第二类型图像样本的人脸特征输入至所述第二分类器中,得到所述第二类型图 像样本的人脸预测结果; 将所述混合类型图像样本的人脸特征输入至所述混合分类器中,得到所述混合类型图 像样本的人脸预测结果。 15、 根据权利要求 14所述的方法,其特征在于,所述方法还包括: 在训练完成的所述神经网络中去除所述第一分类器、 所述第二分类器和所述混合分类 器,得到用于进行人脸识别的神经网络。 16、 一种人脸识别装置,其特征在于,包括: 第一获取单元,用于通过第一摄像头获取第一人脸图像; 第一提取单元,用于提取所述第一人脸图像的第一人脸特征; 对比单元,用于将所述第一人脸特征与预先存储的第二人脸特征进行对比,获得参考 相似度,所述第二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得,所述第二 摄像头与所述第一摄像头属于不同类型的摄像头; 确定单元,用于根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否 对应相同人。 17、 根据权利要求 16所述的装置,其特征在于, 所述第一摄像头为热成像摄像头,所述第二摄像头为可见光摄像头; 或者,所述第一摄像头为可见光摄像头,所述第一摄像头为热成像摄像头。 18、 根据权利要求 16或 17所述的装置,其特征在于, 所述确定单元,具体用于根据所述参考相似度、 参考误报率以及相似度阈值确定所述 第一人脸特征与所述第二人脸特征是否对应相同人;其中,不同的误报率对应不同的相似 度阈值。 19、 根据权利要求 16或 17所述的装置,其特征在于, 所述确定单元,具体用于根据所述参考相似度以及阈值信息确定归一化后的参考相似 度;以及根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否 对应相同人。 20、 根据权利要求 16-19任_项所述的装置,其特征在于, 所述第一提取单元,具体用于将所述第一人脸图像输入预先训练完成的神经网络,通 过所述神经网络输出所述第一人脸图像的第一人脸特征;其中,所述神经网络基于第一类 型图像样本和第二类型图像样本训练得到,所述第一类型图像样本和所述第二类型图像样 本由不同类型的摄像头拍摄得到,且所述第一类型图像样本和所述第二类型图像样本中包 括人脸。 21、 根据权利要求 20 所述的装置,其特征在于,所述神经网络基于所述第一类型图 像样本、 所述第二类型图像样本和混合类型图像样本训练得到,所述混合类型图像样本由 所述第一类型图像样本和所述第二类型图像样本配对而得。 22、 根据权利要求 16-21任一项所述的装置,其特征在于,所述第一摄像头包括车载 摄像头, 所述第一获取单元,具体用于通过所述车载摄像头获取所述第一人脸图像,所述第一 人脸图像包括车辆的用车人的人脸图像。 23、 根据权利要求 22所述的装置,其特征在于,所述用车人包括驾驶所述车辆的人、 乘坐所述车辆的人、 对所述车辆进行修理的人、 给所述车辆加油的人以及控制所述车辆的 人中的一项或多项。 24、 根据权利要求 22所述的装置,其特征在于,所述用车人包括驾驶所述车辆的人, 所述第一获取单元,具体用于在接收到触发指令的情况下,通过所述车载摄像头获取所述 第一人脸图像; 或者,所述第一获取单元,具体用于在所述车辆运行时,通过所述车载摄像头获取所 述第 _人脸图像; 或者,所述第一获取单元,具体用于在所述车辆的运行速度达到参考速度的情况下, 通过所述车载摄像头获取所述第一人脸图像。 25、 根据权利要求 22-24任一项所述的装置,其特征在于,所述第二人脸图像为对所 述用车人进行人脸注册的图像,所述装置还包括: 第二获取单元,用于通过所述第二摄像头获取所述第二人脸图像; 第二提取单元,用于提取所述第二人脸图像的第二人脸特征; 保存单元,用于保存所述第二人脸图像的第二人脸特征。 26、 一种神经网络训练装置,其特征在于,包括: 获取单元,用于获取第一类型图像样本和第二类型图像样本,所述第一类型图像样本 和所述第二类型图像样本由不同类型的摄像头拍摄得到,且所述第一类型图像样本和所述 第二类型图像样本中包括人脸; 训练单元,用于根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。 27、 根据权利要求 26所述的装置,其特征在于,所述训练单元包括: 配对子单元,用于将所述第一类型图像样本和所述第二类型图像样本配对,得到所述 第一类型图像样本和所述第二类型图像样本的混合类型图像样本; 训练子单元,用于根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类 型图像样本,训练所述神经网络。 28、 根据权利要求 27所述的装置,其特征在于, 所述训练子单元,具体用于通过所述神经网络获取所述第一类型图像样本的人脸预测 结果、 所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果; 以及根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、 所述第二类型图 像样本的人脸预测结果和人脸标注结果之间的差异、 以及所述混合类型图像样本的人脸预 测结果和人脸标注结果的差异,训练所述神经网络。 29、 根据权利要求 28 所述的装置,其特征在于,所述神经网络中包括第一分类器、 第二分类器和混合分类器, 所述训练子单元,具体用于将所述第一类型图像样本的人脸特征输入至所述第一分类 器中,得到所述第一类型图像样本的人脸预测结果;以及将所述第二类型图像样本的人脸 特征输入至所述第二分类器中,得到所述第二类型图像样本的人脸预测结果;以及将所述 混合类型图像样本的人脸特征输入至所述混合分类器中,得到所述混合类型图像样本的人 脸预测结果。 30、 根据权利要求 29所述的装置,其特征在于,所述装置还包括: 神经网络应用单元,用于在训练完成的所述神经网络中去除所述第一分类器、 所述第 二分类器和所述混合分类器,得到用于进行人脸识别的神经网络。 31、 一种电子设备,其特征在于,包括处理器和存储器,所述处理器和所述存储器耦 合;其中,所述存储器用于存储程序指令,所述程序指令被所述处理器执行时,使所述处 理器执行权利要求 1-10任一项所述的方法;和/或,使所述处理器执行权利要求 11-15任一 项所述的方法。 32、 一种计算机可读存储介质,其特征在于,所述计算机可读存储介质中存储有计算 机程序,所述计算机程序包括程序指令,所述程序指令当被处理器执行时,使所述处理器 执行权利要求 1-10任一项所述的方法;和/或,使所述处理器执行权利要求 11-15任一项所 述的方法。1. a system for alerting on vision impairment, said system comprising a processing unit configured and operable for receiving scene data being indicative of a scene of at least one consumer in an environment, identifying in the scene data a certain consumer, identifying an event being indicative of a behavioral compensation for vision impairment, and, upon identification of such an event, sending a notification relating to the vision impairment. 2. the system of claim 1, further comprising at least one sensing unit configured and operable for detecting the scene data. 3. the system of claim 2, wherein said at least one sensing unit comprises at least one of: at least one imaging unit configured and operable for capturing at least one image of at least a portion of a consumer\\'s body, at least one motion detector configured and operable for detecting consumer data being indicative of a motion of a consumer, or at least one eye tracker configured and operable for tracking eye motion of a consumer. 4. the system of claim 3, wherein the at least one imaging unit comprises a plurality of cameras placed at different heights. 5. the system of any one of claims 2 to 4, wherein said sensing unit is accommodated in an optical or digital eyewear frame display. 6. the system of any one of claims 1 to 5, wherein said processing unit is configured and operable for identifying a consumer\\'s condition, said consumer\\'s condition comprising consumer data being indicative of the consumer\\'s position and location relative to at least one object in the consumer\\'s environment; said consumer data comprises at least one of a consumer\\'s face, eyewear, posture, position, sound or motion. 7. the system of any one of claims 1 to 6, wherein said event comprises at least one position and orientation of head increase or decrease of viewing distance between the consumer and viewed object and changing the position of eyeglasses worn by the consumer. 8. the system of any one of claims 1 to 7, wherein said event is identified by identifying images having an image feature being indicative of behavioral compensation, performing a bruckner test, performing a hirschberg test, and measuring blink count/ frequency. 9. the system of claim 8, wherein the image feature being indicative of behavioral compensation comprises squinting, head orientation, certain distances between an object and consumer\\'s eyes, certain position of eyeglasses on the consumer\\'s face, strabismus, cataracts, and reflections from the eye. 10. the system of any one of claims 1 to 9, wherein the notification includes at least one of the data indicative of the identified event, data indicative of the identified consumer, ophthalmologic recommendations based on the identified event, or lack of events, or an appointment for a vision test. 11. the system of any one of claims 1 to 10, wherein said processing unit comprises a memory for storing at least one of a reference data indicative of behavioral compensation for vision impairment, data indicative of the notification, or data indicative of a follow-up of the notification. 12. the system of claim 11 , wherein said processing unit is configured for at least one of identifying the event upon comparison between the detected data and the reference data or determining a probability for a vision impairment of the consumer based on the comparison. 13. the system of any one of claims 1 to 12, wherein said processing unit comprises a communication interface being configured for sending the notification to at least one of the identified consumer or a third party. 14. the system of any one of claims 1 to 13, wherein said processing unit is configured for providing a frame recommendation. 15. the system of any one of claims 11 to 14, wherein said memory is configured for storing a database including a multiplicity of data sets related to a plurality of spectacle frame models and sizes. 16. the system according to claim 14 or 15, wherein said processing unit is configured and operable to correlate between frames parameters and ophthalmic prescriptions. 17. the system according to any of claims 14 to 16, wherein said processing unit is configured and operable to correlate between frames parameters and facial features. 18. the system according to any of claims 14 to 17, wherein said processing unit is configured and operable to correlate between frames parameters and eyewear preferences. 19. the system according to any of claims 14 to 18, comprising a server and at least one computer entity linked to the server via a network, wherein said network is configured to receive and respond to requests sent across the network; transmitting one or more modules of computer executable program instructions and displayable data to the network connected user computer platform in response to a request, wherein said modules include modules configured to: receive and transmit image information, transmitting a frame recommendation and an optical lens option recommendation based on received image information, for display by the network connected user computer platform. 20. a computer program instructions stored in the local storage that, when executed by a processing unit, cause the processing unit to: receive data being indicative of a scene of at least one consumer in an environment, identify in the data a certain consumer, identify an event being indicative of a behavioral compensation for vision impairment, and, upon identification of such an event, send a notification relating to the vision impairment. 21. a computer program product stored on a tangible computer readable medium, comprising: a library of software modules which cause a computer executing them to prompt for information pertinent to at least one of an eyeglasses recommendation and an optical lens option recommendation, to store said information or to display eyewear recommendations . 22. the computer program product of claim 21 , wherein said library further comprises a module for frame selection, point of sales and advertising. 23. a computer platform for facilitating eye glasses marketing or selection, comprising: a camera; a processor configured to execute computer program instructions to cause the processor to take an image of a consumer, identify in the image a certain consumer, identify an event being indicative of a behavioral compensation for vision impairment, and, upon identification of such an event, sending a notification relating to the vision impairment; local storage for processor executable instructions for carrying out storage of information. 24. a method for alerting on vision impairment; said method comprising: identifying a certain individual in scene data being indicative of a scene of at least one consumer in an environment; identifying an event being indicative of a behavioral compensation for vision impairment; and upon identification of such an event, sending a notification on the vision impairment. 25. the method of claim 24, further comprising detecting data being indicative of a scene of at least one consumer in a retail environment. 26. the method of claim 24, wherein detecting the data being indicative of at least one consumer comprises at least one of capturing at least one image of at least one consumer, detecting data being indicative of a motion of a consumer, or tracking an eye motion of a consumer. 27. the method of claim 26, wherein capturing at least one image of at least one consumer comprises continuously recording a scene. 28. the method of any one of claims 24 to 27, further comprising identifying, in the data, the consumer\\' s condition including data being indicative of the consumer\\'s position and location relative to the consumer\\'s environment; said data comprising at least one of the consumer\\'s face, posture, position, sound or motion. 29. the method of any one of claims 26 to 28, wherein said event comprises at least one of position and orientation of head, increase or decrease of viewing distance between the consumer and viewed object, or changing the position of eyeglasses worn by the consumer. 30. the method of any one of claims 26 to 29, wherein identifying of the event comprises identifying images having an image feature being indicative of behavioral compensation, performing a bruckner test, performing a hirschberg test, and measuring blink count/frequency. 31. the method of claim 30, wherein the image feature being indicative of behavioral compensation comprises squinting, head orientation, certain distances between an object and a consumer\\'s eyes, certain position of eyeglasses on the consumer\\'s face, strabismus, cataracts, and reflections from the eye. 32. the method of any one of claims 27 to 31, wherein identifying in the at least one image a consumer in a retail environment, comprising at least one of receiving data characterizing the retail environment, or performing face recognition. 33. the method of any one of claims 24 to 32, wherein sending a notification comprising sending the notification to at least one of the identified consumer or a third party. 34. the method of any one of claims 24 to 33, wherein the notification includes at least one of the data indicative of the identified event, data indicative of the identified consumer, ophthalmologic recommendations based on the identified event, or lack of events, and an appointment for a vision test. 35. the method of any one of claims 24 to 34, further comprising storing at least one of a reference data indicative of behavioral compensation for vision impairment, data indicative of the notification, or data indicative of a follow-up of the notification. 36. the method of claim 35, further comprising identifying the event upon comparison between the detected data and the reference data and determining a probability for a vision impairment of the consumer, based on the comparison. 37. a computer program intended to be stored in a memory of a processor unit of a computer system, or in a removable memory medium adapted to cooperate with a reader of the processor unit, comprising instructions for implementing the method according to any of claims 24 to 36.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#white space removal for both sections\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "lowera_text = remove_whitespace(lower_atext)\n",
    "lowera_text\n",
    "lowerc_text = remove_whitespace(lower_ctext)\n",
    "lowerc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94d36101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an electronic apparatus including an image capturing device a storage device and a processor and an operation method thereof are provided the image capturing device captures an image for a user and the storage device records a plurality of modules the processor is coupled to the image capturing device and the storage device and is configured to configure the image capturing device to capture a head image of a user perform a face recognition operation to obtain a face region detect a plurality of facial landmarks within the face region estimate a head posture angle of the user according to the facial landmarks calculate a gaze position where the user gazes on the screen according to the head posture angle a plurality of rotation reference angle and a plurality of predetermined calibration positions and configure the screen to display a corresponding visual effect according to the gaze positionthe present disclosure provides a computation method and product thereof the computation method adopts a fusion method to perform machine learning computations technical effects of the present disclosure include fewer computations and less power consumptiona method for detecting body information on passengers of a vehicle based on humans' status recognition is provided the method includes steps of a passenger body information-detecting device a inputting an interior image of the vehicle into a face recognition network to detect faces of the passengers and output passenger feature information and inputting the interior image into a body recognition network to detect bodies and output body-part length information and b retrieving specific height mapping information by referring to a height mapping table of ratios of segment body portions of human groups to heights per the human groups acquiring a specific height of the specific passenger retrieving specific weight mapping information from a weight mapping table of correlations between the heights and weights per the human groups and acquiring a weight of the specific passenger by referring to the specific heighttechniques related to improved video coding based on face detection region extraction and tracking are discussed such techniques may include performing a facial search of a video frame to determine candidate face regions in the video frame testing the candidate face regions based on skin tone information to determine valid and invalid face regions rejecting invalid face regions and encoding the video frame based on valid face regions to generate a coded bitstreama method for managing a smart database which stores facial images for face recognition is provided the method includes steps of a managing device a counting specific facial images corresponding to a specific person in the smart database where new facial images are continuously stored and determining whether a first counted value representing a count of the specific facial images satisfies a first set value and b if the first counted value satisfies the first set value inputting the specific facial images into a neural aggregation network to generate quality scores of the specific facial images by aggregation of the specific facial images and if a second counted value representing a count of specific quality scores among the quality scores from a highest during counting thereof satisfies a second set value deleting part of the specific facial images corresponding to the uncounted quality scores from the smart databasea system capable of determining which recognition algorithms should be applied to regions of interest within digital representations is presented a preprocessing module utilizes one or more feature identification algorithms to determine regions of interest based on feature density the preprocessing modules leverages the feature density signature for each region to determine which of a plurality of diverse recognition modules should operate on the region of interest a specific embodiment that focuses on structured documents is also presented further the disclosed approach can be enhanced by addition of an object classifier that classifies types of objects found in the regions of interestdisclosed is a mobile terminal the mobile terminal may include a front camera obtaining a d face image of a user a glance sensor tilted by a certain angle and disposed adjacent to the front camera to obtain metadata of the d face image and a controller obtaining a distance between the glance sensor and the front camera the distance enabling an area of an overlap region where a first region representing a range photographable by the front camera overlaps a second region representing a range photographable by the glance sensor to be the maximumthis disclosure provides systems methods and apparatus including computer programs encoded on computer storage media for intelligent routing of notifications related to media programming in one aspect a smart television tv can be implemented to track a user's tv watching behavior and anticipate programming based on that behavior in some other aspects the smart tv can be implemented to detect a user's presence and based on that detection can automatically change the tv channel to media programming analyzed to be desirable to the user in some further aspects the smart tv can be implemented to transmit notification instructions to electronic devices within a network in an attempt to alert the user to upcoming media programming additionally the smart tv can be implemented to transmit detection instructions to the electronic devices within the network whereby the electronic devices attempt to detect a user's presence through voice or facial recognitiona camera is configured to output a test depth+multi-spectral image including a plurality of pixels each pixel corresponds to one of the plurality of sensors of a sensor array of the camera and includes at least a depth value and a spectral value for each spectral light sub-band of a plurality of spectral illuminators of the camera a face recognition machine is previously trained with a set of labeled training depth+multi-spectral images having a same structure as the test depth+multi-spectral image the face recognition machine is configured to output a confidence value indicating a likelihood that the test depth+multi-spectral image includes a faceembodiments of the present disclosure relate to an image processing method and apparatus and an electronic device the method includes acquiring a photo album obtained from face clustering collecting face information of respective images in the photo album and acquiring a face parameter of each image according to the face information selecting a cover image according to the face parameter of each image and taking a face-region image from the cover image and setting the face-region image as a cover of the photo albumtechniques described herein provide location-based access control to secured resources generally described configurations disclosed herein enable a system to dynamically modify access to secured resources based on one or more location-related actions for example techniques disclosed herein can enable a computing system to control access to resources such as computing devices display devices secured locations and secured data in some configurations the techniques disclosed herein can enable controlled access to secured resources based at least in part on an invitation associated with a location and positioning data indicating a location of a userone embodiment provides a method comprising receiving a piece of content and salient moments data for the piece of content the method further comprises based on the salient moments data determining a first path for a viewport for the piece of content the method further comprises displaying the viewport on a display device movement of the viewport is based on the first path during playback of the piece of content the method further comprises generating an augmentation for a salient moment occurring in the piece of content and presenting the augmentation in the viewport during a portion of the playback the augmentation comprises an interactive hint for guiding the viewport to the salient momenta computer-implemented method system and computer program product are provided for facial recognition the method includes receiving by a processor device a plurality of images the method also includes extracting by the processor device with a feature extractor utilizing a convolutional neural network cnn with an enlarged intra-class variance of long-tail classes feature vectors for each of the plurality of images the method additionally includes generating by the processor device with a feature generator discriminative feature vectors for each of the feature vectors the method further includes classifying by the processor device utilizing a fully connected classifier an identity from the discriminative feature vector the method also includes control an operation of a processor-based machine to react in accordance with the identitysome embodiments of the invention provide efficient expressive machine-trained networks for performing machine learning the machine-trained mt networks of some embodiments use novel processing nodes with novel activation functions that allow the mt network to efficiently define with fewer processing node layers a complex mathematical expression that solves a particular problem eg face recognition speech recognition etc in some embodiments the same activation function eg a cup function is used for numerous processing nodes of the mt network but through the machine learning this activation function is configured differently for different processing nodes so that different nodes can emulate or implement two or more different functions eg two or more boolean logical operators such as xor and and the activation function in some embodiments is a periodic function that can be configured to implement different functions eg different sinusoidal functionsmethods and systems may provide for facial recognition of at least one input image utilizing hierarchical feature learning and pair-wise classification receptive field theory may be used on the input image to generate a pre-processed multi-channel image channels in the pre-processed image may be activated based on the amount of feature rich details within the channels similarly local patches may be activated based on the discriminant features within the local patches features may be extracted from the local patches and the most discriminant features may be selected in order to perform feature matching on pair sets the system may utilize patch feature pooling pair-wise matching and large-scale training in order to quickly and accurately perform facial recognition at a low cost for both system memory and computationa method for controlling a terminal is provided the terminal includes a capturing apparatus and at least one processor an image is acquired by the capturing apparatus a motion parameter of the terminal is obtained image processing on the acquired image is controlled to be performed based on the motion parameter being equal to or less than a preset parameter threshold and skipped based on the motion parameter being greater than the preset parameter thresholda drive-through order processing method and apparatus are disclosed the drive-through order processing method includes receiving customer information detected through vision recognition providing product information based on the customer information and processing a product order of a customer according to the present disclosure it is possible to rapidly process an order using customer information based on customer recognition using an artificial intelligence ai model of machine learning through a g networkan image processing method performed at a computing device includes identifying using face recognition one or more faces each face corresponding to a respective person captured in a first image for each identified face extracting a set of profile parameters of a corresponding person in the first image and selecting from a plurality of image tiles a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles and sharing the first image and the second image in a predefined order via a group chat sessionin one embodiment the artificial reality system determines that a performance metric of an eye tracking system is below a first performance threshold the eye tracking system is associated with a head-mounted display worn by a user the artificial reality system receives first inputs associated with the body of a user and determines a region that the user is looking at within a field of view of a head-mounted display based on the received first inputs the system determines a vergence distance of the user based at least on the first inputs associated with the body of the user the region that the user is looking at and locations of one or more objects in a scene displayed by the head-mounted display the system adjusts one or more configurations of the head-mounted display based on the determined vergence distance of the usera computer-implemented method is provided for image-based self-guided object detection the method includes receiving by a processor device a set of images each of the images has a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data the method further includes training by the processor device a grid-based object detector using the grid level label data the method also includes determining by the processor device a respective bounding box for the respective object in each of the images by applying local segmentation to each of the images the method additionally includes training by the processor device a region-based convolutional neural network rcnn for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the rcnna system and method of face recognition comprising multiple phases implemented in a parallel architecture the first phase is a normalization phase whereby a captured image is normalized to the same size orientation and illumination of stored images in a preexisting database the second phase is a feature extractiondistance matrix phase where a distance matrix is generated for the captured image in a coarse recognition phase the generated distance matrix is compared with distance matrices in the database using euclidean distance matches to create candidate lists and in a detailed recognition phase multiple face recognition algorithms are applied to the candidate lists to produce a final result the distance matrices in the normalized database may be broken into parallel lists for parallelization in the feature extractiondistance matrix phase and the candidate lists may also be grouped according to a dissimilarity algorithm for parallel processing in the detailed recognition phasean imaging device including a pixel matrix and a processor is provided the pixel matrix includes a plurality of phase detection pixels and a plurality of regular pixels the processor performs autofocusing according to pixel data of the phase detection pixels and determines an operating resolution of the regular pixels according to autofocused pixel data of the phase detection pixels wherein the phase detection pixels are always-on pixels and the regular pixels are selectively turned on after the autofocusing is accomplishedan apparatus includes a first camera module providing a first image of an object with a first field of view a second camera module providing a second image of the object with a second field of view different from the first field of view a first depth map generator that generates a first depth map of the first image based on the first image and the second image and a second depth map generator that generates a second depth map of the second image based on the first image the second image and the first depth mapmethods systems and apparatus including computer programs encoded on computer storage media for a payment based on a face recognition are provided one of the methods includes acquiring first face image information of a target user extracting first characteristic information from the first face image information wherein the first characteristic information includes head posture information of the target user and gaze information of the target user determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user including determining whether an angle of rotation in each preset direction is less than an angle threshold and whether a probability value that a user gazes at a payment screen is greater than a probability threshold and in response to determining that the target user has a willingness to pay completing a payment operation based on the face recognitiona novel method and apparatus for face authentication is disclosed the disclosed method comprises detecting a motion by a subject within a predetermined area of view assigning a unique session identification number to the subject detected within a predetermined area of view detecting a facial area of the subject detected within a predetermined area of view generating an image of the facial area of the subject assessing a quality of the image of the facial area of the subject conducing an incremental training of the image of the facial area of the subject determining an identity of the subject based on the image of the facial area of the subject identifying an intent of the subject and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subjectdisclosed herein is a robot and an electronic device for acquiring video and a method for acquiring video using the robot the robot includes a camera configured to rotate in the lateral direction and tilt in the vertical direction and controls at least one of a direction of the rotation of the camera an angle of the tilt of the camera and a focal distance of the camera by recognizing and tracking users in a video acquired by the camerasystems and methods are disclosed for inferring topics from a file containing both audio and video for example a multimodal or multimedia file in order to facilitate video indexing a set of entities is extracted from the file and linked to produce a graph and reference information is also obtained for the set of entities entities may be drawn for example from wikipedia categories or other large ontological data sources analysis of the graph using unsupervised learning permits determining clusters in the graph extracting features from the clusters possibly using supervised learning provides for selection of topic identifiers the topic identifiers are then used for indexing the filea face recognition method a neural network training method an apparatus and an electronic device the method comprises obtaining a first face image by means of a first camera  extracting a first face feature of the first face image  comparing the first face feature with a pre-stored second face feature to obtain a reference similarity the second face feature being obtained by extracting a feature of a second face image obtained by a second camera and the second camera and the first camera being different types of cameras  and determining according to the reference similarity whether the first face feature and the second face feature correspond to a same person the present invention discloses a technique for alerting on vision impairment the system comprises a processing unit configured and operable for receiving scene data being indicative of a scene of at least one consumer in an environment identifying in the scene data a certain consumer identifying an event being indicative of a behavioral compensation for vision impairment and upon identification of such an event sending a notification relating to the vision impairment\n",
      " an electronic device  configured to make a screen  to display a plurality of image frames comprising an image capturing device  a storage device  storing a plurality of modules and a processor  coupled to the image capturing device  and the storage device  configured to execute the modules in the storage device  to configure the screen  to display a plurality of marker objects at a plurality of predetermined calibration positions configure the image capturing device  to capture a plurality of first head images when a user is looking at the predetermined calibration positions s perform a plurality of first face recognition operations on the first head images to obtain a plurality of first face regions corresponding to the predetermined calibration positions s detect a plurality of first facial landmarks corresponding to the first face regions s calculate a plurality of rotation reference angles of the user looking at the predetermined calibration positions according to the first facial landmarks configure the image capturing device  to capture a second head image of the user perform a second face recognition operation on the second head image to obtain a second face region detect a plurality of second facial landmarks within the second face region s estimate a head posture angle of the user according to the second facial landmarks calculate a gaze position of the user on the screen  according to the head posture angle the rotation reference angles and the predetermined calibration positions and configure the screen  to display a corresponding visual effect according to the gaze position  the electronic device  according to claim  wherein the gaze position comprises a first coordinate value in a first axial direction and a second coordinate value in a second axial direction  the electronic device  according to claim  wherein the head posture angles comprise a head pitch angle and a head yaw angle and the rotation reference angles comprise a first pitch angle a second pitch angle a first yaw angle and a second yaw angle corresponding to the predetermined calibration positions  the electronic device  according to claim  wherein the processor  performs interpolation operation or extrapolation operation according to the first yaw angle the second yaw angle a first position corresponding to the first yaw angle among the predetermined calibration positions a second position corresponding to the second yaw angle among the predetermined calibration positions and the head yaw angle thereby obtaining the first coordinate value of the gaze position and the processor  performs interpolation operation or extrapolation operation according to the first pitch angle the second pitch angle a third position corresponding to the first pitch angle among the predetermined calibration positions a fourth position corresponding to the second pitch angle among the predetermined calibration positions and the head pitch angle thereby obtaining the second coordinate value of the gaze position  the electronic device  according to claim  wherein the processor  calculates a plurality of first viewing distances between the user and the screen  according to the first facial landmarks the processor  estimates a second viewing distance between the user and the screen  according to the second facial landmarks and the processor  adjusts the rotation reference angles or the gaze position according to the second viewing distance and the first viewing distances  the electronic device  according to claim  wherein the processor  maps a plurality of two-dimensional position coordinates of the second facial landmarks under a plane coordinate system to a plurality of three-dimensional position coordinates under a three-dimensional coordinate system and the processor  estimates the head posture angle according to the three-dimensional position coordinates of the second facial landmarks  the electronic device  according to claim  wherein the second head image comprises a wearable device and the second facial landmarks do not comprise a plurality of third facial landmarks of the user covered by the wearable device  the electronic device  according to claim  wherein the second head image comprises a wearable device and the second facial landmarks comprise one or more simulated landmarks marked by the wearable device  an operating method adapted for an electronic device  comprising an image capturing device  and making a screen  to display a plurality of image frames the method comprising configuring the screen  to display a plurality of marker objects at a plurality of predetermined calibration positions configuring the image capturing device  to capture a plurality of first head images when a user is looking at the predetermined calibration positions s performing a plurality of first face recognition operations on the first head images to obtain a plurality of first face regions corresponding to the predetermined calibration positions s detecting a plurality of first facial landmarks corresponding to the first face regions s calculating a plurality of rotation reference angles of the user looking at the predetermined calibration positions according to the first facial landmarks configuring the image capturing device  to capture a second head image of the user performing a second face recognition operation on the second head image to obtain a second face region s detecting a plurality of second facial landmarks within the second face region estimating a head posture angle of the user according to the second facial landmarks calculating a gaze position of the user on the screen  according to the head posture angle the rotation reference angles and the predetermined calibration positions and s configuring the screen  to display a corresponding visual effect according to the gaze position  the operation method according to claim  wherein the gaze position comprises a first coordinate value in a first axial direction and a second coordinate value in a second axial direction  the operation method according to claim  wherein the head posture angles comprise a head pitch angle and a head yaw angle and the rotation reference angles comprise a first pitch angle a second pitch angle a first yaw angle and a second yaw angle corresponding to the predetermined calibration positions  the operation method according to claim  wherein the step of calculating the gaze position of the user on the screen  according to the head posture angle the rotation reference angles and the predetermined calibration positions comprises performing interpolation operation or extrapolation operation according to the first yaw angle the second yaw angle a first position corresponding to the first yaw angle among the predetermined calibration positions a second position corresponding to the second yaw angle among the predetermined calibration positions and the head yaw angle thereby obtaining the first coordinate value of the gaze position and performing interpolation operation or extrapolation operation according to the first pitch angle the second pitch angle a third position corresponding to the first pitch angle among the predetermined calibration positions a fourth position corresponding to the second pitch angle among the predetermined calibration positions and the head pitch angle thereby obtaining the second coordinate value of the gaze position  the operation method according to claim  wherein the method further comprises calculating a plurality of first viewing distances between the user and the screen  according to the first facial landmarks estimating a second viewing distance between the user and the screen  according to the second facial landmarks and adjusting the rotation reference angles or the gaze position according to the second viewing distance and the first viewing distances  the operation method according to claim  wherein the method further comprises mapping a plurality of two-dimensional position coordinates of the second facial landmarks under a plane coordinate system to a plurality of three-dimensional position coordinates under a three-dimensional coordinate system and estimating the head posture angle according to the three-dimensional position coordinates of the second facial landmarks  the operation method according to claim  wherein the second head image comprises a wearable device and the second facial landmarks do not comprise a plurality of third facial landmarks of the user covered by the wearable device  the operation method according to claim  wherein the second head image comprises a wearable device and the second facial landmarks comprise one or more simulated landmarks marked by the wearable device a computation method applied to a computing system wherein the computing system comprises a control unit a computation group and a general storage unit wherein the control unit comprises a first memory a decoding logic and a controller wherein the computation group comprises a group controller and a plurality of computing units the general storage unit is configured to store data and the computation method comprises receiving by the controller a first level instruction sequence and partitioning by the decoding logic the first level instruction sequence into a plurality of second level instruction sequences creating by the controller m threads for the plurality of second level instruction sequences and allocating by the controller an independent register as well as configuring an independent addressing function for each thread of the m threads wherein m is an integer greater than or equal to  and obtaining by the group controller a plurality of computation types of the plurality of second level instruction sequences obtaining a corresponding fusion computation manner of the computation types according to the plurality of computation types and adopting by the plurality of computing units the fusion computation manner to call the m threads for performing computations on the plurality of second level instruction sequences to obtain a final result  the method of claim  wherein the obtaining by the group controller a plurality of computation types of the plurality of second level instruction sequences obtaining a corresponding fusion computation manner of the computation types according to the plurality of computation types and adopting by the plurality of computing units the fusion computation manner to call the m threads for performing computations on the plurality of second instruction sequences to obtain a final result if the computation types represent computation operations of the same type the group controller calls a combined computation manner in which single instruction multiple data of the same type is in combination with single instruction multiple threads and uses the m threads to perform the combined computation manner to obtain a final result which includes partitioning by the decoding logic the m threads into n wraps for allocating to the the plurality of computing units converting by the group controller the plurality of second instruction sequences into a plurality of second control signals and sending the second control signals to the plurality of computing units calling by the plurality of computing units wraps that are allocated to the computing units and the second control signals to fetch corresponding data according to the independent addressing function performing by the plurality of computing units computations on the data to obtain a plurality of intermediate results and splicing the plurality of intermediate results to obtain a final result  the method of claim  wherein the obtaining by the group controller a plurality of computation types of the plurality of second level instruction sequences obtaining a corresponding fusion computation manner of the computation types according to the plurality of computation types and adopting by the plurality of computing units the fusion computation manner to call the m threads for performing computations on the plurality of second instruction sequences to obtain a final result if the computation types represent computation operations of different types the group controller calls simultaneous multi-threading and the m threads to perform computations to obtain a final result which includes partitioning by the decoding logic the m threads into n wraps converting the plurality of second instruction sequences into a plurality of second control signals obtaining by the group controller computation types supported by the plurality of computing units allocating by the controller the n wraps and the plurality of second control signals to corresponding computing units that support computation types of the wraps and the second control signals calling by the plurality of computing units wraps that are allocated to the computing units and the second control signals fetching by the plurality of computing units corresponding data performing by the plurality of computing units computations on the data to obtain a plurality of intermediate results and splicing all the intermediate results to obtain a final result  the method of claim  or  further comprising if a wrap a in the plurality of wraps is blocked adding the wrap a to a waiting queue and if data of the wrap a are already fetched adding the wrap a to a preparation queue wherein the preparation queue is a queue where a wrap to be scheduled for executing is located when a computing resource is idle  the method of claim  wherein the first level instruction sequence includes a very long instruction and the second level instruction sequence includes an instruction sequence  the method of claim  wherein the computing system further includes a tree module wherein the tree module includes a root port and a plurality of branch ports wherein the root port of the tree module is connected to the group controller and the plurality of branch ports of the tree module are connected to a computing unit of the plurality of computing units respectively and the tree module is configured to forward data blocks wraps or instruction sequences between the group controller and the plurality of computing units  the method of claim  wherein the tree module is an n-ary tree wherein n is an integer greater than or equal to   the method of claim  wherein the computing system further includes a branch processing circuit wherein the branch processing circuit is connected between the group controller and the plurality of computing units and the branch processing circuit is configured to forward data wraps or instruction sequences between the group controller and the plurality of computing units  a computing system comprising a control unit a computation group and a general storage unit wherein the control unit includes a first memory a decoding logic and a controller the computation group includes a group controller and a plurality of computing units the general storage unit is configured to store data the controller is configured to receive a first level instruction sequence and control the first memory and the decoding logic the decoding logic is configured to partition the first level instruction sequence into a plurality of second level instruction sequences the the controller is further configured to create m threads for the plurality of second level instruction sequences and allocate an independent register and configure an independent addressing function for each thread of the m threads m is an integer greater than or equal to  and the controller is further configured to convert the plurality of second instruction sequences into a plurality of control signals for sending to the group controller the group controller is configured to receive the plurality of control signals obtain a plurality of computational types if the plurality of control signals divide the m threads into n wraps and allocate the n wraps and the plurality of control signals to the plurality of computing units according to the plurality of computational types the plurality of computing units are configured to fetch data from the general storage unit through allocated wraps and control signals and perform computations to obtain an intermediate result and the group controller is configured to splice all intermediate results to obtain a final computation result  the computing system of claim  wherein the plurality of computing units includes an addition computing unit a multiplication computing unit an activation computing unit or a dedicated computing unit  the computing system of claim  wherein the dedicated computing unit includes a face recognition computing unit a graphics computing unit a fingerprint computing unit or a neural network computing unit  the computing system of claim  wherein the group controller is configured to if computation types of a plurality of control signals are graphics computations fingerprint identification face recognition or neural network operations allocate the plurality of control signals to the face recognition computing unit the graphics computing unit the fingerprint computing unit or the neural network computing unit respectively  the computing system of claim  wherein the first level instruction sequence includes a very long instruction and the second level instruction sequence includes an instruction sequence  the computing system of claim  further comprising a tree module wherein the tree module includes a root port and a plurality of branch ports wherein the root port of the tree module is connected to the group controller and the plurality of branch ports of the tree module are connected to a computing unit of the plurality of computing units respectively and the tree module is configured to forward data blocks wraps or instruction sequences between the group controller and the plurality of computing units  the computing system of claim  wherein the tree module is an n-ary tree wherein n is an integer greater than or equal to   the computing system of claim  wherein the computing system includes a branch processing circuit the branch processing circuit is connected between the group controller and the plurality of computing units and the branch processing circuit is configured to forward data wraps or instruction sequences between the group controller and the plurality of computing units  a computer program product comprising a non-instant computer readable storage medium wherein a computer program is stored in the non-instant computer readable storage medium and the computer program is capable of causing a computer to perform the method of any of claims - through operations a method for detecting body information on one or more passengers of a vehicle based on humans' status recognition comprising steps of a if at least one interior image of an interior of the vehicle is acquired a passenger body information-detecting device performing i a process of inputting the interior image into a face recognition network to thereby allow the face recognition network to detect each of faces of each of the passengers from the interior image and thus to output multiple pieces of passenger feature information corresponding to each of the detected faces and ii a process of inputting the interior image into a body recognition network to thereby allow the body recognition network to detect each of bodies of each of the passengers from the interior image and thus to output body-part length information of each of the detected bodies and b the passenger body information-detecting device performing a process of retrieving specific height mapping information corresponding to specific passenger feature information on a specific passenger from a height mapping table which stores height mapping information representing respective one or more predetermined ratios of one or more segment body portions of each of human groups to each of heights per each of the human groups a process of acquiring a specific height of the specific passenger from the specific height mapping information by referring to specific body-part length information of the specific passenger a process of retrieving specific weight mapping information corresponding to the specific passenger feature information from a weight mapping table which stores multiple pieces of weight mapping information representing predetermined correlations between each of the heights and each of weights per each of the human groups and a process of acquiring a weight of the specific passenger from the specific weight mapping information by referring to the specific height of the specific passenger  the method of claim  wherein at the step of a the passenger body information-detecting device performs a process of inputting the interior image into the body recognition network to thereby allow the body recognition network to i output one or more feature tensors with one or more channels corresponding to the interior image via a feature extraction network ii generate at least one keypoint heatmap and at least one part affinity field with one or more channels corresponding to each of the feature tensors via a keypoint heatmap & part affinity field extractor and iii extract keypoints from the keypoint heatmap via a keypoint detector to group the extracted keypoints by referring to the part affinity field and thus to generate body parts per the passengers and as a result allow the body recognition network to output multiple pieces of body-part length information on each of the passengers by referring to the body parts per the passengers  the method of claim  wherein the feature extraction network includes at least one convolutional layer and applies at least one convolution operation to the interior image to thereby output the feature tensors  the method of claim  wherein the keypoint heatmap & part affinity field extractor includes one of a fully convolutional network and a × convolutional layer and applies a fully-convolution operation or × convolution operation to the feature tensors to thereby generate the keypoint heatmap and the part affinity field  the method of claim  wherein the keypoint detector connects by referring to the part affinity field pairs respectively having highest mutual connection probabilities of being connected among the extracted keypoints to thereby group the extracted keypoints  the method of claim  wherein the feature extraction network and the keypoint heatmap & part affinity field extractor have been learned by a learning device performing i a process of inputting at least one training image including one or more objects for training into the feature extraction network to thereby allow the feature extraction network to generate one or more feature tensors for training having one or more channels by applying at least one convolutional operation to the training image ii a process of inputting the feature tensors for training into the keypoint heatmap & part affinity field extractor to thereby allow the keypoint heatmap & part affinity field extractor to generate one or more keypoint heatmaps for training and one or more part affinity fields for training having one or more channels for each of the feature tensors for training iii a process of inputting the keypoint heatmaps for training and the part affinity fields for training into the keypoint detector to thereby allow the keypoint detector to extract keypoints for training from each of the keypoint heatmaps for training and a process of grouping the extracted keypoints for training by referring to each of the part affinity fields for training to thereby detect keypoints per each of the objects for training and iv a process of allowing a loss layer to calculate one or more losses by referring to the keypoints per each of the objects for training and their corresponding ground truths to thereby adjust one or more parameters of the feature extraction network and the keypoint heatmap & part affinity field extractor such that the losses are minimized by backpropagation using the losses  the method of claim  wherein at the step of a the passenger body information-detecting device performs a process of inputting the interior image into the face recognition network to thereby allow the face recognition network to detect each of the faces of each of the passengers located in the interior image via a face detector and to output multiple pieces of the passenger feature information on each of the facial images via a facial feature classifier  the method of claim  wherein at the step of a the passenger body information-detecting device performs a process of inputting the interior image into the face recognition network to thereby allow the face recognition network to i apply at least one convolution operation to the interior image and thus to output at least one feature map corresponding to the interior image via at least one convolutional layer ii output one or more proposal boxes where the passengers are estimated as located on the feature map via a region proposal network iii apply pooling operation to one or more regions corresponding to the proposal boxes on the feature map and thus to output at least one feature vector via a pooling layer and iv apply fully-connected operation to the feature vector and thus to output the multiple pieces of the passenger feature information corresponding to each of the faces of each of the passengers corresponding to each of the proposal boxes via a fully connected layer  the method of claim  wherein the multiple pieces of the passenger feature information include each of ages each of genders and each of races corresponding to each of the passengers  a passenger body information-detecting device for detecting body information on one or more passengers of a vehicle based on humans' status recognition comprising at least one memory that stores instructions and at least one processor configured to execute the instructions to perform or support another device to perform i if at least one interior image of an interior of the vehicle is acquired i a process of inputting the interior image into a face recognition network to thereby allow the face recognition network to detect each of faces of each of the passengers from the interior image and thus to output multiple pieces of passenger feature information corresponding to each of the detected faces and ii a process of inputting the interior image into a body recognition network to thereby allow the body recognition network to detect each of bodies of each of the passengers from the interior image and thus to output body-part length information of each of the detected bodies and ii a process of retrieving specific height mapping information corresponding to specific passenger feature information on a specific passenger from a height mapping table which stores height mapping information representing respective one or more predetermined ratios of one or more segment body portions of each of human groups to each of heights per each of the human groups a process of acquiring a specific height of the specific passenger from the specific height mapping information by referring to specific body-part length information of the specific passenger a process of retrieving specific weight mapping information corresponding to the specific passenger feature information from a weight mapping table which stores multiple pieces of weight mapping information representing predetermined correlations between each of the heights and each of weights per each of the human groups and a process of acquiring a weight of the specific passenger from the specific weight mapping information by referring to the specific height of the specific passenger  the passenger body information-detecting device of claim  wherein at the process of i the processor performs a process of inputting the interior image into the body recognition network to thereby allow the body recognition network to i output one or more feature tensors with one or more channels corresponding to the interior image via a feature extraction network ii generate at least one keypoint heatmap and at least one part affinity field with one or more channels corresponding to each of the feature tensors via a keypoint heatmap & part affinity field extractor and iii extract keypoints from the keypoint heatmap via a keypoint detector to group the extracted keypoints by referring to the part affinity field and thus to generate body parts per the passengers and as a result allow the body recognition network to output multiple pieces of body-part length information on each of the passengers by referring to the body parts per the passengers  the passenger body information-detecting device of claim  wherein the keypoint heatmap & part affinity field extractor includes one of a fully convolutional network and a × convolutional layer and applies a fully-convolution operation or × convolution operation to the feature tensors to thereby generate the keypoint heatmap and the part affinity field  the passenger body information-detecting device of claim  wherein the keypoint detector connects by referring to the part affinity field pairs respectively having highest mutual connection probabilities of being connected among the extracted keypoints to thereby group the extracted keypoints  the passenger body information-detecting device of claim  wherein the feature extraction network and the keypoint heatmap & part affinity field extractor have been learned by a learning device performing i a process of inputting at least one training image including one or more objects for training into the feature extraction network to thereby allow the feature extraction network to generate one or more feature tensors for training having one or more channels by applying at least one convolutional operation to the training image ii a process of inputting the feature tensors for training into the keypoint heatmap & part affinity field extractor to thereby allow the keypoint heatmap & part affinity field extractor to generate one or more keypoint heatmaps for training and one or more part affinity fields for training having one or more channels for each of the feature tensors for training iii a process of inputting the keypoint heatmaps for training and the part affinity fields for training into the keypoint detector to thereby allow the keypoint detector to extract keypoints for training from each of the keypoint heatmaps for training and a process of grouping the extracted keypoints for training by referring to each of the part affinity fields for training to thereby detect keypoints per each of the objects for training and iv a process of allowing a loss layer to calculate one or more losses by referring to the keypoints per each of the objects for training and their corresponding ground truths to thereby adjust one or more parameters of the feature extraction network and the keypoint heatmap & part affinity field extractor such that the losses are minimized by backpropagation using the losses  the passenger body information-detecting device of claim  wherein at the process of i the processor performs a process of inputting the interior image into the face recognition network to thereby allow the face recognition network to i apply at least one convolution operation to the interior image and thus to output at least one feature map corresponding to the interior image via at least one convolutional layer ii output one or more proposal boxes where the passengers are estimated as located on the feature map via a region proposal network iii apply pooling operation to one or more regions corresponding to the proposal boxes on the feature map and thus to output at least one feature vector via a pooling layer and iv apply fully-connected operation to the feature vector and thus to output the multiple pieces of the passenger feature information corresponding to each of the faces of each of the passengers corresponding to each of the proposal boxes via a fully connected layer a computer implemented method for performing video coding based on face detection comprising receiving a video frame comprising one of a plurality of video frames of a video sequence determining the video frame is a key frame of the video sequence performing in response to the video frame being a key frame of the video sequence a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region rejecting the second candidate face region and outputting the first candidate face region and encoding the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream  the method of claim  wherein the skin tone information comprises a skin probability map  the method of claim  wherein said testing the first and second candidate face regions based on skin tone information is performed in response to the video frame being a key frame of the video sequence  the method of claim  wherein the first candidate face region comprises a rectangular region the method further comprising determining a free form shape face region corresponding to the first candidate face region wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy  the method of claim  wherein determining the free form shape face region comprises generating an enhanced skip probability map corresponding to the first candidate face region binarizing the enhanced skip probability map and overlaying the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region  the method of claim  wherein a second video frame comprises a non-key frame of the video sequence the method further comprising performing face detection in the second video frame of the video sequence based on the free form shape face region  the method of claim  further comprising tracking a second free form shape face region in the second video frame based on the free form shape face region in the video frame  the method of claim  wherein tracking the second free form shape face region comprises determining a location of a second valid face region in the second video frame based on a displacement offset with respect to the first candidate face region  the method of claim  further comprising determining the displacement offset based on an offset between a centroid of a bounding box around a skin enhanced region corresponding to the first candidate face region and a second centroid of a second bounding box around a second skin enhanced region in the second video frame  the method of claim  wherein encoding the video frame based at least in part on the first candidate face region being a valid face region comprises at least one of reducing a quantization parameter corresponding to the first candidate face region adjusting a lambda value for the first candidate face region or disabling skip coding for the first candidate face region  the method of claim  wherein the bitstream comprises at least one of an hadvanced video coding avc compliant bitstream an hhigh efficiency video coding hevc compliant bitstream a vp compliant bitstream a vp compliant bitstream or an alliance for open media aom av compliant bitstream  a computer implemented method for performing face detection comprising receiving a video frame of a sequence of video frames performing a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region rejecting the second candidate face region and outputting the first candidate face region as a valid face region for further processing and providing an index indicative of a person being present in the video frame based on the valid face region  the method of claim  wherein the sequence of video frames comprises a sequence of surveillance video frames the method further comprising performing face recognition in the surveillance video frames based on the valid face region  the method of claim  wherein the sequence of video frames comprises a sequence of decoded video frames the method further comprising adding a marker corresponding to the received video frame to perform face recognition on the received video frame based on the valid face region  the method of claim  wherein the sequence of video frames is received during a device login attempt the method further comprising performing face recognition based on the valid face region and allowing access to the device if a secured face is recognized  the method of claim  wherein the sequence of video frames comprises a sequence of videoconferencing frames the method further comprising encoding the video frame based at least in part on the valid face region to generate a coded bitstream  the method of claim  wherein encoding the video frame comprises not encoding a background region of the video frame into the bitstream  the method of claim  further comprising encoding the video frame based at least in part on the valid face region to generate a coded bitstream wherein encoding the video frame comprises including metadata corresponding to the valid face region in the bitstream  the method of claim  further comprising decoding the coded bitstream to generate a decoded video frame and to determine the metadata corresponding to the valid face region in the bitstream  the method of claim  further comprising at least one of replacing the valid face region based on the decoded metadata cropping and displaying image data corresponding only to the valid face region based on the decoded metadata or indexing the decoded video frame based on the decoded metadata  a system for performing video coding based on face detection comprising a memory configured to store a video frame comprising one of a plurality of video frames of a video sequence and a processor coupled to the memory the processor to receive the video frame to determine the video frame is a key frame of the video sequence to perform in response to the video frame being a key frame of the video sequence a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame to test the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region to reject the second candidate face region and outputting the first candidate face region and to encode the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream  the system of claim  wherein the skin tone information comprises a skin probability map  the system of claim  wherein the first candidate face region comprises a rectangular region the processor further to determine a free form shape face region corresponding to the first candidate face region wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy  the system of claim  wherein the processor to determine the free form shape face region comprises the processor to generate an enhanced skip probability map corresponding to the first candidate face region to binarize the enhanced skip probability map and to overlay the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region  the system of claim  wherein a second video frame comprises a non-key frame of the video sequence and the processor is further to perform face detection in the second video frame of the video sequence based on the free form shape face region  the system of claim  wherein the processor is further to track a second free form shape face region in the second video frame based on the free form shape face region in the video frame  the system of claim  wherein to encode the video frame based at least in part on the first candidate face region being a valid face region comprises the processor to reduce a quantization parameter corresponding to the first candidate face region adjust a lambda value for the first candidate face region or disable skip coding for the first candidate face region  at least one non-transitory machine readable medium comprising a plurality of instructions that in response to being executed on a device cause the device to perform video coding based on face detection by receiving a video frame comprising one of a plurality of video frames of a video sequence determining the video frame is a key frame of the video sequence performing in response to the video frame being a key frame of the video sequence a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region rejecting the second candidate face region and outputting the first candidate face region and encoding the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream  the non-transitory machine readable medium of claim  wherein the skin tone information comprises a skin probability map  the non-transitory machine readable medium of claim  wherein the first candidate face region comprises a rectangular region the machine readable medium comprising further instructions that in response to being executed on the device cause the device to perform video coding based on face detection by determining a free form shape face region corresponding to the first candidate face region wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy  the non-transitory machine readable medium of claim  wherein determining the free form shape face region comprises generating an enhanced skip probability map corresponding to the first candidate face region binarizing the enhanced skip probability map and overlaying the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region  the non-transitory machine readable medium of claim  wherein a second video frame comprises a non-key frame of the video sequence the machine readable medium comprising further instructions that in response to being executed on the device cause the device to perform video coding based on face detection by performing face detection in the second video frame of the video sequence based on the free form shape face region  the non-transitory machine readable medium of claim  the machine readable medium comprising further instructions that in response to being executed on the device cause the device to perform video coding based on face detection by tracking a second free form shape face region in the second video frame based on the free form shape face region in the video frame  the non-transitory machine readable medium of claim  wherein encoding the video frame based at least in part on the first candidate face region being a valid face region comprises at least one of reducing a quantization parameter corresponding to the first candidate face region adjusting a lambda value for the first candidate face region or disabling skip coding for the first candidate face region a method for managing a smart database which stores facial images for face recognition comprising steps of a a managing device performing a process of counting one or more specific facial images corresponding to at least one specific person stored in the smart database where new facial images for the face recognition are continuously stored and a process of determining whether a first counted value representing a count of the specific facial images satisfies a preset first set value and b if the first counted value is determined as satisfying the first set value the managing device performing a process of inputting the specific facial images into a neural aggregation network to thereby allow the neural aggregation network to generate each of quality scores of each of the specific facial images by aggregation of the specific facial images and a process of sorting the quality scores corresponding to the specific facial images in a descending order of the quality scores a process of counting the sorted specific facial images in the descending order until a second counted value which represents the number of a counted part of the specific facial images becomes equal to a preset second set value and a process of deleting an uncounted part of the specific facial images from the smart database  the method of claim  further comprising a step of c the managing device performing a process of generating at least one optimal feature by weighted summation of one or more features of the specific facial images using the counted part of the quality scores and a process of setting the optimal feature as a representative face corresponding to the specific person  the method of claim  wherein at the step of b the managing device performs a process of inputting the specific facial images into a cnn of the neural aggregation network to thereby allow the cnn to generate one or more features corresponding to each of the specific facial images and a process of inputting at least one feature vector where the features are embedded into an aggregation module including at least two attention blocks to thereby allow the aggregation module to generate each of the quality scores of each of the features  the method of claim  wherein at the step of b the managing device performs a process of matching i i- one or more features corresponding to each of the specific facial images stored in the smart database and i- the quality scores with ii the specific person and a process of storing the matched features and the matched quality scores in the smart database  the method of claim  further comprising a step of d the managing device performing one of i a process of learning a face recognition system by using the specific facial images corresponding to the specific person stored in the smart database and ii a process of transmitting the specific facial images corresponding to the specific person to a learning device corresponding to the face recognition system to thereby allow the learning device to learn the face recognition system using the specific facial images  the method of claim  wherein the neural aggregation network has been learned by a learning device repeating more than once i a process of inputting multiple facial images for training corresponding to an image set of a single face or a video of the single face into a cnn of the neural aggregation network to thereby allow the cnn to generate one or more features for training by applying at least one convolution operation to the facial images for training ii a process of inputting at least one feature vector for training where the features for training are embedded into an aggregation module including at least two attention blocks of the neural aggregation network to thereby allow the aggregation module to generate each of quality scores for training of each of the features for training by aggregation of the features for training using one or more attention parameters learned in a previous iteration iii a process of outputting at least one optimal feature for training by weighted summation of the features for training using the quality scores for training and iv a process of updating the attention parameters learned in the previous iteration of the at least two attention blocks such that one or more losses are minimized which are outputted from a loss layer by referring to the optimal feature for training and its corresponding ground truth  a managing device for managing a smart database which stores facial images for face recognition comprising at least one memory that stores instructions and at least one processor configured to execute the instructions to perform or support another device to perform i a process of counting one or more specific facial images corresponding to at least one specific person stored in the smart database where new facial images for the face recognition are continuously stored and a process of determining whether a first counted value representing a count of the specific facial images satisfies a preset first set value and ii if the first counted value is determined as satisfying the first set value a process of inputting the specific facial images into a neural aggregation network to thereby allow the neural aggregation network to generate each of quality scores of each of the specific facial images by aggregation of the specific facial images and a process of sorting the quality scores corresponding to the specific facial images in a descending order of the quality scores a process of counting the sorted specific facial images in the descending order until a second counted value which represents the number of a counted part of the specific facial images becomes equal to a preset second set value and a process of deleting an uncounted part of the specific facial images from the smart database  the managing device of claim  wherein the processor further performs iii a process of generating at least one optimal feature by weighted summation of one or more features of the specific facial images using the counted part of the quality scores and a process of setting the optimal feature as a representative face corresponding to the specific person  the managing device of claim  wherein at the process of ii the processor performs a process of inputting the specific facial images into a cnn of the neural aggregation network to thereby allow the cnn to generate one or more features corresponding to each of the specific facial images and a process of inputting at least one feature vector where the features are embedded into an aggregation module including at least two attention blocks to thereby allow the aggregation module to generate each of the quality scores of each of the features  the managing device of claim  wherein at the process of ii the processor performs a process of matching i i- one or more features corresponding to each of the specific facial images stored in the smart database and i- the quality scores with ii the specific person and a process of storing the matched features and the matched quality scores in the smart database  the managing device of claim  wherein the processor further performs iv one of i a process of learning a face recognition system by using the specific facial images corresponding to the specific person stored in the smart database and ii a process of transmitting the specific facial images corresponding to the specific person to a learning device corresponding to the face recognition system to thereby allow the learning device to learn the face recognition system using the specific facial images  the managing device of claim  wherein the neural aggregation network has been learned by a learning device repeating more than once i a process of inputting multiple facial images for training corresponding to an image set of a single face or a video of the single face into a cnn of the neural aggregation network to thereby allow the cnn to generate one or more features for training by applying at least one convolution operation to the facial images for training ii a process of inputting at least one feature vector for training where the features for training are embedded into an aggregation module including at least two attention blocks of the neural aggregation network to thereby allow the aggregation module to generate each of quality scores for training of each of the features for training by aggregation of the features for training using one or more attention parameters learned in a previous iteration iii a process of outputting at least one optimal feature for training by weighted summation of the features for training using the quality scores for training and iv a process of updating the attention parameters learned in the previous iteration of the at least two attention blocks such that one or more losses are minimized which are outputted from a loss layer by referring to the optimal feature for training and its corresponding ground truth an object data processing system comprising at least one processor configured to execute at least one implementation of a plurality of recognition algorithms stored on at least one non-transitory computer-readable storage medium each recognition algorithm having feature density selection criteria and data preprocessing code executed by at least one processor the data preprocessing code comprising an invariant feature identification algorithm and configured to obtain a digital representation of a scene the scene comprising one or more textual media generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation cluster the set of invariant features into regions of interest in the digital representation of the scene each region of interest having a region feature density classify by region classifier code at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation wherein the at least one of the classified regions of interest corresponds to text and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type wherein the another of the regions of interest corresponds to a region of interest for images  the system of claim  wherein preprocessing code based on the feature density selection criteria determines that an ocr algorithm is applicable to the text and that other recognition algorithms are applicable to aspects of the photographs and to logos  the system of claim  wherein a user creates a user profile for a camera-equipped smartphone that includes the information that the user is visually impaired which causes prioritized execution of the ocr algorithm such that a text reader program begins reading the text to the user as quickly as possible  the system of claim  further comprising an audio or tactile feedback mechanism that helps the user to position the smart phone relative to the text  the system of claim  further comprising a \"hold still\" audio feedback signal that is sent to the user when the text is at the center of the captured scene  the system of claim  wherein the digital representation comprises at least one of the following types of digital data image data video data and audio data  the system of claim  wherein invariant feature identification algorithm comprises at least one of the following feature identification algorithms fast sift freak brisk harris daisy and mser  the system of claim  wherein the invariant feature identification algorithm includes at least one of the following edge detection algorithm corner detection algorithm saliency map algorithm curve detection algorithm a texton identification algorithm and wavelets algorithm  the system of claim  wherein at least one region of interest represents at least one physical object in the scene  the system of claim  wherein at least one region of interest represents at least one textual media in the scene  the system of claim  wherein the region of interest represents a document as the textual media  the system of claim  wherein the region of interest represents a financial document  the system of claim  wherein the region of interest represents a structured document  the system of claim  wherein at least one implementation of a plurality of recognition algorithms includes at least one of the following a template driven algorithm a face recognition algorithm an optical character recognition algorithm a speech recognition algorithm and an object recognition algorithm  the system of claim  wherein data preprocessing code is further configured to assign each region of interest at least one recognition algorithm as a function of a scene context derived from the digital representation  the system of claim  wherein the scene context includes at least one of the following types of data a location a position a time a user identity a news event a medical event and a promotion  the system of claim  further comprising a mobile device comprising at least one implementation of a plurality of recognition algorithms and data preprocessing code  the system of claim  wherein the mobile device comprises at least one of the following a smart phone a tablet wearable glass a toy a vehicle a computer and a phablet  the system of claim  further comprising a network-accessible server device comprising at least one implementation of a plurality of recognition algorithms and data preprocessing code  the system of claim  wherein the object type includes at least one of the following a face an animal a vehicle a document a plant a building an appliance clothing a body part and a toy  an object data processing system comprising at least one processor configured to execute at least one implementation of a plurality of recognition algorithms stored on at least one non-transitory computer-readable storage medium each recognition algorithm having feature density selection criteria and data preprocessing code executed by at least one processor the data preprocessing code comprising an invariant feature identification algorithm and configured to obtain a digital representation of a scene the scene comprising one or more textual media generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation cluster the set of invariant features into regions of interest in the digital representation of the scene each region of interest having a region feature density classify by region classifier code at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation wherein the at least one of the classified regions of interest corresponds to text and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type wherein the another of the regions of interest corresponds to a region of interest for images assign each region of interest at least one recognition algorithm from at least one implementation of a plurality of diverse recognition algorithms as a function of the region feature density of each region of interest and the feature density selection criteria of the at least one implementation of a plurality of diverse recognition algorithms and configure the assigned recognition algorithms to process their respective regions of interest wherein preprocessing code based on the feature density selection criteria determines that an ocr algorithm is applicable to the text and that other recognition algorithms are applicable to aspects of the photographs and to logos  a device comprising at least one processor configured to execute at least one implementation of a plurality of recognition algorithms stored on at least one non-transitory computer-readable storage medium each recognition algorithm having feature density selection criteria and data preprocessing code executed by at least one processor the data preprocessing code comprising an invariant feature identification algorithm and configured to obtain a digital representation of a scene the scene comprising one or more textual media generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation cluster the set of invariant features into regions of interest in the digital representation of the scene each region of interest having a region feature density and classify by region classifier code at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation wherein the at least one of the classified regions of interest corresponds to text and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type wherein the another of the regions of interest corresponds to a region of interest for images a mobile terminal comprising a front camera configured to obtain a two-dimensional d face image of a user a glance sensor tilted by a certain angle and disposed adjacent to the front camera to obtain metadata of the d face image and a controller obtaining a distance between the glance sensor and the front camera the distance enabling an area of an overlap region where a first region representing a range photographable by the front camera overlaps a second region representing a range photographable by the glance sensor to be the maximum  the mobile terminal of claim  wherein the controller is configured to obtain the distance enabling the area of the overlap region to be the maximum between the glance sensor and the front camera by varying a tilting angle of the glance sensor  the mobile terminal of claim  wherein the controller is configured to set the distance enabling the area of the overlap region to be the maximum between the glance sensor and the front camera and the tilting angle of the glance sensor as an optimal disposition location of the glance sensor  the mobile terminal of claim  wherein the controller is configured to set a disposition location of the front camera as an original point and calculates coordinates of a first triangle representing the first region based on a field of view of the front camera and a maximum photographing distance of the front camera  the mobile terminal of claim  wherein the controller is configured to calculate coordinates of a second triangle representing the second region based on a field of view of the glance sensor a maximum photographing distance of the glance sensor a distance between the front camera and the glance sensor and a tilting angle of the glance sensor  the mobile terminal of claim  wherein before the glance sensor is tilted the controller is configured to calculate coordinates of a third triangle representing a third region photographable by the glance sensor and the controller is configured to rotation-convert the coordinates of the third triangle based on the tilting angle of the glance sensor and calculate the coordinates of the second triangle  the mobile terminal of claim  wherein the controller is configured to calculate coordinates of the overlap region based on the coordinates of the first triangle and the coordinates of the second triangle and calculates the area of the overlap region based on the coordinates of the overlap region  the mobile terminal of claim  wherein the controller is configured to generate three-dimensional d face information based on the d face image obtained by the front camera and metadata obtained by the glance sensor  the mobile terminal of claim  wherein the metadata comprises one or more of an angle of a face of the user a size of the face and a location of the face  the mobile terminal of claim  wherein the angle of the face comprises an angle by which the face is rotated about one or more of a pitch axis a roll axis and a yaw axis  the mobile terminal of claim  further comprising a memory storing the generated d face information wherein the controller is configured to performs a user authentication process by comparing the stored d face information with d face information obtained for user authentication  the mobile terminal of claim  wherein the glance sensor is controlled to be permanently activated with a low power to obtain a front image and metadata of the front image  the mobile terminal of claim  wherein the front camera and the glance sensor are disposed on the same line in an upper end of the mobile terminal  the mobile terminal of claim  wherein the glance sensor is tilted in one direction of an up direction a down direction a left direction and a right direction  the mobile terminal of claim  wherein the metadata is data which is changed when the mobile terminal is tilted by an external physical force a method comprising receiving by a smart television tv an indication of upcoming media programming wherein the upcoming media programming is based on a user profile identifying one or more devices in communication with the smart tv each of the one or more devices including at least one of a microphone or a camera instructing at least one identified device to detect audio signals using its respective microphone or to detect visual signals using its respective camera selecting at least one device of the one or more devices based on the detected audio signal or detected visual signal and providing instructions to the selected device to output a notification related to the upcoming media programming  the method of claim  wherein the upcoming media programming is one of a live television program a recorded television program a broadcast television program or an application-provided program  the method of claim  wherein selecting the first device based on the detected audio signal includes recognizing a voice  the method of claim  further comprising determining a distance to the recognized voice and wherein selecting the first device is further based on the determined distance  the method of claim  wherein selecting the first device based on the detected visual signals includes recognizing a face  the method of claim  wherein recognizing the face includes a face recognition technique  the method of claim  further comprising presenting on the smart tv the upcoming media programming in a favorite channel list  the method of claim  further comprising obtaining media programming viewing data wherein the media programming viewing data includes at least one of a historical time and a historical date that one or more media programs were viewed obtaining at least one of a current time and a current date processing the media programming viewing data to determine a probability of the one or more media programs being viewed based on at least one of the current time and the current date and presenting the favorite channel list based on the determined probability of the one or more media programs being viewed  the method of claim  wherein processing the media programming viewing data includes employing a neural network model  the method of claim  wherein employing the neural network model comprises determining a duration that the one or more media programs were viewed for each of the at least one of the historical time and the historical date setting a threshold time duration comparing the determined duration to the threshold time duration and filtering out the one or more media programs viewed below the threshold time duration  a smart television tv comprising a network interface a non-transitory computer-readable medium and a processor in communication with the network interface and the non-transitory computer-readable medium and capable of executing processor-executable program code stored in the non-transitory computer-readable medium to cause the smart tv to receive an indication of upcoming media programming wherein the upcoming media programming is based on a user profile identify one or more devices in communication with the smart tv each of the one or more devices including at least one of a microphone or a camera instruct at least one identified device to detect audio signals using its respective microphone or to detect visual signals using its respective camera select at least one device of the one or more devices based on the detected audio signal or detected visual signal and provide instructions to the selected device to output a notification related to the upcoming media programming  the smart tv of claim  wherein selecting the first device based on the detected audio signal includes recognizing a voice  the smart tv of claim  wherein the processor is further capable of executing processor-executable program code to determine a distance to the recognized voice and wherein selecting the first device is further based on the determined distance  the smart tv of claim  wherein selecting the first device based on the detected visual signals includes detecting the presence of a user  the smart tv of claim  wherein detecting the presence of the user includes employing one or more of a camera a microphone or a fingerprint sensor associated with at least one of the smart tv a mobile device a smartphone a laptop computer a tablet device a wearable device an internet of things iot device an internet of everything ioe device an iot hub or an ioe hub  a smart television tv comprising means for receiving an indication of upcoming media programming wherein the upcoming media programming is based on a user profile means for identifying one or more devices in communication with the smart tv each of the one or more devices including at least one of a microphone or a camera means for instructing at least one identified device to detect audio signals using its respective microphone or to detect visual signals using its respective camera means for selecting at least one device of the one or more devices based on the detected audio signal or detected visual signal and means for providing instructions to the selected device to output a notification related to the upcoming media programming  the smart tv of claim  wherein the one or more devices includes at least one of a mobile device a smartphone a laptop computer a tablet device a wearable device an internet of things iot device an internet of everything ioe device an iot hub an ioe hub or another smart tv  the smart tv of claim  wherein the upcoming media programming is one of a live television program a recorded television program a broadcast television program or an application-provided program  the smart tv of claim  wherein the notification includes at least one of a push message a sms message a waysms message an audio alert an audio message or an email message  the smart tv of claim  further comprising presenting the upcoming media programming in a favorite channel list  the smart tv of claim  further comprising means for obtaining media programming viewing data wherein the media programming viewing data includes at least one of a historical time and a historical date that one or more media programs were viewed on the smart tv means for obtaining at least one of a current time and a current date means for processing the media programming viewing data to determine a probability of the one or more media programs being viewed on the smart tv based on at least one of the current time and the current date and means for presenting the favorite channel list based on the determined probability of the one or more media programs being viewed  the smart tv of claim  wherein the means for processing the media programming viewing data includes employing a neural network model  the smart tv of claim  wherein employing the neural network model comprises determining a duration that the one or more media programs were viewed on the smart tv for each of the at least one of the historical time and the historical date setting a threshold time duration comparing the determined duration to the threshold time duration and filtering out the one or more media programs viewed below the threshold time duration  the smart tv of claim  further comprising means for adjusting at least one of a volume or a brightness of the smart tv wherein the adjusting is based on at least one of the historical time and the historical date  the smart tv of claim  further comprising means for restricting access to one or more media programs  a non-transitory computer-readable medium comprising processor-executable program code configured to cause a processor of a smart television tv to receive an indication of upcoming media programming wherein the upcoming media programming is based on a user profile identify one or more devices in communication with the smart tv each of the one or more devices including at least one of a microphone or a camera instruct at least one identified device to detect audio signals using its respective microphone or to detect visual signals using its respective camera select at least one device of the one or more devices based on the detected audio signal or detected visual signal and provide instructions to the selected device to output a notification related to the upcoming media programming  the non-transitory computer-readable medium of claim  wherein selecting the first device based on the detected audio signal includes recognizing a voice  the non-transitory computer-readable medium of claim  wherein the processor is further capable of executing processor-executable program code to determine a distance to the recognized voice and wherein selecting the first device is further based on the determined distance  the non-transitory computer-readable medium of claim  wherein selecting the first device based on the detected visual signals includes recognizing a face  the non-transitory computer-readable medium of claim  wherein recognizing the face includes a face recognition technique a camera comprising a sensor array including a plurality of sensors an infrared ir illuminator configured to emit active ir light in an ir light sub-band a plurality of spectral illuminators each spectral illuminator configured to emit active spectral light in a different spectral light sub-band a depth controller machine configured to determine a depth value for each of the plurality of sensors based on the active ir light a spectral controller machine configured to for each of the plurality of sensors determine a spectral value for each spectral light sub-band of the plurality of spectral illuminators and an output machine configured to output a test depth+multi-spectral image including a plurality of pixels each pixel corresponding to one of the plurality of sensors of the sensor array and including at least a depth value and a spectral value for each spectral light sub-band of the plurality of spectral illuminators a face recognition machine previously trained with a set of labeled training depth+multi-spectral images having a same structure as the test depth+multi-spectral image the face recognition machine configured to output a confidence value indicating a likelihood that the test depth+multi-spectral image includes a face  the camera of claim  wherein each spectral value is calculated based on the depth value determined for the sensor that corresponds to the pixel  the camera of claim  wherein the face recognition machine is configured to use a convolutional neural network to determine the confidence value  the camera of claim  wherein the face recognition machine includes a plurality of input nodes wherein each input node is configured to receive a pixel value array corresponding to a different pixel of the plurality of pixels of the test depth+multi-spectral image and wherein the pixel value array includes the depth value and the plurality of multi-spectral values for the pixel  the camera of claim  wherein the plurality of multi-spectral values for the pixel include more than three spectral values  the camera of claim  wherein the output machine is configured to output a surface normal for each pixel of the test depth+multi-spectral image and wherein the pixel value array includes the surface normal  the camera of claim  wherein the output machine is configured to output a curvature for each pixel of the test depth+multi-spectral image and wherein the pixel value array includes the curvature  the camera of claim  wherein the face recognition machine is configured to use a plurality of models to determine the confidence value wherein the plurality of models includes a plurality of channel-specific models wherein each channel-specific model is configured to process a different pixel parameter for the plurality of pixels of the test depth+multi-spectral image wherein each channel-specific model includes a plurality of input nodes and wherein for each channel-specific model each input node is configured to receive a pixel parameter value for a different pixel of the plurality of pixels of the test depth+multi-spectral image  the camera of claim  wherein the face recognition machine is configured to use a statistical model to determine the confidence value  the camera of claim  wherein the statistical model includes a nearest neighbor algorithm  the camera of claim  wherein the statistical model includes a support vector machine  the camera of claim  wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of a bounding box around a recognized face  the camera of claim  wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of an identified two-dimensional d facial feature of a recognized face  the camera of claim  wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of an identified three-dimensional d facial feature of a recognized face  the camera of claim  wherein the face recognition machine is further configured to output a location on the test depth+multi-spectral image of an identified spectral feature on a recognized face  the camera of claim  wherein the face recognition machine is further configured to output for each pixel of the test depth+multi-spectral image a confidence value indicating a likelihood that the pixel is included in a face  the camera of claim  wherein the face recognition machine is further configured to output an identity of a face recognized in the test depth+multi-spectral image  the camera of claim  wherein the plurality of sensors of the sensor array are differential sensors and wherein each spectral value is determined based on a depth value and a differential measurement for that differential sensor  a camera comprising a sensor array including a plurality of sensors an infrared ir illuminator configured to emit active ir light in an ir light sub-band a plurality of spectral illuminators each spectral illuminator configured to emit active spectral light in a different spectral light sub-band a depth controller machine configured to determine a depth value for each of the plurality of sensors based on the active ir light a spectral controller machine configured to for each of the plurality of sensors determine a spectral value for each spectral light sub-band of the plurality of spectral illuminators wherein each spectral value is calculated based on the depth value determined for the sensor that corresponds to the pixel and an output machine configured to output a test depth+multi-spectral image including a plurality of pixels each pixel corresponding to one of the plurality of sensors of the sensor array and including at least a depth value and a spectral value for each spectral light sub-band of the plurality of spectral illuminators and a face recognition machine including a convolutional neural network previously trained with a set of labeled training depth+multi-spectral images having a same structure as the test depth+multi-spectral image the face recognition machine configured to output a confidence value indicating a likelihood that the test depth+multi-spectral image includes a face an image processing method comprising acquiring a photo album obtained from face clustering collecting face information of respective images in the photo album and acquiring a face parameter of each image according to the face information selecting a cover image according to the face parameter of each image and taking a face-region image from the cover image and setting the face-region image as a cover of the photo album wherein selecting the cover image according to the face parameter of each image comprises performing calculation on the face parameter of each image in a preset way to obtain a cover score of each image selecting the image with a highest cover score as the cover image wherein selecting the image with the highest cover score as the cover image comprises acquiring a source of each image and selecting the image with the highest cover score in images coming from a preset source as the cover image  the method according to claim  wherein selecting the image with the highest cover score as the cover image comprises acquiring the number of faces contained in each image determining single-person images according to the number of faces and selecting the single-person image with the highest cover score as the cover image  the method according to claim  wherein selecting the image with the highest cover score as the cover image further comprises when there is no single-person image in the photo album determining images including two faces from the photo album and selecting the image with the highest cover score from the images including two faces as the cover image  the method according to claim  wherein the face information comprises face feature points and the face parameter comprises a face turning angle acquiring the face parameter of each image according to the face information comprises acquiring coordinate values of the face feature points determining distances and angles between the face feature points and determining the face turning angle according to the distances and the angles  the method according to claim  wherein the face parameter comprises a face ratio acquiring the face parameter of each image according to the face information comprises determining a face region of the image according to the face information and calculating a ratio of an area of the face region to an area of the image to obtain the face ratio  the method according to claim  wherein calculating the face ratio comprises when there is more than one face in the image subtracting an area occupied faces other than a face corresponding to the photo album from the face region to obtain a remaining area and calculating a ratio of the remaining area to the area of the image to obtain the face ratio  the method according to claim  wherein collecting face information of respective images in the photo album comprises acquiring image identifications of images in the photo album extracting face information corresponding to the image identifications from a face database the face database being stored with face recognition results of images the face recognition results including the face information  an image processing apparatus comprising a processor and a memory configured to store instructions executable by the processor wherein the processor is configured to run a program corresponding to the instructions by reading the instructions stored in the memory so as to perform acquiring a photo album obtained from face clustering collecting face information of each image in the photo album acquiring a face parameter of each image according to the face information selecting a cover image according to the face parameter of each image taking a face-region image from the cover image and setting the face-region image as a cover of the photo album wherein the processor is configured to perform calculation on the face parameter of each image in a preset way to obtain a cover score of each image and select the image with a highest cover score as the cover image and wherein the processor is configured to acquire a source of each image and select the image with the highest cover score in images coming from a preset source as the cover image  the apparatus according to claim  wherein the processor is configured to acquire the number of faces contained in each image determine single-person images according to the number of faces and select the single-person image with the highest cover score as the cover image  the apparatus according to claim  wherein the processor is further configured to when there is no single-person image in the photo album determine images including two faces from the photo album and select the image with the highest cover score from the images including two faces as the cover image  the apparatus according to claim  wherein the face information comprises face feature points and the face parameter comprises a face turning angle the processor is configured to acquire coordinate values of the face feature points determine distances and angles between the face feature points and determine the face turning angle according to the distances and the angles  the apparatus according to claim  wherein the face parameter comprises a face ratio the processor is configured to determine a face region of the image according to the face information and calculate a ratio of an area of the face region to an area of the image to obtain the face ratio  the apparatus according to claim  wherein the processor is configured to when there is more than one face in the image subtract an area occupied faces other than a face corresponding to the photo album from the face region to obtain a remaining area and calculate a ratio of the remaining area to the area of the image to obtain the face ratio  the apparatus according to claim  wherein the processor is configured to acquire image identifications of images in the photo album extract face information corresponding to the image identifications from a face database the face database being stored with face recognition results of images the face recognition results including the face information  an electronic device comprising a processor a memory a display screen and an input device connected via a system bus wherein the memory is stored with computer programs that when executed by the processor cause the processor to implement an image processing method the image processing method comprising acquiring a photo album obtained from face clustering collecting face information of respective images in the photo album and acquiring a face parameter of each image according to the face information selecting a cover image according to the face parameter of each image and taking a face-region image from the cover image and setting the face-region image as a cover of the photo album wherein selecting the cover image according to the face parameter of each image comprises performing calculation on the face parameter of each image in a preset way to obtain a cover score of each image and selecting the image with a highest cover score as the cover image and wherein selecting the image with the highest cover score as the cover image comprises acquiring a source of each image and selecting the image with the highest cover score in images coming from a preset source as the cover image  the electronic device according to claim  wherein the electronic device comprises at least one of a mobile phone a tablet computer a personal digital assistant and a wearable device a computer-implemented method comprising receiving at a computing device a meeting invitation identifying a location and at least one invitee the meeting invitation configured to provide the at least one invitee with physical access to the location wherein the meeting invitation causes a system to control a pathway allowing physical access to the location providing based on the meeting invitation the at least one invitee with physical access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location wherein the positioning data is based in part on a face recognition camera system identifying the at least one invitee receiving the positioning data from the face recognition camera system identifying the at least one invitee wherein the positioning data indicates a pattern of movement of the at least one invitee determining that the pattern of movement indicates that the at least one invitee has exited the location and revoking physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway in response to determining that the pattern of movement indicates that the at least one invitee has exited the location  the computer-implemented method of claim  wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location in a predetermined direction  the computer-implemented method of claim  wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has moved through an area in a predetermined direction  the computer-implemented method of claim  wherein the positioning data indicates a second pattern of movement of the at least one invitee and wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement  the computer-implemented method of claim  further comprising collating secured data and public data to generate resource data and communicating the resource data to a client computing device associated with the at least one invitee when access of the location is provided  the computer-implemented method of claim  wherein the positioning data indicates that the at least one invitee is at the predetermined location when the at least one invitee passes through the predetermined location  the computer-implemented method of claim  wherein the positioning data indicates that the at least one invitee is at the predetermined location when the at least one invitee passes through the predetermined location near the location in a predetermined direction  a system comprising a processor and a memory in communication with the processor the memory having computer-readable instructions stored thereupon that when executed by the processor cause the processor to receive a meeting invitation indicating a location and an identity the meeting invitation configured to provide at least one invitee with physical access to the location wherein the meeting invitation causes the system to control a pathway allowing physical access to the location provide the at least one invitee associated with the identity access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location wherein the positioning data is based in part on a face recognition camera system identifying the at least one invitee receive the positioning data from the face recognition camera system identifying the at least one invitee wherein the positioning data indicates a pattern of movement of the at least one invitee determine that the pattern of movement indicates that the at least one invitee has exited the location and revoke physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway in response to determining that the pattern of movement indicates that the at least one invitee has exited the location  the system of claim  wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location  the system of claim  wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has moved through an area in a predetermined direction  the system of claim  wherein the positioning data indicates a second pattern of movement of the at least one invitee and wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement  the system of claim  wherein the instructions further cause the processor to collate secured data and public data to generate resource data and communicate the resource data to a client computing device associated with the at least one invitee when access of the location is provided  a non-transitory computer-readable storage medium having computer-executable instructions stored thereupon which when executed by one or more processors of a computing device cause the one or more processors of the computing device to receive a meeting invitation indicating a location and an identity the meeting invitation configured to provide at least one invitee with physical access to the location wherein the meeting invitation causes a system to control a pathway allowing physical access to the location provide the at least one invitee associated with the identity access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location wherein the positioning data is based in part on a face recognition camera system identifying the at least one invitee receive the positioning data from the face recognition camera system identifying the at least one invitee wherein the positioning data indicates a pattern of movement of the at least one invitee determine that the pattern of movement indicates that the at least one invitee has exited the location and revoke physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway in response to determining that the pattern of movement indicates that the at least one invitee has exited the location  the non-transitory computer-readable storage medium of claim  wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location  the non-transitory computer-readable storage medium of claim  wherein the positioning data indicates a second pattern of movement of the at least one invitee and wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement  the non-transitory computer-readable storage medium of claim  wherein the instructions further cause the one or more processors to collate secured data and public data to generate resource data and communicate the resource data to a client computing device associated with the at least one invitee when access of the location is provided a method comprising receiving a piece of content and salient data for the piece of content based on the salient data determining a first path for a viewport for the piece of content wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content providing the viewport on a display device wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback detecting an additional salient event in the piece of content that is not included in the first path for the viewport and providing an indication for the additional salient event in the viewport during the playback  the method of claim  wherein the salient data identifies each salient event in the piece of content and the salient data indicates for each salient event in the piece of content a corresponding point location of the salient event in the piece of content and a corresponding time at which the salient event occurs during the playback  the method of claim  wherein the salient data further indicates for each salient event in the piece of content a corresponding type of the salient event and a corresponding strength value of the salient event  the method of claim  wherein the first path for the viewport controls the movement of the viewport to put the different salient events in a view of the viewport at the different times during the playback  the method of claim  further comprising detecting one or more salient events in the piece of content based on at least one of the following visual data of the piece of content audio data of the piece of content or content consumption experience data for the piece of content wherein the salient data is indicative of each salient event detected  the method of claim  further comprising detecting one or more salient events in the piece of content based on at least one of the following face recognition facial emotion recognition object recognition motion recognition or metadata of the piece of content wherein the salient data is indicative of each salient event detected  the method of claim  further comprising detecting user interaction with the indication wherein the indication comprises an interactive hint and in response to detecting the user interaction adapting the first path for the viewport to a second path for the viewport based on the user interaction wherein the second path for the viewport includes the additional salient event and providing an updated viewport for the piece of content on the display device wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport  the method of claim  further comprising changing a weight assigned to the additional salient event and one or more other salient events in the piece of content having the same type as the additional salient event  the method of claim  wherein the second path for the viewport includes one or more other salient events in the piece of content having the same type as the additional salient event  a system comprising at least one processor and a non-transitory processor-readable memory device storing instructions that when executed by the at least one processor causes the at least one processor to perform operations including receiving a piece of content and salient data for the piece of content based on the salient data determining a first path for a viewport for the piece of content wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content providing the viewport on a display device wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback detecting an additional salient event in the piece of content that is not included in the first path for the viewport and providing an indication for the additional salient event in the viewport during the playback  the system of claim  wherein the salient data identifies each salient event in the piece of content and the salient data indicates for each salient event in the piece of content a corresponding point location of the salient event in the piece of content and a corresponding time at which the salient event occurs during the playback  the system of claim  wherein the salient data further indicates for each salient event in the piece of content a corresponding type of the salient event and a corresponding strength value of the salient event  the system of claim  wherein the salient data is generated offline on a server  the system of claim  the operations further comprising detecting one or more salient events in the piece of content based on at least one of the following visual data of the piece of content audio data of the piece of content or content consumption experience data for the piece of content wherein the salient data is indicative of each salient event detected  the system of claim  the operations further comprising detecting one or more salient events in the piece of content based on at least one of the following face recognition facial emotion recognition object recognition motion recognition or metadata of the piece of content wherein the salient data is indicative of each salient event detected  the system of claim  the operations further comprising detecting user interaction with the indication wherein the indication comprises an interactive hint and in response to detecting the user interaction adapting the first path for the viewport to a second path for the viewport based on the user interaction wherein the second path for the viewport includes the additional salient event and providing an updated viewport for the piece of content on the display device wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport  the system of claim  the operations further comprising changing a weight assigned to the additional salient event and one or more other salient events in the piece of content having the same type as the additional salient event  the system of claim  wherein the second path for the viewport includes one or more other salient events in the piece of content having the same type as the additional salient event  a non-transitory computer readable storage medium including instructions to perform a method comprising receiving a piece of content and salient data for the piece of content based on the salient data determining a first path for a viewport for the piece of content wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content providing the viewport on a display device wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback detecting an additional salient event in the piece of content that is not included in the first path for the viewport and providing an indication for the additional salient event in the viewport during the playback  the computer readable storage medium of claim  the method further comprising detecting user interaction with the indication wherein the indication comprises an interactive hint and in response to detecting the user interaction adapting the first path for the viewport to a second path for the viewport based on the user interaction wherein the second path for the viewport includes the additional salient event and providing an updated viewport for the piece of content on the display device wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport a mobile device with facial recognition the mobile device comprising one or more cameras a processor device and memory coupled to the processor device the processing system programmed to receive a plurality of images from the one or more cameras extract with a feature extractor utilizing a convolutional neural network cnn with an enlarged intra-class variance of long-tail classes feature vectors from each of the plurality of images generate with a feature generator discriminative feature vectors for each of the feature vectors classify with a fully connected classifier an identity from the discriminative feature vectors and control an operation of the mobile device to react in accordance with the identity  the mobile device as recited in claim  further includes a communication system  the mobile device as recited in claim  wherein the operation tags the video with the identity and uploads the video to social media  the mobile device as recited in claim  wherein the operation tags the video with the identity and sends the video to a user  the mobile device as recited in claim  wherein the mobile device is a smart phone  the mobile device as recited in claim  wherein the mobile device is a body cam  the mobile device as recited in claim  further programmed to train the feature extractor the feature generator and the fully connected classifier with an alternative bi-stage strategy  the mobile device as recited in claim  wherein the feature extractor shares covariance matrices across all classes to transfer intra-class variance from regular classes to the long-tail classes  the mobile device as recited in claim  wherein the feature generator optimizes a softmax loss by joint regularization of weights and features through a magnitude of an inner product of the weights and features  the mobile device as recited in claim  wherein the feature extractor averages the feature vector with a flipped feature vector the flipped feature vector being generated from a horizontally flipped frame from one of the plurality of images  the mobile device as recited in claim  wherein each of the plurality of images is selected from the group consisting of an image a video and a frame from the video  the mobile device as recited in claim  wherein the communication system connects to a remote server that includes a facial recognition network  the mobile device as recited in claim  wherein one stage of the alternative bi-stage strategy fixes the feature extractor and applies the feature generator to generate new transferred features that are more diverse and violate a decision boundary  the mobile device as recited in claim  wherein one stage of the alternative bi-stage strategy fixes the fully connected classifier and updates the feature extractor and the feature generator  a computer program product for a mobile device with facial recognition the computer program product comprising a non-transitory computer readable storage medium having program instructions embodied therewith the program instructions executable by a computer to cause the computer to perform a method comprising receiving by a processor device a plurality of images extracting by the processor device with a feature extractor utilizing a convolutional neural network cnn with an enlarged intra-class variance of long-tail classes feature vectors for each of the plurality of images generating by the processor device with a feature generator discriminative feature vectors for each of the feature vectors classifying by the processor device utilizing a fully connected classifier an identity from the discriminative feature vector and controlling an operation of the mobile device to react in accordance with the identity  a computer-implemented method for facial recognition in a mobile device the method comprising receiving by a processor device a plurality of images extracting by the processor device with a feature extractor utilizing a convolutional neural network cnn with an enlarged intra-class variance of long-tail classes feature vectors for each of the plurality of images generating by the processor device with a feature generator discriminative feature vectors for each of the feature vectors classifying by the processor device utilizing a fully connected classifier an identity from the discriminative feature vector and controlling an operation of the mobile device to react in accordance with the identity  the computer-implemented method as recited in claim  wherein controlling includes tagging the video with the identity and uploading the video to social media  the computer-implemented method as recited in claim  wherein controlling includes tagging the video with the identity and sending the video to a user  the computer-implemented method as recited in claim  wherein extracting includes sharing covariance matrices across all classes to transfer intra-class variance from regular classes to the long-tail classes a computing device comprising a non-transitory machine readable medium storing a machine trained mt network comprising a plurality of layers of processing nodes each processing node configured to compute a first output value by combining a set of output values from a set of processing nodes and use a piecewise linear cup function to compute a second output value from the first output value of the processing node wherein the piecewise linear cup function prior to training of the mt network comprises at least i a first linear section with a first slope followed by ii a second linear section with a negative second slope followed by iii a third linear section with a negative third slope that is different from the second slope followed by iv a fourth linear section with a positive fourth slope followed by v a fifth linear section with a positive fifth slope that is different from the fourth slope followed by vi a sixth linear section with a sixth slope wherein the piecewise linear cup function is symmetric about a vertical axis between the third and fourth linear sections prior to training of the mt network a content capturing circuit for capturing content for processing by the mt network and a set of processing units for executing the processing nodes to process content captured by the content capturing circuit wherein by training a set of parameters that define the piecewise linear cup function of each node in first and second pluralities of processing nodes i each processing node in the first plurality of processing nodes is configured to emulate a boolean and operator such that an output value of the processing node is in a range associated with a \"\" value only when a set of inputs to the processing node have a set of values in a range associated with \"\" and ii each processing node in the second plurality of processing nodes is configured to emulate a boolean xnor operator such that an output value of the processing node is in the range associated with \"\" only when a a set of inputs to the node have a set of values in a range associated with \"\" or b the set of inputs to the node have a set of values in a range associated with a \"\" value  the computing device of claim  wherein the third linear section of the piecewise linear cup function of a first processing node in the mt network has a different slope from the third linear section of a second processing node in the mt network  the computing device of claim  wherein the length of the third section of a piecewise linear cup function of a first processing node in the mt network is different from the length of the third section of a piecewise linear cup function of a second processing node in the mt network  the computing device of claim  wherein the sets of parameters are trained in part by a back propagating module for back propagating errors in output values of later layers of processing nodes to earlier layers of processing nodes by adjusting the set of parameters that define the piecewise linear cup functions of the earlier layers of processing nodes  the computing device of claim  wherein each processing node uses a linear function that is defined by a set of parameters to compute the first output value of the processing node wherein the back propagating module back propagates errors in output values of later layers of processing nodes to earlier layers of processing nodes by adjusting the set of parameters that define the linear functions of the earlier layers of processing nodes  the computing device of claim  wherein the first plurality of processing nodes that emulate the boolean and operator and the second plurality of processing nodes that emulate the boolean xnor operator enable the mt network to implement mathematical problems  the computing device of claim  wherein each of a plurality of processing node layers has a plurality of processing nodes that receive as input values the output values from a plurality of processing nodes in a set of prior layers  the computing device of claim  wherein each processing node uses a linear function to compute the first output value of the processing node wherein each processing node's piecewise linear cup function is defined along first and second axes the first axis defining a range of output values from the processing node's linear function and the second axis defining a range of output values produced by the piecewise linear cup function for the range of output values from the processing node's linear function  the computing device of claim  further comprising a content output circuit for presenting an output based on the processing of the content by the mt network  the computing device of claim  wherein the captured content is one of an image and an audio segment and wherein the presented output is an output display on a display screen of the computing device or an audio presentation output on a speaker of the computing device  the computing device of claim  wherein the computing device is a mobile device  the computing device of claim  wherein the mt network is a mt neural network and the processing nodes are mt neurons  the computing device of claim  wherein the set of parameters configured through training for a plurality of the processing nodes comprise at least one of the negative second and third slopes for the second and third linear sections the positive fourth and fifth slopes for the fourth and fifth linear sections a first intercept for the second linear section a second intercept for the fifth linear section and a set of lengths for at least the second third fourth and fifth sections  the computing device of claim  wherein the trained set of parameters that define the piecewise linear cup function of each node comprise a plurality of output values  the computing device of claim  wherein the first and sixth slopes are zerowe claim  a system comprising a memory device to store an input image a processor including an image input interface to receive the input image a pre-processor to model the input image to yield a multi-channel image a feature extractor to extract a set of features based on the multi-channel image a feature selector to select one or more features from the set of features of the multi-channel image wherein the one or more features are selected based on an ability to differentiate features a feature matcher to match the one or more features to a learned feature set and a similarity detector to determine whether the one or more features meet a pre-defined similarity threshold  the system of claim  wherein the pre-processor further is to activate one or more channels of the multi-channel image to yield one or more activated channels  the system of claim  wherein the one or more activated channels are to be determined based on their ability to differentiate features  the system of claim  wherein the pre-processor further is to activate one or more local patches of the one or more activated channels  the system of claim  wherein the one or more local patches are to be determined based on their ability to differentiate features  the system of claim  wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching  an apparatus comprising an image input interface to receive an input image a pre-processor to model the input image to yield a multi-channel image a feature extractor to extract a set of features based on the multi-channel image a feature selector to select one or more features from the set of features of the multi-channel image wherein the one or more features are selected based on an ability to differentiate features a feature matcher to match the one or more features to a learned feature set and a similarity detector to determine whether the one or more features meet a pre-defined similarity threshold  the apparatus of claim  wherein the pre-processor further is to activate one or more channels of the multi-channel image to yield one or more activated channels  the apparatus of claim  wherein the one or more activated channels are to be determined based on their ability to differentiate features  the apparatus of claim  wherein the pre-processor further is to activate one or more local patches of the one or more activated channels  the apparatus of claim  wherein the one or more local patches are to be determined based on their ability to differentiate features  the apparatus of claim  wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching  a method comprising modeling an input image to yield a multi-channel image extracting a set of features based on the multi-channel image selecting one or more features from the set of features of the multi-channel image wherein the one or more features are selected based on an ability to differentiate features matching the one or more features to a learned feature set and determining whether the one or more features meet a pre-defined similarity threshold  the method of claim  wherein modeling the input image further is to include activating one or more channels of the multi-channel image to yield one or more activated channels  the method of claim  wherein the one or more activated channels are to be determined based on their ability to differentiate features  the method of claim  wherein extracting features of the input image further is to include activating one or more local patches of the one or more activated channels  the method of claim  wherein the one or more local patches are to be determined based on their ability to differentiate features  the method of claim  wherein the feature matcher utilizes a large-scale data learning process to perform the feature matching  at least one non-transitory computer readable storage medium comprising a set of instructions which when executed by a computing device cause the computing device to model an input image to yield a multi-channel image extract a set of features based on the multi-channel image select one or more features from the set of features of the multi-channel image wherein the features are selected based on an ability to differentiate features match the one or more features to a learned feature set and determine whether the one or more features meet a pre-defined similarity threshold  the at least one non-transitory computer readable storage medium of claim  wherein the instructions when executed cause a computing device to activate one or more channels of the multi-channel image to yield one or more activated channels  the at least one non-transitory computer readable storage medium of claim  wherein the instructions when executed cause a computing device to determine the one or more activated channels based on their ability to differentiate features  the at least one non-transitory computer readable storage medium of claim  wherein extracting features of the input image is to further include activating one or more local patches of the one or more activated channels  the at least one non-transitory computer readable storage medium of claim  wherein the one or more local patches are to be determined based on their ability to differentiate features  the at least one non-transitory computer readable storage medium of claim  wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching  an apparatus comprising means for modeling an input image to yield a multi-channel image means for extracting a set of features based on the multi-channel image means for selecting one or more features from the set of features of the multi-channel image wherein the one or more features are selected based on an ability to differentiate features means for matching the one or more features to a learned feature set and means for determining whether the one or more features meet a pre-defined similarity threshold a method for controlling a terminal the terminal comprising a capturing apparatus and at least one processor the method comprising acquiring by the capturing apparatus an image obtaining by the at least one processor a motion parameter of the terminal the motion parameter comprising at least one of a motion frequency or a motion time and two or more parameters from among an acceleration an angular velocity a motion amplitude the motion frequency and the motion time transmitting by the at least one processor a parameter threshold obtaining request to a data management server the parameter threshold obtaining request comprising configuration information of the terminal receiving corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request comparing the two or more parameters with the corresponding preset thresholds and controlling by the at least one processor not to perform image processing on the acquired image based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds wherein the acquiring comprises acquiring the image in real time and the obtaining comprises obtaining the motion parameter of the terminal in real time the method further comprising in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold obtaining the motion parameter of the terminal again and in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds performing the image processing on the image acquired at the latest time  the method according to claim  wherein the acquiring comprises controlling by the at least one processor to turn on the capturing apparatus based on a face recognition instruction and acquiring by the capturing apparatus a face image when the capturing apparatus is turned on  the method according to claim  wherein the controlling not to perform the image processing comprises skipping performing face recognition on the acquired face image based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds  the method according to claim  wherein the obtaining comprises at least one of obtaining the acceleration of the terminal by using an acceleration sensor or obtaining the angular velocity of the terminal by using a gyro sensor  the method according to claim  wherein the transmitting comprises transmitting the parameter threshold obtaining request to the data management server according to a preset time period  the method according to claim  further comprising generating prompt information based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold the prompt information being used for prompting the terminal to stop moving  the method according to claim  wherein the motion parameter comprises the motion frequency and the motion time  a terminal comprising a capturing apparatus at least one memory configured to store program code and at least one processor configured to access the at least one memory and operate according to the program code the program code comprising motion parameter obtaining code configured to cause the at least one processor to acquire an image by using the capturing apparatus and obtain a motion parameter of the terminal the motion parameter comprising at least one of a motion frequency or a motion time and two or more parameters from among an acceleration an angular velocity a motion amplitude the motion frequency and the motion time request transmitting code configured to cause the at least one processor to transmit a parameter threshold obtaining request to a data management server the parameter threshold obtaining request comprising configuration information of the terminal parameter threshold receiving code configured to cause the at least one processor to receive corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request comparing code configured to cause the at least one processor to compare the two or more parameters with the corresponding preset thresholds and control code configured to cause the at least one processor not to perform image processing on the acquired image based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds wherein the motion parameter obtaining code causes the at least one processor to acquire the image in real time and obtain the motion parameter of the terminal in real time and in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold obtain the motion parameter of the terminal again and wherein the control code causes the at least one processor to in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds perform the image processing on the image acquired at the latest time  the terminal according to claim  wherein the program code further comprises face instruction receiving code configured to cause the at least one processor to receive a face recognition instruction wherein the motion parameter obtaining code causes the at least one processor to control according to the face recognition instruction the capturing apparatus to turn on and acquire a face image by using the capturing apparatus when the capturing apparatus is turned on and wherein the control code causes the at least one processor to skip performing face recognition on the acquired face image based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds  the terminal according to claim  wherein the request transmitting code causes the at least one processor to transmit the parameter threshold obtaining request to the data management server according to a preset time period  the terminal according to claim  wherein the program code further comprises prompt information generation code configured to cause the at least one processor to generate prompt information based on at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold the prompt information being used for prompting the terminal to stop moving  the terminal according to claim  wherein the motion parameter comprises the motion frequency and the motion time  a non-transitory computer-readable storage medium storing a machine instruction which when executed by one or more processors causes the one or more processors to perform obtaining an image acquired by a capturing apparatus obtaining a motion parameter of a terminal the terminal comprising the capturing apparatus the motion parameter comprising at least one of a motion frequency or a motion time and two or more parameters from among an acceleration an angular velocity a motion amplitude the motion frequency and the motion time transmitting a parameter threshold obtaining request to a data management server the parameter threshold obtaining request comprising configuration information of the terminal receiving corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request comparing the two or more parameters with the corresponding preset thresholds and controlling not to perform image processing on an acquired image based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds wherein the acquiring comprises acquiring the image in real time and the obtaining comprises obtaining the motion parameter of the terminal in real time the method further comprising in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold obtaining the motion parameter of the terminal again and in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds performing the image processing on the image acquired at the latest time  the non-transitory computer-readable storage medium according to claim  wherein the acquired image is a face image and the image processing comprises performing face recognition  the non-transitory computer-readable storage medium according to claim  wherein the obtaining the motion parameter comprises at least one of obtaining the acceleration of the terminal by using an acceleration sensor or obtaining the angular velocity of the terminal by using a gyro sensor  the non-transitory computer-readable storage medium according to claim  wherein the motion parameter comprises the motion frequency and the motion time a method of processing a drive-through order the method comprising receiving customer information detected through vision recognition providing product information to a customer based on the customer information and processing a product order of the customer  the method according to claim  wherein the receiving of customer information comprises at least one of receiving customer information associated with vehicle information detected through vehicle recognition or receiving customer information associated with identification information detected through face recognition  the method according to claim  further comprising determining whether the customer is a pre-order customer based on the customer information wherein when the customer is determined to be a pre-order customer the providing of product information based on the customer information comprises providing pre-order information using at least one of audio or video and the processing of the product order of the customer comprises providing information for promptly guiding a vehicle to a pickup stand using at least one of audio or video and providing information that an additional order is available  the method according to claim  wherein the product information based on the customer information comprises a most recently ordered product component and a most frequently ordered product component in an order history of the customer information  the method according to claim  wherein the receiving of customer information comprises receiving information about an age and gender of a passenger detected through face recognition and the providing of product information to a customer based on the customer information comprises providing recommended menu information differentiated according to the age and gender  the method according to claim  wherein the processing of a product order of the customer comprises determining a product component in a past order history or a component modified from the product component as a product order  the method according to claim  wherein the processing of a product order of the customer comprises paying a product price according to biometrics-based authentication through a communication system of a vehicle or a mobile terminal  the method according to claim  wherein the processing of a product order of the customer comprises issuing a payment number for a divided payment and performing the divided payments according to payment requests of a plurality of mobile terminals to which the payment numbers are inputted  the method according to claim  wherein the processing of a product order of the customer further comprises accumulating mileage in an account corresponding to the mobile terminal undergoing a payment  the method according to claim  wherein the processing of a product order of the customer further comprises suggesting a takeout packaging method according to a temperature of a product an atmospheric temperature weather and a vehicle type  an apparatus configured to process a drive-through order the apparatus comprising a transceiver configured to receive customer information detected through vision recognition a digital signage configured to provide product information to a customer based on the customer information and a processor configured to process a product order of the customer  the apparatus according to claim  wherein the transceiver receives at least one of customer information associated with vehicle information detected through vehicle recognition or customer information associated with identification information detected through face recognition  the apparatus according to claim  wherein the processor is configured to determine whether the customer is a pre-order customer based on the customer information and when the customer is determined to be a pre-order customer perform a control operation to provide pre-order information and control the digital signage to output information for promptly guiding a vehicle to a pickup stand and provide information that an additional order is available  the apparatus according to claim  wherein the product information based on the customer information comprises a most recently ordered product component and a most frequently ordered product component in an order history of the customer information  the apparatus according to claim  wherein the transceiver is configured to receive information about an age and gender of a passenger detected through face recognition and the processor is configured to control the digital signage to provide recommended menu information differentiated according to the age and gender  the apparatus according to claim  wherein the processor is configured to determine a product component in a past order history or a component modified from the product component as the product order  the apparatus according to claim  wherein the processor is configured to pay a product price according to biometrics-based authentication through a communication system of a vehicle or a mobile terminal  the apparatus according to claim  wherein the processor is configured to issue a payment number for a divided payment and perform the divided payments according to requests of a plurality of mobile terminals to which the payment numbers are inputted  the apparatus according to claim  wherein the processor is configured to accumulate mileage in an account corresponding to the mobile terminal undergoing a payment  the apparatus according to claim  wherein the processor is configured to control the digital signage to suggest a takeout packaging method according to a temperature of a product an atmospheric temperature weather and a vehicle type an image information processing method performed at a computing device having one or more processors and memory storing a plurality of programs to be executed by the one or more processors the method comprising identifying using face recognition one or more faces each face corresponding to a respective person captured in a first image for each identified face extracting a set of profile parameters of a corresponding person in the first image and selecting from a plurality of image tiles a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles and sharing the first image and the second image in a predefined order via a group chat session  the method of claim  wherein the first image and the second image are displayed in the group chat session one image at a time such that one of the two images is replaced by the other of the two images periodically  the method of claim  wherein extracting a set of profile parameters of a corresponding person in the first image includes determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model wherein the first machine learning model is trained with the facial images and corresponding descriptive labels  the method of claim  wherein extracting a set of profile parameters of a corresponding person in the first image includes determining an identity of the corresponding person based on the identified face of the corresponding person locating respective profile information of the first person based on the determined identity of the corresponding person and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person  the method of claim  wherein at least a first one of the first image tiles is a dynamic image tile and at least a second one of the first image tiles is a static image tile  the method of claim  including receiving a plurality of user comments from different users of the group chat session each user comment including a descriptive term for a respective person identified in the first image choosing a descriptive label for the respective person according to the plurality of user comments and updating the second image by adding the descriptive label adjacent to the first image tile of the respective person  a computing device for image information processing comprising one or more processors and memory storing instructions which when executed by the one or more processors cause the processors to perform a plurality of operations comprising identifying using face recognition one or more faces each face corresponding to a respective person captured in a first image for each identified face extracting a set of profile parameters of a corresponding person in the first image and selecting from a plurality of image tiles a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles and sharing the first image and the second image in a predefined order via a group chat session  the computing device of claim  wherein the first image and the second image are displayed in the group chat session one image at a time such that one of the two images is replaced by the other of the two images periodically  the computing device of claim  wherein extracting a set of profile parameters of a corresponding person in the first image includes determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model wherein the first machine learning model is trained with the facial images and corresponding descriptive labels  the computing device of claim  wherein extracting a set of profile parameters of a corresponding person in the first image includes determining an identity of the corresponding person based on the identified face of the corresponding person locating respective profile information of the first person based on the determined identity of the corresponding person and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person  the computing device of claim  wherein at least a first one of the first image tiles is a dynamic image tile and at least a second one of the first image tiles is a static image tile  the computing device of claim  wherein the plurality of operations further include receiving a plurality of user comments from different users of the group chat session each user comment including a descriptive term for a respective person identified in the first image choosing a descriptive label for the respective person according to the plurality of user comments and updating the second image by adding the descriptive label adjacent to the first image tile of the respective person  a non-transitory computer-readable storage medium storing instructions which when executed by a computing device having one or more processors cause the computing device to perform a plurality of operations comprising identifying using face recognition one or more faces each face corresponding to a respective person captured in a first image for each identified face extracting a set of profile parameters of a corresponding person in the first image and selecting from a plurality of image tiles a first image tile that matches the face of the corresponding person in the first image in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first image tile generating a second image by covering the faces of respective persons in the first image with their corresponding first image tiles and sharing the first image and the second image in a predefined order via a group chat session  the non-transitory computer-readable storage medium of claim  wherein the first image and the second image are displayed in the group chat session one image at a time such that one of the two images is replaced by the other of the two images periodically  the non-transitory computer-readable storage medium of claim  wherein extracting a set of profile parameters of a corresponding person in the first image includes determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model wherein the first machine learning model is trained with the facial images and corresponding descriptive labels  the non-transitory computer-readable storage medium of claim  wherein extracting a set of profile parameters of a corresponding person in the first image includes determining an identity of the corresponding person based on the identified face of the corresponding person locating respective profile information of the first person based on the determined identity of the corresponding person and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person  the non-transitory computer-readable storage medium of claim  wherein at least a first one of the first image tiles is a dynamic image tile and at least a second one of the first image tiles is a static image tile  the non-transitory computer-readable storage medium of claim  wherein the plurality of operations further include receiving a plurality of user comments from different users of the group chat session each user comment including a descriptive term for a respective person identified in the first image choosing a descriptive label for the respective person according to the plurality of user comments and updating the second image by adding the descriptive label adjacent to the first image tile of the respective person a method comprising by a computing system determining that a performance metric of an eye tracking system is below a first performance threshold wherein the eye tracking system is associated with a head-mounted display worn by a user based on the determination of the performance metric of the eye tracking system being below the first performance threshold the computer system performing receiving one or more first inputs associated with a body of the user estimating a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user determining a vergence distance of the user based at least on the one or more first inputs associated with the body of the user the estimated region that the user is looking at and locations of one or more objects in a scene displayed by the head-mounted display and adjusting one or more configurations of the head-mounted display based on the determined vergence distance of the user  the method of claim  wherein the one or more configurations of the head-mounted display comprise one or more of a rendering image a position of a display screen or a position of an optics block  the method of claim  further comprising determining that the performance metric of the eye tracking system is above a second performance threshold receiving eye tracking data from the eye tracking system and determining the vergence distance of the user based on the eye tracking data and the one or more first inputs associated with the body of the user  the method of claim  further comprising receiving one or more second inputs associated with one or more displaying elements in the scene displayed by the head-mounted display and determining the vergence distance of the user based at least on the eye tracking data the one or more first inputs associated with the body of the user and the one or more second inputs associated with the one or more displaying elements of the scene  the method of claim  further comprising feeding the one or more first inputs associated with the body of the user to a fusion algorithm wherein the fusion algorithm assigns a weight score to each input of the one or more first inputs determining the vergence distance of the user using the fusion algorithm based on the one or more first inputs associated with the body of the user and determining a z-depth of a display screen and a confidence score based on the one or more first inputs associated with the body of the user  the method of claim  further comprising comparing the confidence score to a confidence level threshold in response to a determination that the confidence score is below the confidence level threshold feeding the one or more second inputs associated with the one or more displaying elements of the scene to the fusion algorithm and determining the z-depth of the display screen using the fusion algorithm based on the one or more first inputs associated with the body of the user and the one or more second inputs associated with the one or more displaying elements of the scene  the method of claim  further comparing comparing by the fusion algorithm confidence scores associated with a plurality of combinations of inputs and determining by the fusion algorithm the z-depth of the display screen based on a combination of inputs associated with a highest confidence score  the method of claim  wherein the z-depth and the confidence score are determined by the fusion algorithm using a piecewise comparison of the one or more first inputs and the one or more second inputs  the method of claim  wherein the z-depth and the confidence score are determined based on a correlation between two or more inputs of the one or more first inputs and the one or more second inputs  the method of claim  wherein the fusion algorithm comprises a machine learning ml algorithm and wherein the machine learning ml algorithm determines a combination of first inputs fed to the fusion algorithm  the method of claim  wherein the one or more first inputs associated with the body of the user comprise one or more of a hand position a hand direction a hand movement a hand gesture a head position a head direction a head movement a head gesture a gaze angle rea body gesture a body posture a body movement a behavior of the user or a weighted combination of one or more related parameters  the method of claim  wherein the one or more first inputs associated with the body of the user are received from one or more of a controller a sensor a camera a microphone an accelerometer a headset worn by the user or a mobile device  the method of claim  wherein the one or more second inputs associated with the one or more displaying elements comprise one or more of a z-buffer value associated with a displaying element a displaying element marked by a developer an image analysis result a shape of a displaying element a face recognition result an object recognition result a person identified in a displaying content an object identified in a displaying content a correlation of two or more displaying elements or a weighted combination of the one or more second inputs  the method of claim  further comprising determining that the performance metric of the eye tracking system is below a second performance threshold receiving one or more second inputs associated with one or more displaying elements in the scene displayed by the head-mounted display and determining the vergence distance of the user based at least on the one or more first inputs associated with the body of the user and the one or more second inputs associated with the one or more displaying elements  the method of claim  wherein determining that the performance metric of the eye tracking system is below the second performance threshold comprises determining that the eye tracking system does not exist or fails to provide eye tracking data  the method of claim  wherein the performance metric of the eye tracking system comprises one or more of an accuracy of a parameter from the eye tracking system a precision of a parameter from the eye tracking system a value of a parameter from the eye tracking system a detectability of a pupil a metric based on one or more parameters associated with the user a parameter change a parameter changing trend a data availability or a weighted combination of one or more performance related parameters  the method of claim  wherein the one or more parameters associated with the user comprise one or more of an eye distance of the user a pupil position a pupil status a correlation of two pupils of the user a head size of the user a position of a headset worn by the user an angle of the headset worn by the user a direction of the headset worn by the user an alignment of the eyes of the user or a weighted combination of one or more related parameters associated with the user  the method of claim  wherein the first performance threshold comprises one or more of a pre-determined value a pre-determined range a state of a data a changing speed of a data or a trend of a data change  one or more non-transitory computer-readable storage media embodying software that is operable when executed by a computing system to determine that a performance metric of an eye tracking system is below a first performance threshold wherein the eye tracking system is associated with a head-mounted display worn by a user based on the determination of the performance metric of the eye tracking system being below the first performance threshold the media embodying software operable when executed by the computing system to receive one or more first inputs associated with a body of the user estimate a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user determine a vergence distance of the user based at least on the one or more first inputs associated with the body of the user the estimated region that the user is looking at and locations of one or more objects in a scene displayed by the head-mounted display and adjust one or more configurations of the head-mounted display based on the determined vergence distance of the user  a system comprising one or more non-transitory computer-readable storage media embodying instructions one or more processors coupled to the storage media and operable to execute the instructions to determine that a performance metric of an eye tracking system is below a first performance threshold wherein the eye tracking system is associated with a head-mounted display worn by a user based on the determination of the performance metric of the eye tracking system being below the first performance threshold the system is configured to receive one or more first inputs associated with a body of the user estimate a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user determine a vergence distance of the user based at least on the one or more first inputs associated with the body of the user the estimated region that the user is looking at and locations of one or more objects in a scene displayed by the head-mounted display and adjust one or more configurations of the head-mounted display based on the determined vergence distance of the user a computer-implemented method for image-based self-guided object detection comprising receiving by a processor device a set of images each of the images having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data training by the processor device a grid-based object detector using the grid level label data determining by the processor device a respective bounding box for the respective object in each of the images by applying local segmentation to each of the images and training by the processor device a region-based convolutional neural network rcnn for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the rcnn  the computer-implemented method of claim  further comprising performing an action responsive to the object localization and object classification for a respective new object in a new image to which the rcnn is applied  the computer-implemented method of claim  wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object  the computer-implemented method of claim  wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of images  the computer-implemented method of claim  wherein the local segmentation is applied to each of the images to segment a respective target region therein  the computer-implemented method of claim  wherein the region-based convolutional neural network rcnn forms a model during an object training stage that is to detect objects in new images during an inference stage  the computer-implemented method of claim  wherein the method is performed by a system selected from the group consisting of a surveillance system a face detection system a face recognition system a cancer detection system an object tracking system and an advanced driver-assistance system  a computer program product for image-based self-guided object detection the computer program product comprising a non-transitory computer readable storage medium having program instructions embodied therewith the program instructions executable by a computer to cause the computer to perform a method comprising receiving by a processor device a set of images each of the images having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data training by the processor device a grid-based object detector using the grid level label data determining by the processor device a respective bounding box for the respective object in each of the images by applying local segmentation to each of the images and training by the processor device a region-based convolutional neural network rcnn for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the rcnn  the computer program product of claim  wherein the method further comprises performing an action responsive to the object localization and object classification for a respective new object in a new image to which the rcnn is applied  the computer program product of claim  wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object  the computer program product of claim  wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of images  the computer program product of claim  wherein the local segmentation is applied to each of the images to segment a respective target region therein  the computer program product of claim  wherein the region-based convolutional neural network rcnn forms a model during an object training stage that is to detect objects in new images during an inference stage  the computer program product of claim  wherein the method is performed by a system selected from the group consisting of a surveillance system a face detection system a face recognition system a cancer detection system an object tracking system and an advanced driver-assistance system  a computer processing system for image-based self-guided object detection comprising a memory device for storing program code and a processor device for running the program code to receive a set of images each of the images having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data train a grid-based object detector using the grid level label data determine a respective bounding box for the respective object in each of the images by applying local segmentation to each of the images and train a region-based convolutional neural network rcnn for joint object localization and object classification using the respective bounding box for the respective object in each of the images as an input to the rcnn  the computer processing system of claim  wherein the processor device further runs the program code to perform an action responsive to the object localization and object classification for a respective new object in a new image to which the rcnn is applied  the computer processing system of claim  wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object  the computer processing system of claim  wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of images  the computer processing system of claim  wherein the region-based convolutional neural network rcnn forms a model during an object training stage that is to detect objects in new images during an inference stage  the computer processing system of claim  wherein the computer processing system is comprised in a system selected from the group consisting of a surveillance system a face detection system a face recognition system a cancer detection system an object tracking system and an advanced driver-assistance system a method of scalable parallel cloud-based face recognition utilizing a database of normalized stored images comprising capturing an image using a camera detecting a face in the captured image normalizing the detected facial image to match the normalized stored images identifying facial features in the normalized detected facial image generating a plurality of facial metrics from the facial features calculating euclidean distances between the facial metrics of the normalized detected facial image with corresponding facial metrics of each of the stored images comparing each euclidean distance against a predetermined threshold responsive to the euclidean distance comparison producing a reduced candidate list of best possible image matches from the normalized stored images comparing in parallel the normalized detected facial image with each of the normalized stored images of the reduced candidate list utilizing a plurality of face recognition algorithms where each processor of a parallel processing system uses a different face recognition algorithm responsive to the comparison producing best match results from each parallel subset of the reduced candidate list and selecting a final match from the best match results using a deep learning neural network face recognition algorithm trained on outputs of individual face recognition algorithms  the method of scalable parallel cloud-based face recognition of claim  wherein detecting a face in the captured image comprises utilizing opencv to detect a face in the captured image extracting the location of the eyes and a tip of the nose in the face determining a distance between the eyes cropping the face from the captured image where the width and the height of a cropped face image is a function of the distance between the eyes and rotating the face by an angle of rotation that is a function of the distance between the eyes  the method of scalable parallel cloud-based face recognition of claim  wherein the width of the cropped face image is  times the distance between the eyes the height of the cropped face image is  times the distance between the eyes and the angle of rotation is an angle formed by a straight line joining the eyes and an x-axis of the face  the method of scalable parallel cloud-based face recognition of claim  wherein rotating the face comprises rotating the face to provide a frontal face pattern  the method of scalable parallel cloud-based face recognition of claim  further comprising the step of proportionally rescaling the cropped and rotated image  the method of scalable parallel cloud-based face recognition of claim  where the proportional rescaling yields a cropped and rotated image with a size of = pixels  the method of scalable parallel cloud-based face recognition of claim  wherein the facial features identified in the normalized detected facial image comprise a pair of eyes a tip of a nose a mouth a center of the mouth and a chin area comprising a bottom a top left landmark and a top right landmark  the method of scalable parallel cloud-based face recognition of claim  wherein generating a plurality of facial metrics comprises calculating a distance between the pair of eyes a distance between the eyes and the tip of the nose a distance equal to the width of the mouth a distance between the tip of the nose and the center of mouth a distance between the bottom of chin and the center of mouth a distance between the top left landmark on the chin and the tip of the nose and a distance between the top right landmark on the chin and the tip of the nose  the method of scalable parallel cloud-based face recognition of claim  wherein performing a euclidean distance match further comprises partitioning the normalized stored images into a plurality of substantially equal subsets performing a euclidean distance match between the facial metrics of the normalized detected facial image and corresponding facial metrics of each of the stored images of the subsets of the normalized stored images with a separate processor of a parallel processing system to generate a euclidean distance for each stored image of the subset comparing each euclidean distance against a predetermined threshold with the separate processors responsive to the euclidean distance comparison producing a reduced candidate list of best possible image matches from the normalized stored images of each subset and combining the reduced candidate lists from each subset to produce a single reduced candidate list  the method of scalable parallel cloud-based face recognition of claim  wherein the plurality of face recognition algorithms utilized in comparing in parallel the normalized detected facial image with each of the normalized stored images of the reduced candidate list consists of face recognition algorithms selected from a group consisting of principle component analysis pca-based algorithms linear discriminant analysis lda algorithms independent component analysis ica algorithms kernel-based algorithms feature-based techniques algorithms based on neural networks algorithms based on transforms and model-based face recognition algorithms  the method of scalable parallel cloud-based face recognition of claim  wherein the pca-based algorithms include eigenfaces for face detectionrecognition and the lda algorithms include the fisherfaces method of face recognition  the method of scalable parallel cloud-based face recognition of claim  wherein comparing in parallel the captured image with each of the normalized stored images of the reduced candidate list further comprises partitioning the reduced candidate list into a plurality of substantially equal subsets processing each subset in a different processor of the parallel processing system uses a unique face recognition algorithm to produce the best match results and using a reduce function of a mapreduce program to combine the best match results from each of the subsets to produce a single set of the best match results  the method of scalable parallel cloud-based face recognition of claim  wherein partitioning the reduced candidate list comprises selecting the images comprising each subset by optimizing the variance between of each of the images according to the following equation where m and n are the number of rows and columns of the face vector image n is the number of groups and σij is the standard deviation of image dimension i in the group j of the face image vector  the method of scalable parallel cloud-based face recognition of claim  wherein selecting the images comprising each subset by optimizing the variance between each of the images according to the following equation dμi μj is the euclidean distance between the mean of the group i and the mean of group j i is the face image vector and l is the number of group levels  the method of scalable parallel cloud-based face recognition of claim  where selecting a final match from the best match results utilizing a deep learning neural network face recognition algorithm comprises utilizing either an adaboost machine-learning algorithm or a neural networks machine-learning model  the method of scalable parallel cloud-based face recognition of claim  where normalizing the detected facial image to match the normalized stored images includes normalizing the detected facial image to the same size orientation and illumination of the normalized stored images  a non-transitory computer-readable medium containing executable program instructions for causing a computer to perform a method of face recognition the method comprising detecting a face in an image captured by a camera normalizing the detected facial image to match the normalized stored images identifying facial features in the normalized detected facial image generating a plurality of facial metrics from the facial features calculating euclidean distances between the facial metrics of the normalized detected facial image with corresponding facial metrics of each of the stored images comparing each euclidean distance against a predetermined threshold responsive to the euclidean distance comparison producing a reduced candidate list of best possible image matches from the normalized stored images comparing in parallel the captured image with each of the normalized stored images of the reduced candidate list utilizing a plurality of face recognition algorithms where each processor of a parallel processing system uses a different face recognition algorithm responsive to the comparison producing best match results from each parallel subset of the reduced candidate list and selecting a final match from the best match results using a deep learning neural network face recognition algorithm trained on outputs of individual face recognition algorithms  the non-transitory computer-readable medium containing executable program instructions of claim  wherein the plurality of face recognition algorithms utilized in comparing in parallel the normalized detected facial image with each of the normalized stored images of the reduced candidate list consists of face recognition algorithms selected from a group consisting of principle component analysis pca-based algorithms linear discriminant analysis lda algorithms independent component analysis ica algorithms kernel-based algorithms feature-based techniques algorithms based on neural networks algorithms based on transforms and model-based face recognition algorithms  the non-transitory computer-readable medium containing executable program instructions of claim  wherein the pca-based algorithms include eigenfaces for face detectionrecognition and the lda algorithms include the fisherfaces method of face recognition  the non-transitory computer-readable medium containing executable program instructions of claim  where selecting a final match from the best match results utilizing a deep learning neural network face recognition algorithm comprises utilizing either an adaboost machine-learning algorithm or a neural networks machine-learning model an imaging device comprising a condensing lens an image sensor configured to detect light passing through the condensing lens and comprising a pixel matrix wherein the pixel matrix comprises a plurality of phase detection pixel pairs and a plurality of regular pixels and a processor configured to turn on the phase detection pixel pairs for autofocusing and output autofocused pixel data after completing the autofocusing divide the autofocused pixel data into a first subframe and a second subframe calculate image features of at least one of the first subframe and the second subframe wherein the image features comprise module widths of a finder pattern and the finder pattern has a predetermined ratio a harr-like feature or a gabor feature and determine an operating resolution of the regular pixels according to the image features calculated from at least one of the first subframe and the second subframe divided from the autofocused pixel data  the imaging device as claimed in claim  wherein each of the phase detection pixel pairs comprises a first pixel and a second pixel a cover layer covering upon a first region of the first pixel and upon a second region of the second pixel wherein the first region and the second region are mirror symmetrical to each other and a microlens aligned with at least one of the first pixel and the second pixel  the imaging device as claimed in claim  wherein the first region and the second region are % to % of an area of a single pixel  the imaging device as claimed in claim  wherein the processor is configured to perform the autofocusing using a dual pixel autofocus technique according to pixel data of the phase detection pixel pairs before completing the autofocusing  the imaging device as claimed in claim  wherein the processor is configured to divide pixel data of the phase detection pixel pairs into a third subframe and a fourth subframe before completing the autofocusing and perform the autofocusing according to the third subframe and the fourth subframe  the imaging device as claimed in claim  wherein the processor is further configured to calibrate brightness of the third subframe and the fourth subframe to be identical using a shading algorithm  the imaging device as claimed in claim  wherein the operating resolution is selected as a first resolution smaller than a number of the regular pixels or as a second resolution larger than the first resolution  the imaging device as claimed in claim  wherein the regular pixels are turned off in the autofocusing  the imaging device as claimed in claim  wherein a number of the phase detection pixel pairs is smaller than that of the regular pixels  an imaging device comprising a condensing lens an image sensor configured to detect light passing through the condensing lens and comprising a pixel matrix wherein the pixel matrix comprises a plurality of phase detection pixel pairs and a plurality of regular pixels and a processor configured to turn on the phase detection pixel pairs for autofocusing and output autofocused pixel data after completing the autofocusing divide the autofocused pixel data into a first subframe and a second subframe calculate image features of at least one of the first subframe and the second subframe wherein the image features comprise module widths of a finder pattern and the finder pattern has a predetermined ratio a harr-like feature or a gabor feature and select an image decoding or an image recognition using pixel data of the regular pixels according to the image features calculated from at least one of the first subframe and the second subframe divided from the autofocused pixel data  the imaging device as claimed in claim  wherein each of the phase detection pixel pairs comprises a first pixel and a second pixel a cover layer covering upon a first region of the first pixel and upon a second region of the second pixel wherein the first region and the second region are mirror symmetrical to each other and a microlens aligned with at least one of the first pixel and the second pixel  the imaging device as claimed in claim  wherein the processor is configured to perform the autofocusing using a dual pixel autofocus technique according to pixel data of the phase detection pixel pairs before completing the autofocusing  the imaging device as claimed in claim  wherein the processor is configured to divide the pixel data of the phase detection pixel pairs into a third subframe and a fourth subframe before completing the autofocusing calibrate brightness of the third subframe and the fourth subframe to be identical using a shading algorithm and perform the autofocusing according to the third subframe and the fourth subframe  the imaging device as claimed in claim  wherein the processor is configured to calculate the image features using at least one of a rule based algorithm and a machine learning algorithm  the imaging device as claimed in claim  wherein the image decoding is decoding qr codes and the image recognition is face recognition  an operating method of an imaging device the imaging device comprising a plurality of phase detection pixel pairs and a plurality of regular pixels the operating method comprising turning on the phase detection pixel pairs for autofocusing and outputting autofocused image frame after completing the autofocusing dividing the autofocused image frame acquired by the phase detection pixel pairs into a first subframe and a second subframe calculating image features of at least one of the first subframe and the second subframe wherein the image feature comprise module widths of a finder pattern and the finder pattern has a predetermined ratio a harr-like feature or a gabor feature and selectively activating at least a part of the regular pixels according to the image features calculated from at least one of the first subframe and the second subframe divided from the autofocused image frame  the operating method as claimed in claim  wherein the selectively activating comprises activating a first part of the regular pixels to perform an image decoding according to pixel data of the first part of the regular pixels or activating all the regular pixels to perform an image recognition according to pixel data of the all regular pixels  the operating method as claimed in claim  wherein pixel data of the phase detection pixel pairs captured in a same frame with the pixel data of the regular pixels is also used in performing the image decoding and the image recognition  the operating method as claimed in claim  wherein the image decoding is decoding qr codes and the image recognition is face recognition  the operating method as claimed in claim  wherein the phase detection pixel pairs are partially covered pixels or have a structure of dual pixel an apparatus comprising a first camera module configured to obtain a first image of an object with a first field of view a second camera module configured to obtain a second image of the object with a second field of view different from the first field of view a first depth map generator configured to generate a first depth map of the first image based on the first image and the second image and a second depth map generator configured to generate a second depth map of the second image based on the first image the second image and the first depth map  the apparatus of claim  wherein the first field of view is a narrow angle and the second field of view is a wider angle  the apparatus of claim  wherein the second image is divided into a primary region and a residual region and the second depth map generator comprises a relationship estimating module configured to estimate a relationship between the primary region and the residual region based on the first image and the second image and a depth map estimating module configured to estimate a depth map of the residual region based on the estimated relationship and the first depth map  the apparatus of claim  wherein at least one of the relationship estimating module and the depth map estimating module performs an estimating operation based on a neural network module  the apparatus of claim  further comprising a depth map fusion unit configured to generate a third depth map of the second image by performing a fusion operation based on the first depth map and the second depth map  the apparatus of claim  wherein the depth map fusion unit comprises a tone mapping module configured to generate a tone-mapped second depth map to correspond to the first depth map by performing a bias removing operation on the second depth map and a fusion module configured to generate the third depth map by fusing the tone-mapped second depth map and the first depth map  the apparatus of claim  wherein the depth map fusion unit further comprises a propagating module configured to generate a propagated first depth map in the second image by iterated propagating of the first depth map based on the first depth map and the second image and the fusion module generates the third depth map by fusing the tone-mapped second depth map and the propagated first depth map  the apparatus of claim  wherein the depth map fusion unit further comprises a post-processing module configured to perform a post-processing operation on the third depth map generated by the fusion module to provide the post-processed third depth map  the apparatus of claim  wherein the post-processing module performs the post-processing operation by filtering an interface generated in the third depth map in accordance with fusion of the fusion module  the apparatus of claim  wherein the post-processing module removes artifacts generated in the third depth map in accordance with fusion of the fusion module  the apparatus of claim  wherein the first depth map generator analyses a distance relationship between the first image and the second image and generates a first depth map of the first image based on the distance relationship  a method of processing an image of an electronic apparatus the method comprising obtaining a first image of an object using a first camera module obtaining a second image of the object using a second camera module generating a first depth map of the first image based on the first image and the second image estimating a relationship between a primary region of the second image and a residual region of the second image based on the first image and the second image and generating a second depth map of the second image based on the estimated relationship between the primary region and the residual region and the first depth map  the method of claim  wherein the electronic apparatus comprises a first camera module including a first lens having a first field of view and a second camera module including a second lens having a second field of view wider than the first field of view  the method of claim  wherein the generating of the second depth map comprises estimating a depth map of the residual region based on the estimated relationship between the primary region and the residual region and the first depth map and generating the second depth map based on a depth map of the residual region and the first depth map  the method of claim  wherein the estimating of the relationship between a primary region of the second image is performed using a neural network model  the method of claim  further comprising performing a pre-processing operation on the second depth map and generating a third depth map of the residual image by fusing the second depth map on which the pre-processing operation is performed and the first depth map  the method of claim  wherein the performing of the pre-processing operation comprises performing a tone mapping operation between a depth map of the primary region and a depth map of the residual region based on the second depth map  an operating method for an electronic apparatus the electronic apparatus including a first camera module providing a first image of an object using a first field of view and a second camera module providing a second image of the object using second field of view wider than the first field of view and a processor generating a depth map of the second image based on a primary region of the second image and a residual region of the second image the operating method comprising generating a first depth map of the primary region by estimating a relationship between the first image and the second image estimating a relationship between the primary region and the residual region based on the first image and the second image generating a second depth map of the second image by estimating a depth map of the second region based on the estimated relationship between the primary region and the residual region and generating a depth map of the second image by fusing the first depth map and the second depth map  the operation method of claim  further comprising executing an application that applies an image effect to the second image based on a depth map of the residual image  the operation method of claim  wherein the application applies at least one image effect of auto-focusing out-focusing forebackground separation face recognition object detection within a frame and augmented reality to the second image based on a depth map of the second image a payment method based on a face recognition comprising acquiring first face image information of a target user extracting first characteristic information from the first face image information wherein the first characteristic information includes head posture information of the target user and gaze information of the target user determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user including determining whether an angle of rotation in each preset direction is less than an angle threshold wherein the head posture information includes the angle of rotation in each preset direction determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold wherein the gaze information includes the probability value that a user gazes at a payment screen and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold determining that the target user has a willingness to pay and in response to determining that the target user has a willingness to pay completing a payment operation based on the face recognition  the method as claimed in claim  wherein the completing a payment operation based on the face recognition comprises triggering and performing a payment initiating operation to acquire second face image information based on the face recognition determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay and in response to determining that the second characteristic information indicates that the user has a willingness to pay triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user  the method as claimed in claim  wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises determining whether a current user corresponding to the second face image information is consistent with the target user and in response to determining that the current user is consistent with the target user determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information  the method as claimed in claim  wherein the extracting first characteristic information from the first face image information comprises determining the head posture information of the target user using a head posture recognition model based on the first face image information and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information  the method as claimed in claim  wherein the head posture recognition model is obtained through training by acquiring a first sample data set wherein the first sample data set includes a plurality of pieces of first sample data and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information determining mean image data and variance image data of a plurality of sample face images for each of the plurality of pieces of first sample data preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image setting the preprocessed sample face image and the corresponding head posture information as a first model training sample and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model  the method as claimed in claim  wherein the gaze information recognition model is obtained through training by acquiring a second sample data set wherein the second sample data set includes a plurality of pieces of second sample data and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information determining mean image data and variance image data of a plurality of sample eye images for each of the plurality of pieces of second sample data preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample and performing training using a machine learning method and based on a plurality of second model training samples to obtain the gaze information recognition model  the method as claimed in claim  wherein the angle of rotation in each preset direction comprises a pitch angle a yaw angle and a roll angle wherein the pitch angle refers to an angle of rotation around a x-axis the yaw angle refers to an angle of rotation around a y-axis and the roll angle refers to an angle of rotation around a z-axis  a payment device based on a face recognition comprising a processor and a non-transitory computer-readable storage medium storing instructions executable by the processor to cause the device to perform operations comprising acquiring first face image information of a target user extracting first characteristic information from the first face image information wherein the first characteristic information includes head posture information of the target user and gaze information of the target user determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user including determining whether an angle of rotation in each preset direction is less than an angle threshold wherein the head posture information includes the angle of rotation in each preset direction determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold wherein the gaze information includes the probability value that a user gazes at a payment screen and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold determining that the target user has a willingness to pay and in response to determining that the target user has a willingness to pay completing a payment operation based on the face recognition  the device as claimed in claim  wherein the completing a payment operation based on the face recognition comprises triggering and performing a payment initiating operation to acquire second face image information based on the face recognition determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay and in response to determining that the second characteristic information indicates that the user has a willingness to pay triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user  the device as claimed in claim  wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises determining whether a current user corresponding to the second face image information is consistent with the target user and in response to determining that the current user is consistent with the target user determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information  the device as claimed in claim  wherein the extracting first characteristic information from the first face image information comprises determining the head posture information of the target user using a head posture recognition model based on the first face image information and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information  the device as claimed in claim  wherein the head posture recognition model is obtained through training by acquiring a first sample data set wherein the first sample data set includes a plurality of pieces of first sample data and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information determining mean image data and variance image data of a plurality of sample face images for each of the plurality of pieces of first sample data preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image setting the preprocessed sample face image and the corresponding head posture information as a first model training sample and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model  the device as claimed in claim  wherein the gaze information recognition model is obtained through training by acquiring a second sample data set wherein the second sample data set includes a plurality of pieces of second sample data and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information determining mean image data and variance image data of a plurality of sample eye images for each of the plurality of pieces of second sample data preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample and performing training using a machine learning method and on a plurality of second model training samples to obtain the gaze information recognition model  the device as claimed in claim  wherein the angle of rotation in each preset direction comprises a pitch angle a yaw angle and a roll angle wherein the pitch angle refers to an angle of rotation around a x-axis the yaw angle refers to an angle of rotation around a y-axis and the roll angle refers to an angle of rotation around a z-axis  a non-transitory computer-readable storage medium for a payment based on a face recognition configured with instructions executable by one or more processors to cause the one or more processors to perform operations comprising acquiring first face image information of a target user extracting first characteristic information from the first face image information wherein the first characteristic information includes head posture information of the target user and gaze information of the target user determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user including determining whether an angle of rotation in each preset direction is less than an angle threshold wherein the head posture information includes the angle of rotation in each preset direction determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold wherein the gaze information includes the probability value that a user gazes at a payment screen and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold determining that the target user has a willingness to pay and in response to determining that the target user has a willingness to pay completing a payment operation based on the face recognition  the storage medium as claimed in claim  wherein the completing a payment operation based on the face recognition comprises triggering and performing a payment initiating operation to acquire second face image information based on the face recognition determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay and in response to determining that the second characteristic information indicates that the user has a willingness to pay triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user  the storage medium as claimed in claim  wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises determining whether a current user corresponding to the second face image information is consistent with the target user and in response to determining that the current user is consistent with the target user determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information  the storage medium as claimed in claim  wherein the extracting first characteristic information from the first face image information comprises determining the head posture information of the target user using a head posture recognition model based on the first face image information and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information  the storage medium as claimed in claim  wherein the head posture recognition model is obtained through training by acquiring a first sample data set wherein the first sample data set includes a plurality of pieces of first sample data and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information determining mean image data and variance image data of a plurality of sample face images for each of the plurality of pieces of first sample data preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image setting the preprocessed sample face image and the corresponding head posture information as a first model training sample and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model and wherein the gaze information recognition model is obtained through training by acquiring a second sample data set wherein the second sample data set includes a plurality of pieces of second sample data and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information determining mean image data and variance image data of a plurality of sample eye images for each of the plurality of pieces of second sample data preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample and performing training using a machine learning method and based on a plurality of second model training samples to obtain the gaze information recognition model  the storage medium as claimed in claim  wherein the angle of rotation in each preset direction comprises a pitch angle a yaw angle and a roll angle wherein the pitch angle refers to an angle of rotation around a x-axis the yaw angle refers to an angle of rotation around a y-axis and the roll angle refers to an angle of rotation around a z-axis a method comprising detecting by a motion detection module a motion by a subject within a predetermined area of view assigning a unique session identification number to the subject detected within a predetermined area of view detecting a facial area of the subject detected within a predetermined area of view generating an image of the facial area of the subject assessing a quality of the image of the facial area of the subject determining an identity of the subject based on the image of the facial area of the subject identifying an intent of the subject and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject  the method of claim  further comprising determining one or more additional subjects within the predetermined area of view and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view  the method of claim  wherein the assessing a quality of the image of the facial area of the subject comprises assessing whether the quality of the image of the facial area of the object equates predetermined metric of quality and upon determining that the quality of the image of the facial area of the object is inferior to the predetermined metric of quality discarding the image of the facial area of the subject and generating a second image of the facial area of the subject  the method of claim  further comprising detecting whether the facial area of the subject is photographic image and upon detecting that the facial area of the subject is a photographic image generating a warning and restrict access to the point of entry  the method of claim  further comprising conducing an incremental training of the image of the facial area of the subject  the method of claim  wherein conducing an incremental training of the image of the facial area of the subject comprises capturing a first image of the facial area having facial landmarks converting the first image of the facial area into a first numeric vector capturing a second image of the facial area having facial landmarks converting the second image of the facial area into a second numeric vector calculating a weighted mean of the first numeric vector and the second numeric vector wherein the weighted mean represents a change in a facial area and storing the weighted mean in the database  the method of claim  wherein determining an identity of the subject based on the image of the facial area of the subject comprises comparing the image of the facial area of the subject with a plurality of images stored in a database and authenticating the subject  the method of claim  wherein identifying an intent of the subject comprises upon detecting the facial area in a bounding box commencing authentication of the subject calculating a directional vector of a face of the subject determine an intent of the subject to gain access to the point of entry based on the directional vector of the face of the subject granting the access to the point of entry based on authentication of the subject and based on determining the intent of the subject  a non-transitory computer readable medium having program instructions stored thereon that in response to execution by a computing device cause the computing device to perform operations comprising detecting a motion by a subject within a predetermined area of view assigning a unique session identification number to the subject detected within a predetermined area of view detecting a facial area of the subject detected within a predetermined area of view generating an image of the facial area of the subject assessing a quality of the image of the facial area of the subject determining an identity of the subject based on the image of the facial area of the subject identifying an intent of the subject and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject  the non-transitory computer readable medium of claim  further comprising determining one or more additional subjects within the predetermined area of view and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view  the non-transitory computer readable medium of claim  wherein the assessing a quality of the image of the facial area of the subject comprises assessing whether the quality of the image of the facial area of the object equates predetermined metric of quality and upon determining that the quality of the image of the facial area of the object is inferior to the predetermined metric of quality discarding the image of the facial area of the subject and generating a second image of the facial area of the subject  the non-transitory computer readable medium of claim  further comprising detecting whether the facial area of the subject is photographic image and upon detecting that the facial area of the subject is a photographic image generating a warning and restrict access to the access point  the non-transitory computer readable medium of claim  further comprising conducing an incremental training of the image of the facial area of the subject  the non-transitory computer readable medium of claim  wherein conducing an incremental training of the image of the facial area of the subject comprises capturing a first image of the facial area having facial landmarks converting the first image of the facial area into a first numeric vector capturing a second image of the facial area having facial landmarks converting the second image of the facial area into a second numeric vector calculating a weighted mean of the first numeric vector and the second numeric vector wherein the weighted mean represents a change in a facial area and storing the weighted mean in the database  an apparatus for face recognition comprising a processor and a memory to store computer program instructions the computer program instructions when executed on the processor cause the processor to perform operations comprising detecting a motion by a subject within a predetermined area of view assigning a unique session identification number to the subject detected within a predetermined area of view detecting a facial area of the subject detected within a predetermined area of view generating an image of the facial area of the subject assessing a quality of the image of the facial area of the subject determining an identity of the subject based on the image of the facial area of the subject identifying an intent of the subject and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject  the apparatus of claim  further comprising determining one or more additional subjects within the predetermined area of view and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view  the apparatus of claim  wherein the assessing a quality of the image of the facial area of the subject comprises assessing whether the quality of the image of the facial area of the object equates predetermined metric of quality and upon determining that the quality of the image of the facial area of the object is inferior to the predetermined metric of quality discarding the image of the facial area of the subject and generating a second image of the facial area of the subject  the apparatus of claim  further comprising detecting whether the facial area of the subject is photographic image and upon detecting that the facial area of the subject is a photographic image generating a warning and restrict access to the access point  the apparatus of claim  further comprising conducing an incremental training of the image of the facial area of the subject  the apparatus of claim  wherein conducing an incremental training of the image of the facial area of the subject comprises capturing a first image of the facial area having facial landmarks converting the first image of the facial area into a first numeric vector capturing a second image of the facial area having facial landmarks converting the second image of the facial area into a second numeric vector calculating a weighted mean of the first numeric vector and the second numeric vector wherein the weighted mean represents a change in a facial area and storing the weighted mean in the database a robot comprising a body configured to rotate and to tilt a camera coupled to the body and configured to rotate and tilt according to the rotate and the tilt of the body wherein the camera is configured to acquire a video of a space a face recognition unit configured to recognize respective faces of one or more persons in the video a tracking unit configured to track motion of each of the recognized faces of the one or more persons and a controller configured to calculate a respective size of each of the faces of the one or more persons select a first person from among the one or more persons based on the calculated sizes of the faces and control at least one of a direction of the rotation of the camera an angle of the tilt of the camera and a focal distance of the camera based on the tracked motion of the recognized face of the first person  the robot of claim  wherein the controller is configured to control the direction of the rotation of the camera and the angle of the tilt of the camera to achieve an particular orientation of the camera relative to the face of the first person and control a focal distance of the camera by comparing respective sizes of the face of the first person before and after motion of the first person  the robot of claim  wherein the particular orientation occurs when the camera faces a general direction of the face of the first person  the robot of claim  wherein the controller is configured to normalize sizes of the faces of the one or more persons based on an interocular distance and select the first person based on the normalized sizes of the faces of the one or more persons  the robot of claim  wherein the controller is configured to select a person having a largest face size from among the one or more persons as the first person  the robot of claim  further comprising a microphone configured to receive a spoken audio that is present in the space wherein the controller is further configured to select the first person further based on the received spoken audio  the robot of claim  wherein the controller is further configured to control gain of the microphone by comparing respective sizes of the face of the first person before and after motion of the first person  the robot of claim  wherein the controller is configured to calculate a position from which the spoken audio is provided and select the first person further based on whether the one or more persons are in the position from which the voice signal is provided  the robot of claim  wherein the controller is configured to select a second person as the first person from among the one or more persons when the second person is located in the position from which the spoken audio is provided  the robot of claim  wherein the controller is configured to select a second person having a largest face size as the first person from among the one or more persons when none of the one or more persons is located in the position from which the spoken audio is provided  the robot of claim  wherein the controller is configured to select a second person having a largest face size as the first person from among the one or more persons when a plurality of persons from among the one or more persons are located in the position from which the spoken audio is provided  the robot of claim  further comprising a speaker wherein the controller is configured to control volume of the speaker by comparing respective sizes of the face of the first person before and after motion of the first person  the robot of claim  wherein the body is further configured to rotate in a lateral direction and to tilt in an vertical direction  an electronic device comprising a camera coupled to the body and configured to rotate and to tilt wherein the camera is configured to acquire a video of a space within which one or more persons are positioned and a processor configured to recognize respective faces of the one or more persons in the video track motion of each of the recognized faces of the one or more persons calculate a respective size of each of the faces of the one or more persons select a first person from among the one or more persons based on the calculated sizes of the faces and control at least one of a direction of the rotation of the camera an angle of the tilt of the camera and a focal distance of the camera based on the tracked motion of the recognized face of the first person  a method comprising acquiring by a camera a video of a space within which one or more persons are positioned recognizing respective faces of the one or more persons in the video tracking motion of each of the recognized faces of the one or more persons calculating a respective size of each of the faces of the one or more persons selecting a first person from among the one or more persons based on the calculated sizes of the faces and controlling at least one of a direction of rotation of the camera an angle of tilt of the camera and a focal distance of the camera based on the tracked motion of the recognized face of the first person a method of inferring topics from a multimodal file the method comprising receiving a multimodal file extracting a set of entities from the multimodal file linking the set of entities to produce a set of linked entities obtaining reference information for the set of entities based at least on the reference information generating a graph of the set of linked entities the graph comprising nodes and edges based at least on the nodes and edges of the graph determining clusters in the graph based at least on the clusters in the graph identifying topic candidates extracting features from the clusters in the graph based at least on the extracted features selecting at least one topicid from among the topic candidates to represent at least one cluster and indexing the multimodal file with the at least one topicid  the method of claim  wherein the multimodal file comprises a video portion and an audio portion and wherein extracting a set of entities from the multimodal file comprises detecting objects in the video portion of the multimodal file and detecting text in the audio portion of the multimodal file  the method of claim  wherein detecting objects comprises performing face recognition  the method of claim  wherein detecting text comprises performing a speech to text process  the method of claim  further comprising identifying a language used in the audio portion of the multimodal file and wherein performing a speech to text process comprises performing a speech to text process in the identified language  the method of claim  further comprising translating the detected text  the method of claim  further comprising determining significant clusters and insignificant clusters in the determined clusters and wherein extracting features from the clusters in the graph comprises extracting features from the significant clusters in the graph  the method of claim  wherein extracting features from the clusters in the graph comprises at least one process selected from the list consisting of determining a graph diameter and determining a jaccard coefficient  the method of claim  wherein selecting at least one topicid to represent at least one cluster comprises based at least on the extracted features mapping topic candidates into a probability interval and based at least on the mapping ranking topic candidates within the at least one cluster and selecting the at least one topicid based at least on the ranking  the method of claim  further comprising translating the at least one topicid and wherein indexing the multimodal file with the at least one topicid comprises indexing the multimodal file with the at least one translated topicid  a system for inferring topics from a multimodal file the system comprising an entity extraction component comprising an object detection component and a speech to text component operative to extract a set of entities from a multimodal file comprising a video portion and an audio portion an entity linking component operative to link the extracted set of entities to produce a set of linked entities an information retrieval component operative to obtain reference information for the extracted set of entities a graphing and analysis component operative to generate a graph of the set of linked entities the graph comprising nodes and edges based at least on the nodes and edges of the graph determine clusters in the graph based at least on the clusters in the graph identify topic candidates and extract features from the clusters in the graph a topicid selection component operative to rank the topic candidates within at least one cluster and based at least on the ranking select at least one topicid from among the topic candidates to represent at least one cluster and a video indexer operative to index the multimodal file with the at least one topicid  the system of claim  wherein the object detection component is operative to perform face recognition  the system of claim  wherein the speech to text component is operative to extract entity information in at least two different languages  one or more computer storage devices having computer-executable instructions stored thereon for inferring topics from a multimodal file which on execution by a computer cause the computer to perform operations comprising receiving a multimodal file comprising a video portion and an audio portion extracting a set of entities from the multimodal file wherein extracting a set of entities from the multimodal file comprises detecting objects in the video portion of the multimodal file with face recognition detecting text in the audio portion of the multimodal file with a speech to text process and disambiguating among a set of detected entity names linking the set of entities to produce a set of linked entities obtaining reference information for the set of entities based at least on the reference information generating a graph of the set of linked entities the graph comprising nodes and edges based at least on the nodes and edges of the graph determining clusters in the graph determining significant clusters and insignificant clusters in the determined clusters based at least on the significant clusters in the graph identifying topic candidates extracting features from the significant clusters in the graph based at least on the extracted features mapping the topic candidates into a probability interval based at least on the mapping ranking the topic candidates within at least one significant cluster based on the ranking selecting at least one topicid from among the topic candidates to represent the at least one significant cluster and indexing the multimodal file with the at least one topicid  the one or more computer storage devices of claim  wherein the operations further comprise identifying a language used in the audio portion of the multimodal file and detecting text in the audio portion of the multimodal file with a speech to text process comprises performing a speech to text process in the identified language权利要求 、 一种人脸识别方法其特征在于包括 通过第一摄像头获取第一人脸图像 提取所述第一人脸图像的第一人脸特征 将所述第一人脸特征与预先存储的第二人脸特征进行对比获得参考相似度所述第 二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得所述第二摄像头与所述第 一摄像头属于不同类型的摄像头 根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否对应相同人。 、 根据权利要求 所述的方法其特征在于 所述第一摄像头为热成像摄像头所述第二摄像头为可见光摄像头 或者所述第一摄像头为可见光摄像头所述第一摄像头为热成像摄像头。 、 根据权利要求 或 所述的方法其特征在于所述根据所述参考相似度确定所 述第一人脸特征与所述第二人脸特征是否对应相同人包括 根据所述参考相似度、 参考误报率以及相似度阈值确定所述第一人脸特征与所述第二 人脸特征是否对应相同人其中不同的误报率对应不同的相似度阈值。 、 根据权利要求 或 所述的方法其特征在于所述根据所述参考相似度确定所 述第一人脸特征与所述第二人脸特征是否对应相同人包括 根据所述参考相似度以及阈值信息确定归一化后的参考相似度 根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否对 应相同人。 、 根据权利要求 -任一项所述的方法其特征在于所述提取所述第一人脸图像的 第_人脸特征包括 将所述第一人脸图像输入预先训练完成的神经网络通过所述神经网络输出所述第一 人脸图像的第一人脸特征其中所述神经网络基于第一类型图像样本和第二类型图像样 本训练得到所述第一类型图像样本和所述第二类型图像样本由不同类型的摄像头拍摄得 到且所述第一类型图像样本和所述第二类型图像样本中包括人脸。 、 根据权利要求  所述的方法其特征在于所述神经网络基于所述第一类型图像 样本、 所述第二类型图像样本和混合类型图像样本训练得到所述混合类型图像样本由所 述第一类型图像样本和所述第二类型图像样本配对而得。 、 根据权利要求 -任一项所述的方法其特征在于所述第一摄像头包括车载摄像 头所述通过第一摄像头获取第一人脸图像包括 通过所述车载摄像头获取所述第一人脸图像所述第一人脸图像包括车辆的用车人的 人脸图像。 、 根据权利要求  所述的方法其特征在于所述用车人包括驾驶所述车辆的人、 乘坐所述车辆的人、 对所述车辆进行修理的人、 给所述车辆加油的人以及控制所述车辆的 人中的一项或多项。 、 根据权利要求  所述的方法其特征在于所述用车人包括驾驶所述车辆的人 所述通过所述车载摄像头获取所述第一人脸图像包括 在接收到触发指令的情况下通过所述车载摄像头获取所述第一人脸图像 或者在所述车辆运行时通过所述车载摄像头获取所述第一人脸图像 或者在所述车辆的运行速度达到参考速度的情况下通过所述车载摄像头获取所述 第一人脸图像。 、 根据权利要求 -任一项所述的方法其特征在于所述第二人脸图像为对所述 用车人进行人脸注册的图像所述将所述第一人脸特征与预先存储的第二人脸特征进行对 比之前所述方法还包括 通过所述第二摄像头获取所述第二人脸图像 提取所述第二人脸图像的第二人脸特征 保存所述第二人脸图像的第二人脸特征。 、 一种神经网络训练方法其特征在于包括 获取第一类型图像样本和第二类型图像样本所述第一类型图像样本和所述第二类型 图像样本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述第二类型图像样 本中包括人脸 根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。 、 根据权利要求 所述的方法其特征在于所述根据所述第一类型图像样本和所 述第二类型图像样本训练神经网络包括 将所述第一类型图像样本和所述第二类型图像样本配对得到所述第一类型图像样本 和所述第二类型图像样本的混合类型图像样本 根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类型图像样本训练 所述神经网络。 、 根据权利要求  所述的方法其特征在于所述根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类型图像样本训练所述神经网络包括 通过所述神经网络获取所述第一类型图像样本的人脸预测结果、 所述第二类型图像样 本的人脸预测结果和所述混合类型图像样本的人脸预测结果 根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、 所述第二类型图 像样本的人脸预测结果和人脸标注结果之间的差异、 以及所述混合类型图像样本的人脸预 测结果和人脸标注结果的差异训练所述神经网络。 、 根据权利要求  所述的方法其特征在于所述神经网络中包括第一分类器、 第二分类器和混合分类器所述通过所述神经网络获取所述第一类型图像样本的人脸预测 结果、 所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果 包括 将所述第一类型图像样本的人脸特征输入至所述第一分类器中得到所述第一类型图 像样本的人脸预测结果 将所述第二类型图像样本的人脸特征输入至所述第二分类器中得到所述第二类型图 像样本的人脸预测结果 将所述混合类型图像样本的人脸特征输入至所述混合分类器中得到所述混合类型图 像样本的人脸预测结果。 、 根据权利要求 所述的方法其特征在于所述方法还包括 在训练完成的所述神经网络中去除所述第一分类器、 所述第二分类器和所述混合分类 器得到用于进行人脸识别的神经网络。 、 一种人脸识别装置其特征在于包括 第一获取单元用于通过第一摄像头获取第一人脸图像 第一提取单元用于提取所述第一人脸图像的第一人脸特征 对比单元用于将所述第一人脸特征与预先存储的第二人脸特征进行对比获得参考 相似度所述第二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得所述第二 摄像头与所述第一摄像头属于不同类型的摄像头 确定单元用于根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否 对应相同人。 、 根据权利要求 所述的装置其特征在于 所述第一摄像头为热成像摄像头所述第二摄像头为可见光摄像头 或者所述第一摄像头为可见光摄像头所述第一摄像头为热成像摄像头。 、 根据权利要求 或 所述的装置其特征在于 所述确定单元具体用于根据所述参考相似度、 参考误报率以及相似度阈值确定所述 第一人脸特征与所述第二人脸特征是否对应相同人其中不同的误报率对应不同的相似 度阈值。 、 根据权利要求 或 所述的装置其特征在于 所述确定单元具体用于根据所述参考相似度以及阈值信息确定归一化后的参考相似 度以及根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否 对应相同人。 、 根据权利要求 -任_项所述的装置其特征在于 所述第一提取单元具体用于将所述第一人脸图像输入预先训练完成的神经网络通 过所述神经网络输出所述第一人脸图像的第一人脸特征其中所述神经网络基于第一类 型图像样本和第二类型图像样本训练得到所述第一类型图像样本和所述第二类型图像样 本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述第二类型图像样本中包 括人脸。 、 根据权利要求  所述的装置其特征在于所述神经网络基于所述第一类型图 像样本、 所述第二类型图像样本和混合类型图像样本训练得到所述混合类型图像样本由 所述第一类型图像样本和所述第二类型图像样本配对而得。 、 根据权利要求 -任一项所述的装置其特征在于所述第一摄像头包括车载 摄像头 所述第一获取单元具体用于通过所述车载摄像头获取所述第一人脸图像所述第一 人脸图像包括车辆的用车人的人脸图像。 、 根据权利要求 所述的装置其特征在于所述用车人包括驾驶所述车辆的人、 乘坐所述车辆的人、 对所述车辆进行修理的人、 给所述车辆加油的人以及控制所述车辆的 人中的一项或多项。 、 根据权利要求 所述的装置其特征在于所述用车人包括驾驶所述车辆的人 所述第一获取单元具体用于在接收到触发指令的情况下通过所述车载摄像头获取所述 第一人脸图像 或者所述第一获取单元具体用于在所述车辆运行时通过所述车载摄像头获取所 述第 _人脸图像 或者所述第一获取单元具体用于在所述车辆的运行速度达到参考速度的情况下 通过所述车载摄像头获取所述第一人脸图像。 、 根据权利要求 -任一项所述的装置其特征在于所述第二人脸图像为对所 述用车人进行人脸注册的图像所述装置还包括 第二获取单元用于通过所述第二摄像头获取所述第二人脸图像 第二提取单元用于提取所述第二人脸图像的第二人脸特征 保存单元用于保存所述第二人脸图像的第二人脸特征。 、 一种神经网络训练装置其特征在于包括 获取单元用于获取第一类型图像样本和第二类型图像样本所述第一类型图像样本 和所述第二类型图像样本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述 第二类型图像样本中包括人脸 训练单元用于根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。 、 根据权利要求 所述的装置其特征在于所述训练单元包括 配对子单元用于将所述第一类型图像样本和所述第二类型图像样本配对得到所述 第一类型图像样本和所述第二类型图像样本的混合类型图像样本 训练子单元用于根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类 型图像样本训练所述神经网络。 、 根据权利要求 所述的装置其特征在于 所述训练子单元具体用于通过所述神经网络获取所述第一类型图像样本的人脸预测 结果、 所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果 以及根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、 所述第二类型图 像样本的人脸预测结果和人脸标注结果之间的差异、 以及所述混合类型图像样本的人脸预 测结果和人脸标注结果的差异训练所述神经网络。 、 根据权利要求  所述的装置其特征在于所述神经网络中包括第一分类器、 第二分类器和混合分类器 所述训练子单元具体用于将所述第一类型图像样本的人脸特征输入至所述第一分类 器中得到所述第一类型图像样本的人脸预测结果以及将所述第二类型图像样本的人脸 特征输入至所述第二分类器中得到所述第二类型图像样本的人脸预测结果以及将所述 混合类型图像样本的人脸特征输入至所述混合分类器中得到所述混合类型图像样本的人 脸预测结果。 、 根据权利要求 所述的装置其特征在于所述装置还包括 神经网络应用单元用于在训练完成的所述神经网络中去除所述第一分类器、 所述第 二分类器和所述混合分类器得到用于进行人脸识别的神经网络。 、 一种电子设备其特征在于包括处理器和存储器所述处理器和所述存储器耦 合其中所述存储器用于存储程序指令所述程序指令被所述处理器执行时使所述处 理器执行权利要求 -任一项所述的方法和或使所述处理器执行权利要求 -任一 项所述的方法。 、 一种计算机可读存储介质其特征在于所述计算机可读存储介质中存储有计算 机程序所述计算机程序包括程序指令所述程序指令当被处理器执行时使所述处理器 执行权利要求 -任一项所述的方法和或使所述处理器执行权利要求 -任一项所 述的方法。 a system for alerting on vision impairment said system comprising a processing unit configured and operable for receiving scene data being indicative of a scene of at least one consumer in an environment identifying in the scene data a certain consumer identifying an event being indicative of a behavioral compensation for vision impairment and upon identification of such an event sending a notification relating to the vision impairment  the system of claim  further comprising at least one sensing unit configured and operable for detecting the scene data  the system of claim  wherein said at least one sensing unit comprises at least one of at least one imaging unit configured and operable for capturing at least one image of at least a portion of a consumer's body at least one motion detector configured and operable for detecting consumer data being indicative of a motion of a consumer or at least one eye tracker configured and operable for tracking eye motion of a consumer  the system of claim  wherein the at least one imaging unit comprises a plurality of cameras placed at different heights  the system of any one of claims  to  wherein said sensing unit is accommodated in an optical or digital eyewear frame display  the system of any one of claims  to  wherein said processing unit is configured and operable for identifying a consumer's condition said consumer's condition comprising consumer data being indicative of the consumer's position and location relative to at least one object in the consumer's environment said consumer data comprises at least one of a consumer's face eyewear posture position sound or motion  the system of any one of claims  to  wherein said event comprises at least one position and orientation of head increase or decrease of viewing distance between the consumer and viewed object and changing the position of eyeglasses worn by the consumer  the system of any one of claims  to  wherein said event is identified by identifying images having an image feature being indicative of behavioral compensation performing a bruckner test performing a hirschberg test and measuring blink count frequency  the system of claim  wherein the image feature being indicative of behavioral compensation comprises squinting head orientation certain distances between an object and consumer's eyes certain position of eyeglasses on the consumer's face strabismus cataracts and reflections from the eye  the system of any one of claims  to  wherein the notification includes at least one of the data indicative of the identified event data indicative of the identified consumer ophthalmologic recommendations based on the identified event or lack of events or an appointment for a vision test  the system of any one of claims  to  wherein said processing unit comprises a memory for storing at least one of a reference data indicative of behavioral compensation for vision impairment data indicative of the notification or data indicative of a follow-up of the notification  the system of claim   wherein said processing unit is configured for at least one of identifying the event upon comparison between the detected data and the reference data or determining a probability for a vision impairment of the consumer based on the comparison  the system of any one of claims  to  wherein said processing unit comprises a communication interface being configured for sending the notification to at least one of the identified consumer or a third party  the system of any one of claims  to  wherein said processing unit is configured for providing a frame recommendation  the system of any one of claims  to  wherein said memory is configured for storing a database including a multiplicity of data sets related to a plurality of spectacle frame models and sizes  the system according to claim  or  wherein said processing unit is configured and operable to correlate between frames parameters and ophthalmic prescriptions  the system according to any of claims  to  wherein said processing unit is configured and operable to correlate between frames parameters and facial features  the system according to any of claims  to  wherein said processing unit is configured and operable to correlate between frames parameters and eyewear preferences  the system according to any of claims  to  comprising a server and at least one computer entity linked to the server via a network wherein said network is configured to receive and respond to requests sent across the network transmitting one or more modules of computer executable program instructions and displayable data to the network connected user computer platform in response to a request wherein said modules include modules configured to receive and transmit image information transmitting a frame recommendation and an optical lens option recommendation based on received image information for display by the network connected user computer platform  a computer program instructions stored in the local storage that when executed by a processing unit cause the processing unit to receive data being indicative of a scene of at least one consumer in an environment identify in the data a certain consumer identify an event being indicative of a behavioral compensation for vision impairment and upon identification of such an event send a notification relating to the vision impairment  a computer program product stored on a tangible computer readable medium comprising a library of software modules which cause a computer executing them to prompt for information pertinent to at least one of an eyeglasses recommendation and an optical lens option recommendation to store said information or to display eyewear recommendations   the computer program product of claim   wherein said library further comprises a module for frame selection point of sales and advertising  a computer platform for facilitating eye glasses marketing or selection comprising a camera a processor configured to execute computer program instructions to cause the processor to take an image of a consumer identify in the image a certain consumer identify an event being indicative of a behavioral compensation for vision impairment and upon identification of such an event sending a notification relating to the vision impairment local storage for processor executable instructions for carrying out storage of information  a method for alerting on vision impairment said method comprising identifying a certain individual in scene data being indicative of a scene of at least one consumer in an environment identifying an event being indicative of a behavioral compensation for vision impairment and upon identification of such an event sending a notification on the vision impairment  the method of claim  further comprising detecting data being indicative of a scene of at least one consumer in a retail environment  the method of claim  wherein detecting the data being indicative of at least one consumer comprises at least one of capturing at least one image of at least one consumer detecting data being indicative of a motion of a consumer or tracking an eye motion of a consumer  the method of claim  wherein capturing at least one image of at least one consumer comprises continuously recording a scene  the method of any one of claims  to  further comprising identifying in the data the consumer' s condition including data being indicative of the consumer's position and location relative to the consumer's environment said data comprising at least one of the consumer's face posture position sound or motion  the method of any one of claims  to  wherein said event comprises at least one of position and orientation of head increase or decrease of viewing distance between the consumer and viewed object or changing the position of eyeglasses worn by the consumer  the method of any one of claims  to  wherein identifying of the event comprises identifying images having an image feature being indicative of behavioral compensation performing a bruckner test performing a hirschberg test and measuring blink countfrequency  the method of claim  wherein the image feature being indicative of behavioral compensation comprises squinting head orientation certain distances between an object and a consumer's eyes certain position of eyeglasses on the consumer's face strabismus cataracts and reflections from the eye  the method of any one of claims  to  wherein identifying in the at least one image a consumer in a retail environment comprising at least one of receiving data characterizing the retail environment or performing face recognition  the method of any one of claims  to  wherein sending a notification comprising sending the notification to at least one of the identified consumer or a third party  the method of any one of claims  to  wherein the notification includes at least one of the data indicative of the identified event data indicative of the identified consumer ophthalmologic recommendations based on the identified event or lack of events and an appointment for a vision test  the method of any one of claims  to  further comprising storing at least one of a reference data indicative of behavioral compensation for vision impairment data indicative of the notification or data indicative of a follow-up of the notification  the method of claim  further comprising identifying the event upon comparison between the detected data and the reference data and determining a probability for a vision impairment of the consumer based on the comparison  a computer program intended to be stored in a memory of a processor unit of a computer system or in a removable memory medium adapted to cooperate with a reader of the processor unit comprising instructions for implementing the method according to any of claims  to \n"
     ]
    }
   ],
   "source": [
    "#punctuation and digits removal: we replace any undesired character with a ''\n",
    "for char in '?.,!/;:()1234567890':  \n",
    "    lowera_text = lowera_text.replace(char,'')\n",
    "print(lowera_text)\n",
    "for char in '?.,!/;:()1234567890':  \n",
    "    lowerc_text = lowerc_text.replace(char,'')\n",
    "print(lowerc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7e1cc",
   "metadata": {},
   "source": [
    "### KeyWord Cleaning\n",
    "We do a preliminary keyword removal to clean the text from redundant words that are not needed in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "798a087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing extractor...\n",
      "Loading text...\n",
      "Candidate Selection...\n",
      "Weighting...\n",
      "Selecting 10 best candidates...\n",
      "image\n",
      "computation method\n",
      "user\n",
      "face recognition operation\n",
      "device\n",
      "face region\n",
      "plurality\n",
      "processor\n",
      "system\n",
      "facial recognitiona camera\n",
      "[('image', 0.04694284852728213), ('computation method', 0.031442039493363806), ('user', 0.027788302431730104), ('face recognition operation', 0.02196271915915034), ('device', 0.020319617325812032), ('face region', 0.019573677639206713), ('plurality', 0.01656449323088685), ('processor', 0.01628557320191861), ('system', 0.014369073395178037), ('facial recognitiona camera', 0.013183701899647368)]\n"
     ]
    }
   ],
   "source": [
    "import pke\n",
    "\n",
    "# initialize keyphrase extraction model, here TopicRank\n",
    "print(\"Initializing extractor...\")\n",
    "extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "# load the content of the document, here document is expected to be in raw\n",
    "# format (i.e. a simple text file) and preprocessing is carried out using spacy\n",
    "print(\"Loading text...\");\n",
    "extractor.load_document(input=lowera_text, language='en')\n",
    "\n",
    "# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
    "# and adjectives (i.e. `(Noun|Adj)*`)\n",
    "print(\"Candidate Selection...\")\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
    "print(\"Weighting...\")\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# N-best selection, keyphrases contains the 10 highest scored candidates as\n",
    "# (keyphrase, score) tuples\n",
    "print(\"Selecting 10 best candidates...\")\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "for tuple in keyphrases:\n",
    "    print(tuple[0])\n",
    "    lowera_text = lowera_text.replace(tuple[0],'')\n",
    "print(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e577253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing extractor...\n",
      "Loading text...\n",
      "Candidate Selection...\n",
      "Weighting...\n",
      "Selecting 10 best candidates...\n",
      "claim\n",
      "image\n",
      "method\n",
      "plurality\n",
      "electronic device\n",
      "user\n",
      "faces\n",
      "second face recognition operation\n",
      "processor\n",
      "feature tensors\n",
      "[('claim', 0.04382683192815315), ('image', 0.027245173031580146), ('method', 0.02484087304737162), ('plurality', 0.02188169701653695), ('electronic device', 0.016951377660441365), ('user', 0.01656228675250289), ('faces', 0.015134349086063339), ('second face recognition operation', 0.013276317914395708), ('processor', 0.013040922806219603), ('feature tensors', 0.012906305396095897)]\n"
     ]
    }
   ],
   "source": [
    "# initialize keyphrase extraction model, here TopicRank\n",
    "print(\"Initializing extractor...\")\n",
    "extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "# load the content of the document, here document is expected to be in raw\n",
    "# format (i.e. a simple text file) and preprocessing is carried out using spacy\n",
    "print(\"Loading text...\");\n",
    "extractor.load_document(input=lowerc_text, language='en')\n",
    "\n",
    "# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
    "# and adjectives (i.e. `(Noun|Adj)*`)\n",
    "print(\"Candidate Selection...\")\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
    "print(\"Weighting...\")\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# N-best selection, keyphrases contains the 10 highest scored candidates as\n",
    "# (keyphrase, score) tuples\n",
    "print(\"Selecting 10 best candidates...\")\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "for tuple in keyphrases:\n",
    "    print(tuple[0])\n",
    "    lowerc_text = lowerc_text.replace(tuple[0],'')\n",
    "print(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e04297f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recognition\n",
      "recognizing\n",
      "recognitiona\n",
      "features\n",
      "classification\n",
      "[('detection', 0.4378), ('classifier', 0.4285), ('computing', 0.4276), ('classifying', 0.4272), ('supervised', 0.4258)]\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(lowera_text)\n",
    "for tuple in keywords:\n",
    "    print(tuple[0])\n",
    "    lowera_text = lowera_text.replace(tuple[0],'')\n",
    "\n",
    "print(kw_model.extract_keywords(lowera_text, keyphrase_ngram_range=(1, 1), stop_words=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e866b92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calibration\n",
      "tracking\n",
      "recognition\n",
      "orientation\n",
      "recognizing\n",
      "[('analyses', 0.4213), ('capturing', 0.4131), ('posture', 0.4113), ('gaze', 0.4086), ('vision', 0.4033)]\n"
     ]
    }
   ],
   "source": [
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(lowerc_text)\n",
    "for tuple in keywords:\n",
    "    print(tuple[0])\n",
    "    lowerc_text = lowerc_text.replace(tuple[0],'')\n",
    "\n",
    "print(kw_model.extract_keywords(lowerc_text, keyphrase_ngram_range=(1, 1), stop_words=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ef35c",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "In this step we tokenize the text of both sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "102a8bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' an   configured to make a screen  to display a  of  frames comprising an  capturing device  a storage device  storing a  of modules and a   coupled to the  capturing device  and the storage device  configured to execute the modules in the storage device  to configure the screen  to display a  of marker objects at a  of predetermined  positions configure the  capturing device  to capture a  of first head s when a  is looking at the predetermined  positions s perform a  of first face  operations on the first head s to obtain a  of first face regions corresponding to the predetermined  positions s detect a  of first facial landmarks corresponding to the first face regions s calculate a  of rotation reference angles of the  looking at the predetermined  positions according to the first facial landmarks configure the  capturing device  to capture a second head  of the  perform a  on the second head  to obtain a second face region detect a  of second facial landmarks within the second face region s estimate a head posture angle of the  according to the second facial landmarks calculate a gaze position of the  on the screen  according to the head posture angle the rotation reference angles and the predetermined  positions and configure the screen  to display a corresponding visual effect according to the gaze position  the   according to   wherein the gaze position comprises a first coordinate value in a first axial direction and a second coordinate value in a second axial direction  the   according to   wherein the head posture angles comprise a head pitch angle and a head yaw angle and the rotation reference angles comprise a first pitch angle a second pitch angle a first yaw angle and a second yaw angle corresponding to the predetermined  positions  the   according to   wherein the   performs interpolation operation or extrapolation operation according to the first yaw angle the second yaw angle a first position corresponding to the first yaw angle among the predetermined  positions a second position corresponding to the second yaw angle among the predetermined  positions and the head yaw angle thereby obtaining the first coordinate value of the gaze position and the   performs interpolation operation or extrapolation operation according to the first pitch angle the second pitch angle a third position corresponding to the first pitch angle among the predetermined  positions a fourth position corresponding to the second pitch angle among the predetermined  positions and the head pitch angle thereby obtaining the second coordinate value of the gaze position  the   according to   wherein the   calculates a  of first viewing distances between the  and the screen  according to the first facial landmarks the   estimates a second viewing distance between the  and the screen  according to the second facial landmarks and the   adjusts the rotation reference angles or the gaze position according to the second viewing distance and the first viewing distances  the   according to   wherein the   maps a  of two-dimensional position coordinates of the second facial landmarks under a plane coordinate system to a  of three-dimensional position coordinates under a three-dimensional coordinate system and the   estimates the head posture angle according to the three-dimensional position coordinates of the second facial landmarks  the   according to   wherein the second head  comprises a wearable device and the second facial landmarks do not comprise a  of third facial landmarks of the  covered by the wearable device  the   according to   wherein the second head  comprises a wearable device and the second facial landmarks comprise one or more simulated landmarks marked by the wearable device  an operating  adapted for an   comprising an  capturing device  and making a screen  to display a  of  frames the  comprising configuring the screen  to display a  of marker objects at a  of predetermined  positions configuring the  capturing device  to capture a  of first head s when a  is looking at the predetermined  positions s performing a  of first face  operations on the first head s to obtain a  of first face regions corresponding to the predetermined  positions s detecting a  of first facial landmarks corresponding to the first face regions s calculating a  of rotation reference angles of the  looking at the predetermined  positions according to the first facial landmarks configuring the  capturing device  to capture a second head  of the  performing a  on the second head  to obtain a second face region s detecting a  of second facial landmarks within the second face region estimating a head posture angle of the  according to the second facial landmarks calculating a gaze position of the  on the screen  according to the head posture angle the rotation reference angles and the predetermined  positions and s configuring the screen  to display a corresponding visual effect according to the gaze position  the operation  according to   wherein the gaze position comprises a first coordinate value in a first axial direction and a second coordinate value in a second axial direction  the operation  according to   wherein the head posture angles comprise a head pitch angle and a head yaw angle and the rotation reference angles comprise a first pitch angle a second pitch angle a first yaw angle and a second yaw angle corresponding to the predetermined  positions  the operation  according to   wherein the step of calculating the gaze position of the  on the screen  according to the head posture angle the rotation reference angles and the predetermined  positions comprises performing interpolation operation or extrapolation operation according to the first yaw angle the second yaw angle a first position corresponding to the first yaw angle among the predetermined  positions a second position corresponding to the second yaw angle among the predetermined  positions and the head yaw angle thereby obtaining the first coordinate value of the gaze position and performing interpolation operation or extrapolation operation according to the first pitch angle the second pitch angle a third position corresponding to the first pitch angle among the predetermined  positions a fourth position corresponding to the second pitch angle among the predetermined  positions and the head pitch angle thereby obtaining the second coordinate value of the gaze position  the operation  according to   wherein the  further comprises calculating a  of first viewing distances between the  and the screen  according to the first facial landmarks estimating a second viewing distance between the  and the screen  according to the second facial landmarks and adjusting the rotation reference angles or the gaze position according to the second viewing distance and the first viewing distances  the operation  according to   wherein the  further comprises mapping a  of two-dimensional position coordinates of the second facial landmarks under a plane coordinate system to a  of three-dimensional position coordinates under a three-dimensional coordinate system and estimating the head posture angle according to the three-dimensional position coordinates of the second facial landmarks  the operation  according to   wherein the second head  comprises a wearable device and the second facial landmarks do not comprise a  of third facial landmarks of the  covered by the wearable device  the operation  according to   wherein the second head  comprises a wearable device and the second facial landmarks comprise one or more simulated landmarks marked by the wearable device a computation  applied to a computing system wherein the computing system comprises a control unit a computation group and a general storage unit wherein the control unit comprises a first memory a decoding logic and a controller wherein the computation group comprises a group controller and a  of computing units the general storage unit is configured to store data and the computation  comprises receiving by the controller a first level instruction sequence and partitioning by the decoding logic the first level instruction sequence into a  of second level instruction sequences creating by the controller m threads for the  of second level instruction sequences and allocating by the controller an independent register as well as configuring an independent addressing function for each thread of the m threads wherein m is an integer greater than or equal to  and obtaining by the group controller a  of computation types of the  of second level instruction sequences obtaining a corresponding fusion computation manner of the computation types according to the  of computation types and adopting by the  of computing units the fusion computation manner to call the m threads for performing computations on the  of second level instruction sequences to obtain a final result  the  of   wherein the obtaining by the group controller a  of computation types of the  of second level instruction sequences obtaining a corresponding fusion computation manner of the computation types according to the  of computation types and adopting by the  of computing units the fusion computation manner to call the m threads for performing computations on the  of second instruction sequences to obtain a final result if the computation types represent computation operations of the same type the group controller calls a combined computation manner in which single instruction multiple data of the same type is in combination with single instruction multiple threads and uses the m threads to perform the combined computation manner to obtain a final result which includes partitioning by the decoding logic the m threads into n wraps for allocating to the the  of computing units converting by the group controller the  of second instruction sequences into a  of second control signals and sending the second control signals to the  of computing units calling by the  of computing units wraps that are allocated to the computing units and the second control signals to fetch corresponding data according to the independent addressing function performing by the  of computing units computations on the data to obtain a  of intermediate results and splicing the  of intermediate results to obtain a final result  the  of   wherein the obtaining by the group controller a  of computation types of the  of second level instruction sequences obtaining a corresponding fusion computation manner of the computation types according to the  of computation types and adopting by the  of computing units the fusion computation manner to call the m threads for performing computations on the  of second instruction sequences to obtain a final result if the computation types represent computation operations of different types the group controller calls simultaneous multi-threading and the m threads to perform computations to obtain a final result which includes partitioning by the decoding logic the m threads into n wraps converting the  of second instruction sequences into a  of second control signals obtaining by the group controller computation types supported by the  of computing units allocating by the controller the n wraps and the  of second control signals to corresponding computing units that support computation types of the wraps and the second control signals calling by the  of computing units wraps that are allocated to the computing units and the second control signals fetching by the  of computing units corresponding data performing by the  of computing units computations on the data to obtain a  of intermediate results and splicing all the intermediate results to obtain a final result  the  of   or  further comprising if a wrap a in the  of wraps is blocked adding the wrap a to a waiting queue and if data of the wrap a are already fetched adding the wrap a to a preparation queue wherein the preparation queue is a queue where a wrap to be scheduled for executing is located when a computing resource is idle  the  of   wherein the first level instruction sequence includes a very long instruction and the second level instruction sequence includes an instruction sequence  the  of   wherein the computing system further includes a tree module wherein the tree module includes a root port and a  of branch ports wherein the root port of the tree module is connected to the group controller and the  of branch ports of the tree module are connected to a computing unit of the  of computing units respectively and the tree module is configured to forward data blocks wraps or instruction sequences between the group controller and the  of computing units  the  of   wherein the tree module is an n-ary tree wherein n is an integer greater than or equal to   the  of   wherein the computing system further includes a branch processing circuit wherein the branch processing circuit is connected between the group controller and the  of computing units and the branch processing circuit is configured to forward data wraps or instruction sequences between the group controller and the  of computing units  a computing system comprising a control unit a computation group and a general storage unit wherein the control unit includes a first memory a decoding logic and a controller the computation group includes a group controller and a  of computing units the general storage unit is configured to store data the controller is configured to receive a first level instruction sequence and control the first memory and the decoding logic the decoding logic is configured to partition the first level instruction sequence into a  of second level instruction sequences the the controller is further configured to create m threads for the  of second level instruction sequences and allocate an independent register and configure an independent addressing function for each thread of the m threads m is an integer greater than or equal to  and the controller is further configured to convert the  of second instruction sequences into a  of control signals for sending to the group controller the group controller is configured to receive the  of control signals obtain a  of computational types if the  of control signals divide the m threads into n wraps and allocate the n wraps and the  of control signals to the  of computing units according to the  of computational types the  of computing units are configured to fetch data from the general storage unit through allocated wraps and control signals and perform computations to obtain an intermediate result and the group controller is configured to splice all intermediate results to obtain a final computation result  the computing system of   wherein the  of computing units includes an addition computing unit a multiplication computing unit an activation computing unit or a dedicated computing unit  the computing system of   wherein the dedicated computing unit includes a face  computing unit a graphics computing unit a fingerprint computing unit or a neural network computing unit  the computing system of   wherein the group controller is configured to if computation types of a  of control signals are graphics computations fingerprint identification face  or neural network operations allocate the  of control signals to the face  computing unit the graphics computing unit the fingerprint computing unit or the neural network computing unit respectively  the computing system of   wherein the first level instruction sequence includes a very long instruction and the second level instruction sequence includes an instruction sequence  the computing system of   further comprising a tree module wherein the tree module includes a root port and a  of branch ports wherein the root port of the tree module is connected to the group controller and the  of branch ports of the tree module are connected to a computing unit of the  of computing units respectively and the tree module is configured to forward data blocks wraps or instruction sequences between the group controller and the  of computing units  the computing system of   wherein the tree module is an n-ary tree wherein n is an integer greater than or equal to   the computing system of   wherein the computing system includes a branch processing circuit the branch processing circuit is connected between the group controller and the  of computing units and the branch processing circuit is configured to forward data wraps or instruction sequences between the group controller and the  of computing units  a computer program product comprising a non-instant computer readable storage medium wherein a computer program is stored in the non-instant computer readable storage medium and the computer program is capable of causing a computer to perform the  of any of s - through operations a  for detecting body information on one or more passengers of a vehicle based on humans\\' status  comprising steps of a if at least one interior  of an interior of the vehicle is acquired a passenger body information-detecting device performing i a process of inputting the interior  into a face  network to thereby allow the face  network to detect each of  of each of the passengers from the interior  and thus to output multiple pieces of passenger feature information corresponding to each of the detected  and ii a process of inputting the interior  into a body  network to thereby allow the body  network to detect each of bodies of each of the passengers from the interior  and thus to output body-part length information of each of the detected bodies and b the passenger body information-detecting device performing a process of retrieving specific height mapping information corresponding to specific passenger feature information on a specific passenger from a height mapping table which stores height mapping information representing respective one or more predetermined ratios of one or more segment body portions of each of human groups to each of heights per each of the human groups a process of acquiring a specific height of the specific passenger from the specific height mapping information by referring to specific body-part length information of the specific passenger a process of retrieving specific weight mapping information corresponding to the specific passenger feature information from a weight mapping table which stores multiple pieces of weight mapping information representing predetermined correlations between each of the heights and each of weights per each of the human groups and a process of acquiring a weight of the specific passenger from the specific weight mapping information by referring to the specific height of the specific passenger  the  of   wherein at the step of a the passenger body information-detecting device performs a process of inputting the interior  into the body  network to thereby allow the body  network to i output one or more  with one or more channels corresponding to the interior  via a feature extraction network ii generate at least one keypoint heatmap and at least one part affinity field with one or more channels corresponding to each of the  via a keypoint heatmap & part affinity field extractor and iii extract keypoints from the keypoint heatmap via a keypoint detector to group the extracted keypoints by referring to the part affinity field and thus to generate body parts per the passengers and as a result allow the body  network to output multiple pieces of body-part length information on each of the passengers by referring to the body parts per the passengers  the  of   wherein the feature extraction network includes at least one convolutional layer and applies at least one convolution operation to the interior  to thereby output the   the  of   wherein the keypoint heatmap & part affinity field extractor includes one of a fully convolutional network and a × convolutional layer and applies a fully-convolution operation or × convolution operation to the  to thereby generate the keypoint heatmap and the part affinity field  the  of   wherein the keypoint detector connects by referring to the part affinity field pairs respectively having highest mutual connection probabilities of being connected among the extracted keypoints to thereby group the extracted keypoints  the  of   wherein the feature extraction network and the keypoint heatmap & part affinity field extractor have been learned by a learning device performing i a process of inputting at least one training  including one or more objects for training into the feature extraction network to thereby allow the feature extraction network to generate one or more  for training having one or more channels by applying at least one convolutional operation to the training  ii a process of inputting the  for training into the keypoint heatmap & part affinity field extractor to thereby allow the keypoint heatmap & part affinity field extractor to generate one or more keypoint heatmaps for training and one or more part affinity fields for training having one or more channels for each of the  for training iii a process of inputting the keypoint heatmaps for training and the part affinity fields for training into the keypoint detector to thereby allow the keypoint detector to extract keypoints for training from each of the keypoint heatmaps for training and a process of grouping the extracted keypoints for training by referring to each of the part affinity fields for training to thereby detect keypoints per each of the objects for training and iv a process of allowing a loss layer to calculate one or more losses by referring to the keypoints per each of the objects for training and their corresponding ground truths to thereby adjust one or more parameters of the feature extraction network and the keypoint heatmap & part affinity field extractor such that the losses are minimized by backpropagation using the losses  the  of   wherein at the step of a the passenger body information-detecting device performs a process of inputting the interior  into the face  network to thereby allow the face  network to detect each of the  of each of the passengers located in the interior  via a face detector and to output multiple pieces of the passenger feature information on each of the facial s via a facial feature classifier  the  of   wherein at the step of a the passenger body information-detecting device performs a process of inputting the interior  into the face  network to thereby allow the face  network to i apply at least one convolution operation to the interior  and thus to output at least one feature map corresponding to the interior  via at least one convolutional layer ii output one or more proposal boxes where the passengers are estimated as located on the feature map via a region proposal network iii apply pooling operation to one or more regions corresponding to the proposal boxes on the feature map and thus to output at least one feature vector via a pooling layer and iv apply fully-connected operation to the feature vector and thus to output the multiple pieces of the passenger feature information corresponding to each of the  of each of the passengers corresponding to each of the proposal boxes via a fully connected layer  the  of   wherein the multiple pieces of the passenger feature information include each of ages each of genders and each of races corresponding to each of the passengers  a passenger body information-detecting device for detecting body information on one or more passengers of a vehicle based on humans\\' status  comprising at least one memory that stores instructions and at least one  configured to execute the instructions to perform or support another device to perform i if at least one interior  of an interior of the vehicle is acquired i a process of inputting the interior  into a face  network to thereby allow the face  network to detect each of  of each of the passengers from the interior  and thus to output multiple pieces of passenger feature information corresponding to each of the detected  and ii a process of inputting the interior  into a body  network to thereby allow the body  network to detect each of bodies of each of the passengers from the interior  and thus to output body-part length information of each of the detected bodies and ii a process of retrieving specific height mapping information corresponding to specific passenger feature information on a specific passenger from a height mapping table which stores height mapping information representing respective one or more predetermined ratios of one or more segment body portions of each of human groups to each of heights per each of the human groups a process of acquiring a specific height of the specific passenger from the specific height mapping information by referring to specific body-part length information of the specific passenger a process of retrieving specific weight mapping information corresponding to the specific passenger feature information from a weight mapping table which stores multiple pieces of weight mapping information representing predetermined correlations between each of the heights and each of weights per each of the human groups and a process of acquiring a weight of the specific passenger from the specific weight mapping information by referring to the specific height of the specific passenger  the passenger body information-detecting device of   wherein at the process of i the  performs a process of inputting the interior  into the body  network to thereby allow the body  network to i output one or more  with one or more channels corresponding to the interior  via a feature extraction network ii generate at least one keypoint heatmap and at least one part affinity field with one or more channels corresponding to each of the  via a keypoint heatmap & part affinity field extractor and iii extract keypoints from the keypoint heatmap via a keypoint detector to group the extracted keypoints by referring to the part affinity field and thus to generate body parts per the passengers and as a result allow the body  network to output multiple pieces of body-part length information on each of the passengers by referring to the body parts per the passengers  the passenger body information-detecting device of   wherein the keypoint heatmap & part affinity field extractor includes one of a fully convolutional network and a × convolutional layer and applies a fully-convolution operation or × convolution operation to the  to thereby generate the keypoint heatmap and the part affinity field  the passenger body information-detecting device of   wherein the keypoint detector connects by referring to the part affinity field pairs respectively having highest mutual connection probabilities of being connected among the extracted keypoints to thereby group the extracted keypoints  the passenger body information-detecting device of   wherein the feature extraction network and the keypoint heatmap & part affinity field extractor have been learned by a learning device performing i a process of inputting at least one training  including one or more objects for training into the feature extraction network to thereby allow the feature extraction network to generate one or more  for training having one or more channels by applying at least one convolutional operation to the training  ii a process of inputting the  for training into the keypoint heatmap & part affinity field extractor to thereby allow the keypoint heatmap & part affinity field extractor to generate one or more keypoint heatmaps for training and one or more part affinity fields for training having one or more channels for each of the  for training iii a process of inputting the keypoint heatmaps for training and the part affinity fields for training into the keypoint detector to thereby allow the keypoint detector to extract keypoints for training from each of the keypoint heatmaps for training and a process of grouping the extracted keypoints for training by referring to each of the part affinity fields for training to thereby detect keypoints per each of the objects for training and iv a process of allowing a loss layer to calculate one or more losses by referring to the keypoints per each of the objects for training and their corresponding ground truths to thereby adjust one or more parameters of the feature extraction network and the keypoint heatmap & part affinity field extractor such that the losses are minimized by backpropagation using the losses  the passenger body information-detecting device of   wherein at the process of i the  performs a process of inputting the interior  into the face  network to thereby allow the face  network to i apply at least one convolution operation to the interior  and thus to output at least one feature map corresponding to the interior  via at least one convolutional layer ii output one or more proposal boxes where the passengers are estimated as located on the feature map via a region proposal network iii apply pooling operation to one or more regions corresponding to the proposal boxes on the feature map and thus to output at least one feature vector via a pooling layer and iv apply fully-connected operation to the feature vector and thus to output the multiple pieces of the passenger feature information corresponding to each of the  of each of the passengers corresponding to each of the proposal boxes via a fully connected layer a computer implemented  for performing video coding based on face detection comprising receiving a video frame comprising one of a  of video frames of a video sequence determining the video frame is a key frame of the video sequence performing in response to the video frame being a key frame of the video sequence a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region rejecting the second candidate face region and outputting the first candidate face region and encoding the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream  the  of   wherein the skin tone information comprises a skin probability map  the  of   wherein said testing the first and second candidate face regions based on skin tone information is performed in response to the video frame being a key frame of the video sequence  the  of   wherein the first candidate face region comprises a rectangular region the  further comprising determining a free form shape face region corresponding to the first candidate face region wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy  the  of   wherein determining the free form shape face region comprises generating an enhanced skip probability map corresponding to the first candidate face region binarizing the enhanced skip probability map and overlaying the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region  the  of   wherein a second video frame comprises a non-key frame of the video sequence the  further comprising performing face detection in the second video frame of the video sequence based on the free form shape face region  the  of   further comprising  a second free form shape face region in the second video frame based on the free form shape face region in the video frame  the  of   wherein  the second free form shape face region comprises determining a location of a second valid face region in the second video frame based on a displacement offset with respect to the first candidate face region  the  of   further comprising determining the displacement offset based on an offset between a centroid of a bounding box around a skin enhanced region corresponding to the first candidate face region and a second centroid of a second bounding box around a second skin enhanced region in the second video frame  the  of   wherein encoding the video frame based at least in part on the first candidate face region being a valid face region comprises at least one of reducing a quantization parameter corresponding to the first candidate face region adjusting a lambda value for the first candidate face region or disabling skip coding for the first candidate face region  the  of   wherein the bitstream comprises at least one of an hadvanced video coding avc compliant bitstream an hhigh efficiency video coding hevc compliant bitstream a vp compliant bitstream a vp compliant bitstream or an alliance for open media aom av compliant bitstream  a computer implemented  for performing face detection comprising receiving a video frame of a sequence of video frames performing a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region rejecting the second candidate face region and outputting the first candidate face region as a valid face region for further processing and providing an index indicative of a person being present in the video frame based on the valid face region  the  of   wherein the sequence of video frames comprises a sequence of surveillance video frames the  further comprising performing face  in the surveillance video frames based on the valid face region  the  of   wherein the sequence of video frames comprises a sequence of decoded video frames the  further comprising adding a marker corresponding to the received video frame to perform face  on the received video frame based on the valid face region  the  of   wherein the sequence of video frames is received during a device login attempt the  further comprising performing face  based on the valid face region and allowing access to the device if a secured face is recognized  the  of   wherein the sequence of video frames comprises a sequence of videoconferencing frames the  further comprising encoding the video frame based at least in part on the valid face region to generate a coded bitstream  the  of   wherein encoding the video frame comprises not encoding a background region of the video frame into the bitstream  the  of   further comprising encoding the video frame based at least in part on the valid face region to generate a coded bitstream wherein encoding the video frame comprises including metadata corresponding to the valid face region in the bitstream  the  of   further comprising decoding the coded bitstream to generate a decoded video frame and to determine the metadata corresponding to the valid face region in the bitstream  the  of   further comprising at least one of replacing the valid face region based on the decoded metadata cropping and displaying  data corresponding only to the valid face region based on the decoded metadata or indexing the decoded video frame based on the decoded metadata  a system for performing video coding based on face detection comprising a memory configured to store a video frame comprising one of a  of video frames of a video sequence and a  coupled to the memory the  to receive the video frame to determine the video frame is a key frame of the video sequence to perform in response to the video frame being a key frame of the video sequence a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame to test the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region to reject the second candidate face region and outputting the first candidate face region and to encode the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream  the system of   wherein the skin tone information comprises a skin probability map  the system of   wherein the first candidate face region comprises a rectangular region the  further to determine a free form shape face region corresponding to the first candidate face region wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy  the system of   wherein the  to determine the free form shape face region comprises the  to generate an enhanced skip probability map corresponding to the first candidate face region to binarize the enhanced skip probability map and to overlay the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region  the system of   wherein a second video frame comprises a non-key frame of the video sequence and the  is further to perform face detection in the second video frame of the video sequence based on the free form shape face region  the system of   wherein the  is further to track a second free form shape face region in the second video frame based on the free form shape face region in the video frame  the system of   wherein to encode the video frame based at least in part on the first candidate face region being a valid face region comprises the  to reduce a quantization parameter corresponding to the first candidate face region adjust a lambda value for the first candidate face region or disable skip coding for the first candidate face region  at least one non-transitory machine readable medium comprising a  of instructions that in response to being executed on a device cause the device to perform video coding based on face detection by receiving a video frame comprising one of a  of video frames of a video sequence determining the video frame is a key frame of the video sequence performing in response to the video frame being a key frame of the video sequence a multi-stage facial search of the video frame based on predetermined feature templates and a predetermined number of stages to determine a first candidate face region and a second candidate face region in the video frame testing the first and second candidate face regions based on skin tone information to determine the first candidate face region is a valid face region and the second candidate face region is an invalid face region rejecting the second candidate face region and outputting the first candidate face region and encoding the video frame based at least in part on the first candidate face region being a valid face region to generate a coded bitstream  the non-transitory machine readable medium of   wherein the skin tone information comprises a skin probability map  the non-transitory machine readable medium of   wherein the first candidate face region comprises a rectangular region the machine readable medium comprising further instructions that in response to being executed on the device cause the device to perform video coding based on face detection by determining a free form shape face region corresponding to the first candidate face region wherein the free form shape face region has at least one of a pixel accuracy or a small block of pixels accuracy  the non-transitory machine readable medium of   wherein determining the free form shape face region comprises generating an enhanced skip probability map corresponding to the first candidate face region binarizing the enhanced skip probability map and overlaying the binarized enhanced skip probability map over at least a portion of the video frame to provide the free form shape face region  the non-transitory machine readable medium of   wherein a second video frame comprises a non-key frame of the video sequence the machine readable medium comprising further instructions that in response to being executed on the device cause the device to perform video coding based on face detection by performing face detection in the second video frame of the video sequence based on the free form shape face region  the non-transitory machine readable medium of   the machine readable medium comprising further instructions that in response to being executed on the device cause the device to perform video coding based on face detection by  a second free form shape face region in the second video frame based on the free form shape face region in the video frame  the non-transitory machine readable medium of   wherein encoding the video frame based at least in part on the first candidate face region being a valid face region comprises at least one of reducing a quantization parameter corresponding to the first candidate face region adjusting a lambda value for the first candidate face region or disabling skip coding for the first candidate face region a  for managing a smart database which stores facial s for face  comprising steps of a a managing device performing a process of counting one or more specific facial s corresponding to at least one specific person stored in the smart database where new facial s for the face  are continuously stored and a process of determining whether a first counted value representing a count of the specific facial s satisfies a preset first set value and b if the first counted value is determined as satisfying the first set value the managing device performing a process of inputting the specific facial s into a neural aggregation network to thereby allow the neural aggregation network to generate each of quality scores of each of the specific facial s by aggregation of the specific facial s and a process of sorting the quality scores corresponding to the specific facial s in a descending order of the quality scores a process of counting the sorted specific facial s in the descending order until a second counted value which represents the number of a counted part of the specific facial s becomes equal to a preset second set value and a process of deleting an uncounted part of the specific facial s from the smart database  the  of   further comprising a step of c the managing device performing a process of generating at least one optimal feature by weighted summation of one or more features of the specific facial s using the counted part of the quality scores and a process of setting the optimal feature as a representative face corresponding to the specific person  the  of   wherein at the step of b the managing device performs a process of inputting the specific facial s into a cnn of the neural aggregation network to thereby allow the cnn to generate one or more features corresponding to each of the specific facial s and a process of inputting at least one feature vector where the features are embedded into an aggregation module including at least two attention blocks to thereby allow the aggregation module to generate each of the quality scores of each of the features  the  of   wherein at the step of b the managing device performs a process of matching i i- one or more features corresponding to each of the specific facial s stored in the smart database and i- the quality scores with ii the specific person and a process of storing the matched features and the matched quality scores in the smart database  the  of   further comprising a step of d the managing device performing one of i a process of learning a face  system by using the specific facial s corresponding to the specific person stored in the smart database and ii a process of transmitting the specific facial s corresponding to the specific person to a learning device corresponding to the face  system to thereby allow the learning device to learn the face  system using the specific facial s  the  of   wherein the neural aggregation network has been learned by a learning device repeating more than once i a process of inputting multiple facial s for training corresponding to an  set of a single face or a video of the single face into a cnn of the neural aggregation network to thereby allow the cnn to generate one or more features for training by applying at least one convolution operation to the facial s for training ii a process of inputting at least one feature vector for training where the features for training are embedded into an aggregation module including at least two attention blocks of the neural aggregation network to thereby allow the aggregation module to generate each of quality scores for training of each of the features for training by aggregation of the features for training using one or more attention parameters learned in a previous iteration iii a process of outputting at least one optimal feature for training by weighted summation of the features for training using the quality scores for training and iv a process of updating the attention parameters learned in the previous iteration of the at least two attention blocks such that one or more losses are minimized which are outputted from a loss layer by referring to the optimal feature for training and its corresponding ground truth  a managing device for managing a smart database which stores facial s for face  comprising at least one memory that stores instructions and at least one  configured to execute the instructions to perform or support another device to perform i a process of counting one or more specific facial s corresponding to at least one specific person stored in the smart database where new facial s for the face  are continuously stored and a process of determining whether a first counted value representing a count of the specific facial s satisfies a preset first set value and ii if the first counted value is determined as satisfying the first set value a process of inputting the specific facial s into a neural aggregation network to thereby allow the neural aggregation network to generate each of quality scores of each of the specific facial s by aggregation of the specific facial s and a process of sorting the quality scores corresponding to the specific facial s in a descending order of the quality scores a process of counting the sorted specific facial s in the descending order until a second counted value which represents the number of a counted part of the specific facial s becomes equal to a preset second set value and a process of deleting an uncounted part of the specific facial s from the smart database  the managing device of   wherein the  further performs iii a process of generating at least one optimal feature by weighted summation of one or more features of the specific facial s using the counted part of the quality scores and a process of setting the optimal feature as a representative face corresponding to the specific person  the managing device of   wherein at the process of ii the  performs a process of inputting the specific facial s into a cnn of the neural aggregation network to thereby allow the cnn to generate one or more features corresponding to each of the specific facial s and a process of inputting at least one feature vector where the features are embedded into an aggregation module including at least two attention blocks to thereby allow the aggregation module to generate each of the quality scores of each of the features  the managing device of   wherein at the process of ii the  performs a process of matching i i- one or more features corresponding to each of the specific facial s stored in the smart database and i- the quality scores with ii the specific person and a process of storing the matched features and the matched quality scores in the smart database  the managing device of   wherein the  further performs iv one of i a process of learning a face  system by using the specific facial s corresponding to the specific person stored in the smart database and ii a process of transmitting the specific facial s corresponding to the specific person to a learning device corresponding to the face  system to thereby allow the learning device to learn the face  system using the specific facial s  the managing device of   wherein the neural aggregation network has been learned by a learning device repeating more than once i a process of inputting multiple facial s for training corresponding to an  set of a single face or a video of the single face into a cnn of the neural aggregation network to thereby allow the cnn to generate one or more features for training by applying at least one convolution operation to the facial s for training ii a process of inputting at least one feature vector for training where the features for training are embedded into an aggregation module including at least two attention blocks of the neural aggregation network to thereby allow the aggregation module to generate each of quality scores for training of each of the features for training by aggregation of the features for training using one or more attention parameters learned in a previous iteration iii a process of outputting at least one optimal feature for training by weighted summation of the features for training using the quality scores for training and iv a process of updating the attention parameters learned in the previous iteration of the at least two attention blocks such that one or more losses are minimized which are outputted from a loss layer by referring to the optimal feature for training and its corresponding ground truth an object data processing system comprising at least one  configured to execute at least one implementation of a  of  algorithms stored on at least one non-transitory computer-readable storage medium each  algorithm having feature density selection criteria and data preprocessing code executed by at least one  the data preprocessing code comprising an invariant feature identification algorithm and configured to obtain a digital representation of a scene the scene comprising one or more textual media generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation cluster the set of invariant features into regions of interest in the digital representation of the scene each region of interest having a region feature density classify by region classifier code at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation wherein the at least one of the classified regions of interest corresponds to text and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type wherein the another of the regions of interest corresponds to a region of interest for s  the system of   wherein preprocessing code based on the feature density selection criteria determines that an ocr algorithm is applicable to the text and that other  algorithms are applicable to aspects of the photographs and to logos  the system of   wherein a  creates a  profile for a camera-equipped smartphone that includes the information that the  is visually impaired which causes prioritized execution of the ocr algorithm such that a text reader program begins reading the text to the  as quickly as possible  the system of   further comprising an audio or tactile feedback mechanism that helps the  to position the smart phone relative to the text  the system of   further comprising a \"hold still\" audio feedback signal that is sent to the  when the text is at the center of the captured scene  the system of   wherein the digital representation comprises at least one of the following types of digital data  data video data and audio data  the system of   wherein invariant feature identification algorithm comprises at least one of the following feature identification algorithms fast sift freak brisk harris daisy and mser  the system of   wherein the invariant feature identification algorithm includes at least one of the following edge detection algorithm corner detection algorithm saliency map algorithm curve detection algorithm a texton identification algorithm and wavelets algorithm  the system of   wherein at least one region of interest represents at least one physical object in the scene  the system of   wherein at least one region of interest represents at least one textual media in the scene  the system of   wherein the region of interest represents a document as the textual media  the system of   wherein the region of interest represents a financial document  the system of   wherein the region of interest represents a structured document  the system of   wherein at least one implementation of a  of  algorithms includes at least one of the following a template driven algorithm a face  algorithm an optical character  algorithm a speech  algorithm and an object  algorithm  the system of   wherein data preprocessing code is further configured to assign each region of interest at least one  algorithm as a function of a scene context derived from the digital representation  the system of   wherein the scene context includes at least one of the following types of data a location a position a time a  identity a news event a medical event and a promotion  the system of   further comprising a mobile device comprising at least one implementation of a  of  algorithms and data preprocessing code  the system of   wherein the mobile device comprises at least one of the following a smart phone a tablet wearable glass a toy a vehicle a computer and a phablet  the system of   further comprising a network-accessible server device comprising at least one implementation of a  of  algorithms and data preprocessing code  the system of   wherein the object type includes at least one of the following a face an animal a vehicle a document a plant a building an appliance clothing a body part and a toy  an object data processing system comprising at least one  configured to execute at least one implementation of a  of  algorithms stored on at least one non-transitory computer-readable storage medium each  algorithm having feature density selection criteria and data preprocessing code executed by at least one  the data preprocessing code comprising an invariant feature identification algorithm and configured to obtain a digital representation of a scene the scene comprising one or more textual media generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation cluster the set of invariant features into regions of interest in the digital representation of the scene each region of interest having a region feature density classify by region classifier code at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation wherein the at least one of the classified regions of interest corresponds to text and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type wherein the another of the regions of interest corresponds to a region of interest for s assign each region of interest at least one  algorithm from at least one implementation of a  of diverse  algorithms as a function of the region feature density of each region of interest and the feature density selection criteria of the at least one implementation of a  of diverse  algorithms and configure the assigned  algorithms to process their respective regions of interest wherein preprocessing code based on the feature density selection criteria determines that an ocr algorithm is applicable to the text and that other  algorithms are applicable to aspects of the photographs and to logos  a device comprising at least one  configured to execute at least one implementation of a  of  algorithms stored on at least one non-transitory computer-readable storage medium each  algorithm having feature density selection criteria and data preprocessing code executed by at least one  the data preprocessing code comprising an invariant feature identification algorithm and configured to obtain a digital representation of a scene the scene comprising one or more textual media generate a set of invariant features by applying the invariant feature identification algorithm to the digital representation cluster the set of invariant features into regions of interest in the digital representation of the scene each region of interest having a region feature density and classify by region classifier code at least one of the regions of interest according to object type as a function of attributes derived from the region feature density and the digital representation wherein the at least one of the classified regions of interest corresponds to text and use a classification result corresponding to the at least one of the regions of interest to classify another of the regions of interest according to object type wherein the another of the regions of interest corresponds to a region of interest for s a mobile terminal comprising a front camera configured to obtain a two-dimensional d face  of a  a glance sensor tilted by a certain angle and disposed adjacent to the front camera to obtain metadata of the d face  and a controller obtaining a distance between the glance sensor and the front camera the distance enabling an area of an overlap region where a first region representing a range photographable by the front camera overlaps a second region representing a range photographable by the glance sensor to be the maximum  the mobile terminal of   wherein the controller is configured to obtain the distance enabling the area of the overlap region to be the maximum between the glance sensor and the front camera by varying a tilting angle of the glance sensor  the mobile terminal of   wherein the controller is configured to set the distance enabling the area of the overlap region to be the maximum between the glance sensor and the front camera and the tilting angle of the glance sensor as an optimal disposition location of the glance sensor  the mobile terminal of   wherein the controller is configured to set a disposition location of the front camera as an original point and calculates coordinates of a first triangle representing the first region based on a field of view of the front camera and a maximum photographing distance of the front camera  the mobile terminal of   wherein the controller is configured to calculate coordinates of a second triangle representing the second region based on a field of view of the glance sensor a maximum photographing distance of the glance sensor a distance between the front camera and the glance sensor and a tilting angle of the glance sensor  the mobile terminal of   wherein before the glance sensor is tilted the controller is configured to calculate coordinates of a third triangle representing a third region photographable by the glance sensor and the controller is configured to rotation-convert the coordinates of the third triangle based on the tilting angle of the glance sensor and calculate the coordinates of the second triangle  the mobile terminal of   wherein the controller is configured to calculate coordinates of the overlap region based on the coordinates of the first triangle and the coordinates of the second triangle and calculates the area of the overlap region based on the coordinates of the overlap region  the mobile terminal of   wherein the controller is configured to generate three-dimensional d face information based on the d face  obtained by the front camera and metadata obtained by the glance sensor  the mobile terminal of   wherein the metadata comprises one or more of an angle of a face of the  a size of the face and a location of the face  the mobile terminal of   wherein the angle of the face comprises an angle by which the face is rotated about one or more of a pitch axis a roll axis and a yaw axis  the mobile terminal of   further comprising a memory storing the generated d face information wherein the controller is configured to performs a  authentication process by comparing the stored d face information with d face information obtained for  authentication  the mobile terminal of   wherein the glance sensor is controlled to be permanently activated with a low power to obtain a front  and metadata of the front   the mobile terminal of   wherein the front camera and the glance sensor are disposed on the same line in an upper end of the mobile terminal  the mobile terminal of   wherein the glance sensor is tilted in one direction of an up direction a down direction a left direction and a right direction  the mobile terminal of   wherein the metadata is data which is changed when the mobile terminal is tilted by an external physical force a  comprising receiving by a smart television tv an indication of upcoming media programming wherein the upcoming media programming is based on a  profile identifying one or more devices in communication with the smart tv each of the one or more devices including at least one of a microphone or a camera instructing at least one identified device to detect audio signals using its respective microphone or to detect visual signals using its respective camera selecting at least one device of the one or more devices based on the detected audio signal or detected visual signal and providing instructions to the selected device to output a notification related to the upcoming media programming  the  of   wherein the upcoming media programming is one of a live television program a recorded television program a broadcast television program or an application-provided program  the  of   wherein selecting the first device based on the detected audio signal includes  a voice  the  of   further comprising determining a distance to the recognized voice and wherein selecting the first device is further based on the determined distance  the  of   wherein selecting the first device based on the detected visual signals includes  a face  the  of   wherein  the face includes a face  technique  the  of   further comprising presenting on the smart tv the upcoming media programming in a favorite channel list  the  of   further comprising obtaining media programming viewing data wherein the media programming viewing data includes at least one of a historical time and a historical date that one or more media programs were viewed obtaining at least one of a current time and a current date processing the media programming viewing data to determine a probability of the one or more media programs being viewed based on at least one of the current time and the current date and presenting the favorite channel list based on the determined probability of the one or more media programs being viewed  the  of   wherein processing the media programming viewing data includes employing a neural network model  the  of   wherein employing the neural network model comprises determining a duration that the one or more media programs were viewed for each of the at least one of the historical time and the historical date setting a threshold time duration comparing the determined duration to the threshold time duration and filtering out the one or more media programs viewed below the threshold time duration  a smart television tv comprising a network interface a non-transitory computer-readable medium and a  in communication with the network interface and the non-transitory computer-readable medium and capable of executing -executable program code stored in the non-transitory computer-readable medium to cause the smart tv to receive an indication of upcoming media programming wherein the upcoming media programming is based on a  profile identify one or more devices in communication with the smart tv each of the one or more devices including at least one of a microphone or a camera instruct at least one identified device to detect audio signals using its respective microphone or to detect visual signals using its respective camera select at least one device of the one or more devices based on the detected audio signal or detected visual signal and provide instructions to the selected device to output a notification related to the upcoming media programming  the smart tv of   wherein selecting the first device based on the detected audio signal includes  a voice  the smart tv of   wherein the  is further capable of executing -executable program code to determine a distance to the recognized voice and wherein selecting the first device is further based on the determined distance  the smart tv of   wherein selecting the first device based on the detected visual signals includes detecting the presence of a   the smart tv of   wherein detecting the presence of the  includes employing one or more of a camera a microphone or a fingerprint sensor associated with at least one of the smart tv a mobile device a smartphone a laptop computer a tablet device a wearable device an internet of things iot device an internet of everything ioe device an iot hub or an ioe hub  a smart television tv comprising means for receiving an indication of upcoming media programming wherein the upcoming media programming is based on a  profile means for identifying one or more devices in communication with the smart tv each of the one or more devices including at least one of a microphone or a camera means for instructing at least one identified device to detect audio signals using its respective microphone or to detect visual signals using its respective camera means for selecting at least one device of the one or more devices based on the detected audio signal or detected visual signal and means for providing instructions to the selected device to output a notification related to the upcoming media programming  the smart tv of   wherein the one or more devices includes at least one of a mobile device a smartphone a laptop computer a tablet device a wearable device an internet of things iot device an internet of everything ioe device an iot hub an ioe hub or another smart tv  the smart tv of   wherein the upcoming media programming is one of a live television program a recorded television program a broadcast television program or an application-provided program  the smart tv of   wherein the notification includes at least one of a push message a sms message a waysms message an audio alert an audio message or an email message  the smart tv of   further comprising presenting the upcoming media programming in a favorite channel list  the smart tv of   further comprising means for obtaining media programming viewing data wherein the media programming viewing data includes at least one of a historical time and a historical date that one or more media programs were viewed on the smart tv means for obtaining at least one of a current time and a current date means for processing the media programming viewing data to determine a probability of the one or more media programs being viewed on the smart tv based on at least one of the current time and the current date and means for presenting the favorite channel list based on the determined probability of the one or more media programs being viewed  the smart tv of   wherein the means for processing the media programming viewing data includes employing a neural network model  the smart tv of   wherein employing the neural network model comprises determining a duration that the one or more media programs were viewed on the smart tv for each of the at least one of the historical time and the historical date setting a threshold time duration comparing the determined duration to the threshold time duration and filtering out the one or more media programs viewed below the threshold time duration  the smart tv of   further comprising means for adjusting at least one of a volume or a brightness of the smart tv wherein the adjusting is based on at least one of the historical time and the historical date  the smart tv of   further comprising means for restricting access to one or more media programs  a non-transitory computer-readable medium comprising -executable program code configured to cause a  of a smart television tv to receive an indication of upcoming media programming wherein the upcoming media programming is based on a  profile identify one or more devices in communication with the smart tv each of the one or more devices including at least one of a microphone or a camera instruct at least one identified device to detect audio signals using its respective microphone or to detect visual signals using its respective camera select at least one device of the one or more devices based on the detected audio signal or detected visual signal and provide instructions to the selected device to output a notification related to the upcoming media programming  the non-transitory computer-readable medium of   wherein selecting the first device based on the detected audio signal includes  a voice  the non-transitory computer-readable medium of   wherein the  is further capable of executing -executable program code to determine a distance to the recognized voice and wherein selecting the first device is further based on the determined distance  the non-transitory computer-readable medium of   wherein selecting the first device based on the detected visual signals includes  a face  the non-transitory computer-readable medium of   wherein  the face includes a face  technique a camera comprising a sensor array including a  of sensors an infrared ir illuminator configured to emit active ir light in an ir light sub-band a  of spectral illuminators each spectral illuminator configured to emit active spectral light in a different spectral light sub-band a depth controller machine configured to determine a depth value for each of the  of sensors based on the active ir light a spectral controller machine configured to for each of the  of sensors determine a spectral value for each spectral light sub-band of the  of spectral illuminators and an output machine configured to output a test depth+multi-spectral  including a  of pixels each pixel corresponding to one of the  of sensors of the sensor array and including at least a depth value and a spectral value for each spectral light sub-band of the  of spectral illuminators a face  machine previously trained with a set of labeled training depth+multi-spectral s having a same structure as the test depth+multi-spectral  the face  machine configured to output a confidence value indicating a likelihood that the test depth+multi-spectral  includes a face  the camera of   wherein each spectral value is calculated based on the depth value determined for the sensor that corresponds to the pixel  the camera of   wherein the face  machine is configured to use a convolutional neural network to determine the confidence value  the camera of   wherein the face  machine includes a  of input nodes wherein each input node is configured to receive a pixel value array corresponding to a different pixel of the  of pixels of the test depth+multi-spectral  and wherein the pixel value array includes the depth value and the  of multi-spectral values for the pixel  the camera of   wherein the  of multi-spectral values for the pixel include more than three spectral values  the camera of   wherein the output machine is configured to output a surface normal for each pixel of the test depth+multi-spectral  and wherein the pixel value array includes the surface normal  the camera of   wherein the output machine is configured to output a curvature for each pixel of the test depth+multi-spectral  and wherein the pixel value array includes the curvature  the camera of   wherein the face  machine is configured to use a  of models to determine the confidence value wherein the  of models includes a  of channel-specific models wherein each channel-specific model is configured to process a different pixel parameter for the  of pixels of the test depth+multi-spectral  wherein each channel-specific model includes a  of input nodes and wherein for each channel-specific model each input node is configured to receive a pixel parameter value for a different pixel of the  of pixels of the test depth+multi-spectral   the camera of   wherein the face  machine is configured to use a statistical model to determine the confidence value  the camera of   wherein the statistical model includes a nearest neighbor algorithm  the camera of   wherein the statistical model includes a support vector machine  the camera of   wherein the face  machine is further configured to output a location on the test depth+multi-spectral  of a bounding box around a recognized face  the camera of   wherein the face  machine is further configured to output a location on the test depth+multi-spectral  of an identified two-dimensional d facial feature of a recognized face  the camera of   wherein the face  machine is further configured to output a location on the test depth+multi-spectral  of an identified three-dimensional d facial feature of a recognized face  the camera of   wherein the face  machine is further configured to output a location on the test depth+multi-spectral  of an identified spectral feature on a recognized face  the camera of   wherein the face  machine is further configured to output for each pixel of the test depth+multi-spectral  a confidence value indicating a likelihood that the pixel is included in a face  the camera of   wherein the face  machine is further configured to output an identity of a face recognized in the test depth+multi-spectral   the camera of   wherein the  of sensors of the sensor array are differential sensors and wherein each spectral value is determined based on a depth value and a differential measurement for that differential sensor  a camera comprising a sensor array including a  of sensors an infrared ir illuminator configured to emit active ir light in an ir light sub-band a  of spectral illuminators each spectral illuminator configured to emit active spectral light in a different spectral light sub-band a depth controller machine configured to determine a depth value for each of the  of sensors based on the active ir light a spectral controller machine configured to for each of the  of sensors determine a spectral value for each spectral light sub-band of the  of spectral illuminators wherein each spectral value is calculated based on the depth value determined for the sensor that corresponds to the pixel and an output machine configured to output a test depth+multi-spectral  including a  of pixels each pixel corresponding to one of the  of sensors of the sensor array and including at least a depth value and a spectral value for each spectral light sub-band of the  of spectral illuminators and a face  machine including a convolutional neural network previously trained with a set of labeled training depth+multi-spectral s having a same structure as the test depth+multi-spectral  the face  machine configured to output a confidence value indicating a likelihood that the test depth+multi-spectral  includes a face an  processing  comprising acquiring a photo album obtained from face clustering collecting face information of respective s in the photo album and acquiring a face parameter of each  according to the face information selecting a cover  according to the face parameter of each  and taking a face-region  from the cover  and setting the face-region  as a cover of the photo album wherein selecting the cover  according to the face parameter of each  comprises performing calculation on the face parameter of each  in a preset way to obtain a cover score of each  selecting the  with a highest cover score as the cover  wherein selecting the  with the highest cover score as the cover  comprises acquiring a source of each  and selecting the  with the highest cover score in s coming from a preset source as the cover   the  according to   wherein selecting the  with the highest cover score as the cover  comprises acquiring the number of  contained in each  determining single-person s according to the number of  and selecting the single-person  with the highest cover score as the cover   the  according to   wherein selecting the  with the highest cover score as the cover  further comprises when there is no single-person  in the photo album determining s including two  from the photo album and selecting the  with the highest cover score from the s including two  as the cover   the  according to   wherein the face information comprises face feature points and the face parameter comprises a face turning angle acquiring the face parameter of each  according to the face information comprises acquiring coordinate values of the face feature points determining distances and angles between the face feature points and determining the face turning angle according to the distances and the angles  the  according to   wherein the face parameter comprises a face ratio acquiring the face parameter of each  according to the face information comprises determining a face region of the  according to the face information and calculating a ratio of an area of the face region to an area of the  to obtain the face ratio  the  according to   wherein calculating the face ratio comprises when there is more than one face in the  subtracting an area occupied  other than a face corresponding to the photo album from the face region to obtain a remaining area and calculating a ratio of the remaining area to the area of the  to obtain the face ratio  the  according to   wherein collecting face information of respective s in the photo album comprises acquiring  identifications of s in the photo album extracting face information corresponding to the  identifications from a face database the face database being stored with face  results of s the face  results including the face information  an  processing apparatus comprising a  and a memory configured to store instructions executable by the  wherein the  is configured to run a program corresponding to the instructions by reading the instructions stored in the memory so as to perform acquiring a photo album obtained from face clustering collecting face information of each  in the photo album acquiring a face parameter of each  according to the face information selecting a cover  according to the face parameter of each  taking a face-region  from the cover  and setting the face-region  as a cover of the photo album wherein the  is configured to perform calculation on the face parameter of each  in a preset way to obtain a cover score of each  and select the  with a highest cover score as the cover  and wherein the  is configured to acquire a source of each  and select the  with the highest cover score in s coming from a preset source as the cover   the apparatus according to   wherein the  is configured to acquire the number of  contained in each  determine single-person s according to the number of  and select the single-person  with the highest cover score as the cover   the apparatus according to   wherein the  is further configured to when there is no single-person  in the photo album determine s including two  from the photo album and select the  with the highest cover score from the s including two  as the cover   the apparatus according to   wherein the face information comprises face feature points and the face parameter comprises a face turning angle the  is configured to acquire coordinate values of the face feature points determine distances and angles between the face feature points and determine the face turning angle according to the distances and the angles  the apparatus according to   wherein the face parameter comprises a face ratio the  is configured to determine a face region of the  according to the face information and calculate a ratio of an area of the face region to an area of the  to obtain the face ratio  the apparatus according to   wherein the  is configured to when there is more than one face in the  subtract an area occupied  other than a face corresponding to the photo album from the face region to obtain a remaining area and calculate a ratio of the remaining area to the area of the  to obtain the face ratio  the apparatus according to   wherein the  is configured to acquire  identifications of s in the photo album extract face information corresponding to the  identifications from a face database the face database being stored with face  results of s the face  results including the face information  an  comprising a  a memory a display screen and an input device connected via a system bus wherein the memory is stored with computer programs that when executed by the  cause the  to implement an  processing  the  processing  comprising acquiring a photo album obtained from face clustering collecting face information of respective s in the photo album and acquiring a face parameter of each  according to the face information selecting a cover  according to the face parameter of each  and taking a face-region  from the cover  and setting the face-region  as a cover of the photo album wherein selecting the cover  according to the face parameter of each  comprises performing calculation on the face parameter of each  in a preset way to obtain a cover score of each  and selecting the  with a highest cover score as the cover  and wherein selecting the  with the highest cover score as the cover  comprises acquiring a source of each  and selecting the  with the highest cover score in s coming from a preset source as the cover   the  according to   wherein the  comprises at least one of a mobile phone a tablet computer a personal digital assistant and a wearable device a computer-implemented  comprising receiving at a computing device a meeting invitation identifying a location and at least one invitee the meeting invitation configured to provide the at least one invitee with physical access to the location wherein the meeting invitation causes a system to control a pathway allowing physical access to the location providing based on the meeting invitation the at least one invitee with physical access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location wherein the positioning data is based in part on a face  camera system identifying the at least one invitee receiving the positioning data from the face  camera system identifying the at least one invitee wherein the positioning data indicates a pattern of movement of the at least one invitee determining that the pattern of movement indicates that the at least one invitee has exited the location and revoking physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway in response to determining that the pattern of movement indicates that the at least one invitee has exited the location  the computer-implemented  of   wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location in a predetermined direction  the computer-implemented  of   wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has moved through an area in a predetermined direction  the computer-implemented  of   wherein the positioning data indicates a second pattern of movement of the at least one invitee and wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement  the computer-implemented  of   further comprising collating secured data and public data to generate resource data and communicating the resource data to a client computing device associated with the at least one invitee when access of the location is provided  the computer-implemented  of   wherein the positioning data indicates that the at least one invitee is at the predetermined location when the at least one invitee passes through the predetermined location  the computer-implemented  of   wherein the positioning data indicates that the at least one invitee is at the predetermined location when the at least one invitee passes through the predetermined location near the location in a predetermined direction  a system comprising a  and a memory in communication with the  the memory having computer-readable instructions stored thereupon that when executed by the  cause the  to receive a meeting invitation indicating a location and an identity the meeting invitation configured to provide at least one invitee with physical access to the location wherein the meeting invitation causes the system to control a pathway allowing physical access to the location provide the at least one invitee associated with the identity access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location wherein the positioning data is based in part on a face  camera system identifying the at least one invitee receive the positioning data from the face  camera system identifying the at least one invitee wherein the positioning data indicates a pattern of movement of the at least one invitee determine that the pattern of movement indicates that the at least one invitee has exited the location and revoke physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway in response to determining that the pattern of movement indicates that the at least one invitee has exited the location  the system of   wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location  the system of   wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has moved through an area in a predetermined direction  the system of   wherein the positioning data indicates a second pattern of movement of the at least one invitee and wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement  the system of   wherein the instructions further cause the  to collate secured data and public data to generate resource data and communicate the resource data to a client computing device associated with the at least one invitee when access of the location is provided  a non-transitory computer-readable storage medium having computer-executable instructions stored thereupon which when executed by one or more s of a computing device cause the one or more s of the computing device to receive a meeting invitation indicating a location and an identity the meeting invitation configured to provide at least one invitee with physical access to the location wherein the meeting invitation causes a system to control a pathway allowing physical access to the location provide the at least one invitee associated with the identity access to the location by controlling the pathway allowing the at least one invitee to physically access the location through the pathway in response to positioning data indicating that the at least one invitee is at a predetermined location near the location wherein the positioning data is based in part on a face  camera system identifying the at least one invitee receive the positioning data from the face  camera system identifying the at least one invitee wherein the positioning data indicates a pattern of movement of the at least one invitee determine that the pattern of movement indicates that the at least one invitee has exited the location and revoke physical access to the location identified in the meeting invitation by controlling the pathway to restrict the at least one invitee identified in the meeting invitation from physical access to the location through the pathway in response to determining that the pattern of movement indicates that the at least one invitee has exited the location  the non-transitory computer-readable storage medium of   wherein determining that the at least one invitee has exited the location comprises determining that the at least one invitee has passed through an egress associated with the location  the non-transitory computer-readable storage medium of   wherein the positioning data indicates a second pattern of movement of the at least one invitee and wherein access to secured data associated with the location is provided in response to detecting the second pattern of movement  the non-transitory computer-readable storage medium of   wherein the instructions further cause the one or more s to collate secured data and public data to generate resource data and communicate the resource data to a client computing device associated with the at least one invitee when access of the location is provided a  comprising receiving a piece of content and salient data for the piece of content based on the salient data determining a first path for a viewport for the piece of content wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content providing the viewport on a display device wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback detecting an additional salient event in the piece of content that is not included in the first path for the viewport and providing an indication for the additional salient event in the viewport during the playback  the  of   wherein the salient data identifies each salient event in the piece of content and the salient data indicates for each salient event in the piece of content a corresponding point location of the salient event in the piece of content and a corresponding time at which the salient event occurs during the playback  the  of   wherein the salient data further indicates for each salient event in the piece of content a corresponding type of the salient event and a corresponding strength value of the salient event  the  of   wherein the first path for the viewport controls the movement of the viewport to put the different salient events in a view of the viewport at the different times during the playback  the  of   further comprising detecting one or more salient events in the piece of content based on at least one of the following visual data of the piece of content audio data of the piece of content or content consumption experience data for the piece of content wherein the salient data is indicative of each salient event detected  the  of   further comprising detecting one or more salient events in the piece of content based on at least one of the following face  facial emotion  object  motion  or metadata of the piece of content wherein the salient data is indicative of each salient event detected  the  of   further comprising detecting  interaction with the indication wherein the indication comprises an interactive hint and in response to detecting the  interaction adapting the first path for the viewport to a second path for the viewport based on the  interaction wherein the second path for the viewport includes the additional salient event and providing an updated viewport for the piece of content on the display device wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport  the  of   further comprising changing a weight assigned to the additional salient event and one or more other salient events in the piece of content having the same type as the additional salient event  the  of   wherein the second path for the viewport includes one or more other salient events in the piece of content having the same type as the additional salient event  a system comprising at least one  and a non-transitory -readable memory device storing instructions that when executed by the at least one  causes the at least one  to perform operations including receiving a piece of content and salient data for the piece of content based on the salient data determining a first path for a viewport for the piece of content wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content providing the viewport on a display device wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback detecting an additional salient event in the piece of content that is not included in the first path for the viewport and providing an indication for the additional salient event in the viewport during the playback  the system of   wherein the salient data identifies each salient event in the piece of content and the salient data indicates for each salient event in the piece of content a corresponding point location of the salient event in the piece of content and a corresponding time at which the salient event occurs during the playback  the system of   wherein the salient data further indicates for each salient event in the piece of content a corresponding type of the salient event and a corresponding strength value of the salient event  the system of   wherein the salient data is generated offline on a server  the system of   the operations further comprising detecting one or more salient events in the piece of content based on at least one of the following visual data of the piece of content audio data of the piece of content or content consumption experience data for the piece of content wherein the salient data is indicative of each salient event detected  the system of   the operations further comprising detecting one or more salient events in the piece of content based on at least one of the following face  facial emotion  object  motion  or metadata of the piece of content wherein the salient data is indicative of each salient event detected  the system of   the operations further comprising detecting  interaction with the indication wherein the indication comprises an interactive hint and in response to detecting the  interaction adapting the first path for the viewport to a second path for the viewport based on the  interaction wherein the second path for the viewport includes the additional salient event and providing an updated viewport for the piece of content on the display device wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport  the system of   the operations further comprising changing a weight assigned to the additional salient event and one or more other salient events in the piece of content having the same type as the additional salient event  the system of   wherein the second path for the viewport includes one or more other salient events in the piece of content having the same type as the additional salient event  a non-transitory computer readable storage medium including instructions to perform a  comprising receiving a piece of content and salient data for the piece of content based on the salient data determining a first path for a viewport for the piece of content wherein the first path for the viewport includes different salient events occurring in the piece of content at different times during playback of the piece of content providing the viewport on a display device wherein movement of the viewport is based on the first path for the viewport and the salient data during the playback detecting an additional salient event in the piece of content that is not included in the first path for the viewport and providing an indication for the additional salient event in the viewport during the playback  the computer readable storage medium of   the  further comprising detecting  interaction with the indication wherein the indication comprises an interactive hint and in response to detecting the  interaction adapting the first path for the viewport to a second path for the viewport based on the  interaction wherein the second path for the viewport includes the additional salient event and providing an updated viewport for the piece of content on the display device wherein movement of the updated viewport is based on the second path for the viewport and the salient data during the playback and the second path for the viewport controls the movement of the updated viewport to put the additional salient event in a view of the updated viewport a mobile device with facial  the mobile device comprising one or more cameras a  device and memory coupled to the  device the processing system programmed to receive a  of s from the one or more cameras extract with a feature extractor utilizing a convolutional neural network cnn with an enlarged intra-class variance of long-tail classes feature vectors from each of the  of s generate with a feature generator discriminative feature vectors for each of the feature vectors classify with a fully connected classifier an identity from the discriminative feature vectors and control an operation of the mobile device to react in accordance with the identity  the mobile device as recited in   further includes a communication system  the mobile device as recited in   wherein the operation tags the video with the identity and uploads the video to social media  the mobile device as recited in   wherein the operation tags the video with the identity and sends the video to a   the mobile device as recited in   wherein the mobile device is a smart phone  the mobile device as recited in   wherein the mobile device is a body cam  the mobile device as recited in   further programmed to train the feature extractor the feature generator and the fully connected classifier with an alternative bi-stage strategy  the mobile device as recited in   wherein the feature extractor shares covariance matrices across all classes to transfer intra-class variance from regular classes to the long-tail classes  the mobile device as recited in   wherein the feature generator optimizes a softmax loss by joint regularization of weights and features through a magnitude of an inner product of the weights and features  the mobile device as recited in   wherein the feature extractor averages the feature vector with a flipped feature vector the flipped feature vector being generated from a horizontally flipped frame from one of the  of s  the mobile device as recited in   wherein each of the  of s is selected from the group consisting of an  a video and a frame from the video  the mobile device as recited in   wherein the communication system connects to a remote server that includes a facial  network  the mobile device as recited in   wherein one stage of the alternative bi-stage strategy fixes the feature extractor and applies the feature generator to generate new transferred features that are more diverse and violate a decision boundary  the mobile device as recited in   wherein one stage of the alternative bi-stage strategy fixes the fully connected classifier and updates the feature extractor and the feature generator  a computer program product for a mobile device with facial  the computer program product comprising a non-transitory computer readable storage medium having program instructions embodied therewith the program instructions executable by a computer to cause the computer to perform a  comprising receiving by a  device a  of s extracting by the  device with a feature extractor utilizing a convolutional neural network cnn with an enlarged intra-class variance of long-tail classes feature vectors for each of the  of s generating by the  device with a feature generator discriminative feature vectors for each of the feature vectors classifying by the  device utilizing a fully connected classifier an identity from the discriminative feature vector and controlling an operation of the mobile device to react in accordance with the identity  a computer-implemented  for facial  in a mobile device the  comprising receiving by a  device a  of s extracting by the  device with a feature extractor utilizing a convolutional neural network cnn with an enlarged intra-class variance of long-tail classes feature vectors for each of the  of s generating by the  device with a feature generator discriminative feature vectors for each of the feature vectors classifying by the  device utilizing a fully connected classifier an identity from the discriminative feature vector and controlling an operation of the mobile device to react in accordance with the identity  the computer-implemented  as recited in   wherein controlling includes tagging the video with the identity and uploading the video to social media  the computer-implemented  as recited in   wherein controlling includes tagging the video with the identity and sending the video to a   the computer-implemented  as recited in   wherein extracting includes sharing covariance matrices across all classes to transfer intra-class variance from regular classes to the long-tail classes a computing device comprising a non-transitory machine readable medium storing a machine trained mt network comprising a  of layers of processing nodes each processing node configured to compute a first output value by combining a set of output values from a set of processing nodes and use a piecewise linear cup function to compute a second output value from the first output value of the processing node wherein the piecewise linear cup function prior to training of the mt network comprises at least i a first linear section with a first slope followed by ii a second linear section with a negative second slope followed by iii a third linear section with a negative third slope that is different from the second slope followed by iv a fourth linear section with a positive fourth slope followed by v a fifth linear section with a positive fifth slope that is different from the fourth slope followed by vi a sixth linear section with a sixth slope wherein the piecewise linear cup function is symmetric about a vertical axis between the third and fourth linear sections prior to training of the mt network a content capturing circuit for capturing content for processing by the mt network and a set of processing units for executing the processing nodes to process content captured by the content capturing circuit wherein by training a set of parameters that define the piecewise linear cup function of each node in first and second pluralities of processing nodes i each processing node in the first  of processing nodes is configured to emulate a boolean and operator such that an output value of the processing node is in a range associated with a \"\" value only when a set of inputs to the processing node have a set of values in a range associated with \"\" and ii each processing node in the second  of processing nodes is configured to emulate a boolean xnor operator such that an output value of the processing node is in the range associated with \"\" only when a a set of inputs to the node have a set of values in a range associated with \"\" or b the set of inputs to the node have a set of values in a range associated with a \"\" value  the computing device of   wherein the third linear section of the piecewise linear cup function of a first processing node in the mt network has a different slope from the third linear section of a second processing node in the mt network  the computing device of   wherein the length of the third section of a piecewise linear cup function of a first processing node in the mt network is different from the length of the third section of a piecewise linear cup function of a second processing node in the mt network  the computing device of   wherein the sets of parameters are trained in part by a back propagating module for back propagating errors in output values of later layers of processing nodes to earlier layers of processing nodes by adjusting the set of parameters that define the piecewise linear cup functions of the earlier layers of processing nodes  the computing device of   wherein each processing node uses a linear function that is defined by a set of parameters to compute the first output value of the processing node wherein the back propagating module back propagates errors in output values of later layers of processing nodes to earlier layers of processing nodes by adjusting the set of parameters that define the linear functions of the earlier layers of processing nodes  the computing device of   wherein the first  of processing nodes that emulate the boolean and operator and the second  of processing nodes that emulate the boolean xnor operator enable the mt network to implement mathematical problems  the computing device of   wherein each of a  of processing node layers has a  of processing nodes that receive as input values the output values from a  of processing nodes in a set of prior layers  the computing device of   wherein each processing node uses a linear function to compute the first output value of the processing node wherein each processing node\\'s piecewise linear cup function is defined along first and second axes the first axis defining a range of output values from the processing node\\'s linear function and the second axis defining a range of output values produced by the piecewise linear cup function for the range of output values from the processing node\\'s linear function  the computing device of   further comprising a content output circuit for presenting an output based on the processing of the content by the mt network  the computing device of   wherein the captured content is one of an  and an audio segment and wherein the presented output is an output display on a display screen of the computing device or an audio presentation output on a speaker of the computing device  the computing device of   wherein the computing device is a mobile device  the computing device of   wherein the mt network is a mt neural network and the processing nodes are mt neurons  the computing device of   wherein the set of parameters configured through training for a  of the processing nodes comprise at least one of the negative second and third slopes for the second and third linear sections the positive fourth and fifth slopes for the fourth and fifth linear sections a first intercept for the second linear section a second intercept for the fifth linear section and a set of lengths for at least the second third fourth and fifth sections  the computing device of   wherein the trained set of parameters that define the piecewise linear cup function of each node comprise a  of output values  the computing device of   wherein the first and sixth slopes are zerowe   a system comprising a memory device to store an input  a  including an  input interface to receive the input  a pre- to model the input  to yield a multi-channel  a feature extractor to extract a set of features based on the multi-channel  a feature selector to select one or more features from the set of features of the multi-channel  wherein the one or more features are selected based on an ability to differentiate features a feature matcher to match the one or more features to a learned feature set and a similarity detector to determine whether the one or more features meet a pre-defined similarity threshold  the system of   wherein the pre- further is to activate one or more channels of the multi-channel  to yield one or more activated channels  the system of   wherein the one or more activated channels are to be determined based on their ability to differentiate features  the system of   wherein the pre- further is to activate one or more local patches of the one or more activated channels  the system of   wherein the one or more local patches are to be determined based on their ability to differentiate features  the system of   wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching  an apparatus comprising an  input interface to receive an input  a pre- to model the input  to yield a multi-channel  a feature extractor to extract a set of features based on the multi-channel  a feature selector to select one or more features from the set of features of the multi-channel  wherein the one or more features are selected based on an ability to differentiate features a feature matcher to match the one or more features to a learned feature set and a similarity detector to determine whether the one or more features meet a pre-defined similarity threshold  the apparatus of   wherein the pre- further is to activate one or more channels of the multi-channel  to yield one or more activated channels  the apparatus of   wherein the one or more activated channels are to be determined based on their ability to differentiate features  the apparatus of   wherein the pre- further is to activate one or more local patches of the one or more activated channels  the apparatus of   wherein the one or more local patches are to be determined based on their ability to differentiate features  the apparatus of   wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching  a  comprising modeling an input  to yield a multi-channel  extracting a set of features based on the multi-channel  selecting one or more features from the set of features of the multi-channel  wherein the one or more features are selected based on an ability to differentiate features matching the one or more features to a learned feature set and determining whether the one or more features meet a pre-defined similarity threshold  the  of   wherein modeling the input  further is to include activating one or more channels of the multi-channel  to yield one or more activated channels  the  of   wherein the one or more activated channels are to be determined based on their ability to differentiate features  the  of   wherein extracting features of the input  further is to include activating one or more local patches of the one or more activated channels  the  of   wherein the one or more local patches are to be determined based on their ability to differentiate features  the  of   wherein the feature matcher utilizes a large-scale data learning process to perform the feature matching  at least one non-transitory computer readable storage medium comprising a set of instructions which when executed by a computing device cause the computing device to model an input  to yield a multi-channel  extract a set of features based on the multi-channel  select one or more features from the set of features of the multi-channel  wherein the features are selected based on an ability to differentiate features match the one or more features to a learned feature set and determine whether the one or more features meet a pre-defined similarity threshold  the at least one non-transitory computer readable storage medium of   wherein the instructions when executed cause a computing device to activate one or more channels of the multi-channel  to yield one or more activated channels  the at least one non-transitory computer readable storage medium of   wherein the instructions when executed cause a computing device to determine the one or more activated channels based on their ability to differentiate features  the at least one non-transitory computer readable storage medium of   wherein extracting features of the input  is to further include activating one or more local patches of the one or more activated channels  the at least one non-transitory computer readable storage medium of   wherein the one or more local patches are to be determined based on their ability to differentiate features  the at least one non-transitory computer readable storage medium of   wherein the feature matcher further is to utilize a large-scale data learning process to perform the feature matching  an apparatus comprising means for modeling an input  to yield a multi-channel  means for extracting a set of features based on the multi-channel  means for selecting one or more features from the set of features of the multi-channel  wherein the one or more features are selected based on an ability to differentiate features means for matching the one or more features to a learned feature set and means for determining whether the one or more features meet a pre-defined similarity threshold a  for controlling a terminal the terminal comprising a capturing apparatus and at least one  the  comprising acquiring by the capturing apparatus an  obtaining by the at least one  a motion parameter of the terminal the motion parameter comprising at least one of a motion frequency or a motion time and two or more parameters from among an acceleration an angular velocity a motion amplitude the motion frequency and the motion time transmitting by the at least one  a parameter threshold obtaining request to a data management server the parameter threshold obtaining request comprising configuration information of the terminal receiving corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request comparing the two or more parameters with the corresponding preset thresholds and controlling by the at least one  not to perform  processing on the acquired  based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds wherein the acquiring comprises acquiring the  in real time and the obtaining comprises obtaining the motion parameter of the terminal in real time the  further comprising in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold obtaining the motion parameter of the terminal again and in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds performing the  processing on the  acquired at the latest time  the  according to   wherein the acquiring comprises controlling by the at least one  to turn on the capturing apparatus based on a face  instruction and acquiring by the capturing apparatus a face  when the capturing apparatus is turned on  the  according to   wherein the controlling not to perform the  processing comprises skipping performing face  on the acquired face  based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds  the  according to   wherein the obtaining comprises at least one of obtaining the acceleration of the terminal by using an acceleration sensor or obtaining the angular velocity of the terminal by using a gyro sensor  the  according to   wherein the transmitting comprises transmitting the parameter threshold obtaining request to the data management server according to a preset time period  the  according to   further comprising generating prompt information based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold the prompt information being used for prompting the terminal to stop moving  the  according to   wherein the motion parameter comprises the motion frequency and the motion time  a terminal comprising a capturing apparatus at least one memory configured to store program code and at least one  configured to access the at least one memory and operate according to the program code the program code comprising motion parameter obtaining code configured to cause the at least one  to acquire an  by using the capturing apparatus and obtain a motion parameter of the terminal the motion parameter comprising at least one of a motion frequency or a motion time and two or more parameters from among an acceleration an angular velocity a motion amplitude the motion frequency and the motion time request transmitting code configured to cause the at least one  to transmit a parameter threshold obtaining request to a data management server the parameter threshold obtaining request comprising configuration information of the terminal parameter threshold receiving code configured to cause the at least one  to receive corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request comparing code configured to cause the at least one  to compare the two or more parameters with the corresponding preset thresholds and control code configured to cause the at least one  not to perform  processing on the acquired  based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds wherein the motion parameter obtaining code causes the at least one  to acquire the  in real time and obtain the motion parameter of the terminal in real time and in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold obtain the motion parameter of the terminal again and wherein the control code causes the at least one  to in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds perform the  processing on the  acquired at the latest time  the terminal according to   wherein the program code further comprises face instruction receiving code configured to cause the at least one  to receive a face  instruction wherein the motion parameter obtaining code causes the at least one  to control according to the face  instruction the capturing apparatus to turn on and acquire a face  by using the capturing apparatus when the capturing apparatus is turned on and wherein the control code causes the at least one  to skip performing face  on the acquired face  based on the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds  the terminal according to   wherein the request transmitting code causes the at least one  to transmit the parameter threshold obtaining request to the data management server according to a preset time period  the terminal according to   wherein the program code further comprises prompt information generation code configured to cause the at least one  to generate prompt information based on at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold the prompt information being used for prompting the terminal to stop moving  the terminal according to   wherein the motion parameter comprises the motion frequency and the motion time  a non-transitory computer-readable storage medium storing a machine instruction which when executed by one or more s causes the one or more s to perform obtaining an  acquired by a capturing apparatus obtaining a motion parameter of a terminal the terminal comprising the capturing apparatus the motion parameter comprising at least one of a motion frequency or a motion time and two or more parameters from among an acceleration an angular velocity a motion amplitude the motion frequency and the motion time transmitting a parameter threshold obtaining request to a data management server the parameter threshold obtaining request comprising configuration information of the terminal receiving corresponding preset thresholds that correspond to the configuration information in response to the parameter threshold obtaining request comparing the two or more parameters with the corresponding preset thresholds and controlling not to perform  processing on an acquired  based on at least one of the two or more parameters of the motion parameter being greater than a corresponding preset threshold or based on the two or more parameters of the motion parameter being respectively greater than the corresponding preset thresholds wherein the acquiring comprises acquiring the  in real time and the obtaining comprises obtaining the motion parameter of the terminal in real time the  further comprising in response to the at least one of the two or more parameters of the motion parameter being greater than the corresponding preset threshold obtaining the motion parameter of the terminal again and in response to the two or more parameters of the motion parameter obtained at a latest time being less than or equal to the corresponding preset thresholds performing the  processing on the  acquired at the latest time  the non-transitory computer-readable storage medium according to   wherein the acquired  is a face  and the  processing comprises performing face   the non-transitory computer-readable storage medium according to   wherein the obtaining the motion parameter comprises at least one of obtaining the acceleration of the terminal by using an acceleration sensor or obtaining the angular velocity of the terminal by using a gyro sensor  the non-transitory computer-readable storage medium according to   wherein the motion parameter comprises the motion frequency and the motion time a  of processing a drive-through order the  comprising receiving customer information detected through vision  providing product information to a customer based on the customer information and processing a product order of the customer  the  according to   wherein the receiving of customer information comprises at least one of receiving customer information associated with vehicle information detected through vehicle  or receiving customer information associated with identification information detected through face   the  according to   further comprising determining whether the customer is a pre-order customer based on the customer information wherein when the customer is determined to be a pre-order customer the providing of product information based on the customer information comprises providing pre-order information using at least one of audio or video and the processing of the product order of the customer comprises providing information for promptly guiding a vehicle to a pickup stand using at least one of audio or video and providing information that an additional order is available  the  according to   wherein the product information based on the customer information comprises a most recently ordered product component and a most frequently ordered product component in an order history of the customer information  the  according to   wherein the receiving of customer information comprises receiving information about an age and gender of a passenger detected through face  and the providing of product information to a customer based on the customer information comprises providing recommended menu information differentiated according to the age and gender  the  according to   wherein the processing of a product order of the customer comprises determining a product component in a past order history or a component modified from the product component as a product order  the  according to   wherein the processing of a product order of the customer comprises paying a product price according to biometrics-based authentication through a communication system of a vehicle or a mobile terminal  the  according to   wherein the processing of a product order of the customer comprises issuing a payment number for a divided payment and performing the divided payments according to payment requests of a  of mobile terminals to which the payment numbers are inputted  the  according to   wherein the processing of a product order of the customer further comprises accumulating mileage in an account corresponding to the mobile terminal undergoing a payment  the  according to   wherein the processing of a product order of the customer further comprises suggesting a takeout packaging  according to a temperature of a product an atmospheric temperature weather and a vehicle type  an apparatus configured to process a drive-through order the apparatus comprising a transceiver configured to receive customer information detected through vision  a digital signage configured to provide product information to a customer based on the customer information and a  configured to process a product order of the customer  the apparatus according to   wherein the transceiver receives at least one of customer information associated with vehicle information detected through vehicle  or customer information associated with identification information detected through face   the apparatus according to   wherein the  is configured to determine whether the customer is a pre-order customer based on the customer information and when the customer is determined to be a pre-order customer perform a control operation to provide pre-order information and control the digital signage to output information for promptly guiding a vehicle to a pickup stand and provide information that an additional order is available  the apparatus according to   wherein the product information based on the customer information comprises a most recently ordered product component and a most frequently ordered product component in an order history of the customer information  the apparatus according to   wherein the transceiver is configured to receive information about an age and gender of a passenger detected through face  and the  is configured to control the digital signage to provide recommended menu information differentiated according to the age and gender  the apparatus according to   wherein the  is configured to determine a product component in a past order history or a component modified from the product component as the product order  the apparatus according to   wherein the  is configured to pay a product price according to biometrics-based authentication through a communication system of a vehicle or a mobile terminal  the apparatus according to   wherein the  is configured to issue a payment number for a divided payment and perform the divided payments according to requests of a  of mobile terminals to which the payment numbers are inputted  the apparatus according to   wherein the  is configured to accumulate mileage in an account corresponding to the mobile terminal undergoing a payment  the apparatus according to   wherein the  is configured to control the digital signage to suggest a takeout packaging  according to a temperature of a product an atmospheric temperature weather and a vehicle type an  information processing  performed at a computing device having one or more s and memory storing a  of programs to be executed by the one or more s the  comprising identifying using face  one or more  each face corresponding to a respective person captured in a first  for each identified face extracting a set of profile parameters of a corresponding person in the first  and selecting from a  of  tiles a first  tile that matches the face of the corresponding person in the first  in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first  tile generating a second  by covering the  of respective persons in the first  with their corresponding first  tiles and sharing the first  and the second  in a predefined order via a group chat session  the  of   wherein the first  and the second  are displayed in the group chat session one  at a time such that one of the two s is replaced by the other of the two s periodically  the  of   wherein extracting a set of profile parameters of a corresponding person in the first  includes determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model wherein the first machine learning model is trained with the facial s and corresponding descriptive labels  the  of   wherein extracting a set of profile parameters of a corresponding person in the first  includes determining an identity of the corresponding person based on the identified face of the corresponding person locating respective profile information of the first person based on the determined identity of the corresponding person and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person  the  of   wherein at least a first one of the first  tiles is a dynamic  tile and at least a second one of the first  tiles is a static  tile  the  of   including receiving a  of  comments from different s of the group chat session each  comment including a descriptive term for a respective person identified in the first  choosing a descriptive label for the respective person according to the  of  comments and updating the second  by adding the descriptive label adjacent to the first  tile of the respective person  a computing device for  information processing comprising one or more s and memory storing instructions which when executed by the one or more s cause the s to perform a  of operations comprising identifying using face  one or more  each face corresponding to a respective person captured in a first  for each identified face extracting a set of profile parameters of a corresponding person in the first  and selecting from a  of  tiles a first  tile that matches the face of the corresponding person in the first  in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first  tile generating a second  by covering the  of respective persons in the first  with their corresponding first  tiles and sharing the first  and the second  in a predefined order via a group chat session  the computing device of   wherein the first  and the second  are displayed in the group chat session one  at a time such that one of the two s is replaced by the other of the two s periodically  the computing device of   wherein extracting a set of profile parameters of a corresponding person in the first  includes determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model wherein the first machine learning model is trained with the facial s and corresponding descriptive labels  the computing device of   wherein extracting a set of profile parameters of a corresponding person in the first  includes determining an identity of the corresponding person based on the identified face of the corresponding person locating respective profile information of the first person based on the determined identity of the corresponding person and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person  the computing device of   wherein at least a first one of the first  tiles is a dynamic  tile and at least a second one of the first  tiles is a static  tile  the computing device of   wherein the  of operations further include receiving a  of  comments from different s of the group chat session each  comment including a descriptive term for a respective person identified in the first  choosing a descriptive label for the respective person according to the  of  comments and updating the second  by adding the descriptive label adjacent to the first  tile of the respective person  a non-transitory computer-readable storage medium storing instructions which when executed by a computing device having one or more s cause the computing device to perform a  of operations comprising identifying using face  one or more  each face corresponding to a respective person captured in a first  for each identified face extracting a set of profile parameters of a corresponding person in the first  and selecting from a  of  tiles a first  tile that matches the face of the corresponding person in the first  in accordance with a predefined correspondence between the set of profile parameters of the corresponding person and a set of pre-stored description parameters of the first  tile generating a second  by covering the  of respective persons in the first  with their corresponding first  tiles and sharing the first  and the second  in a predefined order via a group chat session  the non-transitory computer-readable storage medium of   wherein the first  and the second  are displayed in the group chat session one  at a time such that one of the two s is replaced by the other of the two s periodically  the non-transitory computer-readable storage medium of   wherein extracting a set of profile parameters of a corresponding person in the first  includes determining one or more descriptive labels corresponding to the identified face of the corresponding person using a first machine learning model wherein the first machine learning model is trained with the facial s and corresponding descriptive labels  the non-transitory computer-readable storage medium of   wherein extracting a set of profile parameters of a corresponding person in the first  includes determining an identity of the corresponding person based on the identified face of the corresponding person locating respective profile information of the first person based on the determined identity of the corresponding person and using one or more characteristics in the respective profile information of the first person as the set of profile parameters corresponding to the identified face of the corresponding person  the non-transitory computer-readable storage medium of   wherein at least a first one of the first  tiles is a dynamic  tile and at least a second one of the first  tiles is a static  tile  the non-transitory computer-readable storage medium of   wherein the  of operations further include receiving a  of  comments from different s of the group chat session each  comment including a descriptive term for a respective person identified in the first  choosing a descriptive label for the respective person according to the  of  comments and updating the second  by adding the descriptive label adjacent to the first  tile of the respective person a  comprising by a computing system determining that a performance metric of an eye  system is below a first performance threshold wherein the eye  system is associated with a head-mounted display worn by a  based on the determination of the performance metric of the eye  system being below the first performance threshold the computer system performing receiving one or more first inputs associated with a body of the  estimating a region that the  is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the  determining a vergence distance of the  based at least on the one or more first inputs associated with the body of the  the estimated region that the  is looking at and locations of one or more objects in a scene displayed by the head-mounted display and adjusting one or more configurations of the head-mounted display based on the determined vergence distance of the   the  of   wherein the one or more configurations of the head-mounted display comprise one or more of a rendering  a position of a display screen or a position of an optics block  the  of   further comprising determining that the performance metric of the eye  system is above a second performance threshold receiving eye  data from the eye  system and determining the vergence distance of the  based on the eye  data and the one or more first inputs associated with the body of the   the  of   further comprising receiving one or more second inputs associated with one or more displaying elements in the scene displayed by the head-mounted display and determining the vergence distance of the  based at least on the eye  data the one or more first inputs associated with the body of the  and the one or more second inputs associated with the one or more displaying elements of the scene  the  of   further comprising feeding the one or more first inputs associated with the body of the  to a fusion algorithm wherein the fusion algorithm assigns a weight score to each input of the one or more first inputs determining the vergence distance of the  using the fusion algorithm based on the one or more first inputs associated with the body of the  and determining a z-depth of a display screen and a confidence score based on the one or more first inputs associated with the body of the   the  of   further comprising comparing the confidence score to a confidence level threshold in response to a determination that the confidence score is below the confidence level threshold feeding the one or more second inputs associated with the one or more displaying elements of the scene to the fusion algorithm and determining the z-depth of the display screen using the fusion algorithm based on the one or more first inputs associated with the body of the  and the one or more second inputs associated with the one or more displaying elements of the scene  the  of   further comparing comparing by the fusion algorithm confidence scores associated with a  of combinations of inputs and determining by the fusion algorithm the z-depth of the display screen based on a combination of inputs associated with a highest confidence score  the  of   wherein the z-depth and the confidence score are determined by the fusion algorithm using a piecewise comparison of the one or more first inputs and the one or more second inputs  the  of   wherein the z-depth and the confidence score are determined based on a correlation between two or more inputs of the one or more first inputs and the one or more second inputs  the  of   wherein the fusion algorithm comprises a machine learning ml algorithm and wherein the machine learning ml algorithm determines a combination of first inputs fed to the fusion algorithm  the  of   wherein the one or more first inputs associated with the body of the  comprise one or more of a hand position a hand direction a hand movement a hand gesture a head position a head direction a head movement a head gesture a gaze angle rea body gesture a body posture a body movement a behavior of the  or a weighted combination of one or more related parameters  the  of   wherein the one or more first inputs associated with the body of the  are received from one or more of a controller a sensor a camera a microphone an accelerometer a headset worn by the  or a mobile device  the  of   wherein the one or more second inputs associated with the one or more displaying elements comprise one or more of a z-buffer value associated with a displaying element a displaying element marked by a developer an  analysis result a shape of a displaying element a face  result an object  result a person identified in a displaying content an object identified in a displaying content a correlation of two or more displaying elements or a weighted combination of the one or more second inputs  the  of   further comprising determining that the performance metric of the eye  system is below a second performance threshold receiving one or more second inputs associated with one or more displaying elements in the scene displayed by the head-mounted display and determining the vergence distance of the  based at least on the one or more first inputs associated with the body of the  and the one or more second inputs associated with the one or more displaying elements  the  of   wherein determining that the performance metric of the eye  system is below the second performance threshold comprises determining that the eye  system does not exist or fails to provide eye  data  the  of   wherein the performance metric of the eye  system comprises one or more of an accuracy of a parameter from the eye  system a precision of a parameter from the eye  system a value of a parameter from the eye  system a detectability of a pupil a metric based on one or more parameters associated with the  a parameter change a parameter changing trend a data availability or a weighted combination of one or more performance related parameters  the  of   wherein the one or more parameters associated with the  comprise one or more of an eye distance of the  a pupil position a pupil status a correlation of two pupils of the  a head size of the  a position of a headset worn by the  an angle of the headset worn by the  a direction of the headset worn by the  an alignment of the eyes of the  or a weighted combination of one or more related parameters associated with the   the  of   wherein the first performance threshold comprises one or more of a pre-determined value a pre-determined range a state of a data a changing speed of a data or a trend of a data change  one or more non-transitory computer-readable storage media embodying software that is operable when executed by a computing system to determine that a performance metric of an eye  system is below a first performance threshold wherein the eye  system is associated with a head-mounted display worn by a  based on the determination of the performance metric of the eye  system being below the first performance threshold the media embodying software operable when executed by the computing system to receive one or more first inputs associated with a body of the  estimate a region that the  is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the  determine a vergence distance of the  based at least on the one or more first inputs associated with the body of the  the estimated region that the  is looking at and locations of one or more objects in a scene displayed by the head-mounted display and adjust one or more configurations of the head-mounted display based on the determined vergence distance of the   a system comprising one or more non-transitory computer-readable storage media embodying instructions one or more s coupled to the storage media and operable to execute the instructions to determine that a performance metric of an eye  system is below a first performance threshold wherein the eye  system is associated with a head-mounted display worn by a  based on the determination of the performance metric of the eye  system being below the first performance threshold the system is configured to receive one or more first inputs associated with a body of the  estimate a region that the  is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the  determine a vergence distance of the  based at least on the one or more first inputs associated with the body of the  the estimated region that the  is looking at and locations of one or more objects in a scene displayed by the head-mounted display and adjust one or more configurations of the head-mounted display based on the determined vergence distance of the  a computer-implemented  for -based self-guided object detection comprising receiving by a  device a set of s each of the s having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data training by the  device a grid-based object detector using the grid level label data determining by the  device a respective bounding box for the respective object in each of the s by applying local segmentation to each of the s and training by the  device a region-based convolutional neural network rcnn for joint object localization and object classification using the respective bounding box for the respective object in each of the s as an input to the rcnn  the computer-implemented  of   further comprising performing an action responsive to the object localization and object classification for a respective new object in a new  to which the rcnn is applied  the computer-implemented  of   wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object  the computer-implemented  of   wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of s  the computer-implemented  of   wherein the local segmentation is applied to each of the s to segment a respective target region therein  the computer-implemented  of   wherein the region-based convolutional neural network rcnn forms a model during an object training stage that is to detect objects in new s during an inference stage  the computer-implemented  of   wherein the  is performed by a system selected from the group consisting of a surveillance system a face detection system a face  system a cancer detection system an object  system and an advanced driver-assistance system  a computer program product for -based self-guided object detection the computer program product comprising a non-transitory computer readable storage medium having program instructions embodied therewith the program instructions executable by a computer to cause the computer to perform a  comprising receiving by a  device a set of s each of the s having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data training by the  device a grid-based object detector using the grid level label data determining by the  device a respective bounding box for the respective object in each of the s by applying local segmentation to each of the s and training by the  device a region-based convolutional neural network rcnn for joint object localization and object classification using the respective bounding box for the respective object in each of the s as an input to the rcnn  the computer program product of   wherein the  further comprises performing an action responsive to the object localization and object classification for a respective new object in a new  to which the rcnn is applied  the computer program product of   wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object  the computer program product of   wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of s  the computer program product of   wherein the local segmentation is applied to each of the s to segment a respective target region therein  the computer program product of   wherein the region-based convolutional neural network rcnn forms a model during an object training stage that is to detect objects in new s during an inference stage  the computer program product of   wherein the  is performed by a system selected from the group consisting of a surveillance system a face detection system a face  system a cancer detection system an object  system and an advanced driver-assistance system  a computer processing system for -based self-guided object detection comprising a memory device for storing program code and a  device for running the program code to receive a set of s each of the s having a respective grid thereon that is labeled regarding a respective object to be detected using grid level label data train a grid-based object detector using the grid level label data determine a respective bounding box for the respective object in each of the s by applying local segmentation to each of the s and train a region-based convolutional neural network rcnn for joint object localization and object classification using the respective bounding box for the respective object in each of the s as an input to the rcnn  the computer processing system of   wherein the  device further runs the program code to perform an action responsive to the object localization and object classification for a respective new object in a new  to which the rcnn is applied  the computer processing system of   wherein the action comprises autonomously controlling a motor vehicle to avoid a collision with the new object responsive to the object localization and object classification for the respective new object  the computer processing system of   wherein the local segmentation is performed using a self-similarity search and template matching to provide the respective bounding box around the respective object in the set of s  the computer processing system of   wherein the region-based convolutional neural network rcnn forms a model during an object training stage that is to detect objects in new s during an inference stage  the computer processing system of   wherein the computer processing system is comprised in a system selected from the group consisting of a surveillance system a face detection system a face  system a cancer detection system an object  system and an advanced driver-assistance system a  of scalable parallel cloud-based face  utilizing a database of normalized stored s comprising capturing an  using a camera detecting a face in the captured  normalizing the detected facial  to match the normalized stored s identifying facial features in the normalized detected facial  generating a  of facial metrics from the facial features calculating euclidean distances between the facial metrics of the normalized detected facial  with corresponding facial metrics of each of the stored s comparing each euclidean distance against a predetermined threshold responsive to the euclidean distance comparison producing a reduced candidate list of best possible  matches from the normalized stored s comparing in parallel the normalized detected facial  with each of the normalized stored s of the reduced candidate list utilizing a  of face  algorithms where each  of a parallel processing system uses a different face  algorithm responsive to the comparison producing best match results from each parallel subset of the reduced candidate list and selecting a final match from the best match results using a deep learning neural network face  algorithm trained on outputs of individual face  algorithms  the  of scalable parallel cloud-based face  of   wherein detecting a face in the captured  comprises utilizing opencv to detect a face in the captured  extracting the location of the eyes and a tip of the nose in the face determining a distance between the eyes cropping the face from the captured  where the width and the height of a cropped face  is a function of the distance between the eyes and rotating the face by an angle of rotation that is a function of the distance between the eyes  the  of scalable parallel cloud-based face  of   wherein the width of the cropped face  is  times the distance between the eyes the height of the cropped face  is  times the distance between the eyes and the angle of rotation is an angle formed by a straight line joining the eyes and an x-axis of the face  the  of scalable parallel cloud-based face  of   wherein rotating the face comprises rotating the face to provide a frontal face pattern  the  of scalable parallel cloud-based face  of   further comprising the step of proportionally rescaling the cropped and rotated   the  of scalable parallel cloud-based face  of   where the proportional rescaling yields a cropped and rotated  with a size of = pixels  the  of scalable parallel cloud-based face  of   wherein the facial features identified in the normalized detected facial  comprise a pair of eyes a tip of a nose a mouth a center of the mouth and a chin area comprising a bottom a top left landmark and a top right landmark  the  of scalable parallel cloud-based face  of   wherein generating a  of facial metrics comprises calculating a distance between the pair of eyes a distance between the eyes and the tip of the nose a distance equal to the width of the mouth a distance between the tip of the nose and the center of mouth a distance between the bottom of chin and the center of mouth a distance between the top left landmark on the chin and the tip of the nose and a distance between the top right landmark on the chin and the tip of the nose  the  of scalable parallel cloud-based face  of   wherein performing a euclidean distance match further comprises partitioning the normalized stored s into a  of substantially equal subsets performing a euclidean distance match between the facial metrics of the normalized detected facial  and corresponding facial metrics of each of the stored s of the subsets of the normalized stored s with a separate  of a parallel processing system to generate a euclidean distance for each stored  of the subset comparing each euclidean distance against a predetermined threshold with the separate s responsive to the euclidean distance comparison producing a reduced candidate list of best possible  matches from the normalized stored s of each subset and combining the reduced candidate lists from each subset to produce a single reduced candidate list  the  of scalable parallel cloud-based face  of   wherein the  of face  algorithms utilized in comparing in parallel the normalized detected facial  with each of the normalized stored s of the reduced candidate list consists of face  algorithms selected from a group consisting of principle component analysis pca-based algorithms linear discriminant analysis lda algorithms independent component analysis ica algorithms kernel-based algorithms feature-based techniques algorithms based on neural networks algorithms based on transforms and model-based face  algorithms  the  of scalable parallel cloud-based face  of   wherein the pca-based algorithms include eigen for face detection and the lda algorithms include the fisher  of face   the  of scalable parallel cloud-based face  of   wherein comparing in parallel the captured  with each of the normalized stored s of the reduced candidate list further comprises partitioning the reduced candidate list into a  of substantially equal subsets processing each subset in a different  of the parallel processing system uses a unique face  algorithm to produce the best match results and using a reduce function of a mapreduce program to combine the best match results from each of the subsets to produce a single set of the best match results  the  of scalable parallel cloud-based face  of   wherein partitioning the reduced candidate list comprises selecting the s comprising each subset by optimizing the variance between of each of the s according to the following equation where m and n are the number of rows and columns of the face vector  n is the number of groups and σij is the standard deviation of  dimension i in the group j of the face  vector  the  of scalable parallel cloud-based face  of   wherein selecting the s comprising each subset by optimizing the variance between each of the s according to the following equation dμi μj is the euclidean distance between the mean of the group i and the mean of group j i is the face  vector and l is the number of group levels  the  of scalable parallel cloud-based face  of   where selecting a final match from the best match results utilizing a deep learning neural network face  algorithm comprises utilizing either an adaboost machine-learning algorithm or a neural networks machine-learning model  the  of scalable parallel cloud-based face  of   where normalizing the detected facial  to match the normalized stored s includes normalizing the detected facial  to the same size  and illumination of the normalized stored s  a non-transitory computer-readable medium containing executable program instructions for causing a computer to perform a  of face  the  comprising detecting a face in an  captured by a camera normalizing the detected facial  to match the normalized stored s identifying facial features in the normalized detected facial  generating a  of facial metrics from the facial features calculating euclidean distances between the facial metrics of the normalized detected facial  with corresponding facial metrics of each of the stored s comparing each euclidean distance against a predetermined threshold responsive to the euclidean distance comparison producing a reduced candidate list of best possible  matches from the normalized stored s comparing in parallel the captured  with each of the normalized stored s of the reduced candidate list utilizing a  of face  algorithms where each  of a parallel processing system uses a different face  algorithm responsive to the comparison producing best match results from each parallel subset of the reduced candidate list and selecting a final match from the best match results using a deep learning neural network face  algorithm trained on outputs of individual face  algorithms  the non-transitory computer-readable medium containing executable program instructions of   wherein the  of face  algorithms utilized in comparing in parallel the normalized detected facial  with each of the normalized stored s of the reduced candidate list consists of face  algorithms selected from a group consisting of principle component analysis pca-based algorithms linear discriminant analysis lda algorithms independent component analysis ica algorithms kernel-based algorithms feature-based techniques algorithms based on neural networks algorithms based on transforms and model-based face  algorithms  the non-transitory computer-readable medium containing executable program instructions of   wherein the pca-based algorithms include eigen for face detection and the lda algorithms include the fisher  of face   the non-transitory computer-readable medium containing executable program instructions of   where selecting a final match from the best match results utilizing a deep learning neural network face  algorithm comprises utilizing either an adaboost machine-learning algorithm or a neural networks machine-learning model an imaging device comprising a condensing lens an  sensor configured to detect light passing through the condensing lens and comprising a pixel matrix wherein the pixel matrix comprises a  of phase detection pixel pairs and a  of regular pixels and a  configured to turn on the phase detection pixel pairs for autofocusing and output autofocused pixel data after completing the autofocusing divide the autofocused pixel data into a first subframe and a second subframe calculate  features of at least one of the first subframe and the second subframe wherein the  features comprise module widths of a finder pattern and the finder pattern has a predetermined ratio a harr-like feature or a gabor feature and determine an operating resolution of the regular pixels according to the  features calculated from at least one of the first subframe and the second subframe divided from the autofocused pixel data  the imaging device as ed in   wherein each of the phase detection pixel pairs comprises a first pixel and a second pixel a cover layer covering upon a first region of the first pixel and upon a second region of the second pixel wherein the first region and the second region are mirror symmetrical to each other and a microlens aligned with at least one of the first pixel and the second pixel  the imaging device as ed in   wherein the first region and the second region are % to % of an area of a single pixel  the imaging device as ed in   wherein the  is configured to perform the autofocusing using a dual pixel autofocus technique according to pixel data of the phase detection pixel pairs before completing the autofocusing  the imaging device as ed in   wherein the  is configured to divide pixel data of the phase detection pixel pairs into a third subframe and a fourth subframe before completing the autofocusing and perform the autofocusing according to the third subframe and the fourth subframe  the imaging device as ed in   wherein the  is further configured to calibrate brightness of the third subframe and the fourth subframe to be identical using a shading algorithm  the imaging device as ed in   wherein the operating resolution is selected as a first resolution smaller than a number of the regular pixels or as a second resolution larger than the first resolution  the imaging device as ed in   wherein the regular pixels are turned off in the autofocusing  the imaging device as ed in   wherein a number of the phase detection pixel pairs is smaller than that of the regular pixels  an imaging device comprising a condensing lens an  sensor configured to detect light passing through the condensing lens and comprising a pixel matrix wherein the pixel matrix comprises a  of phase detection pixel pairs and a  of regular pixels and a  configured to turn on the phase detection pixel pairs for autofocusing and output autofocused pixel data after completing the autofocusing divide the autofocused pixel data into a first subframe and a second subframe calculate  features of at least one of the first subframe and the second subframe wherein the  features comprise module widths of a finder pattern and the finder pattern has a predetermined ratio a harr-like feature or a gabor feature and select an  decoding or an   using pixel data of the regular pixels according to the  features calculated from at least one of the first subframe and the second subframe divided from the autofocused pixel data  the imaging device as ed in   wherein each of the phase detection pixel pairs comprises a first pixel and a second pixel a cover layer covering upon a first region of the first pixel and upon a second region of the second pixel wherein the first region and the second region are mirror symmetrical to each other and a microlens aligned with at least one of the first pixel and the second pixel  the imaging device as ed in   wherein the  is configured to perform the autofocusing using a dual pixel autofocus technique according to pixel data of the phase detection pixel pairs before completing the autofocusing  the imaging device as ed in   wherein the  is configured to divide the pixel data of the phase detection pixel pairs into a third subframe and a fourth subframe before completing the autofocusing calibrate brightness of the third subframe and the fourth subframe to be identical using a shading algorithm and perform the autofocusing according to the third subframe and the fourth subframe  the imaging device as ed in   wherein the  is configured to calculate the  features using at least one of a rule based algorithm and a machine learning algorithm  the imaging device as ed in   wherein the  decoding is decoding qr codes and the   is face   an operating  of an imaging device the imaging device comprising a  of phase detection pixel pairs and a  of regular pixels the operating  comprising turning on the phase detection pixel pairs for autofocusing and outputting autofocused  frame after completing the autofocusing dividing the autofocused  frame acquired by the phase detection pixel pairs into a first subframe and a second subframe calculating  features of at least one of the first subframe and the second subframe wherein the  feature comprise module widths of a finder pattern and the finder pattern has a predetermined ratio a harr-like feature or a gabor feature and selectively activating at least a part of the regular pixels according to the  features calculated from at least one of the first subframe and the second subframe divided from the autofocused  frame  the operating  as ed in   wherein the selectively activating comprises activating a first part of the regular pixels to perform an  decoding according to pixel data of the first part of the regular pixels or activating all the regular pixels to perform an   according to pixel data of the all regular pixels  the operating  as ed in   wherein pixel data of the phase detection pixel pairs captured in a same frame with the pixel data of the regular pixels is also used in performing the  decoding and the    the operating  as ed in   wherein the  decoding is decoding qr codes and the   is face   the operating  as ed in   wherein the phase detection pixel pairs are partially covered pixels or have a structure of dual pixel an apparatus comprising a first camera module configured to obtain a first  of an object with a first field of view a second camera module configured to obtain a second  of the object with a second field of view different from the first field of view a first depth map generator configured to generate a first depth map of the first  based on the first  and the second  and a second depth map generator configured to generate a second depth map of the second  based on the first  the second  and the first depth map  the apparatus of   wherein the first field of view is a narrow angle and the second field of view is a wider angle  the apparatus of   wherein the second  is divided into a primary region and a residual region and the second depth map generator comprises a relationship estimating module configured to estimate a relationship between the primary region and the residual region based on the first  and the second  and a depth map estimating module configured to estimate a depth map of the residual region based on the estimated relationship and the first depth map  the apparatus of   wherein at least one of the relationship estimating module and the depth map estimating module performs an estimating operation based on a neural network module  the apparatus of   further comprising a depth map fusion unit configured to generate a third depth map of the second  by performing a fusion operation based on the first depth map and the second depth map  the apparatus of   wherein the depth map fusion unit comprises a tone mapping module configured to generate a tone-mapped second depth map to correspond to the first depth map by performing a bias removing operation on the second depth map and a fusion module configured to generate the third depth map by fusing the tone-mapped second depth map and the first depth map  the apparatus of   wherein the depth map fusion unit further comprises a propagating module configured to generate a propagated first depth map in the second  by iterated propagating of the first depth map based on the first depth map and the second  and the fusion module generates the third depth map by fusing the tone-mapped second depth map and the propagated first depth map  the apparatus of   wherein the depth map fusion unit further comprises a post-processing module configured to perform a post-processing operation on the third depth map generated by the fusion module to provide the post-processed third depth map  the apparatus of   wherein the post-processing module performs the post-processing operation by filtering an interface generated in the third depth map in accordance with fusion of the fusion module  the apparatus of   wherein the post-processing module removes artifacts generated in the third depth map in accordance with fusion of the fusion module  the apparatus of   wherein the first depth map generator analyses a distance relationship between the first  and the second  and generates a first depth map of the first  based on the distance relationship  a  of processing an  of an electronic apparatus the  comprising obtaining a first  of an object using a first camera module obtaining a second  of the object using a second camera module generating a first depth map of the first  based on the first  and the second  estimating a relationship between a primary region of the second  and a residual region of the second  based on the first  and the second  and generating a second depth map of the second  based on the estimated relationship between the primary region and the residual region and the first depth map  the  of   wherein the electronic apparatus comprises a first camera module including a first lens having a first field of view and a second camera module including a second lens having a second field of view wider than the first field of view  the  of   wherein the generating of the second depth map comprises estimating a depth map of the residual region based on the estimated relationship between the primary region and the residual region and the first depth map and generating the second depth map based on a depth map of the residual region and the first depth map  the  of   wherein the estimating of the relationship between a primary region of the second  is performed using a neural network model  the  of   further comprising performing a pre-processing operation on the second depth map and generating a third depth map of the residual  by fusing the second depth map on which the pre-processing operation is performed and the first depth map  the  of   wherein the performing of the pre-processing operation comprises performing a tone mapping operation between a depth map of the primary region and a depth map of the residual region based on the second depth map  an operating  for an electronic apparatus the electronic apparatus including a first camera module providing a first  of an object using a first field of view and a second camera module providing a second  of the object using second field of view wider than the first field of view and a  generating a depth map of the second  based on a primary region of the second  and a residual region of the second  the operating  comprising generating a first depth map of the primary region by estimating a relationship between the first  and the second  estimating a relationship between the primary region and the residual region based on the first  and the second  generating a second depth map of the second  by estimating a depth map of the second region based on the estimated relationship between the primary region and the residual region and generating a depth map of the second  by fusing the first depth map and the second depth map  the operation  of   further comprising executing an application that applies an  effect to the second  based on a depth map of the residual   the operation  of   wherein the application applies at least one  effect of auto-focusing out-focusing forebackground separation face  object detection within a frame and augmented reality to the second  based on a depth map of the second  a payment  based on a face  comprising acquiring first face  information of a target  extracting first characteristic information from the first face  information wherein the first characteristic information includes head posture information of the target  and gaze information of the target  determining whether the target  has a willingness to pay according to the head posture information of the target  and the gaze information of the target  including determining whether an angle of rotation in each preset direction is less than an angle threshold wherein the head posture information includes the angle of rotation in each preset direction determining whether a probability value that a  gazes at a payment screen is greater than a probability threshold wherein the gaze information includes the probability value that a  gazes at a payment screen and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a  gazes at a payment screen is greater than the probability threshold determining that the target  has a willingness to pay and in response to determining that the target  has a willingness to pay completing a payment operation based on the face   the  as ed in   wherein the completing a payment operation based on the face  comprises triggering and performing a payment initiating operation to acquire second face  information based on the face  determining whether second characteristic information extracted from the second face  information indicates that the  has a willingness to pay and in response to determining that the second characteristic information indicates that the  has a willingness to pay triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target   the  as ed in   wherein the determining whether second characteristic information extracted from the second face  information indicates that the  has a willingness to pay comprises determining whether a current  corresponding to the second face  information is consistent with the target  and in response to determining that the current  is consistent with the target  determining whether the target  has a willingness to pay according to the second characteristic information extracted from the second face  information  the  as ed in   wherein the extracting first characteristic information from the first face  information comprises determining the head posture information of the target  using a head posture  model based on the first face  information and determining the gaze information of the target  using a gaze information  model based on characteristics of an eye region in the first face  information  the  as ed in   wherein the head posture  model is obtained through training by acquiring a first sample data set wherein the first sample data set includes a  of pieces of first sample data and each of the  of pieces of first sample data includes a correspondence between a sample face  and head posture information determining mean  data and variance  data of a  of sample face s for each of the  of pieces of first sample data preprocessing the sample face  contained in each of the  of pieces of first sample data based on the mean  data and the variance  data to obtain a preprocessed sample face  setting the preprocessed sample face  and the corresponding head posture information as a first model training sample and performing training using a machine learning  and based on a  of first model training samples to obtain the head posture  model  the  as ed in   wherein the gaze information  model is obtained through training by acquiring a second sample data set wherein the second sample data set includes a  of pieces of second sample data and each of the  of pieces of second sample data includes a correspondence between a sample eye  and gaze information determining mean  data and variance  data of a  of sample eye s for each of the  of pieces of second sample data preprocessing the sample eye  contained in each of the  of pieces of second sample data based on the mean  data and the variance  data to obtain a preprocessed sample eye  setting the preprocessed sample eye  and the corresponding gaze information as a second model training sample and performing training using a machine learning  and based on a  of second model training samples to obtain the gaze information  model  the  as ed in   wherein the angle of rotation in each preset direction comprises a pitch angle a yaw angle and a roll angle wherein the pitch angle refers to an angle of rotation around a x-axis the yaw angle refers to an angle of rotation around a y-axis and the roll angle refers to an angle of rotation around a z-axis  a payment device based on a face  comprising a  and a non-transitory computer-readable storage medium storing instructions executable by the  to cause the device to perform operations comprising acquiring first face  information of a target  extracting first characteristic information from the first face  information wherein the first characteristic information includes head posture information of the target  and gaze information of the target  determining whether the target  has a willingness to pay according to the head posture information of the target  and the gaze information of the target  including determining whether an angle of rotation in each preset direction is less than an angle threshold wherein the head posture information includes the angle of rotation in each preset direction determining whether a probability value that a  gazes at a payment screen is greater than a probability threshold wherein the gaze information includes the probability value that a  gazes at a payment screen and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a  gazes at a payment screen is greater than the probability threshold determining that the target  has a willingness to pay and in response to determining that the target  has a willingness to pay completing a payment operation based on the face   the device as ed in   wherein the completing a payment operation based on the face  comprises triggering and performing a payment initiating operation to acquire second face  information based on the face  determining whether second characteristic information extracted from the second face  information indicates that the  has a willingness to pay and in response to determining that the second characteristic information indicates that the  has a willingness to pay triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target   the device as ed in   wherein the determining whether second characteristic information extracted from the second face  information indicates that the  has a willingness to pay comprises determining whether a current  corresponding to the second face  information is consistent with the target  and in response to determining that the current  is consistent with the target  determining whether the target  has a willingness to pay according to the second characteristic information extracted from the second face  information  the device as ed in   wherein the extracting first characteristic information from the first face  information comprises determining the head posture information of the target  using a head posture  model based on the first face  information and determining the gaze information of the target  using a gaze information  model based on characteristics of an eye region in the first face  information  the device as ed in   wherein the head posture  model is obtained through training by acquiring a first sample data set wherein the first sample data set includes a  of pieces of first sample data and each of the  of pieces of first sample data includes a correspondence between a sample face  and head posture information determining mean  data and variance  data of a  of sample face s for each of the  of pieces of first sample data preprocessing the sample face  contained in each of the  of pieces of first sample data based on the mean  data and the variance  data to obtain a preprocessed sample face  setting the preprocessed sample face  and the corresponding head posture information as a first model training sample and performing training using a machine learning  and based on a  of first model training samples to obtain the head posture  model  the device as ed in   wherein the gaze information  model is obtained through training by acquiring a second sample data set wherein the second sample data set includes a  of pieces of second sample data and each of the  of pieces of second sample data includes a correspondence between a sample eye  and gaze information determining mean  data and variance  data of a  of sample eye s for each of the  of pieces of second sample data preprocessing the sample eye  contained in each of the  of pieces of second sample data based on the mean  data and the variance  data to obtain a preprocessed sample eye  setting the preprocessed sample eye  and the corresponding gaze information as a second model training sample and performing training using a machine learning  and on a  of second model training samples to obtain the gaze information  model  the device as ed in   wherein the angle of rotation in each preset direction comprises a pitch angle a yaw angle and a roll angle wherein the pitch angle refers to an angle of rotation around a x-axis the yaw angle refers to an angle of rotation around a y-axis and the roll angle refers to an angle of rotation around a z-axis  a non-transitory computer-readable storage medium for a payment based on a face  configured with instructions executable by one or more s to cause the one or more s to perform operations comprising acquiring first face  information of a target  extracting first characteristic information from the first face  information wherein the first characteristic information includes head posture information of the target  and gaze information of the target  determining whether the target  has a willingness to pay according to the head posture information of the target  and the gaze information of the target  including determining whether an angle of rotation in each preset direction is less than an angle threshold wherein the head posture information includes the angle of rotation in each preset direction determining whether a probability value that a  gazes at a payment screen is greater than a probability threshold wherein the gaze information includes the probability value that a  gazes at a payment screen and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a  gazes at a payment screen is greater than the probability threshold determining that the target  has a willingness to pay and in response to determining that the target  has a willingness to pay completing a payment operation based on the face   the storage medium as ed in   wherein the completing a payment operation based on the face  comprises triggering and performing a payment initiating operation to acquire second face  information based on the face  determining whether second characteristic information extracted from the second face  information indicates that the  has a willingness to pay and in response to determining that the second characteristic information indicates that the  has a willingness to pay triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target   the storage medium as ed in   wherein the determining whether second characteristic information extracted from the second face  information indicates that the  has a willingness to pay comprises determining whether a current  corresponding to the second face  information is consistent with the target  and in response to determining that the current  is consistent with the target  determining whether the target  has a willingness to pay according to the second characteristic information extracted from the second face  information  the storage medium as ed in   wherein the extracting first characteristic information from the first face  information comprises determining the head posture information of the target  using a head posture  model based on the first face  information and determining the gaze information of the target  using a gaze information  model based on characteristics of an eye region in the first face  information  the storage medium as ed in   wherein the head posture  model is obtained through training by acquiring a first sample data set wherein the first sample data set includes a  of pieces of first sample data and each of the  of pieces of first sample data includes a correspondence between a sample face  and head posture information determining mean  data and variance  data of a  of sample face s for each of the  of pieces of first sample data preprocessing the sample face  contained in each of the  of pieces of first sample data based on the mean  data and the variance  data to obtain a preprocessed sample face  setting the preprocessed sample face  and the corresponding head posture information as a first model training sample and performing training using a machine learning  and based on a  of first model training samples to obtain the head posture  model and wherein the gaze information  model is obtained through training by acquiring a second sample data set wherein the second sample data set includes a  of pieces of second sample data and each of the  of pieces of second sample data includes a correspondence between a sample eye  and gaze information determining mean  data and variance  data of a  of sample eye s for each of the  of pieces of second sample data preprocessing the sample eye  contained in each of the  of pieces of second sample data based on the mean  data and the variance  data to obtain a preprocessed sample eye  setting the preprocessed sample eye  and the corresponding gaze information as a second model training sample and performing training using a machine learning  and based on a  of second model training samples to obtain the gaze information  model  the storage medium as ed in   wherein the angle of rotation in each preset direction comprises a pitch angle a yaw angle and a roll angle wherein the pitch angle refers to an angle of rotation around a x-axis the yaw angle refers to an angle of rotation around a y-axis and the roll angle refers to an angle of rotation around a z-axis a  comprising detecting by a motion detection module a motion by a subject within a predetermined area of view assigning a unique session identification number to the subject detected within a predetermined area of view detecting a facial area of the subject detected within a predetermined area of view generating an  of the facial area of the subject assessing a quality of the  of the facial area of the subject determining an identity of the subject based on the  of the facial area of the subject identifying an intent of the subject and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject  the  of   further comprising determining one or more additional subjects within the predetermined area of view and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view  the  of   wherein the assessing a quality of the  of the facial area of the subject comprises assessing whether the quality of the  of the facial area of the object equates predetermined metric of quality and upon determining that the quality of the  of the facial area of the object is inferior to the predetermined metric of quality discarding the  of the facial area of the subject and generating a second  of the facial area of the subject  the  of   further comprising detecting whether the facial area of the subject is photographic  and upon detecting that the facial area of the subject is a photographic  generating a warning and restrict access to the point of entry  the  of   further comprising conducing an incremental training of the  of the facial area of the subject  the  of   wherein conducing an incremental training of the  of the facial area of the subject comprises capturing a first  of the facial area having facial landmarks converting the first  of the facial area into a first numeric vector capturing a second  of the facial area having facial landmarks converting the second  of the facial area into a second numeric vector calculating a weighted mean of the first numeric vector and the second numeric vector wherein the weighted mean represents a change in a facial area and storing the weighted mean in the database  the  of   wherein determining an identity of the subject based on the  of the facial area of the subject comprises comparing the  of the facial area of the subject with a  of s stored in a database and authenticating the subject  the  of   wherein identifying an intent of the subject comprises upon detecting the facial area in a bounding box commencing authentication of the subject calculating a directional vector of a face of the subject determine an intent of the subject to gain access to the point of entry based on the directional vector of the face of the subject granting the access to the point of entry based on authentication of the subject and based on determining the intent of the subject  a non-transitory computer readable medium having program instructions stored thereon that in response to execution by a computing device cause the computing device to perform operations comprising detecting a motion by a subject within a predetermined area of view assigning a unique session identification number to the subject detected within a predetermined area of view detecting a facial area of the subject detected within a predetermined area of view generating an  of the facial area of the subject assessing a quality of the  of the facial area of the subject determining an identity of the subject based on the  of the facial area of the subject identifying an intent of the subject and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject  the non-transitory computer readable medium of   further comprising determining one or more additional subjects within the predetermined area of view and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view  the non-transitory computer readable medium of   wherein the assessing a quality of the  of the facial area of the subject comprises assessing whether the quality of the  of the facial area of the object equates predetermined metric of quality and upon determining that the quality of the  of the facial area of the object is inferior to the predetermined metric of quality discarding the  of the facial area of the subject and generating a second  of the facial area of the subject  the non-transitory computer readable medium of   further comprising detecting whether the facial area of the subject is photographic  and upon detecting that the facial area of the subject is a photographic  generating a warning and restrict access to the access point  the non-transitory computer readable medium of   further comprising conducing an incremental training of the  of the facial area of the subject  the non-transitory computer readable medium of   wherein conducing an incremental training of the  of the facial area of the subject comprises capturing a first  of the facial area having facial landmarks converting the first  of the facial area into a first numeric vector capturing a second  of the facial area having facial landmarks converting the second  of the facial area into a second numeric vector calculating a weighted mean of the first numeric vector and the second numeric vector wherein the weighted mean represents a change in a facial area and storing the weighted mean in the database  an apparatus for face  comprising a  and a memory to store computer program instructions the computer program instructions when executed on the  cause the  to perform operations comprising detecting a motion by a subject within a predetermined area of view assigning a unique session identification number to the subject detected within a predetermined area of view detecting a facial area of the subject detected within a predetermined area of view generating an  of the facial area of the subject assessing a quality of the  of the facial area of the subject determining an identity of the subject based on the  of the facial area of the subject identifying an intent of the subject and authorizing access to a point of entry based on the determined identity of the subject and based on the intent of the subject  the apparatus of   further comprising determining one or more additional subjects within the predetermined area of view and assigning a unique session identification number to each of the one or more additional subjects detected within a predetermined area of view  the apparatus of   wherein the assessing a quality of the  of the facial area of the subject comprises assessing whether the quality of the  of the facial area of the object equates predetermined metric of quality and upon determining that the quality of the  of the facial area of the object is inferior to the predetermined metric of quality discarding the  of the facial area of the subject and generating a second  of the facial area of the subject  the apparatus of   further comprising detecting whether the facial area of the subject is photographic  and upon detecting that the facial area of the subject is a photographic  generating a warning and restrict access to the access point  the apparatus of   further comprising conducing an incremental training of the  of the facial area of the subject  the apparatus of   wherein conducing an incremental training of the  of the facial area of the subject comprises capturing a first  of the facial area having facial landmarks converting the first  of the facial area into a first numeric vector capturing a second  of the facial area having facial landmarks converting the second  of the facial area into a second numeric vector calculating a weighted mean of the first numeric vector and the second numeric vector wherein the weighted mean represents a change in a facial area and storing the weighted mean in the database a robot comprising a body configured to rotate and to tilt a camera coupled to the body and configured to rotate and tilt according to the rotate and the tilt of the body wherein the camera is configured to acquire a video of a space a face  unit configured to recognize respective  of one or more persons in the video a  unit configured to track motion of each of the recognized  of the one or more persons and a controller configured to calculate a respective size of each of the  of the one or more persons select a first person from among the one or more persons based on the calculated sizes of the  and control at least one of a direction of the rotation of the camera an angle of the tilt of the camera and a focal distance of the camera based on the tracked motion of the recognized face of the first person  the robot of   wherein the controller is configured to control the direction of the rotation of the camera and the angle of the tilt of the camera to achieve an particular  of the camera relative to the face of the first person and control a focal distance of the camera by comparing respective sizes of the face of the first person before and after motion of the first person  the robot of   wherein the particular  occurs when the camera  a general direction of the face of the first person  the robot of   wherein the controller is configured to normalize sizes of the  of the one or more persons based on an interocular distance and select the first person based on the normalized sizes of the  of the one or more persons  the robot of   wherein the controller is configured to select a person having a largest face size from among the one or more persons as the first person  the robot of   further comprising a microphone configured to receive a spoken audio that is present in the space wherein the controller is further configured to select the first person further based on the received spoken audio  the robot of   wherein the controller is further configured to control gain of the microphone by comparing respective sizes of the face of the first person before and after motion of the first person  the robot of   wherein the controller is configured to calculate a position from which the spoken audio is provided and select the first person further based on whether the one or more persons are in the position from which the voice signal is provided  the robot of   wherein the controller is configured to select a second person as the first person from among the one or more persons when the second person is located in the position from which the spoken audio is provided  the robot of   wherein the controller is configured to select a second person having a largest face size as the first person from among the one or more persons when none of the one or more persons is located in the position from which the spoken audio is provided  the robot of   wherein the controller is configured to select a second person having a largest face size as the first person from among the one or more persons when a  of persons from among the one or more persons are located in the position from which the spoken audio is provided  the robot of   further comprising a speaker wherein the controller is configured to control volume of the speaker by comparing respective sizes of the face of the first person before and after motion of the first person  the robot of   wherein the body is further configured to rotate in a lateral direction and to tilt in an vertical direction  an  comprising a camera coupled to the body and configured to rotate and to tilt wherein the camera is configured to acquire a video of a space within which one or more persons are positioned and a  configured to recognize respective  of the one or more persons in the video track motion of each of the recognized  of the one or more persons calculate a respective size of each of the  of the one or more persons select a first person from among the one or more persons based on the calculated sizes of the  and control at least one of a direction of the rotation of the camera an angle of the tilt of the camera and a focal distance of the camera based on the tracked motion of the recognized face of the first person  a  comprising acquiring by a camera a video of a space within which one or more persons are positioned  respective  of the one or more persons in the video  motion of each of the recognized  of the one or more persons calculating a respective size of each of the  of the one or more persons selecting a first person from among the one or more persons based on the calculated sizes of the  and controlling at least one of a direction of rotation of the camera an angle of tilt of the camera and a focal distance of the camera based on the tracked motion of the recognized face of the first person a  of inferring topics from a multimodal file the  comprising receiving a multimodal file extracting a set of entities from the multimodal file linking the set of entities to produce a set of linked entities obtaining reference information for the set of entities based at least on the reference information generating a graph of the set of linked entities the graph comprising nodes and edges based at least on the nodes and edges of the graph determining clusters in the graph based at least on the clusters in the graph identifying topic candidates extracting features from the clusters in the graph based at least on the extracted features selecting at least one topicid from among the topic candidates to represent at least one cluster and indexing the multimodal file with the at least one topicid  the  of   wherein the multimodal file comprises a video portion and an audio portion and wherein extracting a set of entities from the multimodal file comprises detecting objects in the video portion of the multimodal file and detecting text in the audio portion of the multimodal file  the  of   wherein detecting objects comprises performing face   the  of   wherein detecting text comprises performing a speech to text process  the  of   further comprising identifying a language used in the audio portion of the multimodal file and wherein performing a speech to text process comprises performing a speech to text process in the identified language  the  of   further comprising translating the detected text  the  of   further comprising determining significant clusters and insignificant clusters in the determined clusters and wherein extracting features from the clusters in the graph comprises extracting features from the significant clusters in the graph  the  of   wherein extracting features from the clusters in the graph comprises at least one process selected from the list consisting of determining a graph diameter and determining a jaccard coefficient  the  of   wherein selecting at least one topicid to represent at least one cluster comprises based at least on the extracted features mapping topic candidates into a probability interval and based at least on the mapping ranking topic candidates within the at least one cluster and selecting the at least one topicid based at least on the ranking  the  of   further comprising translating the at least one topicid and wherein indexing the multimodal file with the at least one topicid comprises indexing the multimodal file with the at least one translated topicid  a system for inferring topics from a multimodal file the system comprising an entity extraction component comprising an object detection component and a speech to text component operative to extract a set of entities from a multimodal file comprising a video portion and an audio portion an entity linking component operative to link the extracted set of entities to produce a set of linked entities an information retrieval component operative to obtain reference information for the extracted set of entities a graphing and analysis component operative to generate a graph of the set of linked entities the graph comprising nodes and edges based at least on the nodes and edges of the graph determine clusters in the graph based at least on the clusters in the graph identify topic candidates and extract features from the clusters in the graph a topicid selection component operative to rank the topic candidates within at least one cluster and based at least on the ranking select at least one topicid from among the topic candidates to represent at least one cluster and a video indexer operative to index the multimodal file with the at least one topicid  the system of   wherein the object detection component is operative to perform face   the system of   wherein the speech to text component is operative to extract entity information in at least two different languages  one or more computer storage devices having computer-executable instructions stored thereon for inferring topics from a multimodal file which on execution by a computer cause the computer to perform operations comprising receiving a multimodal file comprising a video portion and an audio portion extracting a set of entities from the multimodal file wherein extracting a set of entities from the multimodal file comprises detecting objects in the video portion of the multimodal file with face  detecting text in the audio portion of the multimodal file with a speech to text process and disambiguating among a set of detected entity names linking the set of entities to produce a set of linked entities obtaining reference information for the set of entities based at least on the reference information generating a graph of the set of linked entities the graph comprising nodes and edges based at least on the nodes and edges of the graph determining clusters in the graph determining significant clusters and insignificant clusters in the determined clusters based at least on the significant clusters in the graph identifying topic candidates extracting features from the significant clusters in the graph based at least on the extracted features mapping the topic candidates into a probability interval based at least on the mapping ranking the topic candidates within at least one significant cluster based on the ranking selecting at least one topicid from among the topic candidates to represent the at least one significant cluster and indexing the multimodal file with the at least one topicid  the one or more computer storage devices of   wherein the operations further comprise identifying a language used in the audio portion of the multimodal file and detecting text in the audio portion of the multimodal file with a speech to text process comprises performing a speech to text process in the identified language权利要求 、 一种人脸识别方法其特征在于包括 通过第一摄像头获取第一人脸图像 提取所述第一人脸图像的第一人脸特征 将所述第一人脸特征与预先存储的第二人脸特征进行对比获得参考相似度所述第 二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得所述第二摄像头与所述第 一摄像头属于不同类型的摄像头 根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否对应相同人。 、 根据权利要求 所述的方法其特征在于 所述第一摄像头为热成像摄像头所述第二摄像头为可见光摄像头 或者所述第一摄像头为可见光摄像头所述第一摄像头为热成像摄像头。 、 根据权利要求 或 所述的方法其特征在于所述根据所述参考相似度确定所 述第一人脸特征与所述第二人脸特征是否对应相同人包括 根据所述参考相似度、 参考误报率以及相似度阈值确定所述第一人脸特征与所述第二 人脸特征是否对应相同人其中不同的误报率对应不同的相似度阈值。 、 根据权利要求 或 所述的方法其特征在于所述根据所述参考相似度确定所 述第一人脸特征与所述第二人脸特征是否对应相同人包括 根据所述参考相似度以及阈值信息确定归一化后的参考相似度 根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否对 应相同人。 、 根据权利要求 -任一项所述的方法其特征在于所述提取所述第一人脸图像的 第_人脸特征包括 将所述第一人脸图像输入预先训练完成的神经网络通过所述神经网络输出所述第一 人脸图像的第一人脸特征其中所述神经网络基于第一类型图像样本和第二类型图像样 本训练得到所述第一类型图像样本和所述第二类型图像样本由不同类型的摄像头拍摄得 到且所述第一类型图像样本和所述第二类型图像样本中包括人脸。 、 根据权利要求  所述的方法其特征在于所述神经网络基于所述第一类型图像 样本、 所述第二类型图像样本和混合类型图像样本训练得到所述混合类型图像样本由所 述第一类型图像样本和所述第二类型图像样本配对而得。 、 根据权利要求 -任一项所述的方法其特征在于所述第一摄像头包括车载摄像 头所述通过第一摄像头获取第一人脸图像包括 通过所述车载摄像头获取所述第一人脸图像所述第一人脸图像包括车辆的用车人的 人脸图像。 、 根据权利要求  所述的方法其特征在于所述用车人包括驾驶所述车辆的人、 乘坐所述车辆的人、 对所述车辆进行修理的人、 给所述车辆加油的人以及控制所述车辆的 人中的一项或多项。 、 根据权利要求  所述的方法其特征在于所述用车人包括驾驶所述车辆的人 所述通过所述车载摄像头获取所述第一人脸图像包括 在接收到触发指令的情况下通过所述车载摄像头获取所述第一人脸图像 或者在所述车辆运行时通过所述车载摄像头获取所述第一人脸图像 或者在所述车辆的运行速度达到参考速度的情况下通过所述车载摄像头获取所述 第一人脸图像。 、 根据权利要求 -任一项所述的方法其特征在于所述第二人脸图像为对所述 用车人进行人脸注册的图像所述将所述第一人脸特征与预先存储的第二人脸特征进行对 比之前所述方法还包括 通过所述第二摄像头获取所述第二人脸图像 提取所述第二人脸图像的第二人脸特征 保存所述第二人脸图像的第二人脸特征。 、 一种神经网络训练方法其特征在于包括 获取第一类型图像样本和第二类型图像样本所述第一类型图像样本和所述第二类型 图像样本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述第二类型图像样 本中包括人脸 根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。 、 根据权利要求 所述的方法其特征在于所述根据所述第一类型图像样本和所 述第二类型图像样本训练神经网络包括 将所述第一类型图像样本和所述第二类型图像样本配对得到所述第一类型图像样本 和所述第二类型图像样本的混合类型图像样本 根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类型图像样本训练 所述神经网络。 、 根据权利要求  所述的方法其特征在于所述根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类型图像样本训练所述神经网络包括 通过所述神经网络获取所述第一类型图像样本的人脸预测结果、 所述第二类型图像样 本的人脸预测结果和所述混合类型图像样本的人脸预测结果 根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、 所述第二类型图 像样本的人脸预测结果和人脸标注结果之间的差异、 以及所述混合类型图像样本的人脸预 测结果和人脸标注结果的差异训练所述神经网络。 、 根据权利要求  所述的方法其特征在于所述神经网络中包括第一分类器、 第二分类器和混合分类器所述通过所述神经网络获取所述第一类型图像样本的人脸预测 结果、 所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果 包括 将所述第一类型图像样本的人脸特征输入至所述第一分类器中得到所述第一类型图 像样本的人脸预测结果 将所述第二类型图像样本的人脸特征输入至所述第二分类器中得到所述第二类型图 像样本的人脸预测结果 将所述混合类型图像样本的人脸特征输入至所述混合分类器中得到所述混合类型图 像样本的人脸预测结果。 、 根据权利要求 所述的方法其特征在于所述方法还包括 在训练完成的所述神经网络中去除所述第一分类器、 所述第二分类器和所述混合分类 器得到用于进行人脸识别的神经网络。 、 一种人脸识别装置其特征在于包括 第一获取单元用于通过第一摄像头获取第一人脸图像 第一提取单元用于提取所述第一人脸图像的第一人脸特征 对比单元用于将所述第一人脸特征与预先存储的第二人脸特征进行对比获得参考 相似度所述第二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得所述第二 摄像头与所述第一摄像头属于不同类型的摄像头 确定单元用于根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否 对应相同人。 、 根据权利要求 所述的装置其特征在于 所述第一摄像头为热成像摄像头所述第二摄像头为可见光摄像头 或者所述第一摄像头为可见光摄像头所述第一摄像头为热成像摄像头。 、 根据权利要求 或 所述的装置其特征在于 所述确定单元具体用于根据所述参考相似度、 参考误报率以及相似度阈值确定所述 第一人脸特征与所述第二人脸特征是否对应相同人其中不同的误报率对应不同的相似 度阈值。 、 根据权利要求 或 所述的装置其特征在于 所述确定单元具体用于根据所述参考相似度以及阈值信息确定归一化后的参考相似 度以及根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否 对应相同人。 、 根据权利要求 -任_项所述的装置其特征在于 所述第一提取单元具体用于将所述第一人脸图像输入预先训练完成的神经网络通 过所述神经网络输出所述第一人脸图像的第一人脸特征其中所述神经网络基于第一类 型图像样本和第二类型图像样本训练得到所述第一类型图像样本和所述第二类型图像样 本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述第二类型图像样本中包 括人脸。 、 根据权利要求  所述的装置其特征在于所述神经网络基于所述第一类型图 像样本、 所述第二类型图像样本和混合类型图像样本训练得到所述混合类型图像样本由 所述第一类型图像样本和所述第二类型图像样本配对而得。 、 根据权利要求 -任一项所述的装置其特征在于所述第一摄像头包括车载 摄像头 所述第一获取单元具体用于通过所述车载摄像头获取所述第一人脸图像所述第一 人脸图像包括车辆的用车人的人脸图像。 、 根据权利要求 所述的装置其特征在于所述用车人包括驾驶所述车辆的人、 乘坐所述车辆的人、 对所述车辆进行修理的人、 给所述车辆加油的人以及控制所述车辆的 人中的一项或多项。 、 根据权利要求 所述的装置其特征在于所述用车人包括驾驶所述车辆的人 所述第一获取单元具体用于在接收到触发指令的情况下通过所述车载摄像头获取所述 第一人脸图像 或者所述第一获取单元具体用于在所述车辆运行时通过所述车载摄像头获取所 述第 _人脸图像 或者所述第一获取单元具体用于在所述车辆的运行速度达到参考速度的情况下 通过所述车载摄像头获取所述第一人脸图像。 、 根据权利要求 -任一项所述的装置其特征在于所述第二人脸图像为对所 述用车人进行人脸注册的图像所述装置还包括 第二获取单元用于通过所述第二摄像头获取所述第二人脸图像 第二提取单元用于提取所述第二人脸图像的第二人脸特征 保存单元用于保存所述第二人脸图像的第二人脸特征。 、 一种神经网络训练装置其特征在于包括 获取单元用于获取第一类型图像样本和第二类型图像样本所述第一类型图像样本 和所述第二类型图像样本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述 第二类型图像样本中包括人脸 训练单元用于根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。 、 根据权利要求 所述的装置其特征在于所述训练单元包括 配对子单元用于将所述第一类型图像样本和所述第二类型图像样本配对得到所述 第一类型图像样本和所述第二类型图像样本的混合类型图像样本 训练子单元用于根据所述第一类型图像样本、 所述第二类型图像样本和所述混合类 型图像样本训练所述神经网络。 、 根据权利要求 所述的装置其特征在于 所述训练子单元具体用于通过所述神经网络获取所述第一类型图像样本的人脸预测 结果、 所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果 以及根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、 所述第二类型图 像样本的人脸预测结果和人脸标注结果之间的差异、 以及所述混合类型图像样本的人脸预 测结果和人脸标注结果的差异训练所述神经网络。 、 根据权利要求  所述的装置其特征在于所述神经网络中包括第一分类器、 第二分类器和混合分类器 所述训练子单元具体用于将所述第一类型图像样本的人脸特征输入至所述第一分类 器中得到所述第一类型图像样本的人脸预测结果以及将所述第二类型图像样本的人脸 特征输入至所述第二分类器中得到所述第二类型图像样本的人脸预测结果以及将所述 混合类型图像样本的人脸特征输入至所述混合分类器中得到所述混合类型图像样本的人 脸预测结果。 、 根据权利要求 所述的装置其特征在于所述装置还包括 神经网络应用单元用于在训练完成的所述神经网络中去除所述第一分类器、 所述第 二分类器和所述混合分类器得到用于进行人脸识别的神经网络。 、 一种电子设备其特征在于包括处理器和存储器所述处理器和所述存储器耦 合其中所述存储器用于存储程序指令所述程序指令被所述处理器执行时使所述处 理器执行权利要求 -任一项所述的方法和或使所述处理器执行权利要求 -任一 项所述的方法。 、 一种计算机可读存储介质其特征在于所述计算机可读存储介质中存储有计算 机程序所述计算机程序包括程序指令所述程序指令当被处理器执行时使所述处理器 执行权利要求 -任一项所述的方法和或使所述处理器执行权利要求 -任一项所 述的方法。 a system for alerting on vision impairment said system comprising a processing unit configured and operable for receiving scene data being indicative of a scene of at least one consumer in an environment identifying in the scene data a certain consumer identifying an event being indicative of a behavioral compensation for vision impairment and upon identification of such an event sending a notification relating to the vision impairment  the system of   further comprising at least one sensing unit configured and operable for detecting the scene data  the system of   wherein said at least one sensing unit comprises at least one of at least one imaging unit configured and operable for capturing at least one  of at least a portion of a consumer\\'s body at least one motion detector configured and operable for detecting consumer data being indicative of a motion of a consumer or at least one eye tracker configured and operable for  eye motion of a consumer  the system of   wherein the at least one imaging unit comprises a  of cameras placed at different heights  the system of any one of s  to  wherein said sensing unit is accommodated in an optical or digital eyewear frame display  the system of any one of s  to  wherein said processing unit is configured and operable for identifying a consumer\\'s condition said consumer\\'s condition comprising consumer data being indicative of the consumer\\'s position and location relative to at least one object in the consumer\\'s environment said consumer data comprises at least one of a consumer\\'s face eyewear posture position sound or motion  the system of any one of s  to  wherein said event comprises at least one position and  of head increase or decrease of viewing distance between the consumer and viewed object and changing the position of eyeglasses worn by the consumer  the system of any one of s  to  wherein said event is identified by identifying s having an  feature being indicative of behavioral compensation performing a bruckner test performing a hirschberg test and measuring blink count frequency  the system of   wherein the  feature being indicative of behavioral compensation comprises squinting head  certain distances between an object and consumer\\'s eyes certain position of eyeglasses on the consumer\\'s face strabismus cataracts and reflections from the eye  the system of any one of s  to  wherein the notification includes at least one of the data indicative of the identified event data indicative of the identified consumer ophthalmologic recommendations based on the identified event or lack of events or an appointment for a vision test  the system of any one of s  to  wherein said processing unit comprises a memory for storing at least one of a reference data indicative of behavioral compensation for vision impairment data indicative of the notification or data indicative of a follow-up of the notification  the system of    wherein said processing unit is configured for at least one of identifying the event upon comparison between the detected data and the reference data or determining a probability for a vision impairment of the consumer based on the comparison  the system of any one of s  to  wherein said processing unit comprises a communication interface being configured for sending the notification to at least one of the identified consumer or a third party  the system of any one of s  to  wherein said processing unit is configured for providing a frame recommendation  the system of any one of s  to  wherein said memory is configured for storing a database including a multiplicity of data sets related to a  of spectacle frame models and sizes  the system according to   or  wherein said processing unit is configured and operable to correlate between frames parameters and ophthalmic prescriptions  the system according to any of s  to  wherein said processing unit is configured and operable to correlate between frames parameters and facial features  the system according to any of s  to  wherein said processing unit is configured and operable to correlate between frames parameters and eyewear preferences  the system according to any of s  to  comprising a server and at least one computer entity linked to the server via a network wherein said network is configured to receive and respond to requests sent across the network transmitting one or more modules of computer executable program instructions and displayable data to the network connected  computer platform in response to a request wherein said modules include modules configured to receive and transmit  information transmitting a frame recommendation and an optical lens option recommendation based on received  information for display by the network connected  computer platform  a computer program instructions stored in the local storage that when executed by a processing unit cause the processing unit to receive data being indicative of a scene of at least one consumer in an environment identify in the data a certain consumer identify an event being indicative of a behavioral compensation for vision impairment and upon identification of such an event send a notification relating to the vision impairment  a computer program product stored on a tangible computer readable medium comprising a library of software modules which cause a computer executing them to prompt for information pertinent to at least one of an eyeglasses recommendation and an optical lens option recommendation to store said information or to display eyewear recommendations   the computer program product of    wherein said library further comprises a module for frame selection point of sales and advertising  a computer platform for facilitating eye glasses marketing or selection comprising a camera a  configured to execute computer program instructions to cause the  to take an  of a consumer identify in the  a certain consumer identify an event being indicative of a behavioral compensation for vision impairment and upon identification of such an event sending a notification relating to the vision impairment local storage for  executable instructions for carrying out storage of information  a  for alerting on vision impairment said  comprising identifying a certain individual in scene data being indicative of a scene of at least one consumer in an environment identifying an event being indicative of a behavioral compensation for vision impairment and upon identification of such an event sending a notification on the vision impairment  the  of   further comprising detecting data being indicative of a scene of at least one consumer in a retail environment  the  of   wherein detecting the data being indicative of at least one consumer comprises at least one of capturing at least one  of at least one consumer detecting data being indicative of a motion of a consumer or  an eye motion of a consumer  the  of   wherein capturing at least one  of at least one consumer comprises continuously recording a scene  the  of any one of s  to  further comprising identifying in the data the consumer\\' s condition including data being indicative of the consumer\\'s position and location relative to the consumer\\'s environment said data comprising at least one of the consumer\\'s face posture position sound or motion  the  of any one of s  to  wherein said event comprises at least one of position and  of head increase or decrease of viewing distance between the consumer and viewed object or changing the position of eyeglasses worn by the consumer  the  of any one of s  to  wherein identifying of the event comprises identifying s having an  feature being indicative of behavioral compensation performing a bruckner test performing a hirschberg test and measuring blink countfrequency  the  of   wherein the  feature being indicative of behavioral compensation comprises squinting head  certain distances between an object and a consumer\\'s eyes certain position of eyeglasses on the consumer\\'s face strabismus cataracts and reflections from the eye  the  of any one of s  to  wherein identifying in the at least one  a consumer in a retail environment comprising at least one of receiving data characterizing the retail environment or performing face   the  of any one of s  to  wherein sending a notification comprising sending the notification to at least one of the identified consumer or a third party  the  of any one of s  to  wherein the notification includes at least one of the data indicative of the identified event data indicative of the identified consumer ophthalmologic recommendations based on the identified event or lack of events and an appointment for a vision test  the  of any one of s  to  further comprising storing at least one of a reference data indicative of behavioral compensation for vision impairment data indicative of the notification or data indicative of a follow-up of the notification  the  of   further comprising identifying the event upon comparison between the detected data and the reference data and determining a probability for a vision impairment of the consumer based on the comparison  a computer program intended to be stored in a memory of a  unit of a computer system or in a removable memory medium adapted to cooperate with a reader of the  unit comprising instructions for implementing the  according to any of s  to']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# the output is a list, where each element is a sentence of the original text\n",
    "nltk.sent_tokenize(lowera_text)\n",
    "nltk.sent_tokenize(lowerc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1afb3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'electronic', 'apparatus', 'including', 'an', 'capturing', 'a', 'storage', 'and', 'a', 'and', 'an', 'operation', 'method', 'thereof', 'are', 'provided', 'the', 'capturing', 'captures', 'an', 'for', 'a', 'and', 'the', 'storage', 'records', 'a', 'of', 'modules', 'the', 'is', 'coupled', 'to', 'the', 'capturing', 'and', 'the', 'storage', 'and', 'is', 'configured', 'to', 'configure', 'the', 'capturing', 'to', 'capture', 'a', 'head', 'of', 'a', 'perform', 'a', 'to', 'obtain', 'a', 'detect', 'a', 'of', 'facial', 'landmarks', 'within', 'the', 'estimate', 'a', 'head', 'posture', 'angle', 'of', 'the', 'according', 'to', 'the', 'facial', 'landmarks', 'calculate', 'a', 'gaze', 'position', 'where', 'the', 'gazes', 'on', 'the', 'screen', 'according', 'to', 'the', 'head', 'posture', 'angle', 'a', 'of', 'rotation', 'reference', 'angle', 'and', 'a', 'of', 'predetermined', 'calibration', 'positions', 'and', 'configure', 'the', 'screen', 'to', 'display', 'a', 'corresponding', 'visual', 'effect', 'according', 'to', 'the', 'gaze', 'positionthe', 'present', 'disclosure', 'provides', 'a', 'and', 'product', 'thereof', 'the', 'adopts', 'a', 'fusion', 'method', 'to', 'perform', 'machine', 'learning', 'computations', 'technical', 'effects', 'of', 'the', 'present', 'disclosure', 'include', 'fewer', 'computations', 'and', 'less', 'power', 'consumptiona', 'method', 'for', 'detecting', 'body', 'information', 'on', 'passengers', 'of', 'a', 'vehicle', 'based', 'on', 'humans', \"'\", 'status', 'is', 'provided', 'the', 'method', 'includes', 'steps', 'of', 'a', 'passenger', 'body', 'information-detecting', 'a', 'inputting', 'an', 'interior', 'of', 'the', 'vehicle', 'into', 'a', 'face', 'network', 'to', 'detect', 'faces', 'of', 'the', 'passengers', 'and', 'output', 'passenger', 'feature', 'information', 'and', 'inputting', 'the', 'interior', 'into', 'a', 'body', 'network', 'to', 'detect', 'bodies', 'and', 'output', 'body-part', 'length', 'information', 'and', 'b', 'retrieving', 'specific', 'height', 'mapping', 'information', 'by', 'referring', 'to', 'a', 'height', 'mapping', 'table', 'of', 'ratios', 'of', 'segment', 'body', 'portions', 'of', 'human', 'groups', 'to', 'heights', 'per', 'the', 'human', 'groups', 'acquiring', 'a', 'specific', 'height', 'of', 'the', 'specific', 'passenger', 'retrieving', 'specific', 'weight', 'mapping', 'information', 'from', 'a', 'weight', 'mapping', 'table', 'of', 'correlations', 'between', 'the', 'heights', 'and', 'weights', 'per', 'the', 'human', 'groups', 'and', 'acquiring', 'a', 'weight', 'of', 'the', 'specific', 'passenger', 'by', 'referring', 'to', 'the', 'specific', 'heighttechniques', 'related', 'to', 'improved', 'video', 'coding', 'based', 'on', 'face', 'detection', 'region', 'extraction', 'and', 'tracking', 'are', 'discussed', 'such', 'techniques', 'may', 'include', 'performing', 'a', 'facial', 'search', 'of', 'a', 'video', 'frame', 'to', 'determine', 'candidate', 's', 'in', 'the', 'video', 'frame', 'testing', 'the', 'candidate', 's', 'based', 'on', 'skin', 'tone', 'information', 'to', 'determine', 'valid', 'and', 'invalid', 's', 'rejecting', 'invalid', 's', 'and', 'encoding', 'the', 'video', 'frame', 'based', 'on', 'valid', 's', 'to', 'generate', 'a', 'coded', 'bitstreama', 'method', 'for', 'managing', 'a', 'smart', 'database', 'which', 'stores', 'facial', 's', 'for', 'face', 'is', 'provided', 'the', 'method', 'includes', 'steps', 'of', 'a', 'managing', 'a', 'counting', 'specific', 'facial', 's', 'corresponding', 'to', 'a', 'specific', 'person', 'in', 'the', 'smart', 'database', 'where', 'new', 'facial', 's', 'are', 'continuously', 'stored', 'and', 'determining', 'whether', 'a', 'first', 'counted', 'value', 'representing', 'a', 'count', 'of', 'the', 'specific', 'facial', 's', 'satisfies', 'a', 'first', 'set', 'value', 'and', 'b', 'if', 'the', 'first', 'counted', 'value', 'satisfies', 'the', 'first', 'set', 'value', 'inputting', 'the', 'specific', 'facial', 's', 'into', 'a', 'neural', 'aggregation', 'network', 'to', 'generate', 'quality', 'scores', 'of', 'the', 'specific', 'facial', 's', 'by', 'aggregation', 'of', 'the', 'specific', 'facial', 's', 'and', 'if', 'a', 'second', 'counted', 'value', 'representing', 'a', 'count', 'of', 'specific', 'quality', 'scores', 'among', 'the', 'quality', 'scores', 'from', 'a', 'highest', 'during', 'counting', 'thereof', 'satisfies', 'a', 'second', 'set', 'value', 'deleting', 'part', 'of', 'the', 'specific', 'facial', 's', 'corresponding', 'to', 'the', 'uncounted', 'quality', 'scores', 'from', 'the', 'smart', 'databasea', 'capable', 'of', 'determining', 'which', 'algorithms', 'should', 'be', 'applied', 'to', 'regions', 'of', 'interest', 'within', 'digital', 'representations', 'is', 'presented', 'a', 'preprocessing', 'module', 'utilizes', 'one', 'or', 'more', 'feature', 'identification', 'algorithms', 'to', 'determine', 'regions', 'of', 'interest', 'based', 'on', 'feature', 'density', 'the', 'preprocessing', 'modules', 'leverages', 'the', 'feature', 'density', 'signature', 'for', 'each', 'region', 'to', 'determine', 'which', 'of', 'a', 'of', 'diverse', 'modules', 'should', 'operate', 'on', 'the', 'region', 'of', 'interest', 'a', 'specific', 'embodiment', 'that', 'focuses', 'on', 'structured', 'documents', 'is', 'also', 'presented', 'further', 'the', 'disclosed', 'approach', 'can', 'be', 'enhanced', 'by', 'addition', 'of', 'an', 'object', 'classifier', 'that', 'classifies', 'types', 'of', 'objects', 'found', 'in', 'the', 'regions', 'of', 'interestdisclosed', 'is', 'a', 'mobile', 'terminal', 'the', 'mobile', 'terminal', 'may', 'include', 'a', 'front', 'camera', 'obtaining', 'a', 'd', 'face', 'of', 'a', 'a', 'glance', 'sensor', 'tilted', 'by', 'a', 'certain', 'angle', 'and', 'disposed', 'adjacent', 'to', 'the', 'front', 'camera', 'to', 'obtain', 'metadata', 'of', 'the', 'd', 'face', 'and', 'a', 'controller', 'obtaining', 'a', 'distance', 'between', 'the', 'glance', 'sensor', 'and', 'the', 'front', 'camera', 'the', 'distance', 'enabling', 'an', 'area', 'of', 'an', 'overlap', 'region', 'where', 'a', 'first', 'region', 'representing', 'a', 'range', 'photographable', 'by', 'the', 'front', 'camera', 'overlaps', 'a', 'second', 'region', 'representing', 'a', 'range', 'photographable', 'by', 'the', 'glance', 'sensor', 'to', 'be', 'the', 'maximumthis', 'disclosure', 'provides', 's', 'methods', 'and', 'apparatus', 'including', 'computer', 'programs', 'encoded', 'on', 'computer', 'storage', 'media', 'for', 'intelligent', 'routing', 'of', 'notifications', 'related', 'to', 'media', 'programming', 'in', 'one', 'aspect', 'a', 'smart', 'television', 'tv', 'can', 'be', 'implemented', 'to', 'track', 'a', \"'s\", 'tv', 'watching', 'behavior', 'and', 'anticipate', 'programming', 'based', 'on', 'that', 'behavior', 'in', 'some', 'other', 'aspects', 'the', 'smart', 'tv', 'can', 'be', 'implemented', 'to', 'detect', 'a', \"'s\", 'presence', 'and', 'based', 'on', 'that', 'detection', 'can', 'automatically', 'change', 'the', 'tv', 'channel', 'to', 'media', 'programming', 'analyzed', 'to', 'be', 'desirable', 'to', 'the', 'in', 'some', 'further', 'aspects', 'the', 'smart', 'tv', 'can', 'be', 'implemented', 'to', 'transmit', 'notification', 'instructions', 'to', 'electronic', 's', 'within', 'a', 'network', 'in', 'an', 'attempt', 'to', 'alert', 'the', 'to', 'upcoming', 'media', 'programming', 'additionally', 'the', 'smart', 'tv', 'can', 'be', 'implemented', 'to', 'transmit', 'detection', 'instructions', 'to', 'the', 'electronic', 's', 'within', 'the', 'network', 'whereby', 'the', 'electronic', 's', 'attempt', 'to', 'detect', 'a', \"'s\", 'presence', 'through', 'voice', 'or', 'is', 'configured', 'to', 'output', 'a', 'test', 'depth+multi-spectral', 'including', 'a', 'of', 'pixels', 'each', 'pixel', 'corresponds', 'to', 'one', 'of', 'the', 'of', 'sensors', 'of', 'a', 'sensor', 'array', 'of', 'the', 'camera', 'and', 'includes', 'at', 'least', 'a', 'depth', 'value', 'and', 'a', 'spectral', 'value', 'for', 'each', 'spectral', 'light', 'sub-band', 'of', 'a', 'of', 'spectral', 'illuminators', 'of', 'the', 'camera', 'a', 'face', 'machine', 'is', 'previously', 'trained', 'with', 'a', 'set', 'of', 'labeled', 'training', 'depth+multi-spectral', 's', 'having', 'a', 'same', 'structure', 'as', 'the', 'test', 'depth+multi-spectral', 'the', 'face', 'machine', 'is', 'configured', 'to', 'output', 'a', 'confidence', 'value', 'indicating', 'a', 'likelihood', 'that', 'the', 'test', 'depth+multi-spectral', 'includes', 'a', 'faceembodiments', 'of', 'the', 'present', 'disclosure', 'relate', 'to', 'an', 'processing', 'method', 'and', 'apparatus', 'and', 'an', 'electronic', 'the', 'method', 'includes', 'acquiring', 'a', 'photo', 'album', 'obtained', 'from', 'face', 'clustering', 'collecting', 'face', 'information', 'of', 'respective', 's', 'in', 'the', 'photo', 'album', 'and', 'acquiring', 'a', 'face', 'parameter', 'of', 'each', 'according', 'to', 'the', 'face', 'information', 'selecting', 'a', 'cover', 'according', 'to', 'the', 'face', 'parameter', 'of', 'each', 'and', 'taking', 'a', 'face-region', 'from', 'the', 'cover', 'and', 'setting', 'the', 'face-region', 'as', 'a', 'cover', 'of', 'the', 'photo', 'albumtechniques', 'described', 'herein', 'provide', 'location-based', 'access', 'control', 'to', 'secured', 'resources', 'generally', 'described', 'configurations', 'disclosed', 'herein', 'enable', 'a', 'to', 'dynamically', 'modify', 'access', 'to', 'secured', 'resources', 'based', 'on', 'one', 'or', 'more', 'location-related', 'actions', 'for', 'example', 'techniques', 'disclosed', 'herein', 'can', 'enable', 'a', 'computing', 'to', 'control', 'access', 'to', 'resources', 'such', 'as', 'computing', 's', 'display', 's', 'secured', 'locations', 'and', 'secured', 'data', 'in', 'some', 'configurations', 'the', 'techniques', 'disclosed', 'herein', 'can', 'enable', 'controlled', 'access', 'to', 'secured', 'resources', 'based', 'at', 'least', 'in', 'part', 'on', 'an', 'invitation', 'associated', 'with', 'a', 'location', 'and', 'positioning', 'data', 'indicating', 'a', 'location', 'of', 'a', 'one', 'embodiment', 'provides', 'a', 'method', 'comprising', 'receiving', 'a', 'piece', 'of', 'content', 'and', 'salient', 'moments', 'data', 'for', 'the', 'piece', 'of', 'content', 'the', 'method', 'further', 'comprises', 'based', 'on', 'the', 'salient', 'moments', 'data', 'determining', 'a', 'first', 'path', 'for', 'a', 'viewport', 'for', 'the', 'piece', 'of', 'content', 'the', 'method', 'further', 'comprises', 'displaying', 'the', 'viewport', 'on', 'a', 'display', 'movement', 'of', 'the', 'viewport', 'is', 'based', 'on', 'the', 'first', 'path', 'during', 'playback', 'of', 'the', 'piece', 'of', 'content', 'the', 'method', 'further', 'comprises', 'generating', 'an', 'augmentation', 'for', 'a', 'salient', 'moment', 'occurring', 'in', 'the', 'piece', 'of', 'content', 'and', 'presenting', 'the', 'augmentation', 'in', 'the', 'viewport', 'during', 'a', 'portion', 'of', 'the', 'playback', 'the', 'augmentation', 'comprises', 'an', 'interactive', 'hint', 'for', 'guiding', 'the', 'viewport', 'to', 'the', 'salient', 'momenta', 'computer-implemented', 'method', 'and', 'computer', 'program', 'product', 'are', 'provided', 'for', 'facial', 'the', 'method', 'includes', 'receiving', 'by', 'a', 'a', 'of', 's', 'the', 'method', 'also', 'includes', 'extracting', 'by', 'the', 'with', 'a', 'feature', 'extractor', 'utilizing', 'a', 'convolutional', 'neural', 'network', 'cnn', 'with', 'an', 'enlarged', 'intra-class', 'variance', 'of', 'long-tail', 'classes', 'feature', 'vectors', 'for', 'each', 'of', 'the', 'of', 's', 'the', 'method', 'additionally', 'includes', 'generating', 'by', 'the', 'with', 'a', 'feature', 'generator', 'discriminative', 'feature', 'vectors', 'for', 'each', 'of', 'the', 'feature', 'vectors', 'the', 'method', 'further', 'includes', 'classifying', 'by', 'the', 'utilizing', 'a', 'fully', 'connected', 'classifier', 'an', 'identity', 'from', 'the', 'discriminative', 'feature', 'vector', 'the', 'method', 'also', 'includes', 'control', 'an', 'operation', 'of', 'a', '-based', 'machine', 'to', 'react', 'in', 'accordance', 'with', 'the', 'identitysome', 'embodiments', 'of', 'the', 'invention', 'provide', 'efficient', 'expressive', 'machine-trained', 'networks', 'for', 'performing', 'machine', 'learning', 'the', 'machine-trained', 'mt', 'networks', 'of', 'some', 'embodiments', 'use', 'novel', 'processing', 'nodes', 'with', 'novel', 'activation', 'functions', 'that', 'allow', 'the', 'mt', 'network', 'to', 'efficiently', 'define', 'with', 'fewer', 'processing', 'node', 'layers', 'a', 'complex', 'mathematical', 'expression', 'that', 'solves', 'a', 'particular', 'problem', 'eg', 'face', 'speech', 'etc', 'in', 'some', 'embodiments', 'the', 'same', 'activation', 'function', 'eg', 'a', 'cup', 'function', 'is', 'used', 'for', 'numerous', 'processing', 'nodes', 'of', 'the', 'mt', 'network', 'but', 'through', 'the', 'machine', 'learning', 'this', 'activation', 'function', 'is', 'configured', 'differently', 'for', 'different', 'processing', 'nodes', 'so', 'that', 'different', 'nodes', 'can', 'emulate', 'or', 'implement', 'two', 'or', 'more', 'different', 'functions', 'eg', 'two', 'or', 'more', 'boolean', 'logical', 'operators', 'such', 'as', 'xor', 'and', 'and', 'the', 'activation', 'function', 'in', 'some', 'embodiments', 'is', 'a', 'periodic', 'function', 'that', 'can', 'be', 'configured', 'to', 'implement', 'different', 'functions', 'eg', 'different', 'sinusoidal', 'functionsmethods', 'and', 's', 'may', 'provide', 'for', 'facial', 'of', 'at', 'least', 'one', 'input', 'utilizing', 'hierarchical', 'feature', 'learning', 'and', 'pair-wise', 'receptive', 'field', 'theory', 'may', 'be', 'used', 'on', 'the', 'input', 'to', 'generate', 'a', 'pre-processed', 'multi-channel', 'channels', 'in', 'the', 'pre-processed', 'may', 'be', 'activated', 'based', 'on', 'the', 'amount', 'of', 'feature', 'rich', 'details', 'within', 'the', 'channels', 'similarly', 'local', 'patches', 'may', 'be', 'activated', 'based', 'on', 'the', 'discriminant', 'within', 'the', 'local', 'patches', 'may', 'be', 'extracted', 'from', 'the', 'local', 'patches', 'and', 'the', 'most', 'discriminant', 'may', 'be', 'selected', 'in', 'order', 'to', 'perform', 'feature', 'matching', 'on', 'pair', 'sets', 'the', 'may', 'utilize', 'patch', 'feature', 'pooling', 'pair-wise', 'matching', 'and', 'large-scale', 'training', 'in', 'order', 'to', 'quickly', 'and', 'accurately', 'perform', 'facial', 'at', 'a', 'low', 'cost', 'for', 'both', 'memory', 'and', 'computationa', 'method', 'for', 'controlling', 'a', 'terminal', 'is', 'provided', 'the', 'terminal', 'includes', 'a', 'capturing', 'apparatus', 'and', 'at', 'least', 'one', 'an', 'is', 'acquired', 'by', 'the', 'capturing', 'apparatus', 'a', 'motion', 'parameter', 'of', 'the', 'terminal', 'is', 'obtained', 'processing', 'on', 'the', 'acquired', 'is', 'controlled', 'to', 'be', 'performed', 'based', 'on', 'the', 'motion', 'parameter', 'being', 'equal', 'to', 'or', 'less', 'than', 'a', 'preset', 'parameter', 'threshold', 'and', 'skipped', 'based', 'on', 'the', 'motion', 'parameter', 'being', 'greater', 'than', 'the', 'preset', 'parameter', 'thresholda', 'drive-through', 'order', 'processing', 'method', 'and', 'apparatus', 'are', 'disclosed', 'the', 'drive-through', 'order', 'processing', 'method', 'includes', 'receiving', 'customer', 'information', 'detected', 'through', 'vision', 'providing', 'product', 'information', 'based', 'on', 'the', 'customer', 'information', 'and', 'processing', 'a', 'product', 'order', 'of', 'a', 'customer', 'according', 'to', 'the', 'present', 'disclosure', 'it', 'is', 'possible', 'to', 'rapidly', 'process', 'an', 'order', 'using', 'customer', 'information', 'based', 'on', 'customer', 'using', 'an', 'artificial', 'intelligence', 'ai', 'model', 'of', 'machine', 'learning', 'through', 'a', 'g', 'networkan', 'processing', 'method', 'performed', 'at', 'a', 'computing', 'includes', 'identifying', 'using', 'face', 'one', 'or', 'more', 'faces', 'each', 'face', 'corresponding', 'to', 'a', 'respective', 'person', 'captured', 'in', 'a', 'first', 'for', 'each', 'identified', 'face', 'extracting', 'a', 'set', 'of', 'profile', 'parameters', 'of', 'a', 'corresponding', 'person', 'in', 'the', 'first', 'and', 'selecting', 'from', 'a', 'of', 'tiles', 'a', 'first', 'tile', 'that', 'matches', 'the', 'face', 'of', 'the', 'corresponding', 'person', 'in', 'the', 'first', 'in', 'accordance', 'with', 'a', 'predefined', 'correspondence', 'between', 'the', 'set', 'of', 'profile', 'parameters', 'of', 'the', 'corresponding', 'person', 'and', 'a', 'set', 'of', 'pre-stored', 'description', 'parameters', 'of', 'the', 'first', 'tile', 'generating', 'a', 'second', 'by', 'covering', 'the', 'faces', 'of', 'respective', 'persons', 'in', 'the', 'first', 'with', 'their', 'corresponding', 'first', 'tiles', 'and', 'sharing', 'the', 'first', 'and', 'the', 'second', 'in', 'a', 'predefined', 'order', 'via', 'a', 'group', 'chat', 'sessionin', 'one', 'embodiment', 'the', 'artificial', 'reality', 'determines', 'that', 'a', 'performance', 'metric', 'of', 'an', 'eye', 'tracking', 'is', 'below', 'a', 'first', 'performance', 'threshold', 'the', 'eye', 'tracking', 'is', 'associated', 'with', 'a', 'head-mounted', 'display', 'worn', 'by', 'a', 'the', 'artificial', 'reality', 'receives', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'a', 'and', 'determines', 'a', 'region', 'that', 'the', 'is', 'looking', 'at', 'within', 'a', 'field', 'of', 'view', 'of', 'a', 'head-mounted', 'display', 'based', 'on', 'the', 'received', 'first', 'inputs', 'the', 'determines', 'a', 'vergence', 'distance', 'of', 'the', 'based', 'at', 'least', 'on', 'the', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'the', 'region', 'that', 'the', 'is', 'looking', 'at', 'and', 'locations', 'of', 'one', 'or', 'more', 'objects', 'in', 'a', 'scene', 'displayed', 'by', 'the', 'head-mounted', 'display', 'the', 'adjusts', 'one', 'or', 'more', 'configurations', 'of', 'the', 'head-mounted', 'display', 'based', 'on', 'the', 'determined', 'vergence', 'distance', 'of', 'the', 'a', 'computer-implemented', 'method', 'is', 'provided', 'for', '-based', 'self-guided', 'object', 'detection', 'the', 'method', 'includes', 'receiving', 'by', 'a', 'a', 'set', 'of', 's', 'each', 'of', 'the', 's', 'has', 'a', 'respective', 'grid', 'thereon', 'that', 'is', 'labeled', 'regarding', 'a', 'respective', 'object', 'to', 'be', 'detected', 'using', 'grid', 'level', 'label', 'data', 'the', 'method', 'further', 'includes', 'training', 'by', 'the', 'a', 'grid-based', 'object', 'detector', 'using', 'the', 'grid', 'level', 'label', 'data', 'the', 'method', 'also', 'includes', 'determining', 'by', 'the', 'a', 'respective', 'bounding', 'box', 'for', 'the', 'respective', 'object', 'in', 'each', 'of', 'the', 's', 'by', 'applying', 'local', 'segmentation', 'to', 'each', 'of', 'the', 's', 'the', 'method', 'additionally', 'includes', 'training', 'by', 'the', 'a', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'for', 'joint', 'object', 'localization', 'and', 'object', 'using', 'the', 'respective', 'bounding', 'box', 'for', 'the', 'respective', 'object', 'in', 'each', 'of', 'the', 's', 'as', 'an', 'input', 'to', 'the', 'rcnna', 'and', 'method', 'of', 'face', 'comprising', 'multiple', 'phases', 'implemented', 'in', 'a', 'parallel', 'architecture', 'the', 'first', 'phase', 'is', 'a', 'normalization', 'phase', 'whereby', 'a', 'captured', 'is', 'normalized', 'to', 'the', 'same', 'size', 'orientation', 'and', 'illumination', 'of', 'stored', 's', 'in', 'a', 'preexisting', 'database', 'the', 'second', 'phase', 'is', 'a', 'feature', 'extractiondistance', 'matrix', 'phase', 'where', 'a', 'distance', 'matrix', 'is', 'generated', 'for', 'the', 'captured', 'in', 'a', 'coarse', 'phase', 'the', 'generated', 'distance', 'matrix', 'is', 'compared', 'with', 'distance', 'matrices', 'in', 'the', 'database', 'using', 'euclidean', 'distance', 'matches', 'to', 'create', 'candidate', 'lists', 'and', 'in', 'a', 'detailed', 'phase', 'multiple', 'face', 'algorithms', 'are', 'applied', 'to', 'the', 'candidate', 'lists', 'to', 'produce', 'a', 'final', 'result', 'the', 'distance', 'matrices', 'in', 'the', 'normalized', 'database', 'may', 'be', 'broken', 'into', 'parallel', 'lists', 'for', 'parallelization', 'in', 'the', 'feature', 'extractiondistance', 'matrix', 'phase', 'and', 'the', 'candidate', 'lists', 'may', 'also', 'be', 'grouped', 'according', 'to', 'a', 'dissimilarity', 'algorithm', 'for', 'parallel', 'processing', 'in', 'the', 'detailed', 'phasean', 'imaging', 'including', 'a', 'pixel', 'matrix', 'and', 'a', 'is', 'provided', 'the', 'pixel', 'matrix', 'includes', 'a', 'of', 'phase', 'detection', 'pixels', 'and', 'a', 'of', 'regular', 'pixels', 'the', 'performs', 'autofocusing', 'according', 'to', 'pixel', 'data', 'of', 'the', 'phase', 'detection', 'pixels', 'and', 'determines', 'an', 'operating', 'resolution', 'of', 'the', 'regular', 'pixels', 'according', 'to', 'autofocused', 'pixel', 'data', 'of', 'the', 'phase', 'detection', 'pixels', 'wherein', 'the', 'phase', 'detection', 'pixels', 'are', 'always-on', 'pixels', 'and', 'the', 'regular', 'pixels', 'are', 'selectively', 'turned', 'on', 'after', 'the', 'autofocusing', 'is', 'accomplishedan', 'apparatus', 'includes', 'a', 'first', 'camera', 'module', 'providing', 'a', 'first', 'of', 'an', 'object', 'with', 'a', 'first', 'field', 'of', 'view', 'a', 'second', 'camera', 'module', 'providing', 'a', 'second', 'of', 'the', 'object', 'with', 'a', 'second', 'field', 'of', 'view', 'different', 'from', 'the', 'first', 'field', 'of', 'view', 'a', 'first', 'depth', 'map', 'generator', 'that', 'generates', 'a', 'first', 'depth', 'map', 'of', 'the', 'first', 'based', 'on', 'the', 'first', 'and', 'the', 'second', 'and', 'a', 'second', 'depth', 'map', 'generator', 'that', 'generates', 'a', 'second', 'depth', 'map', 'of', 'the', 'second', 'based', 'on', 'the', 'first', 'the', 'second', 'and', 'the', 'first', 'depth', 'mapmethods', 's', 'and', 'apparatus', 'including', 'computer', 'programs', 'encoded', 'on', 'computer', 'storage', 'media', 'for', 'a', 'payment', 'based', 'on', 'a', 'face', 'are', 'provided', 'one', 'of', 'the', 'methods', 'includes', 'acquiring', 'first', 'face', 'information', 'of', 'a', 'target', 'extracting', 'first', 'characteristic', 'information', 'from', 'the', 'first', 'face', 'information', 'wherein', 'the', 'first', 'characteristic', 'information', 'includes', 'head', 'posture', 'information', 'of', 'the', 'target', 'and', 'gaze', 'information', 'of', 'the', 'target', 'determining', 'whether', 'the', 'target', 'has', 'a', 'willingness', 'to', 'pay', 'according', 'to', 'the', 'head', 'posture', 'information', 'of', 'the', 'target', 'and', 'the', 'gaze', 'information', 'of', 'the', 'target', 'including', 'determining', 'whether', 'an', 'angle', 'of', 'rotation', 'in', 'each', 'preset', 'direction', 'is', 'less', 'than', 'an', 'angle', 'threshold', 'and', 'whether', 'a', 'probability', 'value', 'that', 'a', 'gazes', 'at', 'a', 'payment', 'screen', 'is', 'greater', 'than', 'a', 'probability', 'threshold', 'and', 'in', 'response', 'to', 'determining', 'that', 'the', 'target', 'has', 'a', 'willingness', 'to', 'pay', 'completing', 'a', 'payment', 'operation', 'based', 'on', 'the', 'face', 'a', 'novel', 'method', 'and', 'apparatus', 'for', 'face', 'authentication', 'is', 'disclosed', 'the', 'disclosed', 'method', 'comprises', 'detecting', 'a', 'motion', 'by', 'a', 'subject', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'assigning', 'a', 'unique', 'session', 'identification', 'number', 'to', 'the', 'subject', 'detected', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'detecting', 'a', 'facial', 'area', 'of', 'the', 'subject', 'detected', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'generating', 'an', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'assessing', 'a', 'quality', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'conducing', 'an', 'incremental', 'training', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'determining', 'an', 'identity', 'of', 'the', 'subject', 'based', 'on', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'identifying', 'an', 'intent', 'of', 'the', 'subject', 'and', 'authorizing', 'access', 'to', 'a', 'point', 'of', 'entry', 'based', 'on', 'the', 'determined', 'identity', 'of', 'the', 'subject', 'and', 'based', 'on', 'the', 'intent', 'of', 'the', 'subjectdisclosed', 'herein', 'is', 'a', 'robot', 'and', 'an', 'electronic', 'for', 'acquiring', 'video', 'and', 'a', 'method', 'for', 'acquiring', 'video', 'using', 'the', 'robot', 'the', 'robot', 'includes', 'a', 'camera', 'configured', 'to', 'rotate', 'in', 'the', 'lateral', 'direction', 'and', 'tilt', 'in', 'the', 'vertical', 'direction', 'and', 'controls', 'at', 'least', 'one', 'of', 'a', 'direction', 'of', 'the', 'rotation', 'of', 'the', 'camera', 'an', 'angle', 'of', 'the', 'tilt', 'of', 'the', 'camera', 'and', 'a', 'focal', 'distance', 'of', 'the', 'camera', 'by', 'and', 'tracking', 's', 'in', 'a', 'video', 'acquired', 'by', 'the', 'cameras', 'and', 'methods', 'are', 'disclosed', 'for', 'inferring', 'topics', 'from', 'a', 'file', 'containing', 'both', 'audio', 'and', 'video', 'for', 'example', 'a', 'multimodal', 'or', 'multimedia', 'file', 'in', 'order', 'to', 'facilitate', 'video', 'indexing', 'a', 'set', 'of', 'entities', 'is', 'extracted', 'from', 'the', 'file', 'and', 'linked', 'to', 'produce', 'a', 'graph', 'and', 'reference', 'information', 'is', 'also', 'obtained', 'for', 'the', 'set', 'of', 'entities', 'entities', 'may', 'be', 'drawn', 'for', 'example', 'from', 'wikipedia', 'categories', 'or', 'other', 'large', 'ontological', 'data', 'sources', 'analysis', 'of', 'the', 'graph', 'using', 'unsupervised', 'learning', 'permits', 'determining', 'clusters', 'in', 'the', 'graph', 'extracting', 'from', 'the', 'clusters', 'possibly', 'using', 'supervised', 'learning', 'provides', 'for', 'selection', 'of', 'topic', 'identifiers', 'the', 'topic', 'identifiers', 'are', 'then', 'used', 'for', 'indexing', 'the', 'filea', 'face', 'method', 'a', 'neural', 'network', 'training', 'method', 'an', 'apparatus', 'and', 'an', 'electronic', 'the', 'method', 'comprises', 'obtaining', 'a', 'first', 'face', 'by', 'means', 'of', 'a', 'first', 'camera', 'extracting', 'a', 'first', 'face', 'feature', 'of', 'the', 'first', 'face', 'comparing', 'the', 'first', 'face', 'feature', 'with', 'a', 'pre-stored', 'second', 'face', 'feature', 'to', 'obtain', 'a', 'reference', 'similarity', 'the', 'second', 'face', 'feature', 'being', 'obtained', 'by', 'extracting', 'a', 'feature', 'of', 'a', 'second', 'face', 'obtained', 'by', 'a', 'second', 'camera', 'and', 'the', 'second', 'camera', 'and', 'the', 'first', 'camera', 'being', 'different', 'types', 'of', 'cameras', 'and', 'determining', 'according', 'to', 'the', 'reference', 'similarity', 'whether', 'the', 'first', 'face', 'feature', 'and', 'the', 'second', 'face', 'feature', 'correspond', 'to', 'a', 'same', 'person', 'the', 'present', 'invention', 'discloses', 'a', 'technique', 'for', 'alerting', 'on', 'vision', 'impairment', 'the', 'comprises', 'a', 'processing', 'unit', 'configured', 'and', 'operable', 'for', 'receiving', 'scene', 'data', 'being', 'indicative', 'of', 'a', 'scene', 'of', 'at', 'least', 'one', 'consumer', 'in', 'an', 'environment', 'identifying', 'in', 'the', 'scene', 'data', 'a', 'certain', 'consumer', 'identifying', 'an', 'event', 'being', 'indicative', 'of', 'a', 'behavioral', 'compensation', 'for', 'vision', 'impairment', 'and', 'upon', 'identification', 'of', 'such', 'an', 'event', 'sending', 'a', 'notification', 'relating', 'to', 'the', 'vision', 'impairment']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'configured', 'to', 'make', 'a', 'screen', 'to', 'display', 'a', 'of', 'frames', 'comprising', 'an', 'capturing', 'device', 'a', 'storage', 'device', 'storing', 'a', 'of', 'modules', 'and', 'a', 'coupled', 'to', 'the', 'capturing', 'device', 'and', 'the', 'storage', 'device', 'configured', 'to', 'execute', 'the', 'modules', 'in', 'the', 'storage', 'device', 'to', 'configure', 'the', 'screen', 'to', 'display', 'a', 'of', 'marker', 'objects', 'at', 'a', 'of', 'predetermined', 'positions', 'configure', 'the', 'capturing', 'device', 'to', 'capture', 'a', 'of', 'first', 'head', 's', 'when', 'a', 'is', 'looking', 'at', 'the', 'predetermined', 'positions', 's', 'perform', 'a', 'of', 'first', 'face', 'operations', 'on', 'the', 'first', 'head', 's', 'to', 'obtain', 'a', 'of', 'first', 'face', 'regions', 'corresponding', 'to', 'the', 'predetermined', 'positions', 's', 'detect', 'a', 'of', 'first', 'facial', 'landmarks', 'corresponding', 'to', 'the', 'first', 'face', 'regions', 's', 'calculate', 'a', 'of', 'rotation', 'reference', 'angles', 'of', 'the', 'looking', 'at', 'the', 'predetermined', 'positions', 'according', 'to', 'the', 'first', 'facial', 'landmarks', 'configure', 'the', 'capturing', 'device', 'to', 'capture', 'a', 'second', 'head', 'of', 'the', 'perform', 'a', 'on', 'the', 'second', 'head', 'to', 'obtain', 'a', 'second', 'face', 'region', 'detect', 'a', 'of', 'second', 'facial', 'landmarks', 'within', 'the', 'second', 'face', 'region', 's', 'estimate', 'a', 'head', 'posture', 'angle', 'of', 'the', 'according', 'to', 'the', 'second', 'facial', 'landmarks', 'calculate', 'a', 'gaze', 'position', 'of', 'the', 'on', 'the', 'screen', 'according', 'to', 'the', 'head', 'posture', 'angle', 'the', 'rotation', 'reference', 'angles', 'and', 'the', 'predetermined', 'positions', 'and', 'configure', 'the', 'screen', 'to', 'display', 'a', 'corresponding', 'visual', 'effect', 'according', 'to', 'the', 'gaze', 'position', 'the', 'according', 'to', 'wherein', 'the', 'gaze', 'position', 'comprises', 'a', 'first', 'coordinate', 'value', 'in', 'a', 'first', 'axial', 'direction', 'and', 'a', 'second', 'coordinate', 'value', 'in', 'a', 'second', 'axial', 'direction', 'the', 'according', 'to', 'wherein', 'the', 'head', 'posture', 'angles', 'comprise', 'a', 'head', 'pitch', 'angle', 'and', 'a', 'head', 'yaw', 'angle', 'and', 'the', 'rotation', 'reference', 'angles', 'comprise', 'a', 'first', 'pitch', 'angle', 'a', 'second', 'pitch', 'angle', 'a', 'first', 'yaw', 'angle', 'and', 'a', 'second', 'yaw', 'angle', 'corresponding', 'to', 'the', 'predetermined', 'positions', 'the', 'according', 'to', 'wherein', 'the', 'performs', 'interpolation', 'operation', 'or', 'extrapolation', 'operation', 'according', 'to', 'the', 'first', 'yaw', 'angle', 'the', 'second', 'yaw', 'angle', 'a', 'first', 'position', 'corresponding', 'to', 'the', 'first', 'yaw', 'angle', 'among', 'the', 'predetermined', 'positions', 'a', 'second', 'position', 'corresponding', 'to', 'the', 'second', 'yaw', 'angle', 'among', 'the', 'predetermined', 'positions', 'and', 'the', 'head', 'yaw', 'angle', 'thereby', 'obtaining', 'the', 'first', 'coordinate', 'value', 'of', 'the', 'gaze', 'position', 'and', 'the', 'performs', 'interpolation', 'operation', 'or', 'extrapolation', 'operation', 'according', 'to', 'the', 'first', 'pitch', 'angle', 'the', 'second', 'pitch', 'angle', 'a', 'third', 'position', 'corresponding', 'to', 'the', 'first', 'pitch', 'angle', 'among', 'the', 'predetermined', 'positions', 'a', 'fourth', 'position', 'corresponding', 'to', 'the', 'second', 'pitch', 'angle', 'among', 'the', 'predetermined', 'positions', 'and', 'the', 'head', 'pitch', 'angle', 'thereby', 'obtaining', 'the', 'second', 'coordinate', 'value', 'of', 'the', 'gaze', 'position', 'the', 'according', 'to', 'wherein', 'the', 'calculates', 'a', 'of', 'first', 'viewing', 'distances', 'between', 'the', 'and', 'the', 'screen', 'according', 'to', 'the', 'first', 'facial', 'landmarks', 'the', 'estimates', 'a', 'second', 'viewing', 'distance', 'between', 'the', 'and', 'the', 'screen', 'according', 'to', 'the', 'second', 'facial', 'landmarks', 'and', 'the', 'adjusts', 'the', 'rotation', 'reference', 'angles', 'or', 'the', 'gaze', 'position', 'according', 'to', 'the', 'second', 'viewing', 'distance', 'and', 'the', 'first', 'viewing', 'distances', 'the', 'according', 'to', 'wherein', 'the', 'maps', 'a', 'of', 'two-dimensional', 'position', 'coordinates', 'of', 'the', 'second', 'facial', 'landmarks', 'under', 'a', 'plane', 'coordinate', 'system', 'to', 'a', 'of', 'three-dimensional', 'position', 'coordinates', 'under', 'a', 'three-dimensional', 'coordinate', 'system', 'and', 'the', 'estimates', 'the', 'head', 'posture', 'angle', 'according', 'to', 'the', 'three-dimensional', 'position', 'coordinates', 'of', 'the', 'second', 'facial', 'landmarks', 'the', 'according', 'to', 'wherein', 'the', 'second', 'head', 'comprises', 'a', 'wearable', 'device', 'and', 'the', 'second', 'facial', 'landmarks', 'do', 'not', 'comprise', 'a', 'of', 'third', 'facial', 'landmarks', 'of', 'the', 'covered', 'by', 'the', 'wearable', 'device', 'the', 'according', 'to', 'wherein', 'the', 'second', 'head', 'comprises', 'a', 'wearable', 'device', 'and', 'the', 'second', 'facial', 'landmarks', 'comprise', 'one', 'or', 'more', 'simulated', 'landmarks', 'marked', 'by', 'the', 'wearable', 'device', 'an', 'operating', 'adapted', 'for', 'an', 'comprising', 'an', 'capturing', 'device', 'and', 'making', 'a', 'screen', 'to', 'display', 'a', 'of', 'frames', 'the', 'comprising', 'configuring', 'the', 'screen', 'to', 'display', 'a', 'of', 'marker', 'objects', 'at', 'a', 'of', 'predetermined', 'positions', 'configuring', 'the', 'capturing', 'device', 'to', 'capture', 'a', 'of', 'first', 'head', 's', 'when', 'a', 'is', 'looking', 'at', 'the', 'predetermined', 'positions', 's', 'performing', 'a', 'of', 'first', 'face', 'operations', 'on', 'the', 'first', 'head', 's', 'to', 'obtain', 'a', 'of', 'first', 'face', 'regions', 'corresponding', 'to', 'the', 'predetermined', 'positions', 's', 'detecting', 'a', 'of', 'first', 'facial', 'landmarks', 'corresponding', 'to', 'the', 'first', 'face', 'regions', 's', 'calculating', 'a', 'of', 'rotation', 'reference', 'angles', 'of', 'the', 'looking', 'at', 'the', 'predetermined', 'positions', 'according', 'to', 'the', 'first', 'facial', 'landmarks', 'configuring', 'the', 'capturing', 'device', 'to', 'capture', 'a', 'second', 'head', 'of', 'the', 'performing', 'a', 'on', 'the', 'second', 'head', 'to', 'obtain', 'a', 'second', 'face', 'region', 's', 'detecting', 'a', 'of', 'second', 'facial', 'landmarks', 'within', 'the', 'second', 'face', 'region', 'estimating', 'a', 'head', 'posture', 'angle', 'of', 'the', 'according', 'to', 'the', 'second', 'facial', 'landmarks', 'calculating', 'a', 'gaze', 'position', 'of', 'the', 'on', 'the', 'screen', 'according', 'to', 'the', 'head', 'posture', 'angle', 'the', 'rotation', 'reference', 'angles', 'and', 'the', 'predetermined', 'positions', 'and', 's', 'configuring', 'the', 'screen', 'to', 'display', 'a', 'corresponding', 'visual', 'effect', 'according', 'to', 'the', 'gaze', 'position', 'the', 'operation', 'according', 'to', 'wherein', 'the', 'gaze', 'position', 'comprises', 'a', 'first', 'coordinate', 'value', 'in', 'a', 'first', 'axial', 'direction', 'and', 'a', 'second', 'coordinate', 'value', 'in', 'a', 'second', 'axial', 'direction', 'the', 'operation', 'according', 'to', 'wherein', 'the', 'head', 'posture', 'angles', 'comprise', 'a', 'head', 'pitch', 'angle', 'and', 'a', 'head', 'yaw', 'angle', 'and', 'the', 'rotation', 'reference', 'angles', 'comprise', 'a', 'first', 'pitch', 'angle', 'a', 'second', 'pitch', 'angle', 'a', 'first', 'yaw', 'angle', 'and', 'a', 'second', 'yaw', 'angle', 'corresponding', 'to', 'the', 'predetermined', 'positions', 'the', 'operation', 'according', 'to', 'wherein', 'the', 'step', 'of', 'calculating', 'the', 'gaze', 'position', 'of', 'the', 'on', 'the', 'screen', 'according', 'to', 'the', 'head', 'posture', 'angle', 'the', 'rotation', 'reference', 'angles', 'and', 'the', 'predetermined', 'positions', 'comprises', 'performing', 'interpolation', 'operation', 'or', 'extrapolation', 'operation', 'according', 'to', 'the', 'first', 'yaw', 'angle', 'the', 'second', 'yaw', 'angle', 'a', 'first', 'position', 'corresponding', 'to', 'the', 'first', 'yaw', 'angle', 'among', 'the', 'predetermined', 'positions', 'a', 'second', 'position', 'corresponding', 'to', 'the', 'second', 'yaw', 'angle', 'among', 'the', 'predetermined', 'positions', 'and', 'the', 'head', 'yaw', 'angle', 'thereby', 'obtaining', 'the', 'first', 'coordinate', 'value', 'of', 'the', 'gaze', 'position', 'and', 'performing', 'interpolation', 'operation', 'or', 'extrapolation', 'operation', 'according', 'to', 'the', 'first', 'pitch', 'angle', 'the', 'second', 'pitch', 'angle', 'a', 'third', 'position', 'corresponding', 'to', 'the', 'first', 'pitch', 'angle', 'among', 'the', 'predetermined', 'positions', 'a', 'fourth', 'position', 'corresponding', 'to', 'the', 'second', 'pitch', 'angle', 'among', 'the', 'predetermined', 'positions', 'and', 'the', 'head', 'pitch', 'angle', 'thereby', 'obtaining', 'the', 'second', 'coordinate', 'value', 'of', 'the', 'gaze', 'position', 'the', 'operation', 'according', 'to', 'wherein', 'the', 'further', 'comprises', 'calculating', 'a', 'of', 'first', 'viewing', 'distances', 'between', 'the', 'and', 'the', 'screen', 'according', 'to', 'the', 'first', 'facial', 'landmarks', 'estimating', 'a', 'second', 'viewing', 'distance', 'between', 'the', 'and', 'the', 'screen', 'according', 'to', 'the', 'second', 'facial', 'landmarks', 'and', 'adjusting', 'the', 'rotation', 'reference', 'angles', 'or', 'the', 'gaze', 'position', 'according', 'to', 'the', 'second', 'viewing', 'distance', 'and', 'the', 'first', 'viewing', 'distances', 'the', 'operation', 'according', 'to', 'wherein', 'the', 'further', 'comprises', 'mapping', 'a', 'of', 'two-dimensional', 'position', 'coordinates', 'of', 'the', 'second', 'facial', 'landmarks', 'under', 'a', 'plane', 'coordinate', 'system', 'to', 'a', 'of', 'three-dimensional', 'position', 'coordinates', 'under', 'a', 'three-dimensional', 'coordinate', 'system', 'and', 'estimating', 'the', 'head', 'posture', 'angle', 'according', 'to', 'the', 'three-dimensional', 'position', 'coordinates', 'of', 'the', 'second', 'facial', 'landmarks', 'the', 'operation', 'according', 'to', 'wherein', 'the', 'second', 'head', 'comprises', 'a', 'wearable', 'device', 'and', 'the', 'second', 'facial', 'landmarks', 'do', 'not', 'comprise', 'a', 'of', 'third', 'facial', 'landmarks', 'of', 'the', 'covered', 'by', 'the', 'wearable', 'device', 'the', 'operation', 'according', 'to', 'wherein', 'the', 'second', 'head', 'comprises', 'a', 'wearable', 'device', 'and', 'the', 'second', 'facial', 'landmarks', 'comprise', 'one', 'or', 'more', 'simulated', 'landmarks', 'marked', 'by', 'the', 'wearable', 'device', 'a', 'computation', 'applied', 'to', 'a', 'computing', 'system', 'wherein', 'the', 'computing', 'system', 'comprises', 'a', 'control', 'unit', 'a', 'computation', 'group', 'and', 'a', 'general', 'storage', 'unit', 'wherein', 'the', 'control', 'unit', 'comprises', 'a', 'first', 'memory', 'a', 'decoding', 'logic', 'and', 'a', 'controller', 'wherein', 'the', 'computation', 'group', 'comprises', 'a', 'group', 'controller', 'and', 'a', 'of', 'computing', 'units', 'the', 'general', 'storage', 'unit', 'is', 'configured', 'to', 'store', 'data', 'and', 'the', 'computation', 'comprises', 'receiving', 'by', 'the', 'controller', 'a', 'first', 'level', 'instruction', 'sequence', 'and', 'partitioning', 'by', 'the', 'decoding', 'logic', 'the', 'first', 'level', 'instruction', 'sequence', 'into', 'a', 'of', 'second', 'level', 'instruction', 'sequences', 'creating', 'by', 'the', 'controller', 'm', 'threads', 'for', 'the', 'of', 'second', 'level', 'instruction', 'sequences', 'and', 'allocating', 'by', 'the', 'controller', 'an', 'independent', 'register', 'as', 'well', 'as', 'configuring', 'an', 'independent', 'addressing', 'function', 'for', 'each', 'thread', 'of', 'the', 'm', 'threads', 'wherein', 'm', 'is', 'an', 'integer', 'greater', 'than', 'or', 'equal', 'to', 'and', 'obtaining', 'by', 'the', 'group', 'controller', 'a', 'of', 'computation', 'types', 'of', 'the', 'of', 'second', 'level', 'instruction', 'sequences', 'obtaining', 'a', 'corresponding', 'fusion', 'computation', 'manner', 'of', 'the', 'computation', 'types', 'according', 'to', 'the', 'of', 'computation', 'types', 'and', 'adopting', 'by', 'the', 'of', 'computing', 'units', 'the', 'fusion', 'computation', 'manner', 'to', 'call', 'the', 'm', 'threads', 'for', 'performing', 'computations', 'on', 'the', 'of', 'second', 'level', 'instruction', 'sequences', 'to', 'obtain', 'a', 'final', 'result', 'the', 'of', 'wherein', 'the', 'obtaining', 'by', 'the', 'group', 'controller', 'a', 'of', 'computation', 'types', 'of', 'the', 'of', 'second', 'level', 'instruction', 'sequences', 'obtaining', 'a', 'corresponding', 'fusion', 'computation', 'manner', 'of', 'the', 'computation', 'types', 'according', 'to', 'the', 'of', 'computation', 'types', 'and', 'adopting', 'by', 'the', 'of', 'computing', 'units', 'the', 'fusion', 'computation', 'manner', 'to', 'call', 'the', 'm', 'threads', 'for', 'performing', 'computations', 'on', 'the', 'of', 'second', 'instruction', 'sequences', 'to', 'obtain', 'a', 'final', 'result', 'if', 'the', 'computation', 'types', 'represent', 'computation', 'operations', 'of', 'the', 'same', 'type', 'the', 'group', 'controller', 'calls', 'a', 'combined', 'computation', 'manner', 'in', 'which', 'single', 'instruction', 'multiple', 'data', 'of', 'the', 'same', 'type', 'is', 'in', 'combination', 'with', 'single', 'instruction', 'multiple', 'threads', 'and', 'uses', 'the', 'm', 'threads', 'to', 'perform', 'the', 'combined', 'computation', 'manner', 'to', 'obtain', 'a', 'final', 'result', 'which', 'includes', 'partitioning', 'by', 'the', 'decoding', 'logic', 'the', 'm', 'threads', 'into', 'n', 'wraps', 'for', 'allocating', 'to', 'the', 'the', 'of', 'computing', 'units', 'converting', 'by', 'the', 'group', 'controller', 'the', 'of', 'second', 'instruction', 'sequences', 'into', 'a', 'of', 'second', 'control', 'signals', 'and', 'sending', 'the', 'second', 'control', 'signals', 'to', 'the', 'of', 'computing', 'units', 'calling', 'by', 'the', 'of', 'computing', 'units', 'wraps', 'that', 'are', 'allocated', 'to', 'the', 'computing', 'units', 'and', 'the', 'second', 'control', 'signals', 'to', 'fetch', 'corresponding', 'data', 'according', 'to', 'the', 'independent', 'addressing', 'function', 'performing', 'by', 'the', 'of', 'computing', 'units', 'computations', 'on', 'the', 'data', 'to', 'obtain', 'a', 'of', 'intermediate', 'results', 'and', 'splicing', 'the', 'of', 'intermediate', 'results', 'to', 'obtain', 'a', 'final', 'result', 'the', 'of', 'wherein', 'the', 'obtaining', 'by', 'the', 'group', 'controller', 'a', 'of', 'computation', 'types', 'of', 'the', 'of', 'second', 'level', 'instruction', 'sequences', 'obtaining', 'a', 'corresponding', 'fusion', 'computation', 'manner', 'of', 'the', 'computation', 'types', 'according', 'to', 'the', 'of', 'computation', 'types', 'and', 'adopting', 'by', 'the', 'of', 'computing', 'units', 'the', 'fusion', 'computation', 'manner', 'to', 'call', 'the', 'm', 'threads', 'for', 'performing', 'computations', 'on', 'the', 'of', 'second', 'instruction', 'sequences', 'to', 'obtain', 'a', 'final', 'result', 'if', 'the', 'computation', 'types', 'represent', 'computation', 'operations', 'of', 'different', 'types', 'the', 'group', 'controller', 'calls', 'simultaneous', 'multi-threading', 'and', 'the', 'm', 'threads', 'to', 'perform', 'computations', 'to', 'obtain', 'a', 'final', 'result', 'which', 'includes', 'partitioning', 'by', 'the', 'decoding', 'logic', 'the', 'm', 'threads', 'into', 'n', 'wraps', 'converting', 'the', 'of', 'second', 'instruction', 'sequences', 'into', 'a', 'of', 'second', 'control', 'signals', 'obtaining', 'by', 'the', 'group', 'controller', 'computation', 'types', 'supported', 'by', 'the', 'of', 'computing', 'units', 'allocating', 'by', 'the', 'controller', 'the', 'n', 'wraps', 'and', 'the', 'of', 'second', 'control', 'signals', 'to', 'corresponding', 'computing', 'units', 'that', 'support', 'computation', 'types', 'of', 'the', 'wraps', 'and', 'the', 'second', 'control', 'signals', 'calling', 'by', 'the', 'of', 'computing', 'units', 'wraps', 'that', 'are', 'allocated', 'to', 'the', 'computing', 'units', 'and', 'the', 'second', 'control', 'signals', 'fetching', 'by', 'the', 'of', 'computing', 'units', 'corresponding', 'data', 'performing', 'by', 'the', 'of', 'computing', 'units', 'computations', 'on', 'the', 'data', 'to', 'obtain', 'a', 'of', 'intermediate', 'results', 'and', 'splicing', 'all', 'the', 'intermediate', 'results', 'to', 'obtain', 'a', 'final', 'result', 'the', 'of', 'or', 'further', 'comprising', 'if', 'a', 'wrap', 'a', 'in', 'the', 'of', 'wraps', 'is', 'blocked', 'adding', 'the', 'wrap', 'a', 'to', 'a', 'waiting', 'queue', 'and', 'if', 'data', 'of', 'the', 'wrap', 'a', 'are', 'already', 'fetched', 'adding', 'the', 'wrap', 'a', 'to', 'a', 'preparation', 'queue', 'wherein', 'the', 'preparation', 'queue', 'is', 'a', 'queue', 'where', 'a', 'wrap', 'to', 'be', 'scheduled', 'for', 'executing', 'is', 'located', 'when', 'a', 'computing', 'resource', 'is', 'idle', 'the', 'of', 'wherein', 'the', 'first', 'level', 'instruction', 'sequence', 'includes', 'a', 'very', 'long', 'instruction', 'and', 'the', 'second', 'level', 'instruction', 'sequence', 'includes', 'an', 'instruction', 'sequence', 'the', 'of', 'wherein', 'the', 'computing', 'system', 'further', 'includes', 'a', 'tree', 'module', 'wherein', 'the', 'tree', 'module', 'includes', 'a', 'root', 'port', 'and', 'a', 'of', 'branch', 'ports', 'wherein', 'the', 'root', 'port', 'of', 'the', 'tree', 'module', 'is', 'connected', 'to', 'the', 'group', 'controller', 'and', 'the', 'of', 'branch', 'ports', 'of', 'the', 'tree', 'module', 'are', 'connected', 'to', 'a', 'computing', 'unit', 'of', 'the', 'of', 'computing', 'units', 'respectively', 'and', 'the', 'tree', 'module', 'is', 'configured', 'to', 'forward', 'data', 'blocks', 'wraps', 'or', 'instruction', 'sequences', 'between', 'the', 'group', 'controller', 'and', 'the', 'of', 'computing', 'units', 'the', 'of', 'wherein', 'the', 'tree', 'module', 'is', 'an', 'n-ary', 'tree', 'wherein', 'n', 'is', 'an', 'integer', 'greater', 'than', 'or', 'equal', 'to', 'the', 'of', 'wherein', 'the', 'computing', 'system', 'further', 'includes', 'a', 'branch', 'processing', 'circuit', 'wherein', 'the', 'branch', 'processing', 'circuit', 'is', 'connected', 'between', 'the', 'group', 'controller', 'and', 'the', 'of', 'computing', 'units', 'and', 'the', 'branch', 'processing', 'circuit', 'is', 'configured', 'to', 'forward', 'data', 'wraps', 'or', 'instruction', 'sequences', 'between', 'the', 'group', 'controller', 'and', 'the', 'of', 'computing', 'units', 'a', 'computing', 'system', 'comprising', 'a', 'control', 'unit', 'a', 'computation', 'group', 'and', 'a', 'general', 'storage', 'unit', 'wherein', 'the', 'control', 'unit', 'includes', 'a', 'first', 'memory', 'a', 'decoding', 'logic', 'and', 'a', 'controller', 'the', 'computation', 'group', 'includes', 'a', 'group', 'controller', 'and', 'a', 'of', 'computing', 'units', 'the', 'general', 'storage', 'unit', 'is', 'configured', 'to', 'store', 'data', 'the', 'controller', 'is', 'configured', 'to', 'receive', 'a', 'first', 'level', 'instruction', 'sequence', 'and', 'control', 'the', 'first', 'memory', 'and', 'the', 'decoding', 'logic', 'the', 'decoding', 'logic', 'is', 'configured', 'to', 'partition', 'the', 'first', 'level', 'instruction', 'sequence', 'into', 'a', 'of', 'second', 'level', 'instruction', 'sequences', 'the', 'the', 'controller', 'is', 'further', 'configured', 'to', 'create', 'm', 'threads', 'for', 'the', 'of', 'second', 'level', 'instruction', 'sequences', 'and', 'allocate', 'an', 'independent', 'register', 'and', 'configure', 'an', 'independent', 'addressing', 'function', 'for', 'each', 'thread', 'of', 'the', 'm', 'threads', 'm', 'is', 'an', 'integer', 'greater', 'than', 'or', 'equal', 'to', 'and', 'the', 'controller', 'is', 'further', 'configured', 'to', 'convert', 'the', 'of', 'second', 'instruction', 'sequences', 'into', 'a', 'of', 'control', 'signals', 'for', 'sending', 'to', 'the', 'group', 'controller', 'the', 'group', 'controller', 'is', 'configured', 'to', 'receive', 'the', 'of', 'control', 'signals', 'obtain', 'a', 'of', 'computational', 'types', 'if', 'the', 'of', 'control', 'signals', 'divide', 'the', 'm', 'threads', 'into', 'n', 'wraps', 'and', 'allocate', 'the', 'n', 'wraps', 'and', 'the', 'of', 'control', 'signals', 'to', 'the', 'of', 'computing', 'units', 'according', 'to', 'the', 'of', 'computational', 'types', 'the', 'of', 'computing', 'units', 'are', 'configured', 'to', 'fetch', 'data', 'from', 'the', 'general', 'storage', 'unit', 'through', 'allocated', 'wraps', 'and', 'control', 'signals', 'and', 'perform', 'computations', 'to', 'obtain', 'an', 'intermediate', 'result', 'and', 'the', 'group', 'controller', 'is', 'configured', 'to', 'splice', 'all', 'intermediate', 'results', 'to', 'obtain', 'a', 'final', 'computation', 'result', 'the', 'computing', 'system', 'of', 'wherein', 'the', 'of', 'computing', 'units', 'includes', 'an', 'addition', 'computing', 'unit', 'a', 'multiplication', 'computing', 'unit', 'an', 'activation', 'computing', 'unit', 'or', 'a', 'dedicated', 'computing', 'unit', 'the', 'computing', 'system', 'of', 'wherein', 'the', 'dedicated', 'computing', 'unit', 'includes', 'a', 'face', 'computing', 'unit', 'a', 'graphics', 'computing', 'unit', 'a', 'fingerprint', 'computing', 'unit', 'or', 'a', 'neural', 'network', 'computing', 'unit', 'the', 'computing', 'system', 'of', 'wherein', 'the', 'group', 'controller', 'is', 'configured', 'to', 'if', 'computation', 'types', 'of', 'a', 'of', 'control', 'signals', 'are', 'graphics', 'computations', 'fingerprint', 'identification', 'face', 'or', 'neural', 'network', 'operations', 'allocate', 'the', 'of', 'control', 'signals', 'to', 'the', 'face', 'computing', 'unit', 'the', 'graphics', 'computing', 'unit', 'the', 'fingerprint', 'computing', 'unit', 'or', 'the', 'neural', 'network', 'computing', 'unit', 'respectively', 'the', 'computing', 'system', 'of', 'wherein', 'the', 'first', 'level', 'instruction', 'sequence', 'includes', 'a', 'very', 'long', 'instruction', 'and', 'the', 'second', 'level', 'instruction', 'sequence', 'includes', 'an', 'instruction', 'sequence', 'the', 'computing', 'system', 'of', 'further', 'comprising', 'a', 'tree', 'module', 'wherein', 'the', 'tree', 'module', 'includes', 'a', 'root', 'port', 'and', 'a', 'of', 'branch', 'ports', 'wherein', 'the', 'root', 'port', 'of', 'the', 'tree', 'module', 'is', 'connected', 'to', 'the', 'group', 'controller', 'and', 'the', 'of', 'branch', 'ports', 'of', 'the', 'tree', 'module', 'are', 'connected', 'to', 'a', 'computing', 'unit', 'of', 'the', 'of', 'computing', 'units', 'respectively', 'and', 'the', 'tree', 'module', 'is', 'configured', 'to', 'forward', 'data', 'blocks', 'wraps', 'or', 'instruction', 'sequences', 'between', 'the', 'group', 'controller', 'and', 'the', 'of', 'computing', 'units', 'the', 'computing', 'system', 'of', 'wherein', 'the', 'tree', 'module', 'is', 'an', 'n-ary', 'tree', 'wherein', 'n', 'is', 'an', 'integer', 'greater', 'than', 'or', 'equal', 'to', 'the', 'computing', 'system', 'of', 'wherein', 'the', 'computing', 'system', 'includes', 'a', 'branch', 'processing', 'circuit', 'the', 'branch', 'processing', 'circuit', 'is', 'connected', 'between', 'the', 'group', 'controller', 'and', 'the', 'of', 'computing', 'units', 'and', 'the', 'branch', 'processing', 'circuit', 'is', 'configured', 'to', 'forward', 'data', 'wraps', 'or', 'instruction', 'sequences', 'between', 'the', 'group', 'controller', 'and', 'the', 'of', 'computing', 'units', 'a', 'computer', 'program', 'product', 'comprising', 'a', 'non-instant', 'computer', 'readable', 'storage', 'medium', 'wherein', 'a', 'computer', 'program', 'is', 'stored', 'in', 'the', 'non-instant', 'computer', 'readable', 'storage', 'medium', 'and', 'the', 'computer', 'program', 'is', 'capable', 'of', 'causing', 'a', 'computer', 'to', 'perform', 'the', 'of', 'any', 'of', 's', '-', 'through', 'operations', 'a', 'for', 'detecting', 'body', 'information', 'on', 'one', 'or', 'more', 'passengers', 'of', 'a', 'vehicle', 'based', 'on', 'humans', \"'\", 'status', 'comprising', 'steps', 'of', 'a', 'if', 'at', 'least', 'one', 'interior', 'of', 'an', 'interior', 'of', 'the', 'vehicle', 'is', 'acquired', 'a', 'passenger', 'body', 'information-detecting', 'device', 'performing', 'i', 'a', 'process', 'of', 'inputting', 'the', 'interior', 'into', 'a', 'face', 'network', 'to', 'thereby', 'allow', 'the', 'face', 'network', 'to', 'detect', 'each', 'of', 'of', 'each', 'of', 'the', 'passengers', 'from', 'the', 'interior', 'and', 'thus', 'to', 'output', 'multiple', 'pieces', 'of', 'passenger', 'feature', 'information', 'corresponding', 'to', 'each', 'of', 'the', 'detected', 'and', 'ii', 'a', 'process', 'of', 'inputting', 'the', 'interior', 'into', 'a', 'body', 'network', 'to', 'thereby', 'allow', 'the', 'body', 'network', 'to', 'detect', 'each', 'of', 'bodies', 'of', 'each', 'of', 'the', 'passengers', 'from', 'the', 'interior', 'and', 'thus', 'to', 'output', 'body-part', 'length', 'information', 'of', 'each', 'of', 'the', 'detected', 'bodies', 'and', 'b', 'the', 'passenger', 'body', 'information-detecting', 'device', 'performing', 'a', 'process', 'of', 'retrieving', 'specific', 'height', 'mapping', 'information', 'corresponding', 'to', 'specific', 'passenger', 'feature', 'information', 'on', 'a', 'specific', 'passenger', 'from', 'a', 'height', 'mapping', 'table', 'which', 'stores', 'height', 'mapping', 'information', 'representing', 'respective', 'one', 'or', 'more', 'predetermined', 'ratios', 'of', 'one', 'or', 'more', 'segment', 'body', 'portions', 'of', 'each', 'of', 'human', 'groups', 'to', 'each', 'of', 'heights', 'per', 'each', 'of', 'the', 'human', 'groups', 'a', 'process', 'of', 'acquiring', 'a', 'specific', 'height', 'of', 'the', 'specific', 'passenger', 'from', 'the', 'specific', 'height', 'mapping', 'information', 'by', 'referring', 'to', 'specific', 'body-part', 'length', 'information', 'of', 'the', 'specific', 'passenger', 'a', 'process', 'of', 'retrieving', 'specific', 'weight', 'mapping', 'information', 'corresponding', 'to', 'the', 'specific', 'passenger', 'feature', 'information', 'from', 'a', 'weight', 'mapping', 'table', 'which', 'stores', 'multiple', 'pieces', 'of', 'weight', 'mapping', 'information', 'representing', 'predetermined', 'correlations', 'between', 'each', 'of', 'the', 'heights', 'and', 'each', 'of', 'weights', 'per', 'each', 'of', 'the', 'human', 'groups', 'and', 'a', 'process', 'of', 'acquiring', 'a', 'weight', 'of', 'the', 'specific', 'passenger', 'from', 'the', 'specific', 'weight', 'mapping', 'information', 'by', 'referring', 'to', 'the', 'specific', 'height', 'of', 'the', 'specific', 'passenger', 'the', 'of', 'wherein', 'at', 'the', 'step', 'of', 'a', 'the', 'passenger', 'body', 'information-detecting', 'device', 'performs', 'a', 'process', 'of', 'inputting', 'the', 'interior', 'into', 'the', 'body', 'network', 'to', 'thereby', 'allow', 'the', 'body', 'network', 'to', 'i', 'output', 'one', 'or', 'more', 'with', 'one', 'or', 'more', 'channels', 'corresponding', 'to', 'the', 'interior', 'via', 'a', 'feature', 'extraction', 'network', 'ii', 'generate', 'at', 'least', 'one', 'keypoint', 'heatmap', 'and', 'at', 'least', 'one', 'part', 'affinity', 'field', 'with', 'one', 'or', 'more', 'channels', 'corresponding', 'to', 'each', 'of', 'the', 'via', 'a', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'and', 'iii', 'extract', 'keypoints', 'from', 'the', 'keypoint', 'heatmap', 'via', 'a', 'keypoint', 'detector', 'to', 'group', 'the', 'extracted', 'keypoints', 'by', 'referring', 'to', 'the', 'part', 'affinity', 'field', 'and', 'thus', 'to', 'generate', 'body', 'parts', 'per', 'the', 'passengers', 'and', 'as', 'a', 'result', 'allow', 'the', 'body', 'network', 'to', 'output', 'multiple', 'pieces', 'of', 'body-part', 'length', 'information', 'on', 'each', 'of', 'the', 'passengers', 'by', 'referring', 'to', 'the', 'body', 'parts', 'per', 'the', 'passengers', 'the', 'of', 'wherein', 'the', 'feature', 'extraction', 'network', 'includes', 'at', 'least', 'one', 'convolutional', 'layer', 'and', 'applies', 'at', 'least', 'one', 'convolution', 'operation', 'to', 'the', 'interior', 'to', 'thereby', 'output', 'the', 'the', 'of', 'wherein', 'the', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'includes', 'one', 'of', 'a', 'fully', 'convolutional', 'network', 'and', 'a', '×', 'convolutional', 'layer', 'and', 'applies', 'a', 'fully-convolution', 'operation', 'or', '×', 'convolution', 'operation', 'to', 'the', 'to', 'thereby', 'generate', 'the', 'keypoint', 'heatmap', 'and', 'the', 'part', 'affinity', 'field', 'the', 'of', 'wherein', 'the', 'keypoint', 'detector', 'connects', 'by', 'referring', 'to', 'the', 'part', 'affinity', 'field', 'pairs', 'respectively', 'having', 'highest', 'mutual', 'connection', 'probabilities', 'of', 'being', 'connected', 'among', 'the', 'extracted', 'keypoints', 'to', 'thereby', 'group', 'the', 'extracted', 'keypoints', 'the', 'of', 'wherein', 'the', 'feature', 'extraction', 'network', 'and', 'the', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'have', 'been', 'learned', 'by', 'a', 'learning', 'device', 'performing', 'i', 'a', 'process', 'of', 'inputting', 'at', 'least', 'one', 'training', 'including', 'one', 'or', 'more', 'objects', 'for', 'training', 'into', 'the', 'feature', 'extraction', 'network', 'to', 'thereby', 'allow', 'the', 'feature', 'extraction', 'network', 'to', 'generate', 'one', 'or', 'more', 'for', 'training', 'having', 'one', 'or', 'more', 'channels', 'by', 'applying', 'at', 'least', 'one', 'convolutional', 'operation', 'to', 'the', 'training', 'ii', 'a', 'process', 'of', 'inputting', 'the', 'for', 'training', 'into', 'the', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'to', 'thereby', 'allow', 'the', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'to', 'generate', 'one', 'or', 'more', 'keypoint', 'heatmaps', 'for', 'training', 'and', 'one', 'or', 'more', 'part', 'affinity', 'fields', 'for', 'training', 'having', 'one', 'or', 'more', 'channels', 'for', 'each', 'of', 'the', 'for', 'training', 'iii', 'a', 'process', 'of', 'inputting', 'the', 'keypoint', 'heatmaps', 'for', 'training', 'and', 'the', 'part', 'affinity', 'fields', 'for', 'training', 'into', 'the', 'keypoint', 'detector', 'to', 'thereby', 'allow', 'the', 'keypoint', 'detector', 'to', 'extract', 'keypoints', 'for', 'training', 'from', 'each', 'of', 'the', 'keypoint', 'heatmaps', 'for', 'training', 'and', 'a', 'process', 'of', 'grouping', 'the', 'extracted', 'keypoints', 'for', 'training', 'by', 'referring', 'to', 'each', 'of', 'the', 'part', 'affinity', 'fields', 'for', 'training', 'to', 'thereby', 'detect', 'keypoints', 'per', 'each', 'of', 'the', 'objects', 'for', 'training', 'and', 'iv', 'a', 'process', 'of', 'allowing', 'a', 'loss', 'layer', 'to', 'calculate', 'one', 'or', 'more', 'losses', 'by', 'referring', 'to', 'the', 'keypoints', 'per', 'each', 'of', 'the', 'objects', 'for', 'training', 'and', 'their', 'corresponding', 'ground', 'truths', 'to', 'thereby', 'adjust', 'one', 'or', 'more', 'parameters', 'of', 'the', 'feature', 'extraction', 'network', 'and', 'the', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'such', 'that', 'the', 'losses', 'are', 'minimized', 'by', 'backpropagation', 'using', 'the', 'losses', 'the', 'of', 'wherein', 'at', 'the', 'step', 'of', 'a', 'the', 'passenger', 'body', 'information-detecting', 'device', 'performs', 'a', 'process', 'of', 'inputting', 'the', 'interior', 'into', 'the', 'face', 'network', 'to', 'thereby', 'allow', 'the', 'face', 'network', 'to', 'detect', 'each', 'of', 'the', 'of', 'each', 'of', 'the', 'passengers', 'located', 'in', 'the', 'interior', 'via', 'a', 'face', 'detector', 'and', 'to', 'output', 'multiple', 'pieces', 'of', 'the', 'passenger', 'feature', 'information', 'on', 'each', 'of', 'the', 'facial', 's', 'via', 'a', 'facial', 'feature', 'classifier', 'the', 'of', 'wherein', 'at', 'the', 'step', 'of', 'a', 'the', 'passenger', 'body', 'information-detecting', 'device', 'performs', 'a', 'process', 'of', 'inputting', 'the', 'interior', 'into', 'the', 'face', 'network', 'to', 'thereby', 'allow', 'the', 'face', 'network', 'to', 'i', 'apply', 'at', 'least', 'one', 'convolution', 'operation', 'to', 'the', 'interior', 'and', 'thus', 'to', 'output', 'at', 'least', 'one', 'feature', 'map', 'corresponding', 'to', 'the', 'interior', 'via', 'at', 'least', 'one', 'convolutional', 'layer', 'ii', 'output', 'one', 'or', 'more', 'proposal', 'boxes', 'where', 'the', 'passengers', 'are', 'estimated', 'as', 'located', 'on', 'the', 'feature', 'map', 'via', 'a', 'region', 'proposal', 'network', 'iii', 'apply', 'pooling', 'operation', 'to', 'one', 'or', 'more', 'regions', 'corresponding', 'to', 'the', 'proposal', 'boxes', 'on', 'the', 'feature', 'map', 'and', 'thus', 'to', 'output', 'at', 'least', 'one', 'feature', 'vector', 'via', 'a', 'pooling', 'layer', 'and', 'iv', 'apply', 'fully-connected', 'operation', 'to', 'the', 'feature', 'vector', 'and', 'thus', 'to', 'output', 'the', 'multiple', 'pieces', 'of', 'the', 'passenger', 'feature', 'information', 'corresponding', 'to', 'each', 'of', 'the', 'of', 'each', 'of', 'the', 'passengers', 'corresponding', 'to', 'each', 'of', 'the', 'proposal', 'boxes', 'via', 'a', 'fully', 'connected', 'layer', 'the', 'of', 'wherein', 'the', 'multiple', 'pieces', 'of', 'the', 'passenger', 'feature', 'information', 'include', 'each', 'of', 'ages', 'each', 'of', 'genders', 'and', 'each', 'of', 'races', 'corresponding', 'to', 'each', 'of', 'the', 'passengers', 'a', 'passenger', 'body', 'information-detecting', 'device', 'for', 'detecting', 'body', 'information', 'on', 'one', 'or', 'more', 'passengers', 'of', 'a', 'vehicle', 'based', 'on', 'humans', \"'\", 'status', 'comprising', 'at', 'least', 'one', 'memory', 'that', 'stores', 'instructions', 'and', 'at', 'least', 'one', 'configured', 'to', 'execute', 'the', 'instructions', 'to', 'perform', 'or', 'support', 'another', 'device', 'to', 'perform', 'i', 'if', 'at', 'least', 'one', 'interior', 'of', 'an', 'interior', 'of', 'the', 'vehicle', 'is', 'acquired', 'i', 'a', 'process', 'of', 'inputting', 'the', 'interior', 'into', 'a', 'face', 'network', 'to', 'thereby', 'allow', 'the', 'face', 'network', 'to', 'detect', 'each', 'of', 'of', 'each', 'of', 'the', 'passengers', 'from', 'the', 'interior', 'and', 'thus', 'to', 'output', 'multiple', 'pieces', 'of', 'passenger', 'feature', 'information', 'corresponding', 'to', 'each', 'of', 'the', 'detected', 'and', 'ii', 'a', 'process', 'of', 'inputting', 'the', 'interior', 'into', 'a', 'body', 'network', 'to', 'thereby', 'allow', 'the', 'body', 'network', 'to', 'detect', 'each', 'of', 'bodies', 'of', 'each', 'of', 'the', 'passengers', 'from', 'the', 'interior', 'and', 'thus', 'to', 'output', 'body-part', 'length', 'information', 'of', 'each', 'of', 'the', 'detected', 'bodies', 'and', 'ii', 'a', 'process', 'of', 'retrieving', 'specific', 'height', 'mapping', 'information', 'corresponding', 'to', 'specific', 'passenger', 'feature', 'information', 'on', 'a', 'specific', 'passenger', 'from', 'a', 'height', 'mapping', 'table', 'which', 'stores', 'height', 'mapping', 'information', 'representing', 'respective', 'one', 'or', 'more', 'predetermined', 'ratios', 'of', 'one', 'or', 'more', 'segment', 'body', 'portions', 'of', 'each', 'of', 'human', 'groups', 'to', 'each', 'of', 'heights', 'per', 'each', 'of', 'the', 'human', 'groups', 'a', 'process', 'of', 'acquiring', 'a', 'specific', 'height', 'of', 'the', 'specific', 'passenger', 'from', 'the', 'specific', 'height', 'mapping', 'information', 'by', 'referring', 'to', 'specific', 'body-part', 'length', 'information', 'of', 'the', 'specific', 'passenger', 'a', 'process', 'of', 'retrieving', 'specific', 'weight', 'mapping', 'information', 'corresponding', 'to', 'the', 'specific', 'passenger', 'feature', 'information', 'from', 'a', 'weight', 'mapping', 'table', 'which', 'stores', 'multiple', 'pieces', 'of', 'weight', 'mapping', 'information', 'representing', 'predetermined', 'correlations', 'between', 'each', 'of', 'the', 'heights', 'and', 'each', 'of', 'weights', 'per', 'each', 'of', 'the', 'human', 'groups', 'and', 'a', 'process', 'of', 'acquiring', 'a', 'weight', 'of', 'the', 'specific', 'passenger', 'from', 'the', 'specific', 'weight', 'mapping', 'information', 'by', 'referring', 'to', 'the', 'specific', 'height', 'of', 'the', 'specific', 'passenger', 'the', 'passenger', 'body', 'information-detecting', 'device', 'of', 'wherein', 'at', 'the', 'process', 'of', 'i', 'the', 'performs', 'a', 'process', 'of', 'inputting', 'the', 'interior', 'into', 'the', 'body', 'network', 'to', 'thereby', 'allow', 'the', 'body', 'network', 'to', 'i', 'output', 'one', 'or', 'more', 'with', 'one', 'or', 'more', 'channels', 'corresponding', 'to', 'the', 'interior', 'via', 'a', 'feature', 'extraction', 'network', 'ii', 'generate', 'at', 'least', 'one', 'keypoint', 'heatmap', 'and', 'at', 'least', 'one', 'part', 'affinity', 'field', 'with', 'one', 'or', 'more', 'channels', 'corresponding', 'to', 'each', 'of', 'the', 'via', 'a', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'and', 'iii', 'extract', 'keypoints', 'from', 'the', 'keypoint', 'heatmap', 'via', 'a', 'keypoint', 'detector', 'to', 'group', 'the', 'extracted', 'keypoints', 'by', 'referring', 'to', 'the', 'part', 'affinity', 'field', 'and', 'thus', 'to', 'generate', 'body', 'parts', 'per', 'the', 'passengers', 'and', 'as', 'a', 'result', 'allow', 'the', 'body', 'network', 'to', 'output', 'multiple', 'pieces', 'of', 'body-part', 'length', 'information', 'on', 'each', 'of', 'the', 'passengers', 'by', 'referring', 'to', 'the', 'body', 'parts', 'per', 'the', 'passengers', 'the', 'passenger', 'body', 'information-detecting', 'device', 'of', 'wherein', 'the', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'includes', 'one', 'of', 'a', 'fully', 'convolutional', 'network', 'and', 'a', '×', 'convolutional', 'layer', 'and', 'applies', 'a', 'fully-convolution', 'operation', 'or', '×', 'convolution', 'operation', 'to', 'the', 'to', 'thereby', 'generate', 'the', 'keypoint', 'heatmap', 'and', 'the', 'part', 'affinity', 'field', 'the', 'passenger', 'body', 'information-detecting', 'device', 'of', 'wherein', 'the', 'keypoint', 'detector', 'connects', 'by', 'referring', 'to', 'the', 'part', 'affinity', 'field', 'pairs', 'respectively', 'having', 'highest', 'mutual', 'connection', 'probabilities', 'of', 'being', 'connected', 'among', 'the', 'extracted', 'keypoints', 'to', 'thereby', 'group', 'the', 'extracted', 'keypoints', 'the', 'passenger', 'body', 'information-detecting', 'device', 'of', 'wherein', 'the', 'feature', 'extraction', 'network', 'and', 'the', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'have', 'been', 'learned', 'by', 'a', 'learning', 'device', 'performing', 'i', 'a', 'process', 'of', 'inputting', 'at', 'least', 'one', 'training', 'including', 'one', 'or', 'more', 'objects', 'for', 'training', 'into', 'the', 'feature', 'extraction', 'network', 'to', 'thereby', 'allow', 'the', 'feature', 'extraction', 'network', 'to', 'generate', 'one', 'or', 'more', 'for', 'training', 'having', 'one', 'or', 'more', 'channels', 'by', 'applying', 'at', 'least', 'one', 'convolutional', 'operation', 'to', 'the', 'training', 'ii', 'a', 'process', 'of', 'inputting', 'the', 'for', 'training', 'into', 'the', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'to', 'thereby', 'allow', 'the', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'to', 'generate', 'one', 'or', 'more', 'keypoint', 'heatmaps', 'for', 'training', 'and', 'one', 'or', 'more', 'part', 'affinity', 'fields', 'for', 'training', 'having', 'one', 'or', 'more', 'channels', 'for', 'each', 'of', 'the', 'for', 'training', 'iii', 'a', 'process', 'of', 'inputting', 'the', 'keypoint', 'heatmaps', 'for', 'training', 'and', 'the', 'part', 'affinity', 'fields', 'for', 'training', 'into', 'the', 'keypoint', 'detector', 'to', 'thereby', 'allow', 'the', 'keypoint', 'detector', 'to', 'extract', 'keypoints', 'for', 'training', 'from', 'each', 'of', 'the', 'keypoint', 'heatmaps', 'for', 'training', 'and', 'a', 'process', 'of', 'grouping', 'the', 'extracted', 'keypoints', 'for', 'training', 'by', 'referring', 'to', 'each', 'of', 'the', 'part', 'affinity', 'fields', 'for', 'training', 'to', 'thereby', 'detect', 'keypoints', 'per', 'each', 'of', 'the', 'objects', 'for', 'training', 'and', 'iv', 'a', 'process', 'of', 'allowing', 'a', 'loss', 'layer', 'to', 'calculate', 'one', 'or', 'more', 'losses', 'by', 'referring', 'to', 'the', 'keypoints', 'per', 'each', 'of', 'the', 'objects', 'for', 'training', 'and', 'their', 'corresponding', 'ground', 'truths', 'to', 'thereby', 'adjust', 'one', 'or', 'more', 'parameters', 'of', 'the', 'feature', 'extraction', 'network', 'and', 'the', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'such', 'that', 'the', 'losses', 'are', 'minimized', 'by', 'backpropagation', 'using', 'the', 'losses', 'the', 'passenger', 'body', 'information-detecting', 'device', 'of', 'wherein', 'at', 'the', 'process', 'of', 'i', 'the', 'performs', 'a', 'process', 'of', 'inputting', 'the', 'interior', 'into', 'the', 'face', 'network', 'to', 'thereby', 'allow', 'the', 'face', 'network', 'to', 'i', 'apply', 'at', 'least', 'one', 'convolution', 'operation', 'to', 'the', 'interior', 'and', 'thus', 'to', 'output', 'at', 'least', 'one', 'feature', 'map', 'corresponding', 'to', 'the', 'interior', 'via', 'at', 'least', 'one', 'convolutional', 'layer', 'ii', 'output', 'one', 'or', 'more', 'proposal', 'boxes', 'where', 'the', 'passengers', 'are', 'estimated', 'as', 'located', 'on', 'the', 'feature', 'map', 'via', 'a', 'region', 'proposal', 'network', 'iii', 'apply', 'pooling', 'operation', 'to', 'one', 'or', 'more', 'regions', 'corresponding', 'to', 'the', 'proposal', 'boxes', 'on', 'the', 'feature', 'map', 'and', 'thus', 'to', 'output', 'at', 'least', 'one', 'feature', 'vector', 'via', 'a', 'pooling', 'layer', 'and', 'iv', 'apply', 'fully-connected', 'operation', 'to', 'the', 'feature', 'vector', 'and', 'thus', 'to', 'output', 'the', 'multiple', 'pieces', 'of', 'the', 'passenger', 'feature', 'information', 'corresponding', 'to', 'each', 'of', 'the', 'of', 'each', 'of', 'the', 'passengers', 'corresponding', 'to', 'each', 'of', 'the', 'proposal', 'boxes', 'via', 'a', 'fully', 'connected', 'layer', 'a', 'computer', 'implemented', 'for', 'performing', 'video', 'coding', 'based', 'on', 'face', 'detection', 'comprising', 'receiving', 'a', 'video', 'frame', 'comprising', 'one', 'of', 'a', 'of', 'video', 'frames', 'of', 'a', 'video', 'sequence', 'determining', 'the', 'video', 'frame', 'is', 'a', 'key', 'frame', 'of', 'the', 'video', 'sequence', 'performing', 'in', 'response', 'to', 'the', 'video', 'frame', 'being', 'a', 'key', 'frame', 'of', 'the', 'video', 'sequence', 'a', 'multi-stage', 'facial', 'search', 'of', 'the', 'video', 'frame', 'based', 'on', 'predetermined', 'feature', 'templates', 'and', 'a', 'predetermined', 'number', 'of', 'stages', 'to', 'determine', 'a', 'first', 'candidate', 'face', 'region', 'and', 'a', 'second', 'candidate', 'face', 'region', 'in', 'the', 'video', 'frame', 'testing', 'the', 'first', 'and', 'second', 'candidate', 'face', 'regions', 'based', 'on', 'skin', 'tone', 'information', 'to', 'determine', 'the', 'first', 'candidate', 'face', 'region', 'is', 'a', 'valid', 'face', 'region', 'and', 'the', 'second', 'candidate', 'face', 'region', 'is', 'an', 'invalid', 'face', 'region', 'rejecting', 'the', 'second', 'candidate', 'face', 'region', 'and', 'outputting', 'the', 'first', 'candidate', 'face', 'region', 'and', 'encoding', 'the', 'video', 'frame', 'based', 'at', 'least', 'in', 'part', 'on', 'the', 'first', 'candidate', 'face', 'region', 'being', 'a', 'valid', 'face', 'region', 'to', 'generate', 'a', 'coded', 'bitstream', 'the', 'of', 'wherein', 'the', 'skin', 'tone', 'information', 'comprises', 'a', 'skin', 'probability', 'map', 'the', 'of', 'wherein', 'said', 'testing', 'the', 'first', 'and', 'second', 'candidate', 'face', 'regions', 'based', 'on', 'skin', 'tone', 'information', 'is', 'performed', 'in', 'response', 'to', 'the', 'video', 'frame', 'being', 'a', 'key', 'frame', 'of', 'the', 'video', 'sequence', 'the', 'of', 'wherein', 'the', 'first', 'candidate', 'face', 'region', 'comprises', 'a', 'rectangular', 'region', 'the', 'further', 'comprising', 'determining', 'a', 'free', 'form', 'shape', 'face', 'region', 'corresponding', 'to', 'the', 'first', 'candidate', 'face', 'region', 'wherein', 'the', 'free', 'form', 'shape', 'face', 'region', 'has', 'at', 'least', 'one', 'of', 'a', 'pixel', 'accuracy', 'or', 'a', 'small', 'block', 'of', 'pixels', 'accuracy', 'the', 'of', 'wherein', 'determining', 'the', 'free', 'form', 'shape', 'face', 'region', 'comprises', 'generating', 'an', 'enhanced', 'skip', 'probability', 'map', 'corresponding', 'to', 'the', 'first', 'candidate', 'face', 'region', 'binarizing', 'the', 'enhanced', 'skip', 'probability', 'map', 'and', 'overlaying', 'the', 'binarized', 'enhanced', 'skip', 'probability', 'map', 'over', 'at', 'least', 'a', 'portion', 'of', 'the', 'video', 'frame', 'to', 'provide', 'the', 'free', 'form', 'shape', 'face', 'region', 'the', 'of', 'wherein', 'a', 'second', 'video', 'frame', 'comprises', 'a', 'non-key', 'frame', 'of', 'the', 'video', 'sequence', 'the', 'further', 'comprising', 'performing', 'face', 'detection', 'in', 'the', 'second', 'video', 'frame', 'of', 'the', 'video', 'sequence', 'based', 'on', 'the', 'free', 'form', 'shape', 'face', 'region', 'the', 'of', 'further', 'comprising', 'a', 'second', 'free', 'form', 'shape', 'face', 'region', 'in', 'the', 'second', 'video', 'frame', 'based', 'on', 'the', 'free', 'form', 'shape', 'face', 'region', 'in', 'the', 'video', 'frame', 'the', 'of', 'wherein', 'the', 'second', 'free', 'form', 'shape', 'face', 'region', 'comprises', 'determining', 'a', 'location', 'of', 'a', 'second', 'valid', 'face', 'region', 'in', 'the', 'second', 'video', 'frame', 'based', 'on', 'a', 'displacement', 'offset', 'with', 'respect', 'to', 'the', 'first', 'candidate', 'face', 'region', 'the', 'of', 'further', 'comprising', 'determining', 'the', 'displacement', 'offset', 'based', 'on', 'an', 'offset', 'between', 'a', 'centroid', 'of', 'a', 'bounding', 'box', 'around', 'a', 'skin', 'enhanced', 'region', 'corresponding', 'to', 'the', 'first', 'candidate', 'face', 'region', 'and', 'a', 'second', 'centroid', 'of', 'a', 'second', 'bounding', 'box', 'around', 'a', 'second', 'skin', 'enhanced', 'region', 'in', 'the', 'second', 'video', 'frame', 'the', 'of', 'wherein', 'encoding', 'the', 'video', 'frame', 'based', 'at', 'least', 'in', 'part', 'on', 'the', 'first', 'candidate', 'face', 'region', 'being', 'a', 'valid', 'face', 'region', 'comprises', 'at', 'least', 'one', 'of', 'reducing', 'a', 'quantization', 'parameter', 'corresponding', 'to', 'the', 'first', 'candidate', 'face', 'region', 'adjusting', 'a', 'lambda', 'value', 'for', 'the', 'first', 'candidate', 'face', 'region', 'or', 'disabling', 'skip', 'coding', 'for', 'the', 'first', 'candidate', 'face', 'region', 'the', 'of', 'wherein', 'the', 'bitstream', 'comprises', 'at', 'least', 'one', 'of', 'an', 'hadvanced', 'video', 'coding', 'avc', 'compliant', 'bitstream', 'an', 'hhigh', 'efficiency', 'video', 'coding', 'hevc', 'compliant', 'bitstream', 'a', 'vp', 'compliant', 'bitstream', 'a', 'vp', 'compliant', 'bitstream', 'or', 'an', 'alliance', 'for', 'open', 'media', 'aom', 'av', 'compliant', 'bitstream', 'a', 'computer', 'implemented', 'for', 'performing', 'face', 'detection', 'comprising', 'receiving', 'a', 'video', 'frame', 'of', 'a', 'sequence', 'of', 'video', 'frames', 'performing', 'a', 'multi-stage', 'facial', 'search', 'of', 'the', 'video', 'frame', 'based', 'on', 'predetermined', 'feature', 'templates', 'and', 'a', 'predetermined', 'number', 'of', 'stages', 'to', 'determine', 'a', 'first', 'candidate', 'face', 'region', 'and', 'a', 'second', 'candidate', 'face', 'region', 'in', 'the', 'video', 'frame', 'testing', 'the', 'first', 'and', 'second', 'candidate', 'face', 'regions', 'based', 'on', 'skin', 'tone', 'information', 'to', 'determine', 'the', 'first', 'candidate', 'face', 'region', 'is', 'a', 'valid', 'face', 'region', 'and', 'the', 'second', 'candidate', 'face', 'region', 'is', 'an', 'invalid', 'face', 'region', 'rejecting', 'the', 'second', 'candidate', 'face', 'region', 'and', 'outputting', 'the', 'first', 'candidate', 'face', 'region', 'as', 'a', 'valid', 'face', 'region', 'for', 'further', 'processing', 'and', 'providing', 'an', 'index', 'indicative', 'of', 'a', 'person', 'being', 'present', 'in', 'the', 'video', 'frame', 'based', 'on', 'the', 'valid', 'face', 'region', 'the', 'of', 'wherein', 'the', 'sequence', 'of', 'video', 'frames', 'comprises', 'a', 'sequence', 'of', 'surveillance', 'video', 'frames', 'the', 'further', 'comprising', 'performing', 'face', 'in', 'the', 'surveillance', 'video', 'frames', 'based', 'on', 'the', 'valid', 'face', 'region', 'the', 'of', 'wherein', 'the', 'sequence', 'of', 'video', 'frames', 'comprises', 'a', 'sequence', 'of', 'decoded', 'video', 'frames', 'the', 'further', 'comprising', 'adding', 'a', 'marker', 'corresponding', 'to', 'the', 'received', 'video', 'frame', 'to', 'perform', 'face', 'on', 'the', 'received', 'video', 'frame', 'based', 'on', 'the', 'valid', 'face', 'region', 'the', 'of', 'wherein', 'the', 'sequence', 'of', 'video', 'frames', 'is', 'received', 'during', 'a', 'device', 'login', 'attempt', 'the', 'further', 'comprising', 'performing', 'face', 'based', 'on', 'the', 'valid', 'face', 'region', 'and', 'allowing', 'access', 'to', 'the', 'device', 'if', 'a', 'secured', 'face', 'is', 'recognized', 'the', 'of', 'wherein', 'the', 'sequence', 'of', 'video', 'frames', 'comprises', 'a', 'sequence', 'of', 'videoconferencing', 'frames', 'the', 'further', 'comprising', 'encoding', 'the', 'video', 'frame', 'based', 'at', 'least', 'in', 'part', 'on', 'the', 'valid', 'face', 'region', 'to', 'generate', 'a', 'coded', 'bitstream', 'the', 'of', 'wherein', 'encoding', 'the', 'video', 'frame', 'comprises', 'not', 'encoding', 'a', 'background', 'region', 'of', 'the', 'video', 'frame', 'into', 'the', 'bitstream', 'the', 'of', 'further', 'comprising', 'encoding', 'the', 'video', 'frame', 'based', 'at', 'least', 'in', 'part', 'on', 'the', 'valid', 'face', 'region', 'to', 'generate', 'a', 'coded', 'bitstream', 'wherein', 'encoding', 'the', 'video', 'frame', 'comprises', 'including', 'metadata', 'corresponding', 'to', 'the', 'valid', 'face', 'region', 'in', 'the', 'bitstream', 'the', 'of', 'further', 'comprising', 'decoding', 'the', 'coded', 'bitstream', 'to', 'generate', 'a', 'decoded', 'video', 'frame', 'and', 'to', 'determine', 'the', 'metadata', 'corresponding', 'to', 'the', 'valid', 'face', 'region', 'in', 'the', 'bitstream', 'the', 'of', 'further', 'comprising', 'at', 'least', 'one', 'of', 'replacing', 'the', 'valid', 'face', 'region', 'based', 'on', 'the', 'decoded', 'metadata', 'cropping', 'and', 'displaying', 'data', 'corresponding', 'only', 'to', 'the', 'valid', 'face', 'region', 'based', 'on', 'the', 'decoded', 'metadata', 'or', 'indexing', 'the', 'decoded', 'video', 'frame', 'based', 'on', 'the', 'decoded', 'metadata', 'a', 'system', 'for', 'performing', 'video', 'coding', 'based', 'on', 'face', 'detection', 'comprising', 'a', 'memory', 'configured', 'to', 'store', 'a', 'video', 'frame', 'comprising', 'one', 'of', 'a', 'of', 'video', 'frames', 'of', 'a', 'video', 'sequence', 'and', 'a', 'coupled', 'to', 'the', 'memory', 'the', 'to', 'receive', 'the', 'video', 'frame', 'to', 'determine', 'the', 'video', 'frame', 'is', 'a', 'key', 'frame', 'of', 'the', 'video', 'sequence', 'to', 'perform', 'in', 'response', 'to', 'the', 'video', 'frame', 'being', 'a', 'key', 'frame', 'of', 'the', 'video', 'sequence', 'a', 'multi-stage', 'facial', 'search', 'of', 'the', 'video', 'frame', 'based', 'on', 'predetermined', 'feature', 'templates', 'and', 'a', 'predetermined', 'number', 'of', 'stages', 'to', 'determine', 'a', 'first', 'candidate', 'face', 'region', 'and', 'a', 'second', 'candidate', 'face', 'region', 'in', 'the', 'video', 'frame', 'to', 'test', 'the', 'first', 'and', 'second', 'candidate', 'face', 'regions', 'based', 'on', 'skin', 'tone', 'information', 'to', 'determine', 'the', 'first', 'candidate', 'face', 'region', 'is', 'a', 'valid', 'face', 'region', 'and', 'the', 'second', 'candidate', 'face', 'region', 'is', 'an', 'invalid', 'face', 'region', 'to', 'reject', 'the', 'second', 'candidate', 'face', 'region', 'and', 'outputting', 'the', 'first', 'candidate', 'face', 'region', 'and', 'to', 'encode', 'the', 'video', 'frame', 'based', 'at', 'least', 'in', 'part', 'on', 'the', 'first', 'candidate', 'face', 'region', 'being', 'a', 'valid', 'face', 'region', 'to', 'generate', 'a', 'coded', 'bitstream', 'the', 'system', 'of', 'wherein', 'the', 'skin', 'tone', 'information', 'comprises', 'a', 'skin', 'probability', 'map', 'the', 'system', 'of', 'wherein', 'the', 'first', 'candidate', 'face', 'region', 'comprises', 'a', 'rectangular', 'region', 'the', 'further', 'to', 'determine', 'a', 'free', 'form', 'shape', 'face', 'region', 'corresponding', 'to', 'the', 'first', 'candidate', 'face', 'region', 'wherein', 'the', 'free', 'form', 'shape', 'face', 'region', 'has', 'at', 'least', 'one', 'of', 'a', 'pixel', 'accuracy', 'or', 'a', 'small', 'block', 'of', 'pixels', 'accuracy', 'the', 'system', 'of', 'wherein', 'the', 'to', 'determine', 'the', 'free', 'form', 'shape', 'face', 'region', 'comprises', 'the', 'to', 'generate', 'an', 'enhanced', 'skip', 'probability', 'map', 'corresponding', 'to', 'the', 'first', 'candidate', 'face', 'region', 'to', 'binarize', 'the', 'enhanced', 'skip', 'probability', 'map', 'and', 'to', 'overlay', 'the', 'binarized', 'enhanced', 'skip', 'probability', 'map', 'over', 'at', 'least', 'a', 'portion', 'of', 'the', 'video', 'frame', 'to', 'provide', 'the', 'free', 'form', 'shape', 'face', 'region', 'the', 'system', 'of', 'wherein', 'a', 'second', 'video', 'frame', 'comprises', 'a', 'non-key', 'frame', 'of', 'the', 'video', 'sequence', 'and', 'the', 'is', 'further', 'to', 'perform', 'face', 'detection', 'in', 'the', 'second', 'video', 'frame', 'of', 'the', 'video', 'sequence', 'based', 'on', 'the', 'free', 'form', 'shape', 'face', 'region', 'the', 'system', 'of', 'wherein', 'the', 'is', 'further', 'to', 'track', 'a', 'second', 'free', 'form', 'shape', 'face', 'region', 'in', 'the', 'second', 'video', 'frame', 'based', 'on', 'the', 'free', 'form', 'shape', 'face', 'region', 'in', 'the', 'video', 'frame', 'the', 'system', 'of', 'wherein', 'to', 'encode', 'the', 'video', 'frame', 'based', 'at', 'least', 'in', 'part', 'on', 'the', 'first', 'candidate', 'face', 'region', 'being', 'a', 'valid', 'face', 'region', 'comprises', 'the', 'to', 'reduce', 'a', 'quantization', 'parameter', 'corresponding', 'to', 'the', 'first', 'candidate', 'face', 'region', 'adjust', 'a', 'lambda', 'value', 'for', 'the', 'first', 'candidate', 'face', 'region', 'or', 'disable', 'skip', 'coding', 'for', 'the', 'first', 'candidate', 'face', 'region', 'at', 'least', 'one', 'non-transitory', 'machine', 'readable', 'medium', 'comprising', 'a', 'of', 'instructions', 'that', 'in', 'response', 'to', 'being', 'executed', 'on', 'a', 'device', 'cause', 'the', 'device', 'to', 'perform', 'video', 'coding', 'based', 'on', 'face', 'detection', 'by', 'receiving', 'a', 'video', 'frame', 'comprising', 'one', 'of', 'a', 'of', 'video', 'frames', 'of', 'a', 'video', 'sequence', 'determining', 'the', 'video', 'frame', 'is', 'a', 'key', 'frame', 'of', 'the', 'video', 'sequence', 'performing', 'in', 'response', 'to', 'the', 'video', 'frame', 'being', 'a', 'key', 'frame', 'of', 'the', 'video', 'sequence', 'a', 'multi-stage', 'facial', 'search', 'of', 'the', 'video', 'frame', 'based', 'on', 'predetermined', 'feature', 'templates', 'and', 'a', 'predetermined', 'number', 'of', 'stages', 'to', 'determine', 'a', 'first', 'candidate', 'face', 'region', 'and', 'a', 'second', 'candidate', 'face', 'region', 'in', 'the', 'video', 'frame', 'testing', 'the', 'first', 'and', 'second', 'candidate', 'face', 'regions', 'based', 'on', 'skin', 'tone', 'information', 'to', 'determine', 'the', 'first', 'candidate', 'face', 'region', 'is', 'a', 'valid', 'face', 'region', 'and', 'the', 'second', 'candidate', 'face', 'region', 'is', 'an', 'invalid', 'face', 'region', 'rejecting', 'the', 'second', 'candidate', 'face', 'region', 'and', 'outputting', 'the', 'first', 'candidate', 'face', 'region', 'and', 'encoding', 'the', 'video', 'frame', 'based', 'at', 'least', 'in', 'part', 'on', 'the', 'first', 'candidate', 'face', 'region', 'being', 'a', 'valid', 'face', 'region', 'to', 'generate', 'a', 'coded', 'bitstream', 'the', 'non-transitory', 'machine', 'readable', 'medium', 'of', 'wherein', 'the', 'skin', 'tone', 'information', 'comprises', 'a', 'skin', 'probability', 'map', 'the', 'non-transitory', 'machine', 'readable', 'medium', 'of', 'wherein', 'the', 'first', 'candidate', 'face', 'region', 'comprises', 'a', 'rectangular', 'region', 'the', 'machine', 'readable', 'medium', 'comprising', 'further', 'instructions', 'that', 'in', 'response', 'to', 'being', 'executed', 'on', 'the', 'device', 'cause', 'the', 'device', 'to', 'perform', 'video', 'coding', 'based', 'on', 'face', 'detection', 'by', 'determining', 'a', 'free', 'form', 'shape', 'face', 'region', 'corresponding', 'to', 'the', 'first', 'candidate', 'face', 'region', 'wherein', 'the', 'free', 'form', 'shape', 'face', 'region', 'has', 'at', 'least', 'one', 'of', 'a', 'pixel', 'accuracy', 'or', 'a', 'small', 'block', 'of', 'pixels', 'accuracy', 'the', 'non-transitory', 'machine', 'readable', 'medium', 'of', 'wherein', 'determining', 'the', 'free', 'form', 'shape', 'face', 'region', 'comprises', 'generating', 'an', 'enhanced', 'skip', 'probability', 'map', 'corresponding', 'to', 'the', 'first', 'candidate', 'face', 'region', 'binarizing', 'the', 'enhanced', 'skip', 'probability', 'map', 'and', 'overlaying', 'the', 'binarized', 'enhanced', 'skip', 'probability', 'map', 'over', 'at', 'least', 'a', 'portion', 'of', 'the', 'video', 'frame', 'to', 'provide', 'the', 'free', 'form', 'shape', 'face', 'region', 'the', 'non-transitory', 'machine', 'readable', 'medium', 'of', 'wherein', 'a', 'second', 'video', 'frame', 'comprises', 'a', 'non-key', 'frame', 'of', 'the', 'video', 'sequence', 'the', 'machine', 'readable', 'medium', 'comprising', 'further', 'instructions', 'that', 'in', 'response', 'to', 'being', 'executed', 'on', 'the', 'device', 'cause', 'the', 'device', 'to', 'perform', 'video', 'coding', 'based', 'on', 'face', 'detection', 'by', 'performing', 'face', 'detection', 'in', 'the', 'second', 'video', 'frame', 'of', 'the', 'video', 'sequence', 'based', 'on', 'the', 'free', 'form', 'shape', 'face', 'region', 'the', 'non-transitory', 'machine', 'readable', 'medium', 'of', 'the', 'machine', 'readable', 'medium', 'comprising', 'further', 'instructions', 'that', 'in', 'response', 'to', 'being', 'executed', 'on', 'the', 'device', 'cause', 'the', 'device', 'to', 'perform', 'video', 'coding', 'based', 'on', 'face', 'detection', 'by', 'a', 'second', 'free', 'form', 'shape', 'face', 'region', 'in', 'the', 'second', 'video', 'frame', 'based', 'on', 'the', 'free', 'form', 'shape', 'face', 'region', 'in', 'the', 'video', 'frame', 'the', 'non-transitory', 'machine', 'readable', 'medium', 'of', 'wherein', 'encoding', 'the', 'video', 'frame', 'based', 'at', 'least', 'in', 'part', 'on', 'the', 'first', 'candidate', 'face', 'region', 'being', 'a', 'valid', 'face', 'region', 'comprises', 'at', 'least', 'one', 'of', 'reducing', 'a', 'quantization', 'parameter', 'corresponding', 'to', 'the', 'first', 'candidate', 'face', 'region', 'adjusting', 'a', 'lambda', 'value', 'for', 'the', 'first', 'candidate', 'face', 'region', 'or', 'disabling', 'skip', 'coding', 'for', 'the', 'first', 'candidate', 'face', 'region', 'a', 'for', 'managing', 'a', 'smart', 'database', 'which', 'stores', 'facial', 's', 'for', 'face', 'comprising', 'steps', 'of', 'a', 'a', 'managing', 'device', 'performing', 'a', 'process', 'of', 'counting', 'one', 'or', 'more', 'specific', 'facial', 's', 'corresponding', 'to', 'at', 'least', 'one', 'specific', 'person', 'stored', 'in', 'the', 'smart', 'database', 'where', 'new', 'facial', 's', 'for', 'the', 'face', 'are', 'continuously', 'stored', 'and', 'a', 'process', 'of', 'determining', 'whether', 'a', 'first', 'counted', 'value', 'representing', 'a', 'count', 'of', 'the', 'specific', 'facial', 's', 'satisfies', 'a', 'preset', 'first', 'set', 'value', 'and', 'b', 'if', 'the', 'first', 'counted', 'value', 'is', 'determined', 'as', 'satisfying', 'the', 'first', 'set', 'value', 'the', 'managing', 'device', 'performing', 'a', 'process', 'of', 'inputting', 'the', 'specific', 'facial', 's', 'into', 'a', 'neural', 'aggregation', 'network', 'to', 'thereby', 'allow', 'the', 'neural', 'aggregation', 'network', 'to', 'generate', 'each', 'of', 'quality', 'scores', 'of', 'each', 'of', 'the', 'specific', 'facial', 's', 'by', 'aggregation', 'of', 'the', 'specific', 'facial', 's', 'and', 'a', 'process', 'of', 'sorting', 'the', 'quality', 'scores', 'corresponding', 'to', 'the', 'specific', 'facial', 's', 'in', 'a', 'descending', 'order', 'of', 'the', 'quality', 'scores', 'a', 'process', 'of', 'counting', 'the', 'sorted', 'specific', 'facial', 's', 'in', 'the', 'descending', 'order', 'until', 'a', 'second', 'counted', 'value', 'which', 'represents', 'the', 'number', 'of', 'a', 'counted', 'part', 'of', 'the', 'specific', 'facial', 's', 'becomes', 'equal', 'to', 'a', 'preset', 'second', 'set', 'value', 'and', 'a', 'process', 'of', 'deleting', 'an', 'uncounted', 'part', 'of', 'the', 'specific', 'facial', 's', 'from', 'the', 'smart', 'database', 'the', 'of', 'further', 'comprising', 'a', 'step', 'of', 'c', 'the', 'managing', 'device', 'performing', 'a', 'process', 'of', 'generating', 'at', 'least', 'one', 'optimal', 'feature', 'by', 'weighted', 'summation', 'of', 'one', 'or', 'more', 'features', 'of', 'the', 'specific', 'facial', 's', 'using', 'the', 'counted', 'part', 'of', 'the', 'quality', 'scores', 'and', 'a', 'process', 'of', 'setting', 'the', 'optimal', 'feature', 'as', 'a', 'representative', 'face', 'corresponding', 'to', 'the', 'specific', 'person', 'the', 'of', 'wherein', 'at', 'the', 'step', 'of', 'b', 'the', 'managing', 'device', 'performs', 'a', 'process', 'of', 'inputting', 'the', 'specific', 'facial', 's', 'into', 'a', 'cnn', 'of', 'the', 'neural', 'aggregation', 'network', 'to', 'thereby', 'allow', 'the', 'cnn', 'to', 'generate', 'one', 'or', 'more', 'features', 'corresponding', 'to', 'each', 'of', 'the', 'specific', 'facial', 's', 'and', 'a', 'process', 'of', 'inputting', 'at', 'least', 'one', 'feature', 'vector', 'where', 'the', 'features', 'are', 'embedded', 'into', 'an', 'aggregation', 'module', 'including', 'at', 'least', 'two', 'attention', 'blocks', 'to', 'thereby', 'allow', 'the', 'aggregation', 'module', 'to', 'generate', 'each', 'of', 'the', 'quality', 'scores', 'of', 'each', 'of', 'the', 'features', 'the', 'of', 'wherein', 'at', 'the', 'step', 'of', 'b', 'the', 'managing', 'device', 'performs', 'a', 'process', 'of', 'matching', 'i', 'i-', 'one', 'or', 'more', 'features', 'corresponding', 'to', 'each', 'of', 'the', 'specific', 'facial', 's', 'stored', 'in', 'the', 'smart', 'database', 'and', 'i-', 'the', 'quality', 'scores', 'with', 'ii', 'the', 'specific', 'person', 'and', 'a', 'process', 'of', 'storing', 'the', 'matched', 'features', 'and', 'the', 'matched', 'quality', 'scores', 'in', 'the', 'smart', 'database', 'the', 'of', 'further', 'comprising', 'a', 'step', 'of', 'd', 'the', 'managing', 'device', 'performing', 'one', 'of', 'i', 'a', 'process', 'of', 'learning', 'a', 'face', 'system', 'by', 'using', 'the', 'specific', 'facial', 's', 'corresponding', 'to', 'the', 'specific', 'person', 'stored', 'in', 'the', 'smart', 'database', 'and', 'ii', 'a', 'process', 'of', 'transmitting', 'the', 'specific', 'facial', 's', 'corresponding', 'to', 'the', 'specific', 'person', 'to', 'a', 'learning', 'device', 'corresponding', 'to', 'the', 'face', 'system', 'to', 'thereby', 'allow', 'the', 'learning', 'device', 'to', 'learn', 'the', 'face', 'system', 'using', 'the', 'specific', 'facial', 's', 'the', 'of', 'wherein', 'the', 'neural', 'aggregation', 'network', 'has', 'been', 'learned', 'by', 'a', 'learning', 'device', 'repeating', 'more', 'than', 'once', 'i', 'a', 'process', 'of', 'inputting', 'multiple', 'facial', 's', 'for', 'training', 'corresponding', 'to', 'an', 'set', 'of', 'a', 'single', 'face', 'or', 'a', 'video', 'of', 'the', 'single', 'face', 'into', 'a', 'cnn', 'of', 'the', 'neural', 'aggregation', 'network', 'to', 'thereby', 'allow', 'the', 'cnn', 'to', 'generate', 'one', 'or', 'more', 'features', 'for', 'training', 'by', 'applying', 'at', 'least', 'one', 'convolution', 'operation', 'to', 'the', 'facial', 's', 'for', 'training', 'ii', 'a', 'process', 'of', 'inputting', 'at', 'least', 'one', 'feature', 'vector', 'for', 'training', 'where', 'the', 'features', 'for', 'training', 'are', 'embedded', 'into', 'an', 'aggregation', 'module', 'including', 'at', 'least', 'two', 'attention', 'blocks', 'of', 'the', 'neural', 'aggregation', 'network', 'to', 'thereby', 'allow', 'the', 'aggregation', 'module', 'to', 'generate', 'each', 'of', 'quality', 'scores', 'for', 'training', 'of', 'each', 'of', 'the', 'features', 'for', 'training', 'by', 'aggregation', 'of', 'the', 'features', 'for', 'training', 'using', 'one', 'or', 'more', 'attention', 'parameters', 'learned', 'in', 'a', 'previous', 'iteration', 'iii', 'a', 'process', 'of', 'outputting', 'at', 'least', 'one', 'optimal', 'feature', 'for', 'training', 'by', 'weighted', 'summation', 'of', 'the', 'features', 'for', 'training', 'using', 'the', 'quality', 'scores', 'for', 'training', 'and', 'iv', 'a', 'process', 'of', 'updating', 'the', 'attention', 'parameters', 'learned', 'in', 'the', 'previous', 'iteration', 'of', 'the', 'at', 'least', 'two', 'attention', 'blocks', 'such', 'that', 'one', 'or', 'more', 'losses', 'are', 'minimized', 'which', 'are', 'outputted', 'from', 'a', 'loss', 'layer', 'by', 'referring', 'to', 'the', 'optimal', 'feature', 'for', 'training', 'and', 'its', 'corresponding', 'ground', 'truth', 'a', 'managing', 'device', 'for', 'managing', 'a', 'smart', 'database', 'which', 'stores', 'facial', 's', 'for', 'face', 'comprising', 'at', 'least', 'one', 'memory', 'that', 'stores', 'instructions', 'and', 'at', 'least', 'one', 'configured', 'to', 'execute', 'the', 'instructions', 'to', 'perform', 'or', 'support', 'another', 'device', 'to', 'perform', 'i', 'a', 'process', 'of', 'counting', 'one', 'or', 'more', 'specific', 'facial', 's', 'corresponding', 'to', 'at', 'least', 'one', 'specific', 'person', 'stored', 'in', 'the', 'smart', 'database', 'where', 'new', 'facial', 's', 'for', 'the', 'face', 'are', 'continuously', 'stored', 'and', 'a', 'process', 'of', 'determining', 'whether', 'a', 'first', 'counted', 'value', 'representing', 'a', 'count', 'of', 'the', 'specific', 'facial', 's', 'satisfies', 'a', 'preset', 'first', 'set', 'value', 'and', 'ii', 'if', 'the', 'first', 'counted', 'value', 'is', 'determined', 'as', 'satisfying', 'the', 'first', 'set', 'value', 'a', 'process', 'of', 'inputting', 'the', 'specific', 'facial', 's', 'into', 'a', 'neural', 'aggregation', 'network', 'to', 'thereby', 'allow', 'the', 'neural', 'aggregation', 'network', 'to', 'generate', 'each', 'of', 'quality', 'scores', 'of', 'each', 'of', 'the', 'specific', 'facial', 's', 'by', 'aggregation', 'of', 'the', 'specific', 'facial', 's', 'and', 'a', 'process', 'of', 'sorting', 'the', 'quality', 'scores', 'corresponding', 'to', 'the', 'specific', 'facial', 's', 'in', 'a', 'descending', 'order', 'of', 'the', 'quality', 'scores', 'a', 'process', 'of', 'counting', 'the', 'sorted', 'specific', 'facial', 's', 'in', 'the', 'descending', 'order', 'until', 'a', 'second', 'counted', 'value', 'which', 'represents', 'the', 'number', 'of', 'a', 'counted', 'part', 'of', 'the', 'specific', 'facial', 's', 'becomes', 'equal', 'to', 'a', 'preset', 'second', 'set', 'value', 'and', 'a', 'process', 'of', 'deleting', 'an', 'uncounted', 'part', 'of', 'the', 'specific', 'facial', 's', 'from', 'the', 'smart', 'database', 'the', 'managing', 'device', 'of', 'wherein', 'the', 'further', 'performs', 'iii', 'a', 'process', 'of', 'generating', 'at', 'least', 'one', 'optimal', 'feature', 'by', 'weighted', 'summation', 'of', 'one', 'or', 'more', 'features', 'of', 'the', 'specific', 'facial', 's', 'using', 'the', 'counted', 'part', 'of', 'the', 'quality', 'scores', 'and', 'a', 'process', 'of', 'setting', 'the', 'optimal', 'feature', 'as', 'a', 'representative', 'face', 'corresponding', 'to', 'the', 'specific', 'person', 'the', 'managing', 'device', 'of', 'wherein', 'at', 'the', 'process', 'of', 'ii', 'the', 'performs', 'a', 'process', 'of', 'inputting', 'the', 'specific', 'facial', 's', 'into', 'a', 'cnn', 'of', 'the', 'neural', 'aggregation', 'network', 'to', 'thereby', 'allow', 'the', 'cnn', 'to', 'generate', 'one', 'or', 'more', 'features', 'corresponding', 'to', 'each', 'of', 'the', 'specific', 'facial', 's', 'and', 'a', 'process', 'of', 'inputting', 'at', 'least', 'one', 'feature', 'vector', 'where', 'the', 'features', 'are', 'embedded', 'into', 'an', 'aggregation', 'module', 'including', 'at', 'least', 'two', 'attention', 'blocks', 'to', 'thereby', 'allow', 'the', 'aggregation', 'module', 'to', 'generate', 'each', 'of', 'the', 'quality', 'scores', 'of', 'each', 'of', 'the', 'features', 'the', 'managing', 'device', 'of', 'wherein', 'at', 'the', 'process', 'of', 'ii', 'the', 'performs', 'a', 'process', 'of', 'matching', 'i', 'i-', 'one', 'or', 'more', 'features', 'corresponding', 'to', 'each', 'of', 'the', 'specific', 'facial', 's', 'stored', 'in', 'the', 'smart', 'database', 'and', 'i-', 'the', 'quality', 'scores', 'with', 'ii', 'the', 'specific', 'person', 'and', 'a', 'process', 'of', 'storing', 'the', 'matched', 'features', 'and', 'the', 'matched', 'quality', 'scores', 'in', 'the', 'smart', 'database', 'the', 'managing', 'device', 'of', 'wherein', 'the', 'further', 'performs', 'iv', 'one', 'of', 'i', 'a', 'process', 'of', 'learning', 'a', 'face', 'system', 'by', 'using', 'the', 'specific', 'facial', 's', 'corresponding', 'to', 'the', 'specific', 'person', 'stored', 'in', 'the', 'smart', 'database', 'and', 'ii', 'a', 'process', 'of', 'transmitting', 'the', 'specific', 'facial', 's', 'corresponding', 'to', 'the', 'specific', 'person', 'to', 'a', 'learning', 'device', 'corresponding', 'to', 'the', 'face', 'system', 'to', 'thereby', 'allow', 'the', 'learning', 'device', 'to', 'learn', 'the', 'face', 'system', 'using', 'the', 'specific', 'facial', 's', 'the', 'managing', 'device', 'of', 'wherein', 'the', 'neural', 'aggregation', 'network', 'has', 'been', 'learned', 'by', 'a', 'learning', 'device', 'repeating', 'more', 'than', 'once', 'i', 'a', 'process', 'of', 'inputting', 'multiple', 'facial', 's', 'for', 'training', 'corresponding', 'to', 'an', 'set', 'of', 'a', 'single', 'face', 'or', 'a', 'video', 'of', 'the', 'single', 'face', 'into', 'a', 'cnn', 'of', 'the', 'neural', 'aggregation', 'network', 'to', 'thereby', 'allow', 'the', 'cnn', 'to', 'generate', 'one', 'or', 'more', 'features', 'for', 'training', 'by', 'applying', 'at', 'least', 'one', 'convolution', 'operation', 'to', 'the', 'facial', 's', 'for', 'training', 'ii', 'a', 'process', 'of', 'inputting', 'at', 'least', 'one', 'feature', 'vector', 'for', 'training', 'where', 'the', 'features', 'for', 'training', 'are', 'embedded', 'into', 'an', 'aggregation', 'module', 'including', 'at', 'least', 'two', 'attention', 'blocks', 'of', 'the', 'neural', 'aggregation', 'network', 'to', 'thereby', 'allow', 'the', 'aggregation', 'module', 'to', 'generate', 'each', 'of', 'quality', 'scores', 'for', 'training', 'of', 'each', 'of', 'the', 'features', 'for', 'training', 'by', 'aggregation', 'of', 'the', 'features', 'for', 'training', 'using', 'one', 'or', 'more', 'attention', 'parameters', 'learned', 'in', 'a', 'previous', 'iteration', 'iii', 'a', 'process', 'of', 'outputting', 'at', 'least', 'one', 'optimal', 'feature', 'for', 'training', 'by', 'weighted', 'summation', 'of', 'the', 'features', 'for', 'training', 'using', 'the', 'quality', 'scores', 'for', 'training', 'and', 'iv', 'a', 'process', 'of', 'updating', 'the', 'attention', 'parameters', 'learned', 'in', 'the', 'previous', 'iteration', 'of', 'the', 'at', 'least', 'two', 'attention', 'blocks', 'such', 'that', 'one', 'or', 'more', 'losses', 'are', 'minimized', 'which', 'are', 'outputted', 'from', 'a', 'loss', 'layer', 'by', 'referring', 'to', 'the', 'optimal', 'feature', 'for', 'training', 'and', 'its', 'corresponding', 'ground', 'truth', 'an', 'object', 'data', 'processing', 'system', 'comprising', 'at', 'least', 'one', 'configured', 'to', 'execute', 'at', 'least', 'one', 'implementation', 'of', 'a', 'of', 'algorithms', 'stored', 'on', 'at', 'least', 'one', 'non-transitory', 'computer-readable', 'storage', 'medium', 'each', 'algorithm', 'having', 'feature', 'density', 'selection', 'criteria', 'and', 'data', 'preprocessing', 'code', 'executed', 'by', 'at', 'least', 'one', 'the', 'data', 'preprocessing', 'code', 'comprising', 'an', 'invariant', 'feature', 'identification', 'algorithm', 'and', 'configured', 'to', 'obtain', 'a', 'digital', 'representation', 'of', 'a', 'scene', 'the', 'scene', 'comprising', 'one', 'or', 'more', 'textual', 'media', 'generate', 'a', 'set', 'of', 'invariant', 'features', 'by', 'applying', 'the', 'invariant', 'feature', 'identification', 'algorithm', 'to', 'the', 'digital', 'representation', 'cluster', 'the', 'set', 'of', 'invariant', 'features', 'into', 'regions', 'of', 'interest', 'in', 'the', 'digital', 'representation', 'of', 'the', 'scene', 'each', 'region', 'of', 'interest', 'having', 'a', 'region', 'feature', 'density', 'classify', 'by', 'region', 'classifier', 'code', 'at', 'least', 'one', 'of', 'the', 'regions', 'of', 'interest', 'according', 'to', 'object', 'type', 'as', 'a', 'function', 'of', 'attributes', 'derived', 'from', 'the', 'region', 'feature', 'density', 'and', 'the', 'digital', 'representation', 'wherein', 'the', 'at', 'least', 'one', 'of', 'the', 'classified', 'regions', 'of', 'interest', 'corresponds', 'to', 'text', 'and', 'use', 'a', 'classification', 'result', 'corresponding', 'to', 'the', 'at', 'least', 'one', 'of', 'the', 'regions', 'of', 'interest', 'to', 'classify', 'another', 'of', 'the', 'regions', 'of', 'interest', 'according', 'to', 'object', 'type', 'wherein', 'the', 'another', 'of', 'the', 'regions', 'of', 'interest', 'corresponds', 'to', 'a', 'region', 'of', 'interest', 'for', 's', 'the', 'system', 'of', 'wherein', 'preprocessing', 'code', 'based', 'on', 'the', 'feature', 'density', 'selection', 'criteria', 'determines', 'that', 'an', 'ocr', 'algorithm', 'is', 'applicable', 'to', 'the', 'text', 'and', 'that', 'other', 'algorithms', 'are', 'applicable', 'to', 'aspects', 'of', 'the', 'photographs', 'and', 'to', 'logos', 'the', 'system', 'of', 'wherein', 'a', 'creates', 'a', 'profile', 'for', 'a', 'camera-equipped', 'smartphone', 'that', 'includes', 'the', 'information', 'that', 'the', 'is', 'visually', 'impaired', 'which', 'causes', 'prioritized', 'execution', 'of', 'the', 'ocr', 'algorithm', 'such', 'that', 'a', 'text', 'reader', 'program', 'begins', 'reading', 'the', 'text', 'to', 'the', 'as', 'quickly', 'as', 'possible', 'the', 'system', 'of', 'further', 'comprising', 'an', 'audio', 'or', 'tactile', 'feedback', 'mechanism', 'that', 'helps', 'the', 'to', 'position', 'the', 'smart', 'phone', 'relative', 'to', 'the', 'text', 'the', 'system', 'of', 'further', 'comprising', 'a', '``', 'hold', 'still', \"''\", 'audio', 'feedback', 'signal', 'that', 'is', 'sent', 'to', 'the', 'when', 'the', 'text', 'is', 'at', 'the', 'center', 'of', 'the', 'captured', 'scene', 'the', 'system', 'of', 'wherein', 'the', 'digital', 'representation', 'comprises', 'at', 'least', 'one', 'of', 'the', 'following', 'types', 'of', 'digital', 'data', 'data', 'video', 'data', 'and', 'audio', 'data', 'the', 'system', 'of', 'wherein', 'invariant', 'feature', 'identification', 'algorithm', 'comprises', 'at', 'least', 'one', 'of', 'the', 'following', 'feature', 'identification', 'algorithms', 'fast', 'sift', 'freak', 'brisk', 'harris', 'daisy', 'and', 'mser', 'the', 'system', 'of', 'wherein', 'the', 'invariant', 'feature', 'identification', 'algorithm', 'includes', 'at', 'least', 'one', 'of', 'the', 'following', 'edge', 'detection', 'algorithm', 'corner', 'detection', 'algorithm', 'saliency', 'map', 'algorithm', 'curve', 'detection', 'algorithm', 'a', 'texton', 'identification', 'algorithm', 'and', 'wavelets', 'algorithm', 'the', 'system', 'of', 'wherein', 'at', 'least', 'one', 'region', 'of', 'interest', 'represents', 'at', 'least', 'one', 'physical', 'object', 'in', 'the', 'scene', 'the', 'system', 'of', 'wherein', 'at', 'least', 'one', 'region', 'of', 'interest', 'represents', 'at', 'least', 'one', 'textual', 'media', 'in', 'the', 'scene', 'the', 'system', 'of', 'wherein', 'the', 'region', 'of', 'interest', 'represents', 'a', 'document', 'as', 'the', 'textual', 'media', 'the', 'system', 'of', 'wherein', 'the', 'region', 'of', 'interest', 'represents', 'a', 'financial', 'document', 'the', 'system', 'of', 'wherein', 'the', 'region', 'of', 'interest', 'represents', 'a', 'structured', 'document', 'the', 'system', 'of', 'wherein', 'at', 'least', 'one', 'implementation', 'of', 'a', 'of', 'algorithms', 'includes', 'at', 'least', 'one', 'of', 'the', 'following', 'a', 'template', 'driven', 'algorithm', 'a', 'face', 'algorithm', 'an', 'optical', 'character', 'algorithm', 'a', 'speech', 'algorithm', 'and', 'an', 'object', 'algorithm', 'the', 'system', 'of', 'wherein', 'data', 'preprocessing', 'code', 'is', 'further', 'configured', 'to', 'assign', 'each', 'region', 'of', 'interest', 'at', 'least', 'one', 'algorithm', 'as', 'a', 'function', 'of', 'a', 'scene', 'context', 'derived', 'from', 'the', 'digital', 'representation', 'the', 'system', 'of', 'wherein', 'the', 'scene', 'context', 'includes', 'at', 'least', 'one', 'of', 'the', 'following', 'types', 'of', 'data', 'a', 'location', 'a', 'position', 'a', 'time', 'a', 'identity', 'a', 'news', 'event', 'a', 'medical', 'event', 'and', 'a', 'promotion', 'the', 'system', 'of', 'further', 'comprising', 'a', 'mobile', 'device', 'comprising', 'at', 'least', 'one', 'implementation', 'of', 'a', 'of', 'algorithms', 'and', 'data', 'preprocessing', 'code', 'the', 'system', 'of', 'wherein', 'the', 'mobile', 'device', 'comprises', 'at', 'least', 'one', 'of', 'the', 'following', 'a', 'smart', 'phone', 'a', 'tablet', 'wearable', 'glass', 'a', 'toy', 'a', 'vehicle', 'a', 'computer', 'and', 'a', 'phablet', 'the', 'system', 'of', 'further', 'comprising', 'a', 'network-accessible', 'server', 'device', 'comprising', 'at', 'least', 'one', 'implementation', 'of', 'a', 'of', 'algorithms', 'and', 'data', 'preprocessing', 'code', 'the', 'system', 'of', 'wherein', 'the', 'object', 'type', 'includes', 'at', 'least', 'one', 'of', 'the', 'following', 'a', 'face', 'an', 'animal', 'a', 'vehicle', 'a', 'document', 'a', 'plant', 'a', 'building', 'an', 'appliance', 'clothing', 'a', 'body', 'part', 'and', 'a', 'toy', 'an', 'object', 'data', 'processing', 'system', 'comprising', 'at', 'least', 'one', 'configured', 'to', 'execute', 'at', 'least', 'one', 'implementation', 'of', 'a', 'of', 'algorithms', 'stored', 'on', 'at', 'least', 'one', 'non-transitory', 'computer-readable', 'storage', 'medium', 'each', 'algorithm', 'having', 'feature', 'density', 'selection', 'criteria', 'and', 'data', 'preprocessing', 'code', 'executed', 'by', 'at', 'least', 'one', 'the', 'data', 'preprocessing', 'code', 'comprising', 'an', 'invariant', 'feature', 'identification', 'algorithm', 'and', 'configured', 'to', 'obtain', 'a', 'digital', 'representation', 'of', 'a', 'scene', 'the', 'scene', 'comprising', 'one', 'or', 'more', 'textual', 'media', 'generate', 'a', 'set', 'of', 'invariant', 'features', 'by', 'applying', 'the', 'invariant', 'feature', 'identification', 'algorithm', 'to', 'the', 'digital', 'representation', 'cluster', 'the', 'set', 'of', 'invariant', 'features', 'into', 'regions', 'of', 'interest', 'in', 'the', 'digital', 'representation', 'of', 'the', 'scene', 'each', 'region', 'of', 'interest', 'having', 'a', 'region', 'feature', 'density', 'classify', 'by', 'region', 'classifier', 'code', 'at', 'least', 'one', 'of', 'the', 'regions', 'of', 'interest', 'according', 'to', 'object', 'type', 'as', 'a', 'function', 'of', 'attributes', 'derived', 'from', 'the', 'region', 'feature', 'density', 'and', 'the', 'digital', 'representation', 'wherein', 'the', 'at', 'least', 'one', 'of', 'the', 'classified', 'regions', 'of', 'interest', 'corresponds', 'to', 'text', 'and', 'use', 'a', 'classification', 'result', 'corresponding', 'to', 'the', 'at', 'least', 'one', 'of', 'the', 'regions', 'of', 'interest', 'to', 'classify', 'another', 'of', 'the', 'regions', 'of', 'interest', 'according', 'to', 'object', 'type', 'wherein', 'the', 'another', 'of', 'the', 'regions', 'of', 'interest', 'corresponds', 'to', 'a', 'region', 'of', 'interest', 'for', 's', 'assign', 'each', 'region', 'of', 'interest', 'at', 'least', 'one', 'algorithm', 'from', 'at', 'least', 'one', 'implementation', 'of', 'a', 'of', 'diverse', 'algorithms', 'as', 'a', 'function', 'of', 'the', 'region', 'feature', 'density', 'of', 'each', 'region', 'of', 'interest', 'and', 'the', 'feature', 'density', 'selection', 'criteria', 'of', 'the', 'at', 'least', 'one', 'implementation', 'of', 'a', 'of', 'diverse', 'algorithms', 'and', 'configure', 'the', 'assigned', 'algorithms', 'to', 'process', 'their', 'respective', 'regions', 'of', 'interest', 'wherein', 'preprocessing', 'code', 'based', 'on', 'the', 'feature', 'density', 'selection', 'criteria', 'determines', 'that', 'an', 'ocr', 'algorithm', 'is', 'applicable', 'to', 'the', 'text', 'and', 'that', 'other', 'algorithms', 'are', 'applicable', 'to', 'aspects', 'of', 'the', 'photographs', 'and', 'to', 'logos', 'a', 'device', 'comprising', 'at', 'least', 'one', 'configured', 'to', 'execute', 'at', 'least', 'one', 'implementation', 'of', 'a', 'of', 'algorithms', 'stored', 'on', 'at', 'least', 'one', 'non-transitory', 'computer-readable', 'storage', 'medium', 'each', 'algorithm', 'having', 'feature', 'density', 'selection', 'criteria', 'and', 'data', 'preprocessing', 'code', 'executed', 'by', 'at', 'least', 'one', 'the', 'data', 'preprocessing', 'code', 'comprising', 'an', 'invariant', 'feature', 'identification', 'algorithm', 'and', 'configured', 'to', 'obtain', 'a', 'digital', 'representation', 'of', 'a', 'scene', 'the', 'scene', 'comprising', 'one', 'or', 'more', 'textual', 'media', 'generate', 'a', 'set', 'of', 'invariant', 'features', 'by', 'applying', 'the', 'invariant', 'feature', 'identification', 'algorithm', 'to', 'the', 'digital', 'representation', 'cluster', 'the', 'set', 'of', 'invariant', 'features', 'into', 'regions', 'of', 'interest', 'in', 'the', 'digital', 'representation', 'of', 'the', 'scene', 'each', 'region', 'of', 'interest', 'having', 'a', 'region', 'feature', 'density', 'and', 'classify', 'by', 'region', 'classifier', 'code', 'at', 'least', 'one', 'of', 'the', 'regions', 'of', 'interest', 'according', 'to', 'object', 'type', 'as', 'a', 'function', 'of', 'attributes', 'derived', 'from', 'the', 'region', 'feature', 'density', 'and', 'the', 'digital', 'representation', 'wherein', 'the', 'at', 'least', 'one', 'of', 'the', 'classified', 'regions', 'of', 'interest', 'corresponds', 'to', 'text', 'and', 'use', 'a', 'classification', 'result', 'corresponding', 'to', 'the', 'at', 'least', 'one', 'of', 'the', 'regions', 'of', 'interest', 'to', 'classify', 'another', 'of', 'the', 'regions', 'of', 'interest', 'according', 'to', 'object', 'type', 'wherein', 'the', 'another', 'of', 'the', 'regions', 'of', 'interest', 'corresponds', 'to', 'a', 'region', 'of', 'interest', 'for', 's', 'a', 'mobile', 'terminal', 'comprising', 'a', 'front', 'camera', 'configured', 'to', 'obtain', 'a', 'two-dimensional', 'd', 'face', 'of', 'a', 'a', 'glance', 'sensor', 'tilted', 'by', 'a', 'certain', 'angle', 'and', 'disposed', 'adjacent', 'to', 'the', 'front', 'camera', 'to', 'obtain', 'metadata', 'of', 'the', 'd', 'face', 'and', 'a', 'controller', 'obtaining', 'a', 'distance', 'between', 'the', 'glance', 'sensor', 'and', 'the', 'front', 'camera', 'the', 'distance', 'enabling', 'an', 'area', 'of', 'an', 'overlap', 'region', 'where', 'a', 'first', 'region', 'representing', 'a', 'range', 'photographable', 'by', 'the', 'front', 'camera', 'overlaps', 'a', 'second', 'region', 'representing', 'a', 'range', 'photographable', 'by', 'the', 'glance', 'sensor', 'to', 'be', 'the', 'maximum', 'the', 'mobile', 'terminal', 'of', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'obtain', 'the', 'distance', 'enabling', 'the', 'area', 'of', 'the', 'overlap', 'region', 'to', 'be', 'the', 'maximum', 'between', 'the', 'glance', 'sensor', 'and', 'the', 'front', 'camera', 'by', 'varying', 'a', 'tilting', 'angle', 'of', 'the', 'glance', 'sensor', 'the', 'mobile', 'terminal', 'of', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'set', 'the', 'distance', 'enabling', 'the', 'area', 'of', 'the', 'overlap', 'region', 'to', 'be', 'the', 'maximum', 'between', 'the', 'glance', 'sensor', 'and', 'the', 'front', 'camera', 'and', 'the', 'tilting', 'angle', 'of', 'the', 'glance', 'sensor', 'as', 'an', 'optimal', 'disposition', 'location', 'of', 'the', 'glance', 'sensor', 'the', 'mobile', 'terminal', 'of', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'set', 'a', 'disposition', 'location', 'of', 'the', 'front', 'camera', 'as', 'an', 'original', 'point', 'and', 'calculates', 'coordinates', 'of', 'a', 'first', 'triangle', 'representing', 'the', 'first', 'region', 'based', 'on', 'a', 'field', 'of', 'view', 'of', 'the', 'front', 'camera', 'and', 'a', 'maximum', 'photographing', 'distance', 'of', 'the', 'front', 'camera', 'the', 'mobile', 'terminal', 'of', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'calculate', 'coordinates', 'of', 'a', 'second', 'triangle', 'representing', 'the', 'second', 'region', 'based', 'on', 'a', 'field', 'of', 'view', 'of', 'the', 'glance', 'sensor', 'a', 'maximum', 'photographing', 'distance', 'of', 'the', 'glance', 'sensor', 'a', 'distance', 'between', 'the', 'front', 'camera', 'and', 'the', 'glance', 'sensor', 'and', 'a', 'tilting', 'angle', 'of', 'the', 'glance', 'sensor', 'the', 'mobile', 'terminal', 'of', 'wherein', 'before', 'the', 'glance', 'sensor', 'is', 'tilted', 'the', 'controller', 'is', 'configured', 'to', 'calculate', 'coordinates', 'of', 'a', 'third', 'triangle', 'representing', 'a', 'third', 'region', 'photographable', 'by', 'the', 'glance', 'sensor', 'and', 'the', 'controller', 'is', 'configured', 'to', 'rotation-convert', 'the', 'coordinates', 'of', 'the', 'third', 'triangle', 'based', 'on', 'the', 'tilting', 'angle', 'of', 'the', 'glance', 'sensor', 'and', 'calculate', 'the', 'coordinates', 'of', 'the', 'second', 'triangle', 'the', 'mobile', 'terminal', 'of', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'calculate', 'coordinates', 'of', 'the', 'overlap', 'region', 'based', 'on', 'the', 'coordinates', 'of', 'the', 'first', 'triangle', 'and', 'the', 'coordinates', 'of', 'the', 'second', 'triangle', 'and', 'calculates', 'the', 'area', 'of', 'the', 'overlap', 'region', 'based', 'on', 'the', 'coordinates', 'of', 'the', 'overlap', 'region', 'the', 'mobile', 'terminal', 'of', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'generate', 'three-dimensional', 'd', 'face', 'information', 'based', 'on', 'the', 'd', 'face', 'obtained', 'by', 'the', 'front', 'camera', 'and', 'metadata', 'obtained', 'by', 'the', 'glance', 'sensor', 'the', 'mobile', 'terminal', 'of', 'wherein', 'the', 'metadata', 'comprises', 'one', 'or', 'more', 'of', 'an', 'angle', 'of', 'a', 'face', 'of', 'the', 'a', 'size', 'of', 'the', 'face', 'and', 'a', 'location', 'of', 'the', 'face', 'the', 'mobile', 'terminal', 'of', 'wherein', 'the', 'angle', 'of', 'the', 'face', 'comprises', 'an', 'angle', 'by', 'which', 'the', 'face', 'is', 'rotated', 'about', 'one', 'or', 'more', 'of', 'a', 'pitch', 'axis', 'a', 'roll', 'axis', 'and', 'a', 'yaw', 'axis', 'the', 'mobile', 'terminal', 'of', 'further', 'comprising', 'a', 'memory', 'storing', 'the', 'generated', 'd', 'face', 'information', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'performs', 'a', 'authentication', 'process', 'by', 'comparing', 'the', 'stored', 'd', 'face', 'information', 'with', 'd', 'face', 'information', 'obtained', 'for', 'authentication', 'the', 'mobile', 'terminal', 'of', 'wherein', 'the', 'glance', 'sensor', 'is', 'controlled', 'to', 'be', 'permanently', 'activated', 'with', 'a', 'low', 'power', 'to', 'obtain', 'a', 'front', 'and', 'metadata', 'of', 'the', 'front', 'the', 'mobile', 'terminal', 'of', 'wherein', 'the', 'front', 'camera', 'and', 'the', 'glance', 'sensor', 'are', 'disposed', 'on', 'the', 'same', 'line', 'in', 'an', 'upper', 'end', 'of', 'the', 'mobile', 'terminal', 'the', 'mobile', 'terminal', 'of', 'wherein', 'the', 'glance', 'sensor', 'is', 'tilted', 'in', 'one', 'direction', 'of', 'an', 'up', 'direction', 'a', 'down', 'direction', 'a', 'left', 'direction', 'and', 'a', 'right', 'direction', 'the', 'mobile', 'terminal', 'of', 'wherein', 'the', 'metadata', 'is', 'data', 'which', 'is', 'changed', 'when', 'the', 'mobile', 'terminal', 'is', 'tilted', 'by', 'an', 'external', 'physical', 'force', 'a', 'comprising', 'receiving', 'by', 'a', 'smart', 'television', 'tv', 'an', 'indication', 'of', 'upcoming', 'media', 'programming', 'wherein', 'the', 'upcoming', 'media', 'programming', 'is', 'based', 'on', 'a', 'profile', 'identifying', 'one', 'or', 'more', 'devices', 'in', 'communication', 'with', 'the', 'smart', 'tv', 'each', 'of', 'the', 'one', 'or', 'more', 'devices', 'including', 'at', 'least', 'one', 'of', 'a', 'microphone', 'or', 'a', 'camera', 'instructing', 'at', 'least', 'one', 'identified', 'device', 'to', 'detect', 'audio', 'signals', 'using', 'its', 'respective', 'microphone', 'or', 'to', 'detect', 'visual', 'signals', 'using', 'its', 'respective', 'camera', 'selecting', 'at', 'least', 'one', 'device', 'of', 'the', 'one', 'or', 'more', 'devices', 'based', 'on', 'the', 'detected', 'audio', 'signal', 'or', 'detected', 'visual', 'signal', 'and', 'providing', 'instructions', 'to', 'the', 'selected', 'device', 'to', 'output', 'a', 'notification', 'related', 'to', 'the', 'upcoming', 'media', 'programming', 'the', 'of', 'wherein', 'the', 'upcoming', 'media', 'programming', 'is', 'one', 'of', 'a', 'live', 'television', 'program', 'a', 'recorded', 'television', 'program', 'a', 'broadcast', 'television', 'program', 'or', 'an', 'application-provided', 'program', 'the', 'of', 'wherein', 'selecting', 'the', 'first', 'device', 'based', 'on', 'the', 'detected', 'audio', 'signal', 'includes', 'a', 'voice', 'the', 'of', 'further', 'comprising', 'determining', 'a', 'distance', 'to', 'the', 'recognized', 'voice', 'and', 'wherein', 'selecting', 'the', 'first', 'device', 'is', 'further', 'based', 'on', 'the', 'determined', 'distance', 'the', 'of', 'wherein', 'selecting', 'the', 'first', 'device', 'based', 'on', 'the', 'detected', 'visual', 'signals', 'includes', 'a', 'face', 'the', 'of', 'wherein', 'the', 'face', 'includes', 'a', 'face', 'technique', 'the', 'of', 'further', 'comprising', 'presenting', 'on', 'the', 'smart', 'tv', 'the', 'upcoming', 'media', 'programming', 'in', 'a', 'favorite', 'channel', 'list', 'the', 'of', 'further', 'comprising', 'obtaining', 'media', 'programming', 'viewing', 'data', 'wherein', 'the', 'media', 'programming', 'viewing', 'data', 'includes', 'at', 'least', 'one', 'of', 'a', 'historical', 'time', 'and', 'a', 'historical', 'date', 'that', 'one', 'or', 'more', 'media', 'programs', 'were', 'viewed', 'obtaining', 'at', 'least', 'one', 'of', 'a', 'current', 'time', 'and', 'a', 'current', 'date', 'processing', 'the', 'media', 'programming', 'viewing', 'data', 'to', 'determine', 'a', 'probability', 'of', 'the', 'one', 'or', 'more', 'media', 'programs', 'being', 'viewed', 'based', 'on', 'at', 'least', 'one', 'of', 'the', 'current', 'time', 'and', 'the', 'current', 'date', 'and', 'presenting', 'the', 'favorite', 'channel', 'list', 'based', 'on', 'the', 'determined', 'probability', 'of', 'the', 'one', 'or', 'more', 'media', 'programs', 'being', 'viewed', 'the', 'of', 'wherein', 'processing', 'the', 'media', 'programming', 'viewing', 'data', 'includes', 'employing', 'a', 'neural', 'network', 'model', 'the', 'of', 'wherein', 'employing', 'the', 'neural', 'network', 'model', 'comprises', 'determining', 'a', 'duration', 'that', 'the', 'one', 'or', 'more', 'media', 'programs', 'were', 'viewed', 'for', 'each', 'of', 'the', 'at', 'least', 'one', 'of', 'the', 'historical', 'time', 'and', 'the', 'historical', 'date', 'setting', 'a', 'threshold', 'time', 'duration', 'comparing', 'the', 'determined', 'duration', 'to', 'the', 'threshold', 'time', 'duration', 'and', 'filtering', 'out', 'the', 'one', 'or', 'more', 'media', 'programs', 'viewed', 'below', 'the', 'threshold', 'time', 'duration', 'a', 'smart', 'television', 'tv', 'comprising', 'a', 'network', 'interface', 'a', 'non-transitory', 'computer-readable', 'medium', 'and', 'a', 'in', 'communication', 'with', 'the', 'network', 'interface', 'and', 'the', 'non-transitory', 'computer-readable', 'medium', 'and', 'capable', 'of', 'executing', '-executable', 'program', 'code', 'stored', 'in', 'the', 'non-transitory', 'computer-readable', 'medium', 'to', 'cause', 'the', 'smart', 'tv', 'to', 'receive', 'an', 'indication', 'of', 'upcoming', 'media', 'programming', 'wherein', 'the', 'upcoming', 'media', 'programming', 'is', 'based', 'on', 'a', 'profile', 'identify', 'one', 'or', 'more', 'devices', 'in', 'communication', 'with', 'the', 'smart', 'tv', 'each', 'of', 'the', 'one', 'or', 'more', 'devices', 'including', 'at', 'least', 'one', 'of', 'a', 'microphone', 'or', 'a', 'camera', 'instruct', 'at', 'least', 'one', 'identified', 'device', 'to', 'detect', 'audio', 'signals', 'using', 'its', 'respective', 'microphone', 'or', 'to', 'detect', 'visual', 'signals', 'using', 'its', 'respective', 'camera', 'select', 'at', 'least', 'one', 'device', 'of', 'the', 'one', 'or', 'more', 'devices', 'based', 'on', 'the', 'detected', 'audio', 'signal', 'or', 'detected', 'visual', 'signal', 'and', 'provide', 'instructions', 'to', 'the', 'selected', 'device', 'to', 'output', 'a', 'notification', 'related', 'to', 'the', 'upcoming', 'media', 'programming', 'the', 'smart', 'tv', 'of', 'wherein', 'selecting', 'the', 'first', 'device', 'based', 'on', 'the', 'detected', 'audio', 'signal', 'includes', 'a', 'voice', 'the', 'smart', 'tv', 'of', 'wherein', 'the', 'is', 'further', 'capable', 'of', 'executing', '-executable', 'program', 'code', 'to', 'determine', 'a', 'distance', 'to', 'the', 'recognized', 'voice', 'and', 'wherein', 'selecting', 'the', 'first', 'device', 'is', 'further', 'based', 'on', 'the', 'determined', 'distance', 'the', 'smart', 'tv', 'of', 'wherein', 'selecting', 'the', 'first', 'device', 'based', 'on', 'the', 'detected', 'visual', 'signals', 'includes', 'detecting', 'the', 'presence', 'of', 'a', 'the', 'smart', 'tv', 'of', 'wherein', 'detecting', 'the', 'presence', 'of', 'the', 'includes', 'employing', 'one', 'or', 'more', 'of', 'a', 'camera', 'a', 'microphone', 'or', 'a', 'fingerprint', 'sensor', 'associated', 'with', 'at', 'least', 'one', 'of', 'the', 'smart', 'tv', 'a', 'mobile', 'device', 'a', 'smartphone', 'a', 'laptop', 'computer', 'a', 'tablet', 'device', 'a', 'wearable', 'device', 'an', 'internet', 'of', 'things', 'iot', 'device', 'an', 'internet', 'of', 'everything', 'ioe', 'device', 'an', 'iot', 'hub', 'or', 'an', 'ioe', 'hub', 'a', 'smart', 'television', 'tv', 'comprising', 'means', 'for', 'receiving', 'an', 'indication', 'of', 'upcoming', 'media', 'programming', 'wherein', 'the', 'upcoming', 'media', 'programming', 'is', 'based', 'on', 'a', 'profile', 'means', 'for', 'identifying', 'one', 'or', 'more', 'devices', 'in', 'communication', 'with', 'the', 'smart', 'tv', 'each', 'of', 'the', 'one', 'or', 'more', 'devices', 'including', 'at', 'least', 'one', 'of', 'a', 'microphone', 'or', 'a', 'camera', 'means', 'for', 'instructing', 'at', 'least', 'one', 'identified', 'device', 'to', 'detect', 'audio', 'signals', 'using', 'its', 'respective', 'microphone', 'or', 'to', 'detect', 'visual', 'signals', 'using', 'its', 'respective', 'camera', 'means', 'for', 'selecting', 'at', 'least', 'one', 'device', 'of', 'the', 'one', 'or', 'more', 'devices', 'based', 'on', 'the', 'detected', 'audio', 'signal', 'or', 'detected', 'visual', 'signal', 'and', 'means', 'for', 'providing', 'instructions', 'to', 'the', 'selected', 'device', 'to', 'output', 'a', 'notification', 'related', 'to', 'the', 'upcoming', 'media', 'programming', 'the', 'smart', 'tv', 'of', 'wherein', 'the', 'one', 'or', 'more', 'devices', 'includes', 'at', 'least', 'one', 'of', 'a', 'mobile', 'device', 'a', 'smartphone', 'a', 'laptop', 'computer', 'a', 'tablet', 'device', 'a', 'wearable', 'device', 'an', 'internet', 'of', 'things', 'iot', 'device', 'an', 'internet', 'of', 'everything', 'ioe', 'device', 'an', 'iot', 'hub', 'an', 'ioe', 'hub', 'or', 'another', 'smart', 'tv', 'the', 'smart', 'tv', 'of', 'wherein', 'the', 'upcoming', 'media', 'programming', 'is', 'one', 'of', 'a', 'live', 'television', 'program', 'a', 'recorded', 'television', 'program', 'a', 'broadcast', 'television', 'program', 'or', 'an', 'application-provided', 'program', 'the', 'smart', 'tv', 'of', 'wherein', 'the', 'notification', 'includes', 'at', 'least', 'one', 'of', 'a', 'push', 'message', 'a', 'sms', 'message', 'a', 'waysms', 'message', 'an', 'audio', 'alert', 'an', 'audio', 'message', 'or', 'an', 'email', 'message', 'the', 'smart', 'tv', 'of', 'further', 'comprising', 'presenting', 'the', 'upcoming', 'media', 'programming', 'in', 'a', 'favorite', 'channel', 'list', 'the', 'smart', 'tv', 'of', 'further', 'comprising', 'means', 'for', 'obtaining', 'media', 'programming', 'viewing', 'data', 'wherein', 'the', 'media', 'programming', 'viewing', 'data', 'includes', 'at', 'least', 'one', 'of', 'a', 'historical', 'time', 'and', 'a', 'historical', 'date', 'that', 'one', 'or', 'more', 'media', 'programs', 'were', 'viewed', 'on', 'the', 'smart', 'tv', 'means', 'for', 'obtaining', 'at', 'least', 'one', 'of', 'a', 'current', 'time', 'and', 'a', 'current', 'date', 'means', 'for', 'processing', 'the', 'media', 'programming', 'viewing', 'data', 'to', 'determine', 'a', 'probability', 'of', 'the', 'one', 'or', 'more', 'media', 'programs', 'being', 'viewed', 'on', 'the', 'smart', 'tv', 'based', 'on', 'at', 'least', 'one', 'of', 'the', 'current', 'time', 'and', 'the', 'current', 'date', 'and', 'means', 'for', 'presenting', 'the', 'favorite', 'channel', 'list', 'based', 'on', 'the', 'determined', 'probability', 'of', 'the', 'one', 'or', 'more', 'media', 'programs', 'being', 'viewed', 'the', 'smart', 'tv', 'of', 'wherein', 'the', 'means', 'for', 'processing', 'the', 'media', 'programming', 'viewing', 'data', 'includes', 'employing', 'a', 'neural', 'network', 'model', 'the', 'smart', 'tv', 'of', 'wherein', 'employing', 'the', 'neural', 'network', 'model', 'comprises', 'determining', 'a', 'duration', 'that', 'the', 'one', 'or', 'more', 'media', 'programs', 'were', 'viewed', 'on', 'the', 'smart', 'tv', 'for', 'each', 'of', 'the', 'at', 'least', 'one', 'of', 'the', 'historical', 'time', 'and', 'the', 'historical', 'date', 'setting', 'a', 'threshold', 'time', 'duration', 'comparing', 'the', 'determined', 'duration', 'to', 'the', 'threshold', 'time', 'duration', 'and', 'filtering', 'out', 'the', 'one', 'or', 'more', 'media', 'programs', 'viewed', 'below', 'the', 'threshold', 'time', 'duration', 'the', 'smart', 'tv', 'of', 'further', 'comprising', 'means', 'for', 'adjusting', 'at', 'least', 'one', 'of', 'a', 'volume', 'or', 'a', 'brightness', 'of', 'the', 'smart', 'tv', 'wherein', 'the', 'adjusting', 'is', 'based', 'on', 'at', 'least', 'one', 'of', 'the', 'historical', 'time', 'and', 'the', 'historical', 'date', 'the', 'smart', 'tv', 'of', 'further', 'comprising', 'means', 'for', 'restricting', 'access', 'to', 'one', 'or', 'more', 'media', 'programs', 'a', 'non-transitory', 'computer-readable', 'medium', 'comprising', '-executable', 'program', 'code', 'configured', 'to', 'cause', 'a', 'of', 'a', 'smart', 'television', 'tv', 'to', 'receive', 'an', 'indication', 'of', 'upcoming', 'media', 'programming', 'wherein', 'the', 'upcoming', 'media', 'programming', 'is', 'based', 'on', 'a', 'profile', 'identify', 'one', 'or', 'more', 'devices', 'in', 'communication', 'with', 'the', 'smart', 'tv', 'each', 'of', 'the', 'one', 'or', 'more', 'devices', 'including', 'at', 'least', 'one', 'of', 'a', 'microphone', 'or', 'a', 'camera', 'instruct', 'at', 'least', 'one', 'identified', 'device', 'to', 'detect', 'audio', 'signals', 'using', 'its', 'respective', 'microphone', 'or', 'to', 'detect', 'visual', 'signals', 'using', 'its', 'respective', 'camera', 'select', 'at', 'least', 'one', 'device', 'of', 'the', 'one', 'or', 'more', 'devices', 'based', 'on', 'the', 'detected', 'audio', 'signal', 'or', 'detected', 'visual', 'signal', 'and', 'provide', 'instructions', 'to', 'the', 'selected', 'device', 'to', 'output', 'a', 'notification', 'related', 'to', 'the', 'upcoming', 'media', 'programming', 'the', 'non-transitory', 'computer-readable', 'medium', 'of', 'wherein', 'selecting', 'the', 'first', 'device', 'based', 'on', 'the', 'detected', 'audio', 'signal', 'includes', 'a', 'voice', 'the', 'non-transitory', 'computer-readable', 'medium', 'of', 'wherein', 'the', 'is', 'further', 'capable', 'of', 'executing', '-executable', 'program', 'code', 'to', 'determine', 'a', 'distance', 'to', 'the', 'recognized', 'voice', 'and', 'wherein', 'selecting', 'the', 'first', 'device', 'is', 'further', 'based', 'on', 'the', 'determined', 'distance', 'the', 'non-transitory', 'computer-readable', 'medium', 'of', 'wherein', 'selecting', 'the', 'first', 'device', 'based', 'on', 'the', 'detected', 'visual', 'signals', 'includes', 'a', 'face', 'the', 'non-transitory', 'computer-readable', 'medium', 'of', 'wherein', 'the', 'face', 'includes', 'a', 'face', 'technique', 'a', 'camera', 'comprising', 'a', 'sensor', 'array', 'including', 'a', 'of', 'sensors', 'an', 'infrared', 'ir', 'illuminator', 'configured', 'to', 'emit', 'active', 'ir', 'light', 'in', 'an', 'ir', 'light', 'sub-band', 'a', 'of', 'spectral', 'illuminators', 'each', 'spectral', 'illuminator', 'configured', 'to', 'emit', 'active', 'spectral', 'light', 'in', 'a', 'different', 'spectral', 'light', 'sub-band', 'a', 'depth', 'controller', 'machine', 'configured', 'to', 'determine', 'a', 'depth', 'value', 'for', 'each', 'of', 'the', 'of', 'sensors', 'based', 'on', 'the', 'active', 'ir', 'light', 'a', 'spectral', 'controller', 'machine', 'configured', 'to', 'for', 'each', 'of', 'the', 'of', 'sensors', 'determine', 'a', 'spectral', 'value', 'for', 'each', 'spectral', 'light', 'sub-band', 'of', 'the', 'of', 'spectral', 'illuminators', 'and', 'an', 'output', 'machine', 'configured', 'to', 'output', 'a', 'test', 'depth+multi-spectral', 'including', 'a', 'of', 'pixels', 'each', 'pixel', 'corresponding', 'to', 'one', 'of', 'the', 'of', 'sensors', 'of', 'the', 'sensor', 'array', 'and', 'including', 'at', 'least', 'a', 'depth', 'value', 'and', 'a', 'spectral', 'value', 'for', 'each', 'spectral', 'light', 'sub-band', 'of', 'the', 'of', 'spectral', 'illuminators', 'a', 'face', 'machine', 'previously', 'trained', 'with', 'a', 'set', 'of', 'labeled', 'training', 'depth+multi-spectral', 's', 'having', 'a', 'same', 'structure', 'as', 'the', 'test', 'depth+multi-spectral', 'the', 'face', 'machine', 'configured', 'to', 'output', 'a', 'confidence', 'value', 'indicating', 'a', 'likelihood', 'that', 'the', 'test', 'depth+multi-spectral', 'includes', 'a', 'face', 'the', 'camera', 'of', 'wherein', 'each', 'spectral', 'value', 'is', 'calculated', 'based', 'on', 'the', 'depth', 'value', 'determined', 'for', 'the', 'sensor', 'that', 'corresponds', 'to', 'the', 'pixel', 'the', 'camera', 'of', 'wherein', 'the', 'face', 'machine', 'is', 'configured', 'to', 'use', 'a', 'convolutional', 'neural', 'network', 'to', 'determine', 'the', 'confidence', 'value', 'the', 'camera', 'of', 'wherein', 'the', 'face', 'machine', 'includes', 'a', 'of', 'input', 'nodes', 'wherein', 'each', 'input', 'node', 'is', 'configured', 'to', 'receive', 'a', 'pixel', 'value', 'array', 'corresponding', 'to', 'a', 'different', 'pixel', 'of', 'the', 'of', 'pixels', 'of', 'the', 'test', 'depth+multi-spectral', 'and', 'wherein', 'the', 'pixel', 'value', 'array', 'includes', 'the', 'depth', 'value', 'and', 'the', 'of', 'multi-spectral', 'values', 'for', 'the', 'pixel', 'the', 'camera', 'of', 'wherein', 'the', 'of', 'multi-spectral', 'values', 'for', 'the', 'pixel', 'include', 'more', 'than', 'three', 'spectral', 'values', 'the', 'camera', 'of', 'wherein', 'the', 'output', 'machine', 'is', 'configured', 'to', 'output', 'a', 'surface', 'normal', 'for', 'each', 'pixel', 'of', 'the', 'test', 'depth+multi-spectral', 'and', 'wherein', 'the', 'pixel', 'value', 'array', 'includes', 'the', 'surface', 'normal', 'the', 'camera', 'of', 'wherein', 'the', 'output', 'machine', 'is', 'configured', 'to', 'output', 'a', 'curvature', 'for', 'each', 'pixel', 'of', 'the', 'test', 'depth+multi-spectral', 'and', 'wherein', 'the', 'pixel', 'value', 'array', 'includes', 'the', 'curvature', 'the', 'camera', 'of', 'wherein', 'the', 'face', 'machine', 'is', 'configured', 'to', 'use', 'a', 'of', 'models', 'to', 'determine', 'the', 'confidence', 'value', 'wherein', 'the', 'of', 'models', 'includes', 'a', 'of', 'channel-specific', 'models', 'wherein', 'each', 'channel-specific', 'model', 'is', 'configured', 'to', 'process', 'a', 'different', 'pixel', 'parameter', 'for', 'the', 'of', 'pixels', 'of', 'the', 'test', 'depth+multi-spectral', 'wherein', 'each', 'channel-specific', 'model', 'includes', 'a', 'of', 'input', 'nodes', 'and', 'wherein', 'for', 'each', 'channel-specific', 'model', 'each', 'input', 'node', 'is', 'configured', 'to', 'receive', 'a', 'pixel', 'parameter', 'value', 'for', 'a', 'different', 'pixel', 'of', 'the', 'of', 'pixels', 'of', 'the', 'test', 'depth+multi-spectral', 'the', 'camera', 'of', 'wherein', 'the', 'face', 'machine', 'is', 'configured', 'to', 'use', 'a', 'statistical', 'model', 'to', 'determine', 'the', 'confidence', 'value', 'the', 'camera', 'of', 'wherein', 'the', 'statistical', 'model', 'includes', 'a', 'nearest', 'neighbor', 'algorithm', 'the', 'camera', 'of', 'wherein', 'the', 'statistical', 'model', 'includes', 'a', 'support', 'vector', 'machine', 'the', 'camera', 'of', 'wherein', 'the', 'face', 'machine', 'is', 'further', 'configured', 'to', 'output', 'a', 'location', 'on', 'the', 'test', 'depth+multi-spectral', 'of', 'a', 'bounding', 'box', 'around', 'a', 'recognized', 'face', 'the', 'camera', 'of', 'wherein', 'the', 'face', 'machine', 'is', 'further', 'configured', 'to', 'output', 'a', 'location', 'on', 'the', 'test', 'depth+multi-spectral', 'of', 'an', 'identified', 'two-dimensional', 'd', 'facial', 'feature', 'of', 'a', 'recognized', 'face', 'the', 'camera', 'of', 'wherein', 'the', 'face', 'machine', 'is', 'further', 'configured', 'to', 'output', 'a', 'location', 'on', 'the', 'test', 'depth+multi-spectral', 'of', 'an', 'identified', 'three-dimensional', 'd', 'facial', 'feature', 'of', 'a', 'recognized', 'face', 'the', 'camera', 'of', 'wherein', 'the', 'face', 'machine', 'is', 'further', 'configured', 'to', 'output', 'a', 'location', 'on', 'the', 'test', 'depth+multi-spectral', 'of', 'an', 'identified', 'spectral', 'feature', 'on', 'a', 'recognized', 'face', 'the', 'camera', 'of', 'wherein', 'the', 'face', 'machine', 'is', 'further', 'configured', 'to', 'output', 'for', 'each', 'pixel', 'of', 'the', 'test', 'depth+multi-spectral', 'a', 'confidence', 'value', 'indicating', 'a', 'likelihood', 'that', 'the', 'pixel', 'is', 'included', 'in', 'a', 'face', 'the', 'camera', 'of', 'wherein', 'the', 'face', 'machine', 'is', 'further', 'configured', 'to', 'output', 'an', 'identity', 'of', 'a', 'face', 'recognized', 'in', 'the', 'test', 'depth+multi-spectral', 'the', 'camera', 'of', 'wherein', 'the', 'of', 'sensors', 'of', 'the', 'sensor', 'array', 'are', 'differential', 'sensors', 'and', 'wherein', 'each', 'spectral', 'value', 'is', 'determined', 'based', 'on', 'a', 'depth', 'value', 'and', 'a', 'differential', 'measurement', 'for', 'that', 'differential', 'sensor', 'a', 'camera', 'comprising', 'a', 'sensor', 'array', 'including', 'a', 'of', 'sensors', 'an', 'infrared', 'ir', 'illuminator', 'configured', 'to', 'emit', 'active', 'ir', 'light', 'in', 'an', 'ir', 'light', 'sub-band', 'a', 'of', 'spectral', 'illuminators', 'each', 'spectral', 'illuminator', 'configured', 'to', 'emit', 'active', 'spectral', 'light', 'in', 'a', 'different', 'spectral', 'light', 'sub-band', 'a', 'depth', 'controller', 'machine', 'configured', 'to', 'determine', 'a', 'depth', 'value', 'for', 'each', 'of', 'the', 'of', 'sensors', 'based', 'on', 'the', 'active', 'ir', 'light', 'a', 'spectral', 'controller', 'machine', 'configured', 'to', 'for', 'each', 'of', 'the', 'of', 'sensors', 'determine', 'a', 'spectral', 'value', 'for', 'each', 'spectral', 'light', 'sub-band', 'of', 'the', 'of', 'spectral', 'illuminators', 'wherein', 'each', 'spectral', 'value', 'is', 'calculated', 'based', 'on', 'the', 'depth', 'value', 'determined', 'for', 'the', 'sensor', 'that', 'corresponds', 'to', 'the', 'pixel', 'and', 'an', 'output', 'machine', 'configured', 'to', 'output', 'a', 'test', 'depth+multi-spectral', 'including', 'a', 'of', 'pixels', 'each', 'pixel', 'corresponding', 'to', 'one', 'of', 'the', 'of', 'sensors', 'of', 'the', 'sensor', 'array', 'and', 'including', 'at', 'least', 'a', 'depth', 'value', 'and', 'a', 'spectral', 'value', 'for', 'each', 'spectral', 'light', 'sub-band', 'of', 'the', 'of', 'spectral', 'illuminators', 'and', 'a', 'face', 'machine', 'including', 'a', 'convolutional', 'neural', 'network', 'previously', 'trained', 'with', 'a', 'set', 'of', 'labeled', 'training', 'depth+multi-spectral', 's', 'having', 'a', 'same', 'structure', 'as', 'the', 'test', 'depth+multi-spectral', 'the', 'face', 'machine', 'configured', 'to', 'output', 'a', 'confidence', 'value', 'indicating', 'a', 'likelihood', 'that', 'the', 'test', 'depth+multi-spectral', 'includes', 'a', 'face', 'an', 'processing', 'comprising', 'acquiring', 'a', 'photo', 'album', 'obtained', 'from', 'face', 'clustering', 'collecting', 'face', 'information', 'of', 'respective', 's', 'in', 'the', 'photo', 'album', 'and', 'acquiring', 'a', 'face', 'parameter', 'of', 'each', 'according', 'to', 'the', 'face', 'information', 'selecting', 'a', 'cover', 'according', 'to', 'the', 'face', 'parameter', 'of', 'each', 'and', 'taking', 'a', 'face-region', 'from', 'the', 'cover', 'and', 'setting', 'the', 'face-region', 'as', 'a', 'cover', 'of', 'the', 'photo', 'album', 'wherein', 'selecting', 'the', 'cover', 'according', 'to', 'the', 'face', 'parameter', 'of', 'each', 'comprises', 'performing', 'calculation', 'on', 'the', 'face', 'parameter', 'of', 'each', 'in', 'a', 'preset', 'way', 'to', 'obtain', 'a', 'cover', 'score', 'of', 'each', 'selecting', 'the', 'with', 'a', 'highest', 'cover', 'score', 'as', 'the', 'cover', 'wherein', 'selecting', 'the', 'with', 'the', 'highest', 'cover', 'score', 'as', 'the', 'cover', 'comprises', 'acquiring', 'a', 'source', 'of', 'each', 'and', 'selecting', 'the', 'with', 'the', 'highest', 'cover', 'score', 'in', 's', 'coming', 'from', 'a', 'preset', 'source', 'as', 'the', 'cover', 'the', 'according', 'to', 'wherein', 'selecting', 'the', 'with', 'the', 'highest', 'cover', 'score', 'as', 'the', 'cover', 'comprises', 'acquiring', 'the', 'number', 'of', 'contained', 'in', 'each', 'determining', 'single-person', 's', 'according', 'to', 'the', 'number', 'of', 'and', 'selecting', 'the', 'single-person', 'with', 'the', 'highest', 'cover', 'score', 'as', 'the', 'cover', 'the', 'according', 'to', 'wherein', 'selecting', 'the', 'with', 'the', 'highest', 'cover', 'score', 'as', 'the', 'cover', 'further', 'comprises', 'when', 'there', 'is', 'no', 'single-person', 'in', 'the', 'photo', 'album', 'determining', 's', 'including', 'two', 'from', 'the', 'photo', 'album', 'and', 'selecting', 'the', 'with', 'the', 'highest', 'cover', 'score', 'from', 'the', 's', 'including', 'two', 'as', 'the', 'cover', 'the', 'according', 'to', 'wherein', 'the', 'face', 'information', 'comprises', 'face', 'feature', 'points', 'and', 'the', 'face', 'parameter', 'comprises', 'a', 'face', 'turning', 'angle', 'acquiring', 'the', 'face', 'parameter', 'of', 'each', 'according', 'to', 'the', 'face', 'information', 'comprises', 'acquiring', 'coordinate', 'values', 'of', 'the', 'face', 'feature', 'points', 'determining', 'distances', 'and', 'angles', 'between', 'the', 'face', 'feature', 'points', 'and', 'determining', 'the', 'face', 'turning', 'angle', 'according', 'to', 'the', 'distances', 'and', 'the', 'angles', 'the', 'according', 'to', 'wherein', 'the', 'face', 'parameter', 'comprises', 'a', 'face', 'ratio', 'acquiring', 'the', 'face', 'parameter', 'of', 'each', 'according', 'to', 'the', 'face', 'information', 'comprises', 'determining', 'a', 'face', 'region', 'of', 'the', 'according', 'to', 'the', 'face', 'information', 'and', 'calculating', 'a', 'ratio', 'of', 'an', 'area', 'of', 'the', 'face', 'region', 'to', 'an', 'area', 'of', 'the', 'to', 'obtain', 'the', 'face', 'ratio', 'the', 'according', 'to', 'wherein', 'calculating', 'the', 'face', 'ratio', 'comprises', 'when', 'there', 'is', 'more', 'than', 'one', 'face', 'in', 'the', 'subtracting', 'an', 'area', 'occupied', 'other', 'than', 'a', 'face', 'corresponding', 'to', 'the', 'photo', 'album', 'from', 'the', 'face', 'region', 'to', 'obtain', 'a', 'remaining', 'area', 'and', 'calculating', 'a', 'ratio', 'of', 'the', 'remaining', 'area', 'to', 'the', 'area', 'of', 'the', 'to', 'obtain', 'the', 'face', 'ratio', 'the', 'according', 'to', 'wherein', 'collecting', 'face', 'information', 'of', 'respective', 's', 'in', 'the', 'photo', 'album', 'comprises', 'acquiring', 'identifications', 'of', 's', 'in', 'the', 'photo', 'album', 'extracting', 'face', 'information', 'corresponding', 'to', 'the', 'identifications', 'from', 'a', 'face', 'database', 'the', 'face', 'database', 'being', 'stored', 'with', 'face', 'results', 'of', 's', 'the', 'face', 'results', 'including', 'the', 'face', 'information', 'an', 'processing', 'apparatus', 'comprising', 'a', 'and', 'a', 'memory', 'configured', 'to', 'store', 'instructions', 'executable', 'by', 'the', 'wherein', 'the', 'is', 'configured', 'to', 'run', 'a', 'program', 'corresponding', 'to', 'the', 'instructions', 'by', 'reading', 'the', 'instructions', 'stored', 'in', 'the', 'memory', 'so', 'as', 'to', 'perform', 'acquiring', 'a', 'photo', 'album', 'obtained', 'from', 'face', 'clustering', 'collecting', 'face', 'information', 'of', 'each', 'in', 'the', 'photo', 'album', 'acquiring', 'a', 'face', 'parameter', 'of', 'each', 'according', 'to', 'the', 'face', 'information', 'selecting', 'a', 'cover', 'according', 'to', 'the', 'face', 'parameter', 'of', 'each', 'taking', 'a', 'face-region', 'from', 'the', 'cover', 'and', 'setting', 'the', 'face-region', 'as', 'a', 'cover', 'of', 'the', 'photo', 'album', 'wherein', 'the', 'is', 'configured', 'to', 'perform', 'calculation', 'on', 'the', 'face', 'parameter', 'of', 'each', 'in', 'a', 'preset', 'way', 'to', 'obtain', 'a', 'cover', 'score', 'of', 'each', 'and', 'select', 'the', 'with', 'a', 'highest', 'cover', 'score', 'as', 'the', 'cover', 'and', 'wherein', 'the', 'is', 'configured', 'to', 'acquire', 'a', 'source', 'of', 'each', 'and', 'select', 'the', 'with', 'the', 'highest', 'cover', 'score', 'in', 's', 'coming', 'from', 'a', 'preset', 'source', 'as', 'the', 'cover', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'is', 'configured', 'to', 'acquire', 'the', 'number', 'of', 'contained', 'in', 'each', 'determine', 'single-person', 's', 'according', 'to', 'the', 'number', 'of', 'and', 'select', 'the', 'single-person', 'with', 'the', 'highest', 'cover', 'score', 'as', 'the', 'cover', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'is', 'further', 'configured', 'to', 'when', 'there', 'is', 'no', 'single-person', 'in', 'the', 'photo', 'album', 'determine', 's', 'including', 'two', 'from', 'the', 'photo', 'album', 'and', 'select', 'the', 'with', 'the', 'highest', 'cover', 'score', 'from', 'the', 's', 'including', 'two', 'as', 'the', 'cover', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'face', 'information', 'comprises', 'face', 'feature', 'points', 'and', 'the', 'face', 'parameter', 'comprises', 'a', 'face', 'turning', 'angle', 'the', 'is', 'configured', 'to', 'acquire', 'coordinate', 'values', 'of', 'the', 'face', 'feature', 'points', 'determine', 'distances', 'and', 'angles', 'between', 'the', 'face', 'feature', 'points', 'and', 'determine', 'the', 'face', 'turning', 'angle', 'according', 'to', 'the', 'distances', 'and', 'the', 'angles', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'face', 'parameter', 'comprises', 'a', 'face', 'ratio', 'the', 'is', 'configured', 'to', 'determine', 'a', 'face', 'region', 'of', 'the', 'according', 'to', 'the', 'face', 'information', 'and', 'calculate', 'a', 'ratio', 'of', 'an', 'area', 'of', 'the', 'face', 'region', 'to', 'an', 'area', 'of', 'the', 'to', 'obtain', 'the', 'face', 'ratio', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'is', 'configured', 'to', 'when', 'there', 'is', 'more', 'than', 'one', 'face', 'in', 'the', 'subtract', 'an', 'area', 'occupied', 'other', 'than', 'a', 'face', 'corresponding', 'to', 'the', 'photo', 'album', 'from', 'the', 'face', 'region', 'to', 'obtain', 'a', 'remaining', 'area', 'and', 'calculate', 'a', 'ratio', 'of', 'the', 'remaining', 'area', 'to', 'the', 'area', 'of', 'the', 'to', 'obtain', 'the', 'face', 'ratio', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'is', 'configured', 'to', 'acquire', 'identifications', 'of', 's', 'in', 'the', 'photo', 'album', 'extract', 'face', 'information', 'corresponding', 'to', 'the', 'identifications', 'from', 'a', 'face', 'database', 'the', 'face', 'database', 'being', 'stored', 'with', 'face', 'results', 'of', 's', 'the', 'face', 'results', 'including', 'the', 'face', 'information', 'an', 'comprising', 'a', 'a', 'memory', 'a', 'display', 'screen', 'and', 'an', 'input', 'device', 'connected', 'via', 'a', 'system', 'bus', 'wherein', 'the', 'memory', 'is', 'stored', 'with', 'computer', 'programs', 'that', 'when', 'executed', 'by', 'the', 'cause', 'the', 'to', 'implement', 'an', 'processing', 'the', 'processing', 'comprising', 'acquiring', 'a', 'photo', 'album', 'obtained', 'from', 'face', 'clustering', 'collecting', 'face', 'information', 'of', 'respective', 's', 'in', 'the', 'photo', 'album', 'and', 'acquiring', 'a', 'face', 'parameter', 'of', 'each', 'according', 'to', 'the', 'face', 'information', 'selecting', 'a', 'cover', 'according', 'to', 'the', 'face', 'parameter', 'of', 'each', 'and', 'taking', 'a', 'face-region', 'from', 'the', 'cover', 'and', 'setting', 'the', 'face-region', 'as', 'a', 'cover', 'of', 'the', 'photo', 'album', 'wherein', 'selecting', 'the', 'cover', 'according', 'to', 'the', 'face', 'parameter', 'of', 'each', 'comprises', 'performing', 'calculation', 'on', 'the', 'face', 'parameter', 'of', 'each', 'in', 'a', 'preset', 'way', 'to', 'obtain', 'a', 'cover', 'score', 'of', 'each', 'and', 'selecting', 'the', 'with', 'a', 'highest', 'cover', 'score', 'as', 'the', 'cover', 'and', 'wherein', 'selecting', 'the', 'with', 'the', 'highest', 'cover', 'score', 'as', 'the', 'cover', 'comprises', 'acquiring', 'a', 'source', 'of', 'each', 'and', 'selecting', 'the', 'with', 'the', 'highest', 'cover', 'score', 'in', 's', 'coming', 'from', 'a', 'preset', 'source', 'as', 'the', 'cover', 'the', 'according', 'to', 'wherein', 'the', 'comprises', 'at', 'least', 'one', 'of', 'a', 'mobile', 'phone', 'a', 'tablet', 'computer', 'a', 'personal', 'digital', 'assistant', 'and', 'a', 'wearable', 'device', 'a', 'computer-implemented', 'comprising', 'receiving', 'at', 'a', 'computing', 'device', 'a', 'meeting', 'invitation', 'identifying', 'a', 'location', 'and', 'at', 'least', 'one', 'invitee', 'the', 'meeting', 'invitation', 'configured', 'to', 'provide', 'the', 'at', 'least', 'one', 'invitee', 'with', 'physical', 'access', 'to', 'the', 'location', 'wherein', 'the', 'meeting', 'invitation', 'causes', 'a', 'system', 'to', 'control', 'a', 'pathway', 'allowing', 'physical', 'access', 'to', 'the', 'location', 'providing', 'based', 'on', 'the', 'meeting', 'invitation', 'the', 'at', 'least', 'one', 'invitee', 'with', 'physical', 'access', 'to', 'the', 'location', 'by', 'controlling', 'the', 'pathway', 'allowing', 'the', 'at', 'least', 'one', 'invitee', 'to', 'physically', 'access', 'the', 'location', 'through', 'the', 'pathway', 'in', 'response', 'to', 'positioning', 'data', 'indicating', 'that', 'the', 'at', 'least', 'one', 'invitee', 'is', 'at', 'a', 'predetermined', 'location', 'near', 'the', 'location', 'wherein', 'the', 'positioning', 'data', 'is', 'based', 'in', 'part', 'on', 'a', 'face', 'camera', 'system', 'identifying', 'the', 'at', 'least', 'one', 'invitee', 'receiving', 'the', 'positioning', 'data', 'from', 'the', 'face', 'camera', 'system', 'identifying', 'the', 'at', 'least', 'one', 'invitee', 'wherein', 'the', 'positioning', 'data', 'indicates', 'a', 'pattern', 'of', 'movement', 'of', 'the', 'at', 'least', 'one', 'invitee', 'determining', 'that', 'the', 'pattern', 'of', 'movement', 'indicates', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'exited', 'the', 'location', 'and', 'revoking', 'physical', 'access', 'to', 'the', 'location', 'identified', 'in', 'the', 'meeting', 'invitation', 'by', 'controlling', 'the', 'pathway', 'to', 'restrict', 'the', 'at', 'least', 'one', 'invitee', 'identified', 'in', 'the', 'meeting', 'invitation', 'from', 'physical', 'access', 'to', 'the', 'location', 'through', 'the', 'pathway', 'in', 'response', 'to', 'determining', 'that', 'the', 'pattern', 'of', 'movement', 'indicates', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'exited', 'the', 'location', 'the', 'computer-implemented', 'of', 'wherein', 'determining', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'exited', 'the', 'location', 'comprises', 'determining', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'passed', 'through', 'an', 'egress', 'associated', 'with', 'the', 'location', 'in', 'a', 'predetermined', 'direction', 'the', 'computer-implemented', 'of', 'wherein', 'determining', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'exited', 'the', 'location', 'comprises', 'determining', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'moved', 'through', 'an', 'area', 'in', 'a', 'predetermined', 'direction', 'the', 'computer-implemented', 'of', 'wherein', 'the', 'positioning', 'data', 'indicates', 'a', 'second', 'pattern', 'of', 'movement', 'of', 'the', 'at', 'least', 'one', 'invitee', 'and', 'wherein', 'access', 'to', 'secured', 'data', 'associated', 'with', 'the', 'location', 'is', 'provided', 'in', 'response', 'to', 'detecting', 'the', 'second', 'pattern', 'of', 'movement', 'the', 'computer-implemented', 'of', 'further', 'comprising', 'collating', 'secured', 'data', 'and', 'public', 'data', 'to', 'generate', 'resource', 'data', 'and', 'communicating', 'the', 'resource', 'data', 'to', 'a', 'client', 'computing', 'device', 'associated', 'with', 'the', 'at', 'least', 'one', 'invitee', 'when', 'access', 'of', 'the', 'location', 'is', 'provided', 'the', 'computer-implemented', 'of', 'wherein', 'the', 'positioning', 'data', 'indicates', 'that', 'the', 'at', 'least', 'one', 'invitee', 'is', 'at', 'the', 'predetermined', 'location', 'when', 'the', 'at', 'least', 'one', 'invitee', 'passes', 'through', 'the', 'predetermined', 'location', 'the', 'computer-implemented', 'of', 'wherein', 'the', 'positioning', 'data', 'indicates', 'that', 'the', 'at', 'least', 'one', 'invitee', 'is', 'at', 'the', 'predetermined', 'location', 'when', 'the', 'at', 'least', 'one', 'invitee', 'passes', 'through', 'the', 'predetermined', 'location', 'near', 'the', 'location', 'in', 'a', 'predetermined', 'direction', 'a', 'system', 'comprising', 'a', 'and', 'a', 'memory', 'in', 'communication', 'with', 'the', 'the', 'memory', 'having', 'computer-readable', 'instructions', 'stored', 'thereupon', 'that', 'when', 'executed', 'by', 'the', 'cause', 'the', 'to', 'receive', 'a', 'meeting', 'invitation', 'indicating', 'a', 'location', 'and', 'an', 'identity', 'the', 'meeting', 'invitation', 'configured', 'to', 'provide', 'at', 'least', 'one', 'invitee', 'with', 'physical', 'access', 'to', 'the', 'location', 'wherein', 'the', 'meeting', 'invitation', 'causes', 'the', 'system', 'to', 'control', 'a', 'pathway', 'allowing', 'physical', 'access', 'to', 'the', 'location', 'provide', 'the', 'at', 'least', 'one', 'invitee', 'associated', 'with', 'the', 'identity', 'access', 'to', 'the', 'location', 'by', 'controlling', 'the', 'pathway', 'allowing', 'the', 'at', 'least', 'one', 'invitee', 'to', 'physically', 'access', 'the', 'location', 'through', 'the', 'pathway', 'in', 'response', 'to', 'positioning', 'data', 'indicating', 'that', 'the', 'at', 'least', 'one', 'invitee', 'is', 'at', 'a', 'predetermined', 'location', 'near', 'the', 'location', 'wherein', 'the', 'positioning', 'data', 'is', 'based', 'in', 'part', 'on', 'a', 'face', 'camera', 'system', 'identifying', 'the', 'at', 'least', 'one', 'invitee', 'receive', 'the', 'positioning', 'data', 'from', 'the', 'face', 'camera', 'system', 'identifying', 'the', 'at', 'least', 'one', 'invitee', 'wherein', 'the', 'positioning', 'data', 'indicates', 'a', 'pattern', 'of', 'movement', 'of', 'the', 'at', 'least', 'one', 'invitee', 'determine', 'that', 'the', 'pattern', 'of', 'movement', 'indicates', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'exited', 'the', 'location', 'and', 'revoke', 'physical', 'access', 'to', 'the', 'location', 'identified', 'in', 'the', 'meeting', 'invitation', 'by', 'controlling', 'the', 'pathway', 'to', 'restrict', 'the', 'at', 'least', 'one', 'invitee', 'identified', 'in', 'the', 'meeting', 'invitation', 'from', 'physical', 'access', 'to', 'the', 'location', 'through', 'the', 'pathway', 'in', 'response', 'to', 'determining', 'that', 'the', 'pattern', 'of', 'movement', 'indicates', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'exited', 'the', 'location', 'the', 'system', 'of', 'wherein', 'determining', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'exited', 'the', 'location', 'comprises', 'determining', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'passed', 'through', 'an', 'egress', 'associated', 'with', 'the', 'location', 'the', 'system', 'of', 'wherein', 'determining', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'exited', 'the', 'location', 'comprises', 'determining', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'moved', 'through', 'an', 'area', 'in', 'a', 'predetermined', 'direction', 'the', 'system', 'of', 'wherein', 'the', 'positioning', 'data', 'indicates', 'a', 'second', 'pattern', 'of', 'movement', 'of', 'the', 'at', 'least', 'one', 'invitee', 'and', 'wherein', 'access', 'to', 'secured', 'data', 'associated', 'with', 'the', 'location', 'is', 'provided', 'in', 'response', 'to', 'detecting', 'the', 'second', 'pattern', 'of', 'movement', 'the', 'system', 'of', 'wherein', 'the', 'instructions', 'further', 'cause', 'the', 'to', 'collate', 'secured', 'data', 'and', 'public', 'data', 'to', 'generate', 'resource', 'data', 'and', 'communicate', 'the', 'resource', 'data', 'to', 'a', 'client', 'computing', 'device', 'associated', 'with', 'the', 'at', 'least', 'one', 'invitee', 'when', 'access', 'of', 'the', 'location', 'is', 'provided', 'a', 'non-transitory', 'computer-readable', 'storage', 'medium', 'having', 'computer-executable', 'instructions', 'stored', 'thereupon', 'which', 'when', 'executed', 'by', 'one', 'or', 'more', 's', 'of', 'a', 'computing', 'device', 'cause', 'the', 'one', 'or', 'more', 's', 'of', 'the', 'computing', 'device', 'to', 'receive', 'a', 'meeting', 'invitation', 'indicating', 'a', 'location', 'and', 'an', 'identity', 'the', 'meeting', 'invitation', 'configured', 'to', 'provide', 'at', 'least', 'one', 'invitee', 'with', 'physical', 'access', 'to', 'the', 'location', 'wherein', 'the', 'meeting', 'invitation', 'causes', 'a', 'system', 'to', 'control', 'a', 'pathway', 'allowing', 'physical', 'access', 'to', 'the', 'location', 'provide', 'the', 'at', 'least', 'one', 'invitee', 'associated', 'with', 'the', 'identity', 'access', 'to', 'the', 'location', 'by', 'controlling', 'the', 'pathway', 'allowing', 'the', 'at', 'least', 'one', 'invitee', 'to', 'physically', 'access', 'the', 'location', 'through', 'the', 'pathway', 'in', 'response', 'to', 'positioning', 'data', 'indicating', 'that', 'the', 'at', 'least', 'one', 'invitee', 'is', 'at', 'a', 'predetermined', 'location', 'near', 'the', 'location', 'wherein', 'the', 'positioning', 'data', 'is', 'based', 'in', 'part', 'on', 'a', 'face', 'camera', 'system', 'identifying', 'the', 'at', 'least', 'one', 'invitee', 'receive', 'the', 'positioning', 'data', 'from', 'the', 'face', 'camera', 'system', 'identifying', 'the', 'at', 'least', 'one', 'invitee', 'wherein', 'the', 'positioning', 'data', 'indicates', 'a', 'pattern', 'of', 'movement', 'of', 'the', 'at', 'least', 'one', 'invitee', 'determine', 'that', 'the', 'pattern', 'of', 'movement', 'indicates', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'exited', 'the', 'location', 'and', 'revoke', 'physical', 'access', 'to', 'the', 'location', 'identified', 'in', 'the', 'meeting', 'invitation', 'by', 'controlling', 'the', 'pathway', 'to', 'restrict', 'the', 'at', 'least', 'one', 'invitee', 'identified', 'in', 'the', 'meeting', 'invitation', 'from', 'physical', 'access', 'to', 'the', 'location', 'through', 'the', 'pathway', 'in', 'response', 'to', 'determining', 'that', 'the', 'pattern', 'of', 'movement', 'indicates', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'exited', 'the', 'location', 'the', 'non-transitory', 'computer-readable', 'storage', 'medium', 'of', 'wherein', 'determining', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'exited', 'the', 'location', 'comprises', 'determining', 'that', 'the', 'at', 'least', 'one', 'invitee', 'has', 'passed', 'through', 'an', 'egress', 'associated', 'with', 'the', 'location', 'the', 'non-transitory', 'computer-readable', 'storage', 'medium', 'of', 'wherein', 'the', 'positioning', 'data', 'indicates', 'a', 'second', 'pattern', 'of', 'movement', 'of', 'the', 'at', 'least', 'one', 'invitee', 'and', 'wherein', 'access', 'to', 'secured', 'data', 'associated', 'with', 'the', 'location', 'is', 'provided', 'in', 'response', 'to', 'detecting', 'the', 'second', 'pattern', 'of', 'movement', 'the', 'non-transitory', 'computer-readable', 'storage', 'medium', 'of', 'wherein', 'the', 'instructions', 'further', 'cause', 'the', 'one', 'or', 'more', 's', 'to', 'collate', 'secured', 'data', 'and', 'public', 'data', 'to', 'generate', 'resource', 'data', 'and', 'communicate', 'the', 'resource', 'data', 'to', 'a', 'client', 'computing', 'device', 'associated', 'with', 'the', 'at', 'least', 'one', 'invitee', 'when', 'access', 'of', 'the', 'location', 'is', 'provided', 'a', 'comprising', 'receiving', 'a', 'piece', 'of', 'content', 'and', 'salient', 'data', 'for', 'the', 'piece', 'of', 'content', 'based', 'on', 'the', 'salient', 'data', 'determining', 'a', 'first', 'path', 'for', 'a', 'viewport', 'for', 'the', 'piece', 'of', 'content', 'wherein', 'the', 'first', 'path', 'for', 'the', 'viewport', 'includes', 'different', 'salient', 'events', 'occurring', 'in', 'the', 'piece', 'of', 'content', 'at', 'different', 'times', 'during', 'playback', 'of', 'the', 'piece', 'of', 'content', 'providing', 'the', 'viewport', 'on', 'a', 'display', 'device', 'wherein', 'movement', 'of', 'the', 'viewport', 'is', 'based', 'on', 'the', 'first', 'path', 'for', 'the', 'viewport', 'and', 'the', 'salient', 'data', 'during', 'the', 'playback', 'detecting', 'an', 'additional', 'salient', 'event', 'in', 'the', 'piece', 'of', 'content', 'that', 'is', 'not', 'included', 'in', 'the', 'first', 'path', 'for', 'the', 'viewport', 'and', 'providing', 'an', 'indication', 'for', 'the', 'additional', 'salient', 'event', 'in', 'the', 'viewport', 'during', 'the', 'playback', 'the', 'of', 'wherein', 'the', 'salient', 'data', 'identifies', 'each', 'salient', 'event', 'in', 'the', 'piece', 'of', 'content', 'and', 'the', 'salient', 'data', 'indicates', 'for', 'each', 'salient', 'event', 'in', 'the', 'piece', 'of', 'content', 'a', 'corresponding', 'point', 'location', 'of', 'the', 'salient', 'event', 'in', 'the', 'piece', 'of', 'content', 'and', 'a', 'corresponding', 'time', 'at', 'which', 'the', 'salient', 'event', 'occurs', 'during', 'the', 'playback', 'the', 'of', 'wherein', 'the', 'salient', 'data', 'further', 'indicates', 'for', 'each', 'salient', 'event', 'in', 'the', 'piece', 'of', 'content', 'a', 'corresponding', 'type', 'of', 'the', 'salient', 'event', 'and', 'a', 'corresponding', 'strength', 'value', 'of', 'the', 'salient', 'event', 'the', 'of', 'wherein', 'the', 'first', 'path', 'for', 'the', 'viewport', 'controls', 'the', 'movement', 'of', 'the', 'viewport', 'to', 'put', 'the', 'different', 'salient', 'events', 'in', 'a', 'view', 'of', 'the', 'viewport', 'at', 'the', 'different', 'times', 'during', 'the', 'playback', 'the', 'of', 'further', 'comprising', 'detecting', 'one', 'or', 'more', 'salient', 'events', 'in', 'the', 'piece', 'of', 'content', 'based', 'on', 'at', 'least', 'one', 'of', 'the', 'following', 'visual', 'data', 'of', 'the', 'piece', 'of', 'content', 'audio', 'data', 'of', 'the', 'piece', 'of', 'content', 'or', 'content', 'consumption', 'experience', 'data', 'for', 'the', 'piece', 'of', 'content', 'wherein', 'the', 'salient', 'data', 'is', 'indicative', 'of', 'each', 'salient', 'event', 'detected', 'the', 'of', 'further', 'comprising', 'detecting', 'one', 'or', 'more', 'salient', 'events', 'in', 'the', 'piece', 'of', 'content', 'based', 'on', 'at', 'least', 'one', 'of', 'the', 'following', 'face', 'facial', 'emotion', 'object', 'motion', 'or', 'metadata', 'of', 'the', 'piece', 'of', 'content', 'wherein', 'the', 'salient', 'data', 'is', 'indicative', 'of', 'each', 'salient', 'event', 'detected', 'the', 'of', 'further', 'comprising', 'detecting', 'interaction', 'with', 'the', 'indication', 'wherein', 'the', 'indication', 'comprises', 'an', 'interactive', 'hint', 'and', 'in', 'response', 'to', 'detecting', 'the', 'interaction', 'adapting', 'the', 'first', 'path', 'for', 'the', 'viewport', 'to', 'a', 'second', 'path', 'for', 'the', 'viewport', 'based', 'on', 'the', 'interaction', 'wherein', 'the', 'second', 'path', 'for', 'the', 'viewport', 'includes', 'the', 'additional', 'salient', 'event', 'and', 'providing', 'an', 'updated', 'viewport', 'for', 'the', 'piece', 'of', 'content', 'on', 'the', 'display', 'device', 'wherein', 'movement', 'of', 'the', 'updated', 'viewport', 'is', 'based', 'on', 'the', 'second', 'path', 'for', 'the', 'viewport', 'and', 'the', 'salient', 'data', 'during', 'the', 'playback', 'and', 'the', 'second', 'path', 'for', 'the', 'viewport', 'controls', 'the', 'movement', 'of', 'the', 'updated', 'viewport', 'to', 'put', 'the', 'additional', 'salient', 'event', 'in', 'a', 'view', 'of', 'the', 'updated', 'viewport', 'the', 'of', 'further', 'comprising', 'changing', 'a', 'weight', 'assigned', 'to', 'the', 'additional', 'salient', 'event', 'and', 'one', 'or', 'more', 'other', 'salient', 'events', 'in', 'the', 'piece', 'of', 'content', 'having', 'the', 'same', 'type', 'as', 'the', 'additional', 'salient', 'event', 'the', 'of', 'wherein', 'the', 'second', 'path', 'for', 'the', 'viewport', 'includes', 'one', 'or', 'more', 'other', 'salient', 'events', 'in', 'the', 'piece', 'of', 'content', 'having', 'the', 'same', 'type', 'as', 'the', 'additional', 'salient', 'event', 'a', 'system', 'comprising', 'at', 'least', 'one', 'and', 'a', 'non-transitory', '-readable', 'memory', 'device', 'storing', 'instructions', 'that', 'when', 'executed', 'by', 'the', 'at', 'least', 'one', 'causes', 'the', 'at', 'least', 'one', 'to', 'perform', 'operations', 'including', 'receiving', 'a', 'piece', 'of', 'content', 'and', 'salient', 'data', 'for', 'the', 'piece', 'of', 'content', 'based', 'on', 'the', 'salient', 'data', 'determining', 'a', 'first', 'path', 'for', 'a', 'viewport', 'for', 'the', 'piece', 'of', 'content', 'wherein', 'the', 'first', 'path', 'for', 'the', 'viewport', 'includes', 'different', 'salient', 'events', 'occurring', 'in', 'the', 'piece', 'of', 'content', 'at', 'different', 'times', 'during', 'playback', 'of', 'the', 'piece', 'of', 'content', 'providing', 'the', 'viewport', 'on', 'a', 'display', 'device', 'wherein', 'movement', 'of', 'the', 'viewport', 'is', 'based', 'on', 'the', 'first', 'path', 'for', 'the', 'viewport', 'and', 'the', 'salient', 'data', 'during', 'the', 'playback', 'detecting', 'an', 'additional', 'salient', 'event', 'in', 'the', 'piece', 'of', 'content', 'that', 'is', 'not', 'included', 'in', 'the', 'first', 'path', 'for', 'the', 'viewport', 'and', 'providing', 'an', 'indication', 'for', 'the', 'additional', 'salient', 'event', 'in', 'the', 'viewport', 'during', 'the', 'playback', 'the', 'system', 'of', 'wherein', 'the', 'salient', 'data', 'identifies', 'each', 'salient', 'event', 'in', 'the', 'piece', 'of', 'content', 'and', 'the', 'salient', 'data', 'indicates', 'for', 'each', 'salient', 'event', 'in', 'the', 'piece', 'of', 'content', 'a', 'corresponding', 'point', 'location', 'of', 'the', 'salient', 'event', 'in', 'the', 'piece', 'of', 'content', 'and', 'a', 'corresponding', 'time', 'at', 'which', 'the', 'salient', 'event', 'occurs', 'during', 'the', 'playback', 'the', 'system', 'of', 'wherein', 'the', 'salient', 'data', 'further', 'indicates', 'for', 'each', 'salient', 'event', 'in', 'the', 'piece', 'of', 'content', 'a', 'corresponding', 'type', 'of', 'the', 'salient', 'event', 'and', 'a', 'corresponding', 'strength', 'value', 'of', 'the', 'salient', 'event', 'the', 'system', 'of', 'wherein', 'the', 'salient', 'data', 'is', 'generated', 'offline', 'on', 'a', 'server', 'the', 'system', 'of', 'the', 'operations', 'further', 'comprising', 'detecting', 'one', 'or', 'more', 'salient', 'events', 'in', 'the', 'piece', 'of', 'content', 'based', 'on', 'at', 'least', 'one', 'of', 'the', 'following', 'visual', 'data', 'of', 'the', 'piece', 'of', 'content', 'audio', 'data', 'of', 'the', 'piece', 'of', 'content', 'or', 'content', 'consumption', 'experience', 'data', 'for', 'the', 'piece', 'of', 'content', 'wherein', 'the', 'salient', 'data', 'is', 'indicative', 'of', 'each', 'salient', 'event', 'detected', 'the', 'system', 'of', 'the', 'operations', 'further', 'comprising', 'detecting', 'one', 'or', 'more', 'salient', 'events', 'in', 'the', 'piece', 'of', 'content', 'based', 'on', 'at', 'least', 'one', 'of', 'the', 'following', 'face', 'facial', 'emotion', 'object', 'motion', 'or', 'metadata', 'of', 'the', 'piece', 'of', 'content', 'wherein', 'the', 'salient', 'data', 'is', 'indicative', 'of', 'each', 'salient', 'event', 'detected', 'the', 'system', 'of', 'the', 'operations', 'further', 'comprising', 'detecting', 'interaction', 'with', 'the', 'indication', 'wherein', 'the', 'indication', 'comprises', 'an', 'interactive', 'hint', 'and', 'in', 'response', 'to', 'detecting', 'the', 'interaction', 'adapting', 'the', 'first', 'path', 'for', 'the', 'viewport', 'to', 'a', 'second', 'path', 'for', 'the', 'viewport', 'based', 'on', 'the', 'interaction', 'wherein', 'the', 'second', 'path', 'for', 'the', 'viewport', 'includes', 'the', 'additional', 'salient', 'event', 'and', 'providing', 'an', 'updated', 'viewport', 'for', 'the', 'piece', 'of', 'content', 'on', 'the', 'display', 'device', 'wherein', 'movement', 'of', 'the', 'updated', 'viewport', 'is', 'based', 'on', 'the', 'second', 'path', 'for', 'the', 'viewport', 'and', 'the', 'salient', 'data', 'during', 'the', 'playback', 'and', 'the', 'second', 'path', 'for', 'the', 'viewport', 'controls', 'the', 'movement', 'of', 'the', 'updated', 'viewport', 'to', 'put', 'the', 'additional', 'salient', 'event', 'in', 'a', 'view', 'of', 'the', 'updated', 'viewport', 'the', 'system', 'of', 'the', 'operations', 'further', 'comprising', 'changing', 'a', 'weight', 'assigned', 'to', 'the', 'additional', 'salient', 'event', 'and', 'one', 'or', 'more', 'other', 'salient', 'events', 'in', 'the', 'piece', 'of', 'content', 'having', 'the', 'same', 'type', 'as', 'the', 'additional', 'salient', 'event', 'the', 'system', 'of', 'wherein', 'the', 'second', 'path', 'for', 'the', 'viewport', 'includes', 'one', 'or', 'more', 'other', 'salient', 'events', 'in', 'the', 'piece', 'of', 'content', 'having', 'the', 'same', 'type', 'as', 'the', 'additional', 'salient', 'event', 'a', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'including', 'instructions', 'to', 'perform', 'a', 'comprising', 'receiving', 'a', 'piece', 'of', 'content', 'and', 'salient', 'data', 'for', 'the', 'piece', 'of', 'content', 'based', 'on', 'the', 'salient', 'data', 'determining', 'a', 'first', 'path', 'for', 'a', 'viewport', 'for', 'the', 'piece', 'of', 'content', 'wherein', 'the', 'first', 'path', 'for', 'the', 'viewport', 'includes', 'different', 'salient', 'events', 'occurring', 'in', 'the', 'piece', 'of', 'content', 'at', 'different', 'times', 'during', 'playback', 'of', 'the', 'piece', 'of', 'content', 'providing', 'the', 'viewport', 'on', 'a', 'display', 'device', 'wherein', 'movement', 'of', 'the', 'viewport', 'is', 'based', 'on', 'the', 'first', 'path', 'for', 'the', 'viewport', 'and', 'the', 'salient', 'data', 'during', 'the', 'playback', 'detecting', 'an', 'additional', 'salient', 'event', 'in', 'the', 'piece', 'of', 'content', 'that', 'is', 'not', 'included', 'in', 'the', 'first', 'path', 'for', 'the', 'viewport', 'and', 'providing', 'an', 'indication', 'for', 'the', 'additional', 'salient', 'event', 'in', 'the', 'viewport', 'during', 'the', 'playback', 'the', 'computer', 'readable', 'storage', 'medium', 'of', 'the', 'further', 'comprising', 'detecting', 'interaction', 'with', 'the', 'indication', 'wherein', 'the', 'indication', 'comprises', 'an', 'interactive', 'hint', 'and', 'in', 'response', 'to', 'detecting', 'the', 'interaction', 'adapting', 'the', 'first', 'path', 'for', 'the', 'viewport', 'to', 'a', 'second', 'path', 'for', 'the', 'viewport', 'based', 'on', 'the', 'interaction', 'wherein', 'the', 'second', 'path', 'for', 'the', 'viewport', 'includes', 'the', 'additional', 'salient', 'event', 'and', 'providing', 'an', 'updated', 'viewport', 'for', 'the', 'piece', 'of', 'content', 'on', 'the', 'display', 'device', 'wherein', 'movement', 'of', 'the', 'updated', 'viewport', 'is', 'based', 'on', 'the', 'second', 'path', 'for', 'the', 'viewport', 'and', 'the', 'salient', 'data', 'during', 'the', 'playback', 'and', 'the', 'second', 'path', 'for', 'the', 'viewport', 'controls', 'the', 'movement', 'of', 'the', 'updated', 'viewport', 'to', 'put', 'the', 'additional', 'salient', 'event', 'in', 'a', 'view', 'of', 'the', 'updated', 'viewport', 'a', 'mobile', 'device', 'with', 'facial', 'the', 'mobile', 'device', 'comprising', 'one', 'or', 'more', 'cameras', 'a', 'device', 'and', 'memory', 'coupled', 'to', 'the', 'device', 'the', 'processing', 'system', 'programmed', 'to', 'receive', 'a', 'of', 's', 'from', 'the', 'one', 'or', 'more', 'cameras', 'extract', 'with', 'a', 'feature', 'extractor', 'utilizing', 'a', 'convolutional', 'neural', 'network', 'cnn', 'with', 'an', 'enlarged', 'intra-class', 'variance', 'of', 'long-tail', 'classes', 'feature', 'vectors', 'from', 'each', 'of', 'the', 'of', 's', 'generate', 'with', 'a', 'feature', 'generator', 'discriminative', 'feature', 'vectors', 'for', 'each', 'of', 'the', 'feature', 'vectors', 'classify', 'with', 'a', 'fully', 'connected', 'classifier', 'an', 'identity', 'from', 'the', 'discriminative', 'feature', 'vectors', 'and', 'control', 'an', 'operation', 'of', 'the', 'mobile', 'device', 'to', 'react', 'in', 'accordance', 'with', 'the', 'identity', 'the', 'mobile', 'device', 'as', 'recited', 'in', 'further', 'includes', 'a', 'communication', 'system', 'the', 'mobile', 'device', 'as', 'recited', 'in', 'wherein', 'the', 'operation', 'tags', 'the', 'video', 'with', 'the', 'identity', 'and', 'uploads', 'the', 'video', 'to', 'social', 'media', 'the', 'mobile', 'device', 'as', 'recited', 'in', 'wherein', 'the', 'operation', 'tags', 'the', 'video', 'with', 'the', 'identity', 'and', 'sends', 'the', 'video', 'to', 'a', 'the', 'mobile', 'device', 'as', 'recited', 'in', 'wherein', 'the', 'mobile', 'device', 'is', 'a', 'smart', 'phone', 'the', 'mobile', 'device', 'as', 'recited', 'in', 'wherein', 'the', 'mobile', 'device', 'is', 'a', 'body', 'cam', 'the', 'mobile', 'device', 'as', 'recited', 'in', 'further', 'programmed', 'to', 'train', 'the', 'feature', 'extractor', 'the', 'feature', 'generator', 'and', 'the', 'fully', 'connected', 'classifier', 'with', 'an', 'alternative', 'bi-stage', 'strategy', 'the', 'mobile', 'device', 'as', 'recited', 'in', 'wherein', 'the', 'feature', 'extractor', 'shares', 'covariance', 'matrices', 'across', 'all', 'classes', 'to', 'transfer', 'intra-class', 'variance', 'from', 'regular', 'classes', 'to', 'the', 'long-tail', 'classes', 'the', 'mobile', 'device', 'as', 'recited', 'in', 'wherein', 'the', 'feature', 'generator', 'optimizes', 'a', 'softmax', 'loss', 'by', 'joint', 'regularization', 'of', 'weights', 'and', 'features', 'through', 'a', 'magnitude', 'of', 'an', 'inner', 'product', 'of', 'the', 'weights', 'and', 'features', 'the', 'mobile', 'device', 'as', 'recited', 'in', 'wherein', 'the', 'feature', 'extractor', 'averages', 'the', 'feature', 'vector', 'with', 'a', 'flipped', 'feature', 'vector', 'the', 'flipped', 'feature', 'vector', 'being', 'generated', 'from', 'a', 'horizontally', 'flipped', 'frame', 'from', 'one', 'of', 'the', 'of', 's', 'the', 'mobile', 'device', 'as', 'recited', 'in', 'wherein', 'each', 'of', 'the', 'of', 's', 'is', 'selected', 'from', 'the', 'group', 'consisting', 'of', 'an', 'a', 'video', 'and', 'a', 'frame', 'from', 'the', 'video', 'the', 'mobile', 'device', 'as', 'recited', 'in', 'wherein', 'the', 'communication', 'system', 'connects', 'to', 'a', 'remote', 'server', 'that', 'includes', 'a', 'facial', 'network', 'the', 'mobile', 'device', 'as', 'recited', 'in', 'wherein', 'one', 'stage', 'of', 'the', 'alternative', 'bi-stage', 'strategy', 'fixes', 'the', 'feature', 'extractor', 'and', 'applies', 'the', 'feature', 'generator', 'to', 'generate', 'new', 'transferred', 'features', 'that', 'are', 'more', 'diverse', 'and', 'violate', 'a', 'decision', 'boundary', 'the', 'mobile', 'device', 'as', 'recited', 'in', 'wherein', 'one', 'stage', 'of', 'the', 'alternative', 'bi-stage', 'strategy', 'fixes', 'the', 'fully', 'connected', 'classifier', 'and', 'updates', 'the', 'feature', 'extractor', 'and', 'the', 'feature', 'generator', 'a', 'computer', 'program', 'product', 'for', 'a', 'mobile', 'device', 'with', 'facial', 'the', 'computer', 'program', 'product', 'comprising', 'a', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'having', 'program', 'instructions', 'embodied', 'therewith', 'the', 'program', 'instructions', 'executable', 'by', 'a', 'computer', 'to', 'cause', 'the', 'computer', 'to', 'perform', 'a', 'comprising', 'receiving', 'by', 'a', 'device', 'a', 'of', 's', 'extracting', 'by', 'the', 'device', 'with', 'a', 'feature', 'extractor', 'utilizing', 'a', 'convolutional', 'neural', 'network', 'cnn', 'with', 'an', 'enlarged', 'intra-class', 'variance', 'of', 'long-tail', 'classes', 'feature', 'vectors', 'for', 'each', 'of', 'the', 'of', 's', 'generating', 'by', 'the', 'device', 'with', 'a', 'feature', 'generator', 'discriminative', 'feature', 'vectors', 'for', 'each', 'of', 'the', 'feature', 'vectors', 'classifying', 'by', 'the', 'device', 'utilizing', 'a', 'fully', 'connected', 'classifier', 'an', 'identity', 'from', 'the', 'discriminative', 'feature', 'vector', 'and', 'controlling', 'an', 'operation', 'of', 'the', 'mobile', 'device', 'to', 'react', 'in', 'accordance', 'with', 'the', 'identity', 'a', 'computer-implemented', 'for', 'facial', 'in', 'a', 'mobile', 'device', 'the', 'comprising', 'receiving', 'by', 'a', 'device', 'a', 'of', 's', 'extracting', 'by', 'the', 'device', 'with', 'a', 'feature', 'extractor', 'utilizing', 'a', 'convolutional', 'neural', 'network', 'cnn', 'with', 'an', 'enlarged', 'intra-class', 'variance', 'of', 'long-tail', 'classes', 'feature', 'vectors', 'for', 'each', 'of', 'the', 'of', 's', 'generating', 'by', 'the', 'device', 'with', 'a', 'feature', 'generator', 'discriminative', 'feature', 'vectors', 'for', 'each', 'of', 'the', 'feature', 'vectors', 'classifying', 'by', 'the', 'device', 'utilizing', 'a', 'fully', 'connected', 'classifier', 'an', 'identity', 'from', 'the', 'discriminative', 'feature', 'vector', 'and', 'controlling', 'an', 'operation', 'of', 'the', 'mobile', 'device', 'to', 'react', 'in', 'accordance', 'with', 'the', 'identity', 'the', 'computer-implemented', 'as', 'recited', 'in', 'wherein', 'controlling', 'includes', 'tagging', 'the', 'video', 'with', 'the', 'identity', 'and', 'uploading', 'the', 'video', 'to', 'social', 'media', 'the', 'computer-implemented', 'as', 'recited', 'in', 'wherein', 'controlling', 'includes', 'tagging', 'the', 'video', 'with', 'the', 'identity', 'and', 'sending', 'the', 'video', 'to', 'a', 'the', 'computer-implemented', 'as', 'recited', 'in', 'wherein', 'extracting', 'includes', 'sharing', 'covariance', 'matrices', 'across', 'all', 'classes', 'to', 'transfer', 'intra-class', 'variance', 'from', 'regular', 'classes', 'to', 'the', 'long-tail', 'classes', 'a', 'computing', 'device', 'comprising', 'a', 'non-transitory', 'machine', 'readable', 'medium', 'storing', 'a', 'machine', 'trained', 'mt', 'network', 'comprising', 'a', 'of', 'layers', 'of', 'processing', 'nodes', 'each', 'processing', 'node', 'configured', 'to', 'compute', 'a', 'first', 'output', 'value', 'by', 'combining', 'a', 'set', 'of', 'output', 'values', 'from', 'a', 'set', 'of', 'processing', 'nodes', 'and', 'use', 'a', 'piecewise', 'linear', 'cup', 'function', 'to', 'compute', 'a', 'second', 'output', 'value', 'from', 'the', 'first', 'output', 'value', 'of', 'the', 'processing', 'node', 'wherein', 'the', 'piecewise', 'linear', 'cup', 'function', 'prior', 'to', 'training', 'of', 'the', 'mt', 'network', 'comprises', 'at', 'least', 'i', 'a', 'first', 'linear', 'section', 'with', 'a', 'first', 'slope', 'followed', 'by', 'ii', 'a', 'second', 'linear', 'section', 'with', 'a', 'negative', 'second', 'slope', 'followed', 'by', 'iii', 'a', 'third', 'linear', 'section', 'with', 'a', 'negative', 'third', 'slope', 'that', 'is', 'different', 'from', 'the', 'second', 'slope', 'followed', 'by', 'iv', 'a', 'fourth', 'linear', 'section', 'with', 'a', 'positive', 'fourth', 'slope', 'followed', 'by', 'v', 'a', 'fifth', 'linear', 'section', 'with', 'a', 'positive', 'fifth', 'slope', 'that', 'is', 'different', 'from', 'the', 'fourth', 'slope', 'followed', 'by', 'vi', 'a', 'sixth', 'linear', 'section', 'with', 'a', 'sixth', 'slope', 'wherein', 'the', 'piecewise', 'linear', 'cup', 'function', 'is', 'symmetric', 'about', 'a', 'vertical', 'axis', 'between', 'the', 'third', 'and', 'fourth', 'linear', 'sections', 'prior', 'to', 'training', 'of', 'the', 'mt', 'network', 'a', 'content', 'capturing', 'circuit', 'for', 'capturing', 'content', 'for', 'processing', 'by', 'the', 'mt', 'network', 'and', 'a', 'set', 'of', 'processing', 'units', 'for', 'executing', 'the', 'processing', 'nodes', 'to', 'process', 'content', 'captured', 'by', 'the', 'content', 'capturing', 'circuit', 'wherein', 'by', 'training', 'a', 'set', 'of', 'parameters', 'that', 'define', 'the', 'piecewise', 'linear', 'cup', 'function', 'of', 'each', 'node', 'in', 'first', 'and', 'second', 'pluralities', 'of', 'processing', 'nodes', 'i', 'each', 'processing', 'node', 'in', 'the', 'first', 'of', 'processing', 'nodes', 'is', 'configured', 'to', 'emulate', 'a', 'boolean', 'and', 'operator', 'such', 'that', 'an', 'output', 'value', 'of', 'the', 'processing', 'node', 'is', 'in', 'a', 'range', 'associated', 'with', 'a', '``', \"''\", 'value', 'only', 'when', 'a', 'set', 'of', 'inputs', 'to', 'the', 'processing', 'node', 'have', 'a', 'set', 'of', 'values', 'in', 'a', 'range', 'associated', 'with', '``', \"''\", 'and', 'ii', 'each', 'processing', 'node', 'in', 'the', 'second', 'of', 'processing', 'nodes', 'is', 'configured', 'to', 'emulate', 'a', 'boolean', 'xnor', 'operator', 'such', 'that', 'an', 'output', 'value', 'of', 'the', 'processing', 'node', 'is', 'in', 'the', 'range', 'associated', 'with', '``', \"''\", 'only', 'when', 'a', 'a', 'set', 'of', 'inputs', 'to', 'the', 'node', 'have', 'a', 'set', 'of', 'values', 'in', 'a', 'range', 'associated', 'with', '``', \"''\", 'or', 'b', 'the', 'set', 'of', 'inputs', 'to', 'the', 'node', 'have', 'a', 'set', 'of', 'values', 'in', 'a', 'range', 'associated', 'with', 'a', '``', \"''\", 'value', 'the', 'computing', 'device', 'of', 'wherein', 'the', 'third', 'linear', 'section', 'of', 'the', 'piecewise', 'linear', 'cup', 'function', 'of', 'a', 'first', 'processing', 'node', 'in', 'the', 'mt', 'network', 'has', 'a', 'different', 'slope', 'from', 'the', 'third', 'linear', 'section', 'of', 'a', 'second', 'processing', 'node', 'in', 'the', 'mt', 'network', 'the', 'computing', 'device', 'of', 'wherein', 'the', 'length', 'of', 'the', 'third', 'section', 'of', 'a', 'piecewise', 'linear', 'cup', 'function', 'of', 'a', 'first', 'processing', 'node', 'in', 'the', 'mt', 'network', 'is', 'different', 'from', 'the', 'length', 'of', 'the', 'third', 'section', 'of', 'a', 'piecewise', 'linear', 'cup', 'function', 'of', 'a', 'second', 'processing', 'node', 'in', 'the', 'mt', 'network', 'the', 'computing', 'device', 'of', 'wherein', 'the', 'sets', 'of', 'parameters', 'are', 'trained', 'in', 'part', 'by', 'a', 'back', 'propagating', 'module', 'for', 'back', 'propagating', 'errors', 'in', 'output', 'values', 'of', 'later', 'layers', 'of', 'processing', 'nodes', 'to', 'earlier', 'layers', 'of', 'processing', 'nodes', 'by', 'adjusting', 'the', 'set', 'of', 'parameters', 'that', 'define', 'the', 'piecewise', 'linear', 'cup', 'functions', 'of', 'the', 'earlier', 'layers', 'of', 'processing', 'nodes', 'the', 'computing', 'device', 'of', 'wherein', 'each', 'processing', 'node', 'uses', 'a', 'linear', 'function', 'that', 'is', 'defined', 'by', 'a', 'set', 'of', 'parameters', 'to', 'compute', 'the', 'first', 'output', 'value', 'of', 'the', 'processing', 'node', 'wherein', 'the', 'back', 'propagating', 'module', 'back', 'propagates', 'errors', 'in', 'output', 'values', 'of', 'later', 'layers', 'of', 'processing', 'nodes', 'to', 'earlier', 'layers', 'of', 'processing', 'nodes', 'by', 'adjusting', 'the', 'set', 'of', 'parameters', 'that', 'define', 'the', 'linear', 'functions', 'of', 'the', 'earlier', 'layers', 'of', 'processing', 'nodes', 'the', 'computing', 'device', 'of', 'wherein', 'the', 'first', 'of', 'processing', 'nodes', 'that', 'emulate', 'the', 'boolean', 'and', 'operator', 'and', 'the', 'second', 'of', 'processing', 'nodes', 'that', 'emulate', 'the', 'boolean', 'xnor', 'operator', 'enable', 'the', 'mt', 'network', 'to', 'implement', 'mathematical', 'problems', 'the', 'computing', 'device', 'of', 'wherein', 'each', 'of', 'a', 'of', 'processing', 'node', 'layers', 'has', 'a', 'of', 'processing', 'nodes', 'that', 'receive', 'as', 'input', 'values', 'the', 'output', 'values', 'from', 'a', 'of', 'processing', 'nodes', 'in', 'a', 'set', 'of', 'prior', 'layers', 'the', 'computing', 'device', 'of', 'wherein', 'each', 'processing', 'node', 'uses', 'a', 'linear', 'function', 'to', 'compute', 'the', 'first', 'output', 'value', 'of', 'the', 'processing', 'node', 'wherein', 'each', 'processing', 'node', \"'s\", 'piecewise', 'linear', 'cup', 'function', 'is', 'defined', 'along', 'first', 'and', 'second', 'axes', 'the', 'first', 'axis', 'defining', 'a', 'range', 'of', 'output', 'values', 'from', 'the', 'processing', 'node', \"'s\", 'linear', 'function', 'and', 'the', 'second', 'axis', 'defining', 'a', 'range', 'of', 'output', 'values', 'produced', 'by', 'the', 'piecewise', 'linear', 'cup', 'function', 'for', 'the', 'range', 'of', 'output', 'values', 'from', 'the', 'processing', 'node', \"'s\", 'linear', 'function', 'the', 'computing', 'device', 'of', 'further', 'comprising', 'a', 'content', 'output', 'circuit', 'for', 'presenting', 'an', 'output', 'based', 'on', 'the', 'processing', 'of', 'the', 'content', 'by', 'the', 'mt', 'network', 'the', 'computing', 'device', 'of', 'wherein', 'the', 'captured', 'content', 'is', 'one', 'of', 'an', 'and', 'an', 'audio', 'segment', 'and', 'wherein', 'the', 'presented', 'output', 'is', 'an', 'output', 'display', 'on', 'a', 'display', 'screen', 'of', 'the', 'computing', 'device', 'or', 'an', 'audio', 'presentation', 'output', 'on', 'a', 'speaker', 'of', 'the', 'computing', 'device', 'the', 'computing', 'device', 'of', 'wherein', 'the', 'computing', 'device', 'is', 'a', 'mobile', 'device', 'the', 'computing', 'device', 'of', 'wherein', 'the', 'mt', 'network', 'is', 'a', 'mt', 'neural', 'network', 'and', 'the', 'processing', 'nodes', 'are', 'mt', 'neurons', 'the', 'computing', 'device', 'of', 'wherein', 'the', 'set', 'of', 'parameters', 'configured', 'through', 'training', 'for', 'a', 'of', 'the', 'processing', 'nodes', 'comprise', 'at', 'least', 'one', 'of', 'the', 'negative', 'second', 'and', 'third', 'slopes', 'for', 'the', 'second', 'and', 'third', 'linear', 'sections', 'the', 'positive', 'fourth', 'and', 'fifth', 'slopes', 'for', 'the', 'fourth', 'and', 'fifth', 'linear', 'sections', 'a', 'first', 'intercept', 'for', 'the', 'second', 'linear', 'section', 'a', 'second', 'intercept', 'for', 'the', 'fifth', 'linear', 'section', 'and', 'a', 'set', 'of', 'lengths', 'for', 'at', 'least', 'the', 'second', 'third', 'fourth', 'and', 'fifth', 'sections', 'the', 'computing', 'device', 'of', 'wherein', 'the', 'trained', 'set', 'of', 'parameters', 'that', 'define', 'the', 'piecewise', 'linear', 'cup', 'function', 'of', 'each', 'node', 'comprise', 'a', 'of', 'output', 'values', 'the', 'computing', 'device', 'of', 'wherein', 'the', 'first', 'and', 'sixth', 'slopes', 'are', 'zerowe', 'a', 'system', 'comprising', 'a', 'memory', 'device', 'to', 'store', 'an', 'input', 'a', 'including', 'an', 'input', 'interface', 'to', 'receive', 'the', 'input', 'a', 'pre-', 'to', 'model', 'the', 'input', 'to', 'yield', 'a', 'multi-channel', 'a', 'feature', 'extractor', 'to', 'extract', 'a', 'set', 'of', 'features', 'based', 'on', 'the', 'multi-channel', 'a', 'feature', 'selector', 'to', 'select', 'one', 'or', 'more', 'features', 'from', 'the', 'set', 'of', 'features', 'of', 'the', 'multi-channel', 'wherein', 'the', 'one', 'or', 'more', 'features', 'are', 'selected', 'based', 'on', 'an', 'ability', 'to', 'differentiate', 'features', 'a', 'feature', 'matcher', 'to', 'match', 'the', 'one', 'or', 'more', 'features', 'to', 'a', 'learned', 'feature', 'set', 'and', 'a', 'similarity', 'detector', 'to', 'determine', 'whether', 'the', 'one', 'or', 'more', 'features', 'meet', 'a', 'pre-defined', 'similarity', 'threshold', 'the', 'system', 'of', 'wherein', 'the', 'pre-', 'further', 'is', 'to', 'activate', 'one', 'or', 'more', 'channels', 'of', 'the', 'multi-channel', 'to', 'yield', 'one', 'or', 'more', 'activated', 'channels', 'the', 'system', 'of', 'wherein', 'the', 'one', 'or', 'more', 'activated', 'channels', 'are', 'to', 'be', 'determined', 'based', 'on', 'their', 'ability', 'to', 'differentiate', 'features', 'the', 'system', 'of', 'wherein', 'the', 'pre-', 'further', 'is', 'to', 'activate', 'one', 'or', 'more', 'local', 'patches', 'of', 'the', 'one', 'or', 'more', 'activated', 'channels', 'the', 'system', 'of', 'wherein', 'the', 'one', 'or', 'more', 'local', 'patches', 'are', 'to', 'be', 'determined', 'based', 'on', 'their', 'ability', 'to', 'differentiate', 'features', 'the', 'system', 'of', 'wherein', 'the', 'feature', 'matcher', 'further', 'is', 'to', 'utilize', 'a', 'large-scale', 'data', 'learning', 'process', 'to', 'perform', 'the', 'feature', 'matching', 'an', 'apparatus', 'comprising', 'an', 'input', 'interface', 'to', 'receive', 'an', 'input', 'a', 'pre-', 'to', 'model', 'the', 'input', 'to', 'yield', 'a', 'multi-channel', 'a', 'feature', 'extractor', 'to', 'extract', 'a', 'set', 'of', 'features', 'based', 'on', 'the', 'multi-channel', 'a', 'feature', 'selector', 'to', 'select', 'one', 'or', 'more', 'features', 'from', 'the', 'set', 'of', 'features', 'of', 'the', 'multi-channel', 'wherein', 'the', 'one', 'or', 'more', 'features', 'are', 'selected', 'based', 'on', 'an', 'ability', 'to', 'differentiate', 'features', 'a', 'feature', 'matcher', 'to', 'match', 'the', 'one', 'or', 'more', 'features', 'to', 'a', 'learned', 'feature', 'set', 'and', 'a', 'similarity', 'detector', 'to', 'determine', 'whether', 'the', 'one', 'or', 'more', 'features', 'meet', 'a', 'pre-defined', 'similarity', 'threshold', 'the', 'apparatus', 'of', 'wherein', 'the', 'pre-', 'further', 'is', 'to', 'activate', 'one', 'or', 'more', 'channels', 'of', 'the', 'multi-channel', 'to', 'yield', 'one', 'or', 'more', 'activated', 'channels', 'the', 'apparatus', 'of', 'wherein', 'the', 'one', 'or', 'more', 'activated', 'channels', 'are', 'to', 'be', 'determined', 'based', 'on', 'their', 'ability', 'to', 'differentiate', 'features', 'the', 'apparatus', 'of', 'wherein', 'the', 'pre-', 'further', 'is', 'to', 'activate', 'one', 'or', 'more', 'local', 'patches', 'of', 'the', 'one', 'or', 'more', 'activated', 'channels', 'the', 'apparatus', 'of', 'wherein', 'the', 'one', 'or', 'more', 'local', 'patches', 'are', 'to', 'be', 'determined', 'based', 'on', 'their', 'ability', 'to', 'differentiate', 'features', 'the', 'apparatus', 'of', 'wherein', 'the', 'feature', 'matcher', 'further', 'is', 'to', 'utilize', 'a', 'large-scale', 'data', 'learning', 'process', 'to', 'perform', 'the', 'feature', 'matching', 'a', 'comprising', 'modeling', 'an', 'input', 'to', 'yield', 'a', 'multi-channel', 'extracting', 'a', 'set', 'of', 'features', 'based', 'on', 'the', 'multi-channel', 'selecting', 'one', 'or', 'more', 'features', 'from', 'the', 'set', 'of', 'features', 'of', 'the', 'multi-channel', 'wherein', 'the', 'one', 'or', 'more', 'features', 'are', 'selected', 'based', 'on', 'an', 'ability', 'to', 'differentiate', 'features', 'matching', 'the', 'one', 'or', 'more', 'features', 'to', 'a', 'learned', 'feature', 'set', 'and', 'determining', 'whether', 'the', 'one', 'or', 'more', 'features', 'meet', 'a', 'pre-defined', 'similarity', 'threshold', 'the', 'of', 'wherein', 'modeling', 'the', 'input', 'further', 'is', 'to', 'include', 'activating', 'one', 'or', 'more', 'channels', 'of', 'the', 'multi-channel', 'to', 'yield', 'one', 'or', 'more', 'activated', 'channels', 'the', 'of', 'wherein', 'the', 'one', 'or', 'more', 'activated', 'channels', 'are', 'to', 'be', 'determined', 'based', 'on', 'their', 'ability', 'to', 'differentiate', 'features', 'the', 'of', 'wherein', 'extracting', 'features', 'of', 'the', 'input', 'further', 'is', 'to', 'include', 'activating', 'one', 'or', 'more', 'local', 'patches', 'of', 'the', 'one', 'or', 'more', 'activated', 'channels', 'the', 'of', 'wherein', 'the', 'one', 'or', 'more', 'local', 'patches', 'are', 'to', 'be', 'determined', 'based', 'on', 'their', 'ability', 'to', 'differentiate', 'features', 'the', 'of', 'wherein', 'the', 'feature', 'matcher', 'utilizes', 'a', 'large-scale', 'data', 'learning', 'process', 'to', 'perform', 'the', 'feature', 'matching', 'at', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'comprising', 'a', 'set', 'of', 'instructions', 'which', 'when', 'executed', 'by', 'a', 'computing', 'device', 'cause', 'the', 'computing', 'device', 'to', 'model', 'an', 'input', 'to', 'yield', 'a', 'multi-channel', 'extract', 'a', 'set', 'of', 'features', 'based', 'on', 'the', 'multi-channel', 'select', 'one', 'or', 'more', 'features', 'from', 'the', 'set', 'of', 'features', 'of', 'the', 'multi-channel', 'wherein', 'the', 'features', 'are', 'selected', 'based', 'on', 'an', 'ability', 'to', 'differentiate', 'features', 'match', 'the', 'one', 'or', 'more', 'features', 'to', 'a', 'learned', 'feature', 'set', 'and', 'determine', 'whether', 'the', 'one', 'or', 'more', 'features', 'meet', 'a', 'pre-defined', 'similarity', 'threshold', 'the', 'at', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'of', 'wherein', 'the', 'instructions', 'when', 'executed', 'cause', 'a', 'computing', 'device', 'to', 'activate', 'one', 'or', 'more', 'channels', 'of', 'the', 'multi-channel', 'to', 'yield', 'one', 'or', 'more', 'activated', 'channels', 'the', 'at', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'of', 'wherein', 'the', 'instructions', 'when', 'executed', 'cause', 'a', 'computing', 'device', 'to', 'determine', 'the', 'one', 'or', 'more', 'activated', 'channels', 'based', 'on', 'their', 'ability', 'to', 'differentiate', 'features', 'the', 'at', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'of', 'wherein', 'extracting', 'features', 'of', 'the', 'input', 'is', 'to', 'further', 'include', 'activating', 'one', 'or', 'more', 'local', 'patches', 'of', 'the', 'one', 'or', 'more', 'activated', 'channels', 'the', 'at', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'of', 'wherein', 'the', 'one', 'or', 'more', 'local', 'patches', 'are', 'to', 'be', 'determined', 'based', 'on', 'their', 'ability', 'to', 'differentiate', 'features', 'the', 'at', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'of', 'wherein', 'the', 'feature', 'matcher', 'further', 'is', 'to', 'utilize', 'a', 'large-scale', 'data', 'learning', 'process', 'to', 'perform', 'the', 'feature', 'matching', 'an', 'apparatus', 'comprising', 'means', 'for', 'modeling', 'an', 'input', 'to', 'yield', 'a', 'multi-channel', 'means', 'for', 'extracting', 'a', 'set', 'of', 'features', 'based', 'on', 'the', 'multi-channel', 'means', 'for', 'selecting', 'one', 'or', 'more', 'features', 'from', 'the', 'set', 'of', 'features', 'of', 'the', 'multi-channel', 'wherein', 'the', 'one', 'or', 'more', 'features', 'are', 'selected', 'based', 'on', 'an', 'ability', 'to', 'differentiate', 'features', 'means', 'for', 'matching', 'the', 'one', 'or', 'more', 'features', 'to', 'a', 'learned', 'feature', 'set', 'and', 'means', 'for', 'determining', 'whether', 'the', 'one', 'or', 'more', 'features', 'meet', 'a', 'pre-defined', 'similarity', 'threshold', 'a', 'for', 'controlling', 'a', 'terminal', 'the', 'terminal', 'comprising', 'a', 'capturing', 'apparatus', 'and', 'at', 'least', 'one', 'the', 'comprising', 'acquiring', 'by', 'the', 'capturing', 'apparatus', 'an', 'obtaining', 'by', 'the', 'at', 'least', 'one', 'a', 'motion', 'parameter', 'of', 'the', 'terminal', 'the', 'motion', 'parameter', 'comprising', 'at', 'least', 'one', 'of', 'a', 'motion', 'frequency', 'or', 'a', 'motion', 'time', 'and', 'two', 'or', 'more', 'parameters', 'from', 'among', 'an', 'acceleration', 'an', 'angular', 'velocity', 'a', 'motion', 'amplitude', 'the', 'motion', 'frequency', 'and', 'the', 'motion', 'time', 'transmitting', 'by', 'the', 'at', 'least', 'one', 'a', 'parameter', 'threshold', 'obtaining', 'request', 'to', 'a', 'data', 'management', 'server', 'the', 'parameter', 'threshold', 'obtaining', 'request', 'comprising', 'configuration', 'information', 'of', 'the', 'terminal', 'receiving', 'corresponding', 'preset', 'thresholds', 'that', 'correspond', 'to', 'the', 'configuration', 'information', 'in', 'response', 'to', 'the', 'parameter', 'threshold', 'obtaining', 'request', 'comparing', 'the', 'two', 'or', 'more', 'parameters', 'with', 'the', 'corresponding', 'preset', 'thresholds', 'and', 'controlling', 'by', 'the', 'at', 'least', 'one', 'not', 'to', 'perform', 'processing', 'on', 'the', 'acquired', 'based', 'on', 'at', 'least', 'one', 'of', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'greater', 'than', 'a', 'corresponding', 'preset', 'threshold', 'or', 'based', 'on', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'respectively', 'greater', 'than', 'the', 'corresponding', 'preset', 'thresholds', 'wherein', 'the', 'acquiring', 'comprises', 'acquiring', 'the', 'in', 'real', 'time', 'and', 'the', 'obtaining', 'comprises', 'obtaining', 'the', 'motion', 'parameter', 'of', 'the', 'terminal', 'in', 'real', 'time', 'the', 'further', 'comprising', 'in', 'response', 'to', 'the', 'at', 'least', 'one', 'of', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'greater', 'than', 'the', 'corresponding', 'preset', 'threshold', 'obtaining', 'the', 'motion', 'parameter', 'of', 'the', 'terminal', 'again', 'and', 'in', 'response', 'to', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'obtained', 'at', 'a', 'latest', 'time', 'being', 'less', 'than', 'or', 'equal', 'to', 'the', 'corresponding', 'preset', 'thresholds', 'performing', 'the', 'processing', 'on', 'the', 'acquired', 'at', 'the', 'latest', 'time', 'the', 'according', 'to', 'wherein', 'the', 'acquiring', 'comprises', 'controlling', 'by', 'the', 'at', 'least', 'one', 'to', 'turn', 'on', 'the', 'capturing', 'apparatus', 'based', 'on', 'a', 'face', 'instruction', 'and', 'acquiring', 'by', 'the', 'capturing', 'apparatus', 'a', 'face', 'when', 'the', 'capturing', 'apparatus', 'is', 'turned', 'on', 'the', 'according', 'to', 'wherein', 'the', 'controlling', 'not', 'to', 'perform', 'the', 'processing', 'comprises', 'skipping', 'performing', 'face', 'on', 'the', 'acquired', 'face', 'based', 'on', 'the', 'at', 'least', 'one', 'of', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'greater', 'than', 'the', 'corresponding', 'preset', 'threshold', 'or', 'based', 'on', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'respectively', 'greater', 'than', 'the', 'corresponding', 'preset', 'thresholds', 'the', 'according', 'to', 'wherein', 'the', 'obtaining', 'comprises', 'at', 'least', 'one', 'of', 'obtaining', 'the', 'acceleration', 'of', 'the', 'terminal', 'by', 'using', 'an', 'acceleration', 'sensor', 'or', 'obtaining', 'the', 'angular', 'velocity', 'of', 'the', 'terminal', 'by', 'using', 'a', 'gyro', 'sensor', 'the', 'according', 'to', 'wherein', 'the', 'transmitting', 'comprises', 'transmitting', 'the', 'parameter', 'threshold', 'obtaining', 'request', 'to', 'the', 'data', 'management', 'server', 'according', 'to', 'a', 'preset', 'time', 'period', 'the', 'according', 'to', 'further', 'comprising', 'generating', 'prompt', 'information', 'based', 'on', 'the', 'at', 'least', 'one', 'of', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'greater', 'than', 'the', 'corresponding', 'preset', 'threshold', 'the', 'prompt', 'information', 'being', 'used', 'for', 'prompting', 'the', 'terminal', 'to', 'stop', 'moving', 'the', 'according', 'to', 'wherein', 'the', 'motion', 'parameter', 'comprises', 'the', 'motion', 'frequency', 'and', 'the', 'motion', 'time', 'a', 'terminal', 'comprising', 'a', 'capturing', 'apparatus', 'at', 'least', 'one', 'memory', 'configured', 'to', 'store', 'program', 'code', 'and', 'at', 'least', 'one', 'configured', 'to', 'access', 'the', 'at', 'least', 'one', 'memory', 'and', 'operate', 'according', 'to', 'the', 'program', 'code', 'the', 'program', 'code', 'comprising', 'motion', 'parameter', 'obtaining', 'code', 'configured', 'to', 'cause', 'the', 'at', 'least', 'one', 'to', 'acquire', 'an', 'by', 'using', 'the', 'capturing', 'apparatus', 'and', 'obtain', 'a', 'motion', 'parameter', 'of', 'the', 'terminal', 'the', 'motion', 'parameter', 'comprising', 'at', 'least', 'one', 'of', 'a', 'motion', 'frequency', 'or', 'a', 'motion', 'time', 'and', 'two', 'or', 'more', 'parameters', 'from', 'among', 'an', 'acceleration', 'an', 'angular', 'velocity', 'a', 'motion', 'amplitude', 'the', 'motion', 'frequency', 'and', 'the', 'motion', 'time', 'request', 'transmitting', 'code', 'configured', 'to', 'cause', 'the', 'at', 'least', 'one', 'to', 'transmit', 'a', 'parameter', 'threshold', 'obtaining', 'request', 'to', 'a', 'data', 'management', 'server', 'the', 'parameter', 'threshold', 'obtaining', 'request', 'comprising', 'configuration', 'information', 'of', 'the', 'terminal', 'parameter', 'threshold', 'receiving', 'code', 'configured', 'to', 'cause', 'the', 'at', 'least', 'one', 'to', 'receive', 'corresponding', 'preset', 'thresholds', 'that', 'correspond', 'to', 'the', 'configuration', 'information', 'in', 'response', 'to', 'the', 'parameter', 'threshold', 'obtaining', 'request', 'comparing', 'code', 'configured', 'to', 'cause', 'the', 'at', 'least', 'one', 'to', 'compare', 'the', 'two', 'or', 'more', 'parameters', 'with', 'the', 'corresponding', 'preset', 'thresholds', 'and', 'control', 'code', 'configured', 'to', 'cause', 'the', 'at', 'least', 'one', 'not', 'to', 'perform', 'processing', 'on', 'the', 'acquired', 'based', 'on', 'at', 'least', 'one', 'of', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'greater', 'than', 'a', 'corresponding', 'preset', 'threshold', 'or', 'based', 'on', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'respectively', 'greater', 'than', 'the', 'corresponding', 'preset', 'thresholds', 'wherein', 'the', 'motion', 'parameter', 'obtaining', 'code', 'causes', 'the', 'at', 'least', 'one', 'to', 'acquire', 'the', 'in', 'real', 'time', 'and', 'obtain', 'the', 'motion', 'parameter', 'of', 'the', 'terminal', 'in', 'real', 'time', 'and', 'in', 'response', 'to', 'the', 'at', 'least', 'one', 'of', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'greater', 'than', 'the', 'corresponding', 'preset', 'threshold', 'obtain', 'the', 'motion', 'parameter', 'of', 'the', 'terminal', 'again', 'and', 'wherein', 'the', 'control', 'code', 'causes', 'the', 'at', 'least', 'one', 'to', 'in', 'response', 'to', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'obtained', 'at', 'a', 'latest', 'time', 'being', 'less', 'than', 'or', 'equal', 'to', 'the', 'corresponding', 'preset', 'thresholds', 'perform', 'the', 'processing', 'on', 'the', 'acquired', 'at', 'the', 'latest', 'time', 'the', 'terminal', 'according', 'to', 'wherein', 'the', 'program', 'code', 'further', 'comprises', 'face', 'instruction', 'receiving', 'code', 'configured', 'to', 'cause', 'the', 'at', 'least', 'one', 'to', 'receive', 'a', 'face', 'instruction', 'wherein', 'the', 'motion', 'parameter', 'obtaining', 'code', 'causes', 'the', 'at', 'least', 'one', 'to', 'control', 'according', 'to', 'the', 'face', 'instruction', 'the', 'capturing', 'apparatus', 'to', 'turn', 'on', 'and', 'acquire', 'a', 'face', 'by', 'using', 'the', 'capturing', 'apparatus', 'when', 'the', 'capturing', 'apparatus', 'is', 'turned', 'on', 'and', 'wherein', 'the', 'control', 'code', 'causes', 'the', 'at', 'least', 'one', 'to', 'skip', 'performing', 'face', 'on', 'the', 'acquired', 'face', 'based', 'on', 'the', 'at', 'least', 'one', 'of', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'greater', 'than', 'the', 'corresponding', 'preset', 'threshold', 'or', 'based', 'on', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'respectively', 'greater', 'than', 'the', 'corresponding', 'preset', 'thresholds', 'the', 'terminal', 'according', 'to', 'wherein', 'the', 'request', 'transmitting', 'code', 'causes', 'the', 'at', 'least', 'one', 'to', 'transmit', 'the', 'parameter', 'threshold', 'obtaining', 'request', 'to', 'the', 'data', 'management', 'server', 'according', 'to', 'a', 'preset', 'time', 'period', 'the', 'terminal', 'according', 'to', 'wherein', 'the', 'program', 'code', 'further', 'comprises', 'prompt', 'information', 'generation', 'code', 'configured', 'to', 'cause', 'the', 'at', 'least', 'one', 'to', 'generate', 'prompt', 'information', 'based', 'on', 'at', 'least', 'one', 'of', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'greater', 'than', 'the', 'corresponding', 'preset', 'threshold', 'the', 'prompt', 'information', 'being', 'used', 'for', 'prompting', 'the', 'terminal', 'to', 'stop', 'moving', 'the', 'terminal', 'according', 'to', 'wherein', 'the', 'motion', 'parameter', 'comprises', 'the', 'motion', 'frequency', 'and', 'the', 'motion', 'time', 'a', 'non-transitory', 'computer-readable', 'storage', 'medium', 'storing', 'a', 'machine', 'instruction', 'which', 'when', 'executed', 'by', 'one', 'or', 'more', 's', 'causes', 'the', 'one', 'or', 'more', 's', 'to', 'perform', 'obtaining', 'an', 'acquired', 'by', 'a', 'capturing', 'apparatus', 'obtaining', 'a', 'motion', 'parameter', 'of', 'a', 'terminal', 'the', 'terminal', 'comprising', 'the', 'capturing', 'apparatus', 'the', 'motion', 'parameter', 'comprising', 'at', 'least', 'one', 'of', 'a', 'motion', 'frequency', 'or', 'a', 'motion', 'time', 'and', 'two', 'or', 'more', 'parameters', 'from', 'among', 'an', 'acceleration', 'an', 'angular', 'velocity', 'a', 'motion', 'amplitude', 'the', 'motion', 'frequency', 'and', 'the', 'motion', 'time', 'transmitting', 'a', 'parameter', 'threshold', 'obtaining', 'request', 'to', 'a', 'data', 'management', 'server', 'the', 'parameter', 'threshold', 'obtaining', 'request', 'comprising', 'configuration', 'information', 'of', 'the', 'terminal', 'receiving', 'corresponding', 'preset', 'thresholds', 'that', 'correspond', 'to', 'the', 'configuration', 'information', 'in', 'response', 'to', 'the', 'parameter', 'threshold', 'obtaining', 'request', 'comparing', 'the', 'two', 'or', 'more', 'parameters', 'with', 'the', 'corresponding', 'preset', 'thresholds', 'and', 'controlling', 'not', 'to', 'perform', 'processing', 'on', 'an', 'acquired', 'based', 'on', 'at', 'least', 'one', 'of', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'greater', 'than', 'a', 'corresponding', 'preset', 'threshold', 'or', 'based', 'on', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'respectively', 'greater', 'than', 'the', 'corresponding', 'preset', 'thresholds', 'wherein', 'the', 'acquiring', 'comprises', 'acquiring', 'the', 'in', 'real', 'time', 'and', 'the', 'obtaining', 'comprises', 'obtaining', 'the', 'motion', 'parameter', 'of', 'the', 'terminal', 'in', 'real', 'time', 'the', 'further', 'comprising', 'in', 'response', 'to', 'the', 'at', 'least', 'one', 'of', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'being', 'greater', 'than', 'the', 'corresponding', 'preset', 'threshold', 'obtaining', 'the', 'motion', 'parameter', 'of', 'the', 'terminal', 'again', 'and', 'in', 'response', 'to', 'the', 'two', 'or', 'more', 'parameters', 'of', 'the', 'motion', 'parameter', 'obtained', 'at', 'a', 'latest', 'time', 'being', 'less', 'than', 'or', 'equal', 'to', 'the', 'corresponding', 'preset', 'thresholds', 'performing', 'the', 'processing', 'on', 'the', 'acquired', 'at', 'the', 'latest', 'time', 'the', 'non-transitory', 'computer-readable', 'storage', 'medium', 'according', 'to', 'wherein', 'the', 'acquired', 'is', 'a', 'face', 'and', 'the', 'processing', 'comprises', 'performing', 'face', 'the', 'non-transitory', 'computer-readable', 'storage', 'medium', 'according', 'to', 'wherein', 'the', 'obtaining', 'the', 'motion', 'parameter', 'comprises', 'at', 'least', 'one', 'of', 'obtaining', 'the', 'acceleration', 'of', 'the', 'terminal', 'by', 'using', 'an', 'acceleration', 'sensor', 'or', 'obtaining', 'the', 'angular', 'velocity', 'of', 'the', 'terminal', 'by', 'using', 'a', 'gyro', 'sensor', 'the', 'non-transitory', 'computer-readable', 'storage', 'medium', 'according', 'to', 'wherein', 'the', 'motion', 'parameter', 'comprises', 'the', 'motion', 'frequency', 'and', 'the', 'motion', 'time', 'a', 'of', 'processing', 'a', 'drive-through', 'order', 'the', 'comprising', 'receiving', 'customer', 'information', 'detected', 'through', 'vision', 'providing', 'product', 'information', 'to', 'a', 'customer', 'based', 'on', 'the', 'customer', 'information', 'and', 'processing', 'a', 'product', 'order', 'of', 'the', 'customer', 'the', 'according', 'to', 'wherein', 'the', 'receiving', 'of', 'customer', 'information', 'comprises', 'at', 'least', 'one', 'of', 'receiving', 'customer', 'information', 'associated', 'with', 'vehicle', 'information', 'detected', 'through', 'vehicle', 'or', 'receiving', 'customer', 'information', 'associated', 'with', 'identification', 'information', 'detected', 'through', 'face', 'the', 'according', 'to', 'further', 'comprising', 'determining', 'whether', 'the', 'customer', 'is', 'a', 'pre-order', 'customer', 'based', 'on', 'the', 'customer', 'information', 'wherein', 'when', 'the', 'customer', 'is', 'determined', 'to', 'be', 'a', 'pre-order', 'customer', 'the', 'providing', 'of', 'product', 'information', 'based', 'on', 'the', 'customer', 'information', 'comprises', 'providing', 'pre-order', 'information', 'using', 'at', 'least', 'one', 'of', 'audio', 'or', 'video', 'and', 'the', 'processing', 'of', 'the', 'product', 'order', 'of', 'the', 'customer', 'comprises', 'providing', 'information', 'for', 'promptly', 'guiding', 'a', 'vehicle', 'to', 'a', 'pickup', 'stand', 'using', 'at', 'least', 'one', 'of', 'audio', 'or', 'video', 'and', 'providing', 'information', 'that', 'an', 'additional', 'order', 'is', 'available', 'the', 'according', 'to', 'wherein', 'the', 'product', 'information', 'based', 'on', 'the', 'customer', 'information', 'comprises', 'a', 'most', 'recently', 'ordered', 'product', 'component', 'and', 'a', 'most', 'frequently', 'ordered', 'product', 'component', 'in', 'an', 'order', 'history', 'of', 'the', 'customer', 'information', 'the', 'according', 'to', 'wherein', 'the', 'receiving', 'of', 'customer', 'information', 'comprises', 'receiving', 'information', 'about', 'an', 'age', 'and', 'gender', 'of', 'a', 'passenger', 'detected', 'through', 'face', 'and', 'the', 'providing', 'of', 'product', 'information', 'to', 'a', 'customer', 'based', 'on', 'the', 'customer', 'information', 'comprises', 'providing', 'recommended', 'menu', 'information', 'differentiated', 'according', 'to', 'the', 'age', 'and', 'gender', 'the', 'according', 'to', 'wherein', 'the', 'processing', 'of', 'a', 'product', 'order', 'of', 'the', 'customer', 'comprises', 'determining', 'a', 'product', 'component', 'in', 'a', 'past', 'order', 'history', 'or', 'a', 'component', 'modified', 'from', 'the', 'product', 'component', 'as', 'a', 'product', 'order', 'the', 'according', 'to', 'wherein', 'the', 'processing', 'of', 'a', 'product', 'order', 'of', 'the', 'customer', 'comprises', 'paying', 'a', 'product', 'price', 'according', 'to', 'biometrics-based', 'authentication', 'through', 'a', 'communication', 'system', 'of', 'a', 'vehicle', 'or', 'a', 'mobile', 'terminal', 'the', 'according', 'to', 'wherein', 'the', 'processing', 'of', 'a', 'product', 'order', 'of', 'the', 'customer', 'comprises', 'issuing', 'a', 'payment', 'number', 'for', 'a', 'divided', 'payment', 'and', 'performing', 'the', 'divided', 'payments', 'according', 'to', 'payment', 'requests', 'of', 'a', 'of', 'mobile', 'terminals', 'to', 'which', 'the', 'payment', 'numbers', 'are', 'inputted', 'the', 'according', 'to', 'wherein', 'the', 'processing', 'of', 'a', 'product', 'order', 'of', 'the', 'customer', 'further', 'comprises', 'accumulating', 'mileage', 'in', 'an', 'account', 'corresponding', 'to', 'the', 'mobile', 'terminal', 'undergoing', 'a', 'payment', 'the', 'according', 'to', 'wherein', 'the', 'processing', 'of', 'a', 'product', 'order', 'of', 'the', 'customer', 'further', 'comprises', 'suggesting', 'a', 'takeout', 'packaging', 'according', 'to', 'a', 'temperature', 'of', 'a', 'product', 'an', 'atmospheric', 'temperature', 'weather', 'and', 'a', 'vehicle', 'type', 'an', 'apparatus', 'configured', 'to', 'process', 'a', 'drive-through', 'order', 'the', 'apparatus', 'comprising', 'a', 'transceiver', 'configured', 'to', 'receive', 'customer', 'information', 'detected', 'through', 'vision', 'a', 'digital', 'signage', 'configured', 'to', 'provide', 'product', 'information', 'to', 'a', 'customer', 'based', 'on', 'the', 'customer', 'information', 'and', 'a', 'configured', 'to', 'process', 'a', 'product', 'order', 'of', 'the', 'customer', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'transceiver', 'receives', 'at', 'least', 'one', 'of', 'customer', 'information', 'associated', 'with', 'vehicle', 'information', 'detected', 'through', 'vehicle', 'or', 'customer', 'information', 'associated', 'with', 'identification', 'information', 'detected', 'through', 'face', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'is', 'configured', 'to', 'determine', 'whether', 'the', 'customer', 'is', 'a', 'pre-order', 'customer', 'based', 'on', 'the', 'customer', 'information', 'and', 'when', 'the', 'customer', 'is', 'determined', 'to', 'be', 'a', 'pre-order', 'customer', 'perform', 'a', 'control', 'operation', 'to', 'provide', 'pre-order', 'information', 'and', 'control', 'the', 'digital', 'signage', 'to', 'output', 'information', 'for', 'promptly', 'guiding', 'a', 'vehicle', 'to', 'a', 'pickup', 'stand', 'and', 'provide', 'information', 'that', 'an', 'additional', 'order', 'is', 'available', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'product', 'information', 'based', 'on', 'the', 'customer', 'information', 'comprises', 'a', 'most', 'recently', 'ordered', 'product', 'component', 'and', 'a', 'most', 'frequently', 'ordered', 'product', 'component', 'in', 'an', 'order', 'history', 'of', 'the', 'customer', 'information', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'transceiver', 'is', 'configured', 'to', 'receive', 'information', 'about', 'an', 'age', 'and', 'gender', 'of', 'a', 'passenger', 'detected', 'through', 'face', 'and', 'the', 'is', 'configured', 'to', 'control', 'the', 'digital', 'signage', 'to', 'provide', 'recommended', 'menu', 'information', 'differentiated', 'according', 'to', 'the', 'age', 'and', 'gender', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'is', 'configured', 'to', 'determine', 'a', 'product', 'component', 'in', 'a', 'past', 'order', 'history', 'or', 'a', 'component', 'modified', 'from', 'the', 'product', 'component', 'as', 'the', 'product', 'order', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'is', 'configured', 'to', 'pay', 'a', 'product', 'price', 'according', 'to', 'biometrics-based', 'authentication', 'through', 'a', 'communication', 'system', 'of', 'a', 'vehicle', 'or', 'a', 'mobile', 'terminal', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'is', 'configured', 'to', 'issue', 'a', 'payment', 'number', 'for', 'a', 'divided', 'payment', 'and', 'perform', 'the', 'divided', 'payments', 'according', 'to', 'requests', 'of', 'a', 'of', 'mobile', 'terminals', 'to', 'which', 'the', 'payment', 'numbers', 'are', 'inputted', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'is', 'configured', 'to', 'accumulate', 'mileage', 'in', 'an', 'account', 'corresponding', 'to', 'the', 'mobile', 'terminal', 'undergoing', 'a', 'payment', 'the', 'apparatus', 'according', 'to', 'wherein', 'the', 'is', 'configured', 'to', 'control', 'the', 'digital', 'signage', 'to', 'suggest', 'a', 'takeout', 'packaging', 'according', 'to', 'a', 'temperature', 'of', 'a', 'product', 'an', 'atmospheric', 'temperature', 'weather', 'and', 'a', 'vehicle', 'type', 'an', 'information', 'processing', 'performed', 'at', 'a', 'computing', 'device', 'having', 'one', 'or', 'more', 's', 'and', 'memory', 'storing', 'a', 'of', 'programs', 'to', 'be', 'executed', 'by', 'the', 'one', 'or', 'more', 's', 'the', 'comprising', 'identifying', 'using', 'face', 'one', 'or', 'more', 'each', 'face', 'corresponding', 'to', 'a', 'respective', 'person', 'captured', 'in', 'a', 'first', 'for', 'each', 'identified', 'face', 'extracting', 'a', 'set', 'of', 'profile', 'parameters', 'of', 'a', 'corresponding', 'person', 'in', 'the', 'first', 'and', 'selecting', 'from', 'a', 'of', 'tiles', 'a', 'first', 'tile', 'that', 'matches', 'the', 'face', 'of', 'the', 'corresponding', 'person', 'in', 'the', 'first', 'in', 'accordance', 'with', 'a', 'predefined', 'correspondence', 'between', 'the', 'set', 'of', 'profile', 'parameters', 'of', 'the', 'corresponding', 'person', 'and', 'a', 'set', 'of', 'pre-stored', 'description', 'parameters', 'of', 'the', 'first', 'tile', 'generating', 'a', 'second', 'by', 'covering', 'the', 'of', 'respective', 'persons', 'in', 'the', 'first', 'with', 'their', 'corresponding', 'first', 'tiles', 'and', 'sharing', 'the', 'first', 'and', 'the', 'second', 'in', 'a', 'predefined', 'order', 'via', 'a', 'group', 'chat', 'session', 'the', 'of', 'wherein', 'the', 'first', 'and', 'the', 'second', 'are', 'displayed', 'in', 'the', 'group', 'chat', 'session', 'one', 'at', 'a', 'time', 'such', 'that', 'one', 'of', 'the', 'two', 's', 'is', 'replaced', 'by', 'the', 'other', 'of', 'the', 'two', 's', 'periodically', 'the', 'of', 'wherein', 'extracting', 'a', 'set', 'of', 'profile', 'parameters', 'of', 'a', 'corresponding', 'person', 'in', 'the', 'first', 'includes', 'determining', 'one', 'or', 'more', 'descriptive', 'labels', 'corresponding', 'to', 'the', 'identified', 'face', 'of', 'the', 'corresponding', 'person', 'using', 'a', 'first', 'machine', 'learning', 'model', 'wherein', 'the', 'first', 'machine', 'learning', 'model', 'is', 'trained', 'with', 'the', 'facial', 's', 'and', 'corresponding', 'descriptive', 'labels', 'the', 'of', 'wherein', 'extracting', 'a', 'set', 'of', 'profile', 'parameters', 'of', 'a', 'corresponding', 'person', 'in', 'the', 'first', 'includes', 'determining', 'an', 'identity', 'of', 'the', 'corresponding', 'person', 'based', 'on', 'the', 'identified', 'face', 'of', 'the', 'corresponding', 'person', 'locating', 'respective', 'profile', 'information', 'of', 'the', 'first', 'person', 'based', 'on', 'the', 'determined', 'identity', 'of', 'the', 'corresponding', 'person', 'and', 'using', 'one', 'or', 'more', 'characteristics', 'in', 'the', 'respective', 'profile', 'information', 'of', 'the', 'first', 'person', 'as', 'the', 'set', 'of', 'profile', 'parameters', 'corresponding', 'to', 'the', 'identified', 'face', 'of', 'the', 'corresponding', 'person', 'the', 'of', 'wherein', 'at', 'least', 'a', 'first', 'one', 'of', 'the', 'first', 'tiles', 'is', 'a', 'dynamic', 'tile', 'and', 'at', 'least', 'a', 'second', 'one', 'of', 'the', 'first', 'tiles', 'is', 'a', 'static', 'tile', 'the', 'of', 'including', 'receiving', 'a', 'of', 'comments', 'from', 'different', 's', 'of', 'the', 'group', 'chat', 'session', 'each', 'comment', 'including', 'a', 'descriptive', 'term', 'for', 'a', 'respective', 'person', 'identified', 'in', 'the', 'first', 'choosing', 'a', 'descriptive', 'label', 'for', 'the', 'respective', 'person', 'according', 'to', 'the', 'of', 'comments', 'and', 'updating', 'the', 'second', 'by', 'adding', 'the', 'descriptive', 'label', 'adjacent', 'to', 'the', 'first', 'tile', 'of', 'the', 'respective', 'person', 'a', 'computing', 'device', 'for', 'information', 'processing', 'comprising', 'one', 'or', 'more', 's', 'and', 'memory', 'storing', 'instructions', 'which', 'when', 'executed', 'by', 'the', 'one', 'or', 'more', 's', 'cause', 'the', 's', 'to', 'perform', 'a', 'of', 'operations', 'comprising', 'identifying', 'using', 'face', 'one', 'or', 'more', 'each', 'face', 'corresponding', 'to', 'a', 'respective', 'person', 'captured', 'in', 'a', 'first', 'for', 'each', 'identified', 'face', 'extracting', 'a', 'set', 'of', 'profile', 'parameters', 'of', 'a', 'corresponding', 'person', 'in', 'the', 'first', 'and', 'selecting', 'from', 'a', 'of', 'tiles', 'a', 'first', 'tile', 'that', 'matches', 'the', 'face', 'of', 'the', 'corresponding', 'person', 'in', 'the', 'first', 'in', 'accordance', 'with', 'a', 'predefined', 'correspondence', 'between', 'the', 'set', 'of', 'profile', 'parameters', 'of', 'the', 'corresponding', 'person', 'and', 'a', 'set', 'of', 'pre-stored', 'description', 'parameters', 'of', 'the', 'first', 'tile', 'generating', 'a', 'second', 'by', 'covering', 'the', 'of', 'respective', 'persons', 'in', 'the', 'first', 'with', 'their', 'corresponding', 'first', 'tiles', 'and', 'sharing', 'the', 'first', 'and', 'the', 'second', 'in', 'a', 'predefined', 'order', 'via', 'a', 'group', 'chat', 'session', 'the', 'computing', 'device', 'of', 'wherein', 'the', 'first', 'and', 'the', 'second', 'are', 'displayed', 'in', 'the', 'group', 'chat', 'session', 'one', 'at', 'a', 'time', 'such', 'that', 'one', 'of', 'the', 'two', 's', 'is', 'replaced', 'by', 'the', 'other', 'of', 'the', 'two', 's', 'periodically', 'the', 'computing', 'device', 'of', 'wherein', 'extracting', 'a', 'set', 'of', 'profile', 'parameters', 'of', 'a', 'corresponding', 'person', 'in', 'the', 'first', 'includes', 'determining', 'one', 'or', 'more', 'descriptive', 'labels', 'corresponding', 'to', 'the', 'identified', 'face', 'of', 'the', 'corresponding', 'person', 'using', 'a', 'first', 'machine', 'learning', 'model', 'wherein', 'the', 'first', 'machine', 'learning', 'model', 'is', 'trained', 'with', 'the', 'facial', 's', 'and', 'corresponding', 'descriptive', 'labels', 'the', 'computing', 'device', 'of', 'wherein', 'extracting', 'a', 'set', 'of', 'profile', 'parameters', 'of', 'a', 'corresponding', 'person', 'in', 'the', 'first', 'includes', 'determining', 'an', 'identity', 'of', 'the', 'corresponding', 'person', 'based', 'on', 'the', 'identified', 'face', 'of', 'the', 'corresponding', 'person', 'locating', 'respective', 'profile', 'information', 'of', 'the', 'first', 'person', 'based', 'on', 'the', 'determined', 'identity', 'of', 'the', 'corresponding', 'person', 'and', 'using', 'one', 'or', 'more', 'characteristics', 'in', 'the', 'respective', 'profile', 'information', 'of', 'the', 'first', 'person', 'as', 'the', 'set', 'of', 'profile', 'parameters', 'corresponding', 'to', 'the', 'identified', 'face', 'of', 'the', 'corresponding', 'person', 'the', 'computing', 'device', 'of', 'wherein', 'at', 'least', 'a', 'first', 'one', 'of', 'the', 'first', 'tiles', 'is', 'a', 'dynamic', 'tile', 'and', 'at', 'least', 'a', 'second', 'one', 'of', 'the', 'first', 'tiles', 'is', 'a', 'static', 'tile', 'the', 'computing', 'device', 'of', 'wherein', 'the', 'of', 'operations', 'further', 'include', 'receiving', 'a', 'of', 'comments', 'from', 'different', 's', 'of', 'the', 'group', 'chat', 'session', 'each', 'comment', 'including', 'a', 'descriptive', 'term', 'for', 'a', 'respective', 'person', 'identified', 'in', 'the', 'first', 'choosing', 'a', 'descriptive', 'label', 'for', 'the', 'respective', 'person', 'according', 'to', 'the', 'of', 'comments', 'and', 'updating', 'the', 'second', 'by', 'adding', 'the', 'descriptive', 'label', 'adjacent', 'to', 'the', 'first', 'tile', 'of', 'the', 'respective', 'person', 'a', 'non-transitory', 'computer-readable', 'storage', 'medium', 'storing', 'instructions', 'which', 'when', 'executed', 'by', 'a', 'computing', 'device', 'having', 'one', 'or', 'more', 's', 'cause', 'the', 'computing', 'device', 'to', 'perform', 'a', 'of', 'operations', 'comprising', 'identifying', 'using', 'face', 'one', 'or', 'more', 'each', 'face', 'corresponding', 'to', 'a', 'respective', 'person', 'captured', 'in', 'a', 'first', 'for', 'each', 'identified', 'face', 'extracting', 'a', 'set', 'of', 'profile', 'parameters', 'of', 'a', 'corresponding', 'person', 'in', 'the', 'first', 'and', 'selecting', 'from', 'a', 'of', 'tiles', 'a', 'first', 'tile', 'that', 'matches', 'the', 'face', 'of', 'the', 'corresponding', 'person', 'in', 'the', 'first', 'in', 'accordance', 'with', 'a', 'predefined', 'correspondence', 'between', 'the', 'set', 'of', 'profile', 'parameters', 'of', 'the', 'corresponding', 'person', 'and', 'a', 'set', 'of', 'pre-stored', 'description', 'parameters', 'of', 'the', 'first', 'tile', 'generating', 'a', 'second', 'by', 'covering', 'the', 'of', 'respective', 'persons', 'in', 'the', 'first', 'with', 'their', 'corresponding', 'first', 'tiles', 'and', 'sharing', 'the', 'first', 'and', 'the', 'second', 'in', 'a', 'predefined', 'order', 'via', 'a', 'group', 'chat', 'session', 'the', 'non-transitory', 'computer-readable', 'storage', 'medium', 'of', 'wherein', 'the', 'first', 'and', 'the', 'second', 'are', 'displayed', 'in', 'the', 'group', 'chat', 'session', 'one', 'at', 'a', 'time', 'such', 'that', 'one', 'of', 'the', 'two', 's', 'is', 'replaced', 'by', 'the', 'other', 'of', 'the', 'two', 's', 'periodically', 'the', 'non-transitory', 'computer-readable', 'storage', 'medium', 'of', 'wherein', 'extracting', 'a', 'set', 'of', 'profile', 'parameters', 'of', 'a', 'corresponding', 'person', 'in', 'the', 'first', 'includes', 'determining', 'one', 'or', 'more', 'descriptive', 'labels', 'corresponding', 'to', 'the', 'identified', 'face', 'of', 'the', 'corresponding', 'person', 'using', 'a', 'first', 'machine', 'learning', 'model', 'wherein', 'the', 'first', 'machine', 'learning', 'model', 'is', 'trained', 'with', 'the', 'facial', 's', 'and', 'corresponding', 'descriptive', 'labels', 'the', 'non-transitory', 'computer-readable', 'storage', 'medium', 'of', 'wherein', 'extracting', 'a', 'set', 'of', 'profile', 'parameters', 'of', 'a', 'corresponding', 'person', 'in', 'the', 'first', 'includes', 'determining', 'an', 'identity', 'of', 'the', 'corresponding', 'person', 'based', 'on', 'the', 'identified', 'face', 'of', 'the', 'corresponding', 'person', 'locating', 'respective', 'profile', 'information', 'of', 'the', 'first', 'person', 'based', 'on', 'the', 'determined', 'identity', 'of', 'the', 'corresponding', 'person', 'and', 'using', 'one', 'or', 'more', 'characteristics', 'in', 'the', 'respective', 'profile', 'information', 'of', 'the', 'first', 'person', 'as', 'the', 'set', 'of', 'profile', 'parameters', 'corresponding', 'to', 'the', 'identified', 'face', 'of', 'the', 'corresponding', 'person', 'the', 'non-transitory', 'computer-readable', 'storage', 'medium', 'of', 'wherein', 'at', 'least', 'a', 'first', 'one', 'of', 'the', 'first', 'tiles', 'is', 'a', 'dynamic', 'tile', 'and', 'at', 'least', 'a', 'second', 'one', 'of', 'the', 'first', 'tiles', 'is', 'a', 'static', 'tile', 'the', 'non-transitory', 'computer-readable', 'storage', 'medium', 'of', 'wherein', 'the', 'of', 'operations', 'further', 'include', 'receiving', 'a', 'of', 'comments', 'from', 'different', 's', 'of', 'the', 'group', 'chat', 'session', 'each', 'comment', 'including', 'a', 'descriptive', 'term', 'for', 'a', 'respective', 'person', 'identified', 'in', 'the', 'first', 'choosing', 'a', 'descriptive', 'label', 'for', 'the', 'respective', 'person', 'according', 'to', 'the', 'of', 'comments', 'and', 'updating', 'the', 'second', 'by', 'adding', 'the', 'descriptive', 'label', 'adjacent', 'to', 'the', 'first', 'tile', 'of', 'the', 'respective', 'person', 'a', 'comprising', 'by', 'a', 'computing', 'system', 'determining', 'that', 'a', 'performance', 'metric', 'of', 'an', 'eye', 'system', 'is', 'below', 'a', 'first', 'performance', 'threshold', 'wherein', 'the', 'eye', 'system', 'is', 'associated', 'with', 'a', 'head-mounted', 'display', 'worn', 'by', 'a', 'based', 'on', 'the', 'determination', 'of', 'the', 'performance', 'metric', 'of', 'the', 'eye', 'system', 'being', 'below', 'the', 'first', 'performance', 'threshold', 'the', 'computer', 'system', 'performing', 'receiving', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'a', 'body', 'of', 'the', 'estimating', 'a', 'region', 'that', 'the', 'is', 'looking', 'at', 'within', 'a', 'field', 'of', 'view', 'of', 'the', 'head-mounted', 'display', 'based', 'on', 'the', 'received', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'determining', 'a', 'vergence', 'distance', 'of', 'the', 'based', 'at', 'least', 'on', 'the', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'the', 'estimated', 'region', 'that', 'the', 'is', 'looking', 'at', 'and', 'locations', 'of', 'one', 'or', 'more', 'objects', 'in', 'a', 'scene', 'displayed', 'by', 'the', 'head-mounted', 'display', 'and', 'adjusting', 'one', 'or', 'more', 'configurations', 'of', 'the', 'head-mounted', 'display', 'based', 'on', 'the', 'determined', 'vergence', 'distance', 'of', 'the', 'the', 'of', 'wherein', 'the', 'one', 'or', 'more', 'configurations', 'of', 'the', 'head-mounted', 'display', 'comprise', 'one', 'or', 'more', 'of', 'a', 'rendering', 'a', 'position', 'of', 'a', 'display', 'screen', 'or', 'a', 'position', 'of', 'an', 'optics', 'block', 'the', 'of', 'further', 'comprising', 'determining', 'that', 'the', 'performance', 'metric', 'of', 'the', 'eye', 'system', 'is', 'above', 'a', 'second', 'performance', 'threshold', 'receiving', 'eye', 'data', 'from', 'the', 'eye', 'system', 'and', 'determining', 'the', 'vergence', 'distance', 'of', 'the', 'based', 'on', 'the', 'eye', 'data', 'and', 'the', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'the', 'of', 'further', 'comprising', 'receiving', 'one', 'or', 'more', 'second', 'inputs', 'associated', 'with', 'one', 'or', 'more', 'displaying', 'elements', 'in', 'the', 'scene', 'displayed', 'by', 'the', 'head-mounted', 'display', 'and', 'determining', 'the', 'vergence', 'distance', 'of', 'the', 'based', 'at', 'least', 'on', 'the', 'eye', 'data', 'the', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'and', 'the', 'one', 'or', 'more', 'second', 'inputs', 'associated', 'with', 'the', 'one', 'or', 'more', 'displaying', 'elements', 'of', 'the', 'scene', 'the', 'of', 'further', 'comprising', 'feeding', 'the', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'to', 'a', 'fusion', 'algorithm', 'wherein', 'the', 'fusion', 'algorithm', 'assigns', 'a', 'weight', 'score', 'to', 'each', 'input', 'of', 'the', 'one', 'or', 'more', 'first', 'inputs', 'determining', 'the', 'vergence', 'distance', 'of', 'the', 'using', 'the', 'fusion', 'algorithm', 'based', 'on', 'the', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'and', 'determining', 'a', 'z-depth', 'of', 'a', 'display', 'screen', 'and', 'a', 'confidence', 'score', 'based', 'on', 'the', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'the', 'of', 'further', 'comprising', 'comparing', 'the', 'confidence', 'score', 'to', 'a', 'confidence', 'level', 'threshold', 'in', 'response', 'to', 'a', 'determination', 'that', 'the', 'confidence', 'score', 'is', 'below', 'the', 'confidence', 'level', 'threshold', 'feeding', 'the', 'one', 'or', 'more', 'second', 'inputs', 'associated', 'with', 'the', 'one', 'or', 'more', 'displaying', 'elements', 'of', 'the', 'scene', 'to', 'the', 'fusion', 'algorithm', 'and', 'determining', 'the', 'z-depth', 'of', 'the', 'display', 'screen', 'using', 'the', 'fusion', 'algorithm', 'based', 'on', 'the', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'and', 'the', 'one', 'or', 'more', 'second', 'inputs', 'associated', 'with', 'the', 'one', 'or', 'more', 'displaying', 'elements', 'of', 'the', 'scene', 'the', 'of', 'further', 'comparing', 'comparing', 'by', 'the', 'fusion', 'algorithm', 'confidence', 'scores', 'associated', 'with', 'a', 'of', 'combinations', 'of', 'inputs', 'and', 'determining', 'by', 'the', 'fusion', 'algorithm', 'the', 'z-depth', 'of', 'the', 'display', 'screen', 'based', 'on', 'a', 'combination', 'of', 'inputs', 'associated', 'with', 'a', 'highest', 'confidence', 'score', 'the', 'of', 'wherein', 'the', 'z-depth', 'and', 'the', 'confidence', 'score', 'are', 'determined', 'by', 'the', 'fusion', 'algorithm', 'using', 'a', 'piecewise', 'comparison', 'of', 'the', 'one', 'or', 'more', 'first', 'inputs', 'and', 'the', 'one', 'or', 'more', 'second', 'inputs', 'the', 'of', 'wherein', 'the', 'z-depth', 'and', 'the', 'confidence', 'score', 'are', 'determined', 'based', 'on', 'a', 'correlation', 'between', 'two', 'or', 'more', 'inputs', 'of', 'the', 'one', 'or', 'more', 'first', 'inputs', 'and', 'the', 'one', 'or', 'more', 'second', 'inputs', 'the', 'of', 'wherein', 'the', 'fusion', 'algorithm', 'comprises', 'a', 'machine', 'learning', 'ml', 'algorithm', 'and', 'wherein', 'the', 'machine', 'learning', 'ml', 'algorithm', 'determines', 'a', 'combination', 'of', 'first', 'inputs', 'fed', 'to', 'the', 'fusion', 'algorithm', 'the', 'of', 'wherein', 'the', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'comprise', 'one', 'or', 'more', 'of', 'a', 'hand', 'position', 'a', 'hand', 'direction', 'a', 'hand', 'movement', 'a', 'hand', 'gesture', 'a', 'head', 'position', 'a', 'head', 'direction', 'a', 'head', 'movement', 'a', 'head', 'gesture', 'a', 'gaze', 'angle', 'rea', 'body', 'gesture', 'a', 'body', 'posture', 'a', 'body', 'movement', 'a', 'behavior', 'of', 'the', 'or', 'a', 'weighted', 'combination', 'of', 'one', 'or', 'more', 'related', 'parameters', 'the', 'of', 'wherein', 'the', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'are', 'received', 'from', 'one', 'or', 'more', 'of', 'a', 'controller', 'a', 'sensor', 'a', 'camera', 'a', 'microphone', 'an', 'accelerometer', 'a', 'headset', 'worn', 'by', 'the', 'or', 'a', 'mobile', 'device', 'the', 'of', 'wherein', 'the', 'one', 'or', 'more', 'second', 'inputs', 'associated', 'with', 'the', 'one', 'or', 'more', 'displaying', 'elements', 'comprise', 'one', 'or', 'more', 'of', 'a', 'z-buffer', 'value', 'associated', 'with', 'a', 'displaying', 'element', 'a', 'displaying', 'element', 'marked', 'by', 'a', 'developer', 'an', 'analysis', 'result', 'a', 'shape', 'of', 'a', 'displaying', 'element', 'a', 'face', 'result', 'an', 'object', 'result', 'a', 'person', 'identified', 'in', 'a', 'displaying', 'content', 'an', 'object', 'identified', 'in', 'a', 'displaying', 'content', 'a', 'correlation', 'of', 'two', 'or', 'more', 'displaying', 'elements', 'or', 'a', 'weighted', 'combination', 'of', 'the', 'one', 'or', 'more', 'second', 'inputs', 'the', 'of', 'further', 'comprising', 'determining', 'that', 'the', 'performance', 'metric', 'of', 'the', 'eye', 'system', 'is', 'below', 'a', 'second', 'performance', 'threshold', 'receiving', 'one', 'or', 'more', 'second', 'inputs', 'associated', 'with', 'one', 'or', 'more', 'displaying', 'elements', 'in', 'the', 'scene', 'displayed', 'by', 'the', 'head-mounted', 'display', 'and', 'determining', 'the', 'vergence', 'distance', 'of', 'the', 'based', 'at', 'least', 'on', 'the', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'and', 'the', 'one', 'or', 'more', 'second', 'inputs', 'associated', 'with', 'the', 'one', 'or', 'more', 'displaying', 'elements', 'the', 'of', 'wherein', 'determining', 'that', 'the', 'performance', 'metric', 'of', 'the', 'eye', 'system', 'is', 'below', 'the', 'second', 'performance', 'threshold', 'comprises', 'determining', 'that', 'the', 'eye', 'system', 'does', 'not', 'exist', 'or', 'fails', 'to', 'provide', 'eye', 'data', 'the', 'of', 'wherein', 'the', 'performance', 'metric', 'of', 'the', 'eye', 'system', 'comprises', 'one', 'or', 'more', 'of', 'an', 'accuracy', 'of', 'a', 'parameter', 'from', 'the', 'eye', 'system', 'a', 'precision', 'of', 'a', 'parameter', 'from', 'the', 'eye', 'system', 'a', 'value', 'of', 'a', 'parameter', 'from', 'the', 'eye', 'system', 'a', 'detectability', 'of', 'a', 'pupil', 'a', 'metric', 'based', 'on', 'one', 'or', 'more', 'parameters', 'associated', 'with', 'the', 'a', 'parameter', 'change', 'a', 'parameter', 'changing', 'trend', 'a', 'data', 'availability', 'or', 'a', 'weighted', 'combination', 'of', 'one', 'or', 'more', 'performance', 'related', 'parameters', 'the', 'of', 'wherein', 'the', 'one', 'or', 'more', 'parameters', 'associated', 'with', 'the', 'comprise', 'one', 'or', 'more', 'of', 'an', 'eye', 'distance', 'of', 'the', 'a', 'pupil', 'position', 'a', 'pupil', 'status', 'a', 'correlation', 'of', 'two', 'pupils', 'of', 'the', 'a', 'head', 'size', 'of', 'the', 'a', 'position', 'of', 'a', 'headset', 'worn', 'by', 'the', 'an', 'angle', 'of', 'the', 'headset', 'worn', 'by', 'the', 'a', 'direction', 'of', 'the', 'headset', 'worn', 'by', 'the', 'an', 'alignment', 'of', 'the', 'eyes', 'of', 'the', 'or', 'a', 'weighted', 'combination', 'of', 'one', 'or', 'more', 'related', 'parameters', 'associated', 'with', 'the', 'the', 'of', 'wherein', 'the', 'first', 'performance', 'threshold', 'comprises', 'one', 'or', 'more', 'of', 'a', 'pre-determined', 'value', 'a', 'pre-determined', 'range', 'a', 'state', 'of', 'a', 'data', 'a', 'changing', 'speed', 'of', 'a', 'data', 'or', 'a', 'trend', 'of', 'a', 'data', 'change', 'one', 'or', 'more', 'non-transitory', 'computer-readable', 'storage', 'media', 'embodying', 'software', 'that', 'is', 'operable', 'when', 'executed', 'by', 'a', 'computing', 'system', 'to', 'determine', 'that', 'a', 'performance', 'metric', 'of', 'an', 'eye', 'system', 'is', 'below', 'a', 'first', 'performance', 'threshold', 'wherein', 'the', 'eye', 'system', 'is', 'associated', 'with', 'a', 'head-mounted', 'display', 'worn', 'by', 'a', 'based', 'on', 'the', 'determination', 'of', 'the', 'performance', 'metric', 'of', 'the', 'eye', 'system', 'being', 'below', 'the', 'first', 'performance', 'threshold', 'the', 'media', 'embodying', 'software', 'operable', 'when', 'executed', 'by', 'the', 'computing', 'system', 'to', 'receive', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'a', 'body', 'of', 'the', 'estimate', 'a', 'region', 'that', 'the', 'is', 'looking', 'at', 'within', 'a', 'field', 'of', 'view', 'of', 'the', 'head-mounted', 'display', 'based', 'on', 'the', 'received', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'determine', 'a', 'vergence', 'distance', 'of', 'the', 'based', 'at', 'least', 'on', 'the', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'the', 'estimated', 'region', 'that', 'the', 'is', 'looking', 'at', 'and', 'locations', 'of', 'one', 'or', 'more', 'objects', 'in', 'a', 'scene', 'displayed', 'by', 'the', 'head-mounted', 'display', 'and', 'adjust', 'one', 'or', 'more', 'configurations', 'of', 'the', 'head-mounted', 'display', 'based', 'on', 'the', 'determined', 'vergence', 'distance', 'of', 'the', 'a', 'system', 'comprising', 'one', 'or', 'more', 'non-transitory', 'computer-readable', 'storage', 'media', 'embodying', 'instructions', 'one', 'or', 'more', 's', 'coupled', 'to', 'the', 'storage', 'media', 'and', 'operable', 'to', 'execute', 'the', 'instructions', 'to', 'determine', 'that', 'a', 'performance', 'metric', 'of', 'an', 'eye', 'system', 'is', 'below', 'a', 'first', 'performance', 'threshold', 'wherein', 'the', 'eye', 'system', 'is', 'associated', 'with', 'a', 'head-mounted', 'display', 'worn', 'by', 'a', 'based', 'on', 'the', 'determination', 'of', 'the', 'performance', 'metric', 'of', 'the', 'eye', 'system', 'being', 'below', 'the', 'first', 'performance', 'threshold', 'the', 'system', 'is', 'configured', 'to', 'receive', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'a', 'body', 'of', 'the', 'estimate', 'a', 'region', 'that', 'the', 'is', 'looking', 'at', 'within', 'a', 'field', 'of', 'view', 'of', 'the', 'head-mounted', 'display', 'based', 'on', 'the', 'received', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'determine', 'a', 'vergence', 'distance', 'of', 'the', 'based', 'at', 'least', 'on', 'the', 'one', 'or', 'more', 'first', 'inputs', 'associated', 'with', 'the', 'body', 'of', 'the', 'the', 'estimated', 'region', 'that', 'the', 'is', 'looking', 'at', 'and', 'locations', 'of', 'one', 'or', 'more', 'objects', 'in', 'a', 'scene', 'displayed', 'by', 'the', 'head-mounted', 'display', 'and', 'adjust', 'one', 'or', 'more', 'configurations', 'of', 'the', 'head-mounted', 'display', 'based', 'on', 'the', 'determined', 'vergence', 'distance', 'of', 'the', 'a', 'computer-implemented', 'for', '-based', 'self-guided', 'object', 'detection', 'comprising', 'receiving', 'by', 'a', 'device', 'a', 'set', 'of', 's', 'each', 'of', 'the', 's', 'having', 'a', 'respective', 'grid', 'thereon', 'that', 'is', 'labeled', 'regarding', 'a', 'respective', 'object', 'to', 'be', 'detected', 'using', 'grid', 'level', 'label', 'data', 'training', 'by', 'the', 'device', 'a', 'grid-based', 'object', 'detector', 'using', 'the', 'grid', 'level', 'label', 'data', 'determining', 'by', 'the', 'device', 'a', 'respective', 'bounding', 'box', 'for', 'the', 'respective', 'object', 'in', 'each', 'of', 'the', 's', 'by', 'applying', 'local', 'segmentation', 'to', 'each', 'of', 'the', 's', 'and', 'training', 'by', 'the', 'device', 'a', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'for', 'joint', 'object', 'localization', 'and', 'object', 'classification', 'using', 'the', 'respective', 'bounding', 'box', 'for', 'the', 'respective', 'object', 'in', 'each', 'of', 'the', 's', 'as', 'an', 'input', 'to', 'the', 'rcnn', 'the', 'computer-implemented', 'of', 'further', 'comprising', 'performing', 'an', 'action', 'responsive', 'to', 'the', 'object', 'localization', 'and', 'object', 'classification', 'for', 'a', 'respective', 'new', 'object', 'in', 'a', 'new', 'to', 'which', 'the', 'rcnn', 'is', 'applied', 'the', 'computer-implemented', 'of', 'wherein', 'the', 'action', 'comprises', 'autonomously', 'controlling', 'a', 'motor', 'vehicle', 'to', 'avoid', 'a', 'collision', 'with', 'the', 'new', 'object', 'responsive', 'to', 'the', 'object', 'localization', 'and', 'object', 'classification', 'for', 'the', 'respective', 'new', 'object', 'the', 'computer-implemented', 'of', 'wherein', 'the', 'local', 'segmentation', 'is', 'performed', 'using', 'a', 'self-similarity', 'search', 'and', 'template', 'matching', 'to', 'provide', 'the', 'respective', 'bounding', 'box', 'around', 'the', 'respective', 'object', 'in', 'the', 'set', 'of', 's', 'the', 'computer-implemented', 'of', 'wherein', 'the', 'local', 'segmentation', 'is', 'applied', 'to', 'each', 'of', 'the', 's', 'to', 'segment', 'a', 'respective', 'target', 'region', 'therein', 'the', 'computer-implemented', 'of', 'wherein', 'the', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'forms', 'a', 'model', 'during', 'an', 'object', 'training', 'stage', 'that', 'is', 'to', 'detect', 'objects', 'in', 'new', 's', 'during', 'an', 'inference', 'stage', 'the', 'computer-implemented', 'of', 'wherein', 'the', 'is', 'performed', 'by', 'a', 'system', 'selected', 'from', 'the', 'group', 'consisting', 'of', 'a', 'surveillance', 'system', 'a', 'face', 'detection', 'system', 'a', 'face', 'system', 'a', 'cancer', 'detection', 'system', 'an', 'object', 'system', 'and', 'an', 'advanced', 'driver-assistance', 'system', 'a', 'computer', 'program', 'product', 'for', '-based', 'self-guided', 'object', 'detection', 'the', 'computer', 'program', 'product', 'comprising', 'a', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'having', 'program', 'instructions', 'embodied', 'therewith', 'the', 'program', 'instructions', 'executable', 'by', 'a', 'computer', 'to', 'cause', 'the', 'computer', 'to', 'perform', 'a', 'comprising', 'receiving', 'by', 'a', 'device', 'a', 'set', 'of', 's', 'each', 'of', 'the', 's', 'having', 'a', 'respective', 'grid', 'thereon', 'that', 'is', 'labeled', 'regarding', 'a', 'respective', 'object', 'to', 'be', 'detected', 'using', 'grid', 'level', 'label', 'data', 'training', 'by', 'the', 'device', 'a', 'grid-based', 'object', 'detector', 'using', 'the', 'grid', 'level', 'label', 'data', 'determining', 'by', 'the', 'device', 'a', 'respective', 'bounding', 'box', 'for', 'the', 'respective', 'object', 'in', 'each', 'of', 'the', 's', 'by', 'applying', 'local', 'segmentation', 'to', 'each', 'of', 'the', 's', 'and', 'training', 'by', 'the', 'device', 'a', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'for', 'joint', 'object', 'localization', 'and', 'object', 'classification', 'using', 'the', 'respective', 'bounding', 'box', 'for', 'the', 'respective', 'object', 'in', 'each', 'of', 'the', 's', 'as', 'an', 'input', 'to', 'the', 'rcnn', 'the', 'computer', 'program', 'product', 'of', 'wherein', 'the', 'further', 'comprises', 'performing', 'an', 'action', 'responsive', 'to', 'the', 'object', 'localization', 'and', 'object', 'classification', 'for', 'a', 'respective', 'new', 'object', 'in', 'a', 'new', 'to', 'which', 'the', 'rcnn', 'is', 'applied', 'the', 'computer', 'program', 'product', 'of', 'wherein', 'the', 'action', 'comprises', 'autonomously', 'controlling', 'a', 'motor', 'vehicle', 'to', 'avoid', 'a', 'collision', 'with', 'the', 'new', 'object', 'responsive', 'to', 'the', 'object', 'localization', 'and', 'object', 'classification', 'for', 'the', 'respective', 'new', 'object', 'the', 'computer', 'program', 'product', 'of', 'wherein', 'the', 'local', 'segmentation', 'is', 'performed', 'using', 'a', 'self-similarity', 'search', 'and', 'template', 'matching', 'to', 'provide', 'the', 'respective', 'bounding', 'box', 'around', 'the', 'respective', 'object', 'in', 'the', 'set', 'of', 's', 'the', 'computer', 'program', 'product', 'of', 'wherein', 'the', 'local', 'segmentation', 'is', 'applied', 'to', 'each', 'of', 'the', 's', 'to', 'segment', 'a', 'respective', 'target', 'region', 'therein', 'the', 'computer', 'program', 'product', 'of', 'wherein', 'the', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'forms', 'a', 'model', 'during', 'an', 'object', 'training', 'stage', 'that', 'is', 'to', 'detect', 'objects', 'in', 'new', 's', 'during', 'an', 'inference', 'stage', 'the', 'computer', 'program', 'product', 'of', 'wherein', 'the', 'is', 'performed', 'by', 'a', 'system', 'selected', 'from', 'the', 'group', 'consisting', 'of', 'a', 'surveillance', 'system', 'a', 'face', 'detection', 'system', 'a', 'face', 'system', 'a', 'cancer', 'detection', 'system', 'an', 'object', 'system', 'and', 'an', 'advanced', 'driver-assistance', 'system', 'a', 'computer', 'processing', 'system', 'for', '-based', 'self-guided', 'object', 'detection', 'comprising', 'a', 'memory', 'device', 'for', 'storing', 'program', 'code', 'and', 'a', 'device', 'for', 'running', 'the', 'program', 'code', 'to', 'receive', 'a', 'set', 'of', 's', 'each', 'of', 'the', 's', 'having', 'a', 'respective', 'grid', 'thereon', 'that', 'is', 'labeled', 'regarding', 'a', 'respective', 'object', 'to', 'be', 'detected', 'using', 'grid', 'level', 'label', 'data', 'train', 'a', 'grid-based', 'object', 'detector', 'using', 'the', 'grid', 'level', 'label', 'data', 'determine', 'a', 'respective', 'bounding', 'box', 'for', 'the', 'respective', 'object', 'in', 'each', 'of', 'the', 's', 'by', 'applying', 'local', 'segmentation', 'to', 'each', 'of', 'the', 's', 'and', 'train', 'a', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'for', 'joint', 'object', 'localization', 'and', 'object', 'classification', 'using', 'the', 'respective', 'bounding', 'box', 'for', 'the', 'respective', 'object', 'in', 'each', 'of', 'the', 's', 'as', 'an', 'input', 'to', 'the', 'rcnn', 'the', 'computer', 'processing', 'system', 'of', 'wherein', 'the', 'device', 'further', 'runs', 'the', 'program', 'code', 'to', 'perform', 'an', 'action', 'responsive', 'to', 'the', 'object', 'localization', 'and', 'object', 'classification', 'for', 'a', 'respective', 'new', 'object', 'in', 'a', 'new', 'to', 'which', 'the', 'rcnn', 'is', 'applied', 'the', 'computer', 'processing', 'system', 'of', 'wherein', 'the', 'action', 'comprises', 'autonomously', 'controlling', 'a', 'motor', 'vehicle', 'to', 'avoid', 'a', 'collision', 'with', 'the', 'new', 'object', 'responsive', 'to', 'the', 'object', 'localization', 'and', 'object', 'classification', 'for', 'the', 'respective', 'new', 'object', 'the', 'computer', 'processing', 'system', 'of', 'wherein', 'the', 'local', 'segmentation', 'is', 'performed', 'using', 'a', 'self-similarity', 'search', 'and', 'template', 'matching', 'to', 'provide', 'the', 'respective', 'bounding', 'box', 'around', 'the', 'respective', 'object', 'in', 'the', 'set', 'of', 's', 'the', 'computer', 'processing', 'system', 'of', 'wherein', 'the', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'forms', 'a', 'model', 'during', 'an', 'object', 'training', 'stage', 'that', 'is', 'to', 'detect', 'objects', 'in', 'new', 's', 'during', 'an', 'inference', 'stage', 'the', 'computer', 'processing', 'system', 'of', 'wherein', 'the', 'computer', 'processing', 'system', 'is', 'comprised', 'in', 'a', 'system', 'selected', 'from', 'the', 'group', 'consisting', 'of', 'a', 'surveillance', 'system', 'a', 'face', 'detection', 'system', 'a', 'face', 'system', 'a', 'cancer', 'detection', 'system', 'an', 'object', 'system', 'and', 'an', 'advanced', 'driver-assistance', 'system', 'a', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'utilizing', 'a', 'database', 'of', 'normalized', 'stored', 's', 'comprising', 'capturing', 'an', 'using', 'a', 'camera', 'detecting', 'a', 'face', 'in', 'the', 'captured', 'normalizing', 'the', 'detected', 'facial', 'to', 'match', 'the', 'normalized', 'stored', 's', 'identifying', 'facial', 'features', 'in', 'the', 'normalized', 'detected', 'facial', 'generating', 'a', 'of', 'facial', 'metrics', 'from', 'the', 'facial', 'features', 'calculating', 'euclidean', 'distances', 'between', 'the', 'facial', 'metrics', 'of', 'the', 'normalized', 'detected', 'facial', 'with', 'corresponding', 'facial', 'metrics', 'of', 'each', 'of', 'the', 'stored', 's', 'comparing', 'each', 'euclidean', 'distance', 'against', 'a', 'predetermined', 'threshold', 'responsive', 'to', 'the', 'euclidean', 'distance', 'comparison', 'producing', 'a', 'reduced', 'candidate', 'list', 'of', 'best', 'possible', 'matches', 'from', 'the', 'normalized', 'stored', 's', 'comparing', 'in', 'parallel', 'the', 'normalized', 'detected', 'facial', 'with', 'each', 'of', 'the', 'normalized', 'stored', 's', 'of', 'the', 'reduced', 'candidate', 'list', 'utilizing', 'a', 'of', 'face', 'algorithms', 'where', 'each', 'of', 'a', 'parallel', 'processing', 'system', 'uses', 'a', 'different', 'face', 'algorithm', 'responsive', 'to', 'the', 'comparison', 'producing', 'best', 'match', 'results', 'from', 'each', 'parallel', 'subset', 'of', 'the', 'reduced', 'candidate', 'list', 'and', 'selecting', 'a', 'final', 'match', 'from', 'the', 'best', 'match', 'results', 'using', 'a', 'deep', 'learning', 'neural', 'network', 'face', 'algorithm', 'trained', 'on', 'outputs', 'of', 'individual', 'face', 'algorithms', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'wherein', 'detecting', 'a', 'face', 'in', 'the', 'captured', 'comprises', 'utilizing', 'opencv', 'to', 'detect', 'a', 'face', 'in', 'the', 'captured', 'extracting', 'the', 'location', 'of', 'the', 'eyes', 'and', 'a', 'tip', 'of', 'the', 'nose', 'in', 'the', 'face', 'determining', 'a', 'distance', 'between', 'the', 'eyes', 'cropping', 'the', 'face', 'from', 'the', 'captured', 'where', 'the', 'width', 'and', 'the', 'height', 'of', 'a', 'cropped', 'face', 'is', 'a', 'function', 'of', 'the', 'distance', 'between', 'the', 'eyes', 'and', 'rotating', 'the', 'face', 'by', 'an', 'angle', 'of', 'rotation', 'that', 'is', 'a', 'function', 'of', 'the', 'distance', 'between', 'the', 'eyes', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'wherein', 'the', 'width', 'of', 'the', 'cropped', 'face', 'is', 'times', 'the', 'distance', 'between', 'the', 'eyes', 'the', 'height', 'of', 'the', 'cropped', 'face', 'is', 'times', 'the', 'distance', 'between', 'the', 'eyes', 'and', 'the', 'angle', 'of', 'rotation', 'is', 'an', 'angle', 'formed', 'by', 'a', 'straight', 'line', 'joining', 'the', 'eyes', 'and', 'an', 'x-axis', 'of', 'the', 'face', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'wherein', 'rotating', 'the', 'face', 'comprises', 'rotating', 'the', 'face', 'to', 'provide', 'a', 'frontal', 'face', 'pattern', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'further', 'comprising', 'the', 'step', 'of', 'proportionally', 'rescaling', 'the', 'cropped', 'and', 'rotated', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'where', 'the', 'proportional', 'rescaling', 'yields', 'a', 'cropped', 'and', 'rotated', 'with', 'a', 'size', 'of', '=', 'pixels', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'wherein', 'the', 'facial', 'features', 'identified', 'in', 'the', 'normalized', 'detected', 'facial', 'comprise', 'a', 'pair', 'of', 'eyes', 'a', 'tip', 'of', 'a', 'nose', 'a', 'mouth', 'a', 'center', 'of', 'the', 'mouth', 'and', 'a', 'chin', 'area', 'comprising', 'a', 'bottom', 'a', 'top', 'left', 'landmark', 'and', 'a', 'top', 'right', 'landmark', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'wherein', 'generating', 'a', 'of', 'facial', 'metrics', 'comprises', 'calculating', 'a', 'distance', 'between', 'the', 'pair', 'of', 'eyes', 'a', 'distance', 'between', 'the', 'eyes', 'and', 'the', 'tip', 'of', 'the', 'nose', 'a', 'distance', 'equal', 'to', 'the', 'width', 'of', 'the', 'mouth', 'a', 'distance', 'between', 'the', 'tip', 'of', 'the', 'nose', 'and', 'the', 'center', 'of', 'mouth', 'a', 'distance', 'between', 'the', 'bottom', 'of', 'chin', 'and', 'the', 'center', 'of', 'mouth', 'a', 'distance', 'between', 'the', 'top', 'left', 'landmark', 'on', 'the', 'chin', 'and', 'the', 'tip', 'of', 'the', 'nose', 'and', 'a', 'distance', 'between', 'the', 'top', 'right', 'landmark', 'on', 'the', 'chin', 'and', 'the', 'tip', 'of', 'the', 'nose', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'wherein', 'performing', 'a', 'euclidean', 'distance', 'match', 'further', 'comprises', 'partitioning', 'the', 'normalized', 'stored', 's', 'into', 'a', 'of', 'substantially', 'equal', 'subsets', 'performing', 'a', 'euclidean', 'distance', 'match', 'between', 'the', 'facial', 'metrics', 'of', 'the', 'normalized', 'detected', 'facial', 'and', 'corresponding', 'facial', 'metrics', 'of', 'each', 'of', 'the', 'stored', 's', 'of', 'the', 'subsets', 'of', 'the', 'normalized', 'stored', 's', 'with', 'a', 'separate', 'of', 'a', 'parallel', 'processing', 'system', 'to', 'generate', 'a', 'euclidean', 'distance', 'for', 'each', 'stored', 'of', 'the', 'subset', 'comparing', 'each', 'euclidean', 'distance', 'against', 'a', 'predetermined', 'threshold', 'with', 'the', 'separate', 's', 'responsive', 'to', 'the', 'euclidean', 'distance', 'comparison', 'producing', 'a', 'reduced', 'candidate', 'list', 'of', 'best', 'possible', 'matches', 'from', 'the', 'normalized', 'stored', 's', 'of', 'each', 'subset', 'and', 'combining', 'the', 'reduced', 'candidate', 'lists', 'from', 'each', 'subset', 'to', 'produce', 'a', 'single', 'reduced', 'candidate', 'list', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'wherein', 'the', 'of', 'face', 'algorithms', 'utilized', 'in', 'comparing', 'in', 'parallel', 'the', 'normalized', 'detected', 'facial', 'with', 'each', 'of', 'the', 'normalized', 'stored', 's', 'of', 'the', 'reduced', 'candidate', 'list', 'consists', 'of', 'face', 'algorithms', 'selected', 'from', 'a', 'group', 'consisting', 'of', 'principle', 'component', 'analysis', 'pca-based', 'algorithms', 'linear', 'discriminant', 'analysis', 'lda', 'algorithms', 'independent', 'component', 'analysis', 'ica', 'algorithms', 'kernel-based', 'algorithms', 'feature-based', 'techniques', 'algorithms', 'based', 'on', 'neural', 'networks', 'algorithms', 'based', 'on', 'transforms', 'and', 'model-based', 'face', 'algorithms', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'wherein', 'the', 'pca-based', 'algorithms', 'include', 'eigen', 'for', 'face', 'detection', 'and', 'the', 'lda', 'algorithms', 'include', 'the', 'fisher', 'of', 'face', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'wherein', 'comparing', 'in', 'parallel', 'the', 'captured', 'with', 'each', 'of', 'the', 'normalized', 'stored', 's', 'of', 'the', 'reduced', 'candidate', 'list', 'further', 'comprises', 'partitioning', 'the', 'reduced', 'candidate', 'list', 'into', 'a', 'of', 'substantially', 'equal', 'subsets', 'processing', 'each', 'subset', 'in', 'a', 'different', 'of', 'the', 'parallel', 'processing', 'system', 'uses', 'a', 'unique', 'face', 'algorithm', 'to', 'produce', 'the', 'best', 'match', 'results', 'and', 'using', 'a', 'reduce', 'function', 'of', 'a', 'mapreduce', 'program', 'to', 'combine', 'the', 'best', 'match', 'results', 'from', 'each', 'of', 'the', 'subsets', 'to', 'produce', 'a', 'single', 'set', 'of', 'the', 'best', 'match', 'results', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'wherein', 'partitioning', 'the', 'reduced', 'candidate', 'list', 'comprises', 'selecting', 'the', 's', 'comprising', 'each', 'subset', 'by', 'optimizing', 'the', 'variance', 'between', 'of', 'each', 'of', 'the', 's', 'according', 'to', 'the', 'following', 'equation', 'where', 'm', 'and', 'n', 'are', 'the', 'number', 'of', 'rows', 'and', 'columns', 'of', 'the', 'face', 'vector', 'n', 'is', 'the', 'number', 'of', 'groups', 'and', 'σij', 'is', 'the', 'standard', 'deviation', 'of', 'dimension', 'i', 'in', 'the', 'group', 'j', 'of', 'the', 'face', 'vector', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'wherein', 'selecting', 'the', 's', 'comprising', 'each', 'subset', 'by', 'optimizing', 'the', 'variance', 'between', 'each', 'of', 'the', 's', 'according', 'to', 'the', 'following', 'equation', 'dμi', 'μj', 'is', 'the', 'euclidean', 'distance', 'between', 'the', 'mean', 'of', 'the', 'group', 'i', 'and', 'the', 'mean', 'of', 'group', 'j', 'i', 'is', 'the', 'face', 'vector', 'and', 'l', 'is', 'the', 'number', 'of', 'group', 'levels', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'where', 'selecting', 'a', 'final', 'match', 'from', 'the', 'best', 'match', 'results', 'utilizing', 'a', 'deep', 'learning', 'neural', 'network', 'face', 'algorithm', 'comprises', 'utilizing', 'either', 'an', 'adaboost', 'machine-learning', 'algorithm', 'or', 'a', 'neural', 'networks', 'machine-learning', 'model', 'the', 'of', 'scalable', 'parallel', 'cloud-based', 'face', 'of', 'where', 'normalizing', 'the', 'detected', 'facial', 'to', 'match', 'the', 'normalized', 'stored', 's', 'includes', 'normalizing', 'the', 'detected', 'facial', 'to', 'the', 'same', 'size', 'and', 'illumination', 'of', 'the', 'normalized', 'stored', 's', 'a', 'non-transitory', 'computer-readable', 'medium', 'containing', 'executable', 'program', 'instructions', 'for', 'causing', 'a', 'computer', 'to', 'perform', 'a', 'of', 'face', 'the', 'comprising', 'detecting', 'a', 'face', 'in', 'an', 'captured', 'by', 'a', 'camera', 'normalizing', 'the', 'detected', 'facial', 'to', 'match', 'the', 'normalized', 'stored', 's', 'identifying', 'facial', 'features', 'in', 'the', 'normalized', 'detected', 'facial', 'generating', 'a', 'of', 'facial', 'metrics', 'from', 'the', 'facial', 'features', 'calculating', 'euclidean', 'distances', 'between', 'the', 'facial', 'metrics', 'of', 'the', 'normalized', 'detected', 'facial', 'with', 'corresponding', 'facial', 'metrics', 'of', 'each', 'of', 'the', 'stored', 's', 'comparing', 'each', 'euclidean', 'distance', 'against', 'a', 'predetermined', 'threshold', 'responsive', 'to', 'the', 'euclidean', 'distance', 'comparison', 'producing', 'a', 'reduced', 'candidate', 'list', 'of', 'best', 'possible', 'matches', 'from', 'the', 'normalized', 'stored', 's', 'comparing', 'in', 'parallel', 'the', 'captured', 'with', 'each', 'of', 'the', 'normalized', 'stored', 's', 'of', 'the', 'reduced', 'candidate', 'list', 'utilizing', 'a', 'of', 'face', 'algorithms', 'where', 'each', 'of', 'a', 'parallel', 'processing', 'system', 'uses', 'a', 'different', 'face', 'algorithm', 'responsive', 'to', 'the', 'comparison', 'producing', 'best', 'match', 'results', 'from', 'each', 'parallel', 'subset', 'of', 'the', 'reduced', 'candidate', 'list', 'and', 'selecting', 'a', 'final', 'match', 'from', 'the', 'best', 'match', 'results', 'using', 'a', 'deep', 'learning', 'neural', 'network', 'face', 'algorithm', 'trained', 'on', 'outputs', 'of', 'individual', 'face', 'algorithms', 'the', 'non-transitory', 'computer-readable', 'medium', 'containing', 'executable', 'program', 'instructions', 'of', 'wherein', 'the', 'of', 'face', 'algorithms', 'utilized', 'in', 'comparing', 'in', 'parallel', 'the', 'normalized', 'detected', 'facial', 'with', 'each', 'of', 'the', 'normalized', 'stored', 's', 'of', 'the', 'reduced', 'candidate', 'list', 'consists', 'of', 'face', 'algorithms', 'selected', 'from', 'a', 'group', 'consisting', 'of', 'principle', 'component', 'analysis', 'pca-based', 'algorithms', 'linear', 'discriminant', 'analysis', 'lda', 'algorithms', 'independent', 'component', 'analysis', 'ica', 'algorithms', 'kernel-based', 'algorithms', 'feature-based', 'techniques', 'algorithms', 'based', 'on', 'neural', 'networks', 'algorithms', 'based', 'on', 'transforms', 'and', 'model-based', 'face', 'algorithms', 'the', 'non-transitory', 'computer-readable', 'medium', 'containing', 'executable', 'program', 'instructions', 'of', 'wherein', 'the', 'pca-based', 'algorithms', 'include', 'eigen', 'for', 'face', 'detection', 'and', 'the', 'lda', 'algorithms', 'include', 'the', 'fisher', 'of', 'face', 'the', 'non-transitory', 'computer-readable', 'medium', 'containing', 'executable', 'program', 'instructions', 'of', 'where', 'selecting', 'a', 'final', 'match', 'from', 'the', 'best', 'match', 'results', 'utilizing', 'a', 'deep', 'learning', 'neural', 'network', 'face', 'algorithm', 'comprises', 'utilizing', 'either', 'an', 'adaboost', 'machine-learning', 'algorithm', 'or', 'a', 'neural', 'networks', 'machine-learning', 'model', 'an', 'imaging', 'device', 'comprising', 'a', 'condensing', 'lens', 'an', 'sensor', 'configured', 'to', 'detect', 'light', 'passing', 'through', 'the', 'condensing', 'lens', 'and', 'comprising', 'a', 'pixel', 'matrix', 'wherein', 'the', 'pixel', 'matrix', 'comprises', 'a', 'of', 'phase', 'detection', 'pixel', 'pairs', 'and', 'a', 'of', 'regular', 'pixels', 'and', 'a', 'configured', 'to', 'turn', 'on', 'the', 'phase', 'detection', 'pixel', 'pairs', 'for', 'autofocusing', 'and', 'output', 'autofocused', 'pixel', 'data', 'after', 'completing', 'the', 'autofocusing', 'divide', 'the', 'autofocused', 'pixel', 'data', 'into', 'a', 'first', 'subframe', 'and', 'a', 'second', 'subframe', 'calculate', 'features', 'of', 'at', 'least', 'one', 'of', 'the', 'first', 'subframe', 'and', 'the', 'second', 'subframe', 'wherein', 'the', 'features', 'comprise', 'module', 'widths', 'of', 'a', 'finder', 'pattern', 'and', 'the', 'finder', 'pattern', 'has', 'a', 'predetermined', 'ratio', 'a', 'harr-like', 'feature', 'or', 'a', 'gabor', 'feature', 'and', 'determine', 'an', 'operating', 'resolution', 'of', 'the', 'regular', 'pixels', 'according', 'to', 'the', 'features', 'calculated', 'from', 'at', 'least', 'one', 'of', 'the', 'first', 'subframe', 'and', 'the', 'second', 'subframe', 'divided', 'from', 'the', 'autofocused', 'pixel', 'data', 'the', 'imaging', 'device', 'as', 'ed', 'in', 'wherein', 'each', 'of', 'the', 'phase', 'detection', 'pixel', 'pairs', 'comprises', 'a', 'first', 'pixel', 'and', 'a', 'second', 'pixel', 'a', 'cover', 'layer', 'covering', 'upon', 'a', 'first', 'region', 'of', 'the', 'first', 'pixel', 'and', 'upon', 'a', 'second', 'region', 'of', 'the', 'second', 'pixel', 'wherein', 'the', 'first', 'region', 'and', 'the', 'second', 'region', 'are', 'mirror', 'symmetrical', 'to', 'each', 'other', 'and', 'a', 'microlens', 'aligned', 'with', 'at', 'least', 'one', 'of', 'the', 'first', 'pixel', 'and', 'the', 'second', 'pixel', 'the', 'imaging', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'first', 'region', 'and', 'the', 'second', 'region', 'are', '%', 'to', '%', 'of', 'an', 'area', 'of', 'a', 'single', 'pixel', 'the', 'imaging', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'is', 'configured', 'to', 'perform', 'the', 'autofocusing', 'using', 'a', 'dual', 'pixel', 'autofocus', 'technique', 'according', 'to', 'pixel', 'data', 'of', 'the', 'phase', 'detection', 'pixel', 'pairs', 'before', 'completing', 'the', 'autofocusing', 'the', 'imaging', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'is', 'configured', 'to', 'divide', 'pixel', 'data', 'of', 'the', 'phase', 'detection', 'pixel', 'pairs', 'into', 'a', 'third', 'subframe', 'and', 'a', 'fourth', 'subframe', 'before', 'completing', 'the', 'autofocusing', 'and', 'perform', 'the', 'autofocusing', 'according', 'to', 'the', 'third', 'subframe', 'and', 'the', 'fourth', 'subframe', 'the', 'imaging', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'is', 'further', 'configured', 'to', 'calibrate', 'brightness', 'of', 'the', 'third', 'subframe', 'and', 'the', 'fourth', 'subframe', 'to', 'be', 'identical', 'using', 'a', 'shading', 'algorithm', 'the', 'imaging', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'operating', 'resolution', 'is', 'selected', 'as', 'a', 'first', 'resolution', 'smaller', 'than', 'a', 'number', 'of', 'the', 'regular', 'pixels', 'or', 'as', 'a', 'second', 'resolution', 'larger', 'than', 'the', 'first', 'resolution', 'the', 'imaging', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'regular', 'pixels', 'are', 'turned', 'off', 'in', 'the', 'autofocusing', 'the', 'imaging', 'device', 'as', 'ed', 'in', 'wherein', 'a', 'number', 'of', 'the', 'phase', 'detection', 'pixel', 'pairs', 'is', 'smaller', 'than', 'that', 'of', 'the', 'regular', 'pixels', 'an', 'imaging', 'device', 'comprising', 'a', 'condensing', 'lens', 'an', 'sensor', 'configured', 'to', 'detect', 'light', 'passing', 'through', 'the', 'condensing', 'lens', 'and', 'comprising', 'a', 'pixel', 'matrix', 'wherein', 'the', 'pixel', 'matrix', 'comprises', 'a', 'of', 'phase', 'detection', 'pixel', 'pairs', 'and', 'a', 'of', 'regular', 'pixels', 'and', 'a', 'configured', 'to', 'turn', 'on', 'the', 'phase', 'detection', 'pixel', 'pairs', 'for', 'autofocusing', 'and', 'output', 'autofocused', 'pixel', 'data', 'after', 'completing', 'the', 'autofocusing', 'divide', 'the', 'autofocused', 'pixel', 'data', 'into', 'a', 'first', 'subframe', 'and', 'a', 'second', 'subframe', 'calculate', 'features', 'of', 'at', 'least', 'one', 'of', 'the', 'first', 'subframe', 'and', 'the', 'second', 'subframe', 'wherein', 'the', 'features', 'comprise', 'module', 'widths', 'of', 'a', 'finder', 'pattern', 'and', 'the', 'finder', 'pattern', 'has', 'a', 'predetermined', 'ratio', 'a', 'harr-like', 'feature', 'or', 'a', 'gabor', 'feature', 'and', 'select', 'an', 'decoding', 'or', 'an', 'using', 'pixel', 'data', 'of', 'the', 'regular', 'pixels', 'according', 'to', 'the', 'features', 'calculated', 'from', 'at', 'least', 'one', 'of', 'the', 'first', 'subframe', 'and', 'the', 'second', 'subframe', 'divided', 'from', 'the', 'autofocused', 'pixel', 'data', 'the', 'imaging', 'device', 'as', 'ed', 'in', 'wherein', 'each', 'of', 'the', 'phase', 'detection', 'pixel', 'pairs', 'comprises', 'a', 'first', 'pixel', 'and', 'a', 'second', 'pixel', 'a', 'cover', 'layer', 'covering', 'upon', 'a', 'first', 'region', 'of', 'the', 'first', 'pixel', 'and', 'upon', 'a', 'second', 'region', 'of', 'the', 'second', 'pixel', 'wherein', 'the', 'first', 'region', 'and', 'the', 'second', 'region', 'are', 'mirror', 'symmetrical', 'to', 'each', 'other', 'and', 'a', 'microlens', 'aligned', 'with', 'at', 'least', 'one', 'of', 'the', 'first', 'pixel', 'and', 'the', 'second', 'pixel', 'the', 'imaging', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'is', 'configured', 'to', 'perform', 'the', 'autofocusing', 'using', 'a', 'dual', 'pixel', 'autofocus', 'technique', 'according', 'to', 'pixel', 'data', 'of', 'the', 'phase', 'detection', 'pixel', 'pairs', 'before', 'completing', 'the', 'autofocusing', 'the', 'imaging', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'is', 'configured', 'to', 'divide', 'the', 'pixel', 'data', 'of', 'the', 'phase', 'detection', 'pixel', 'pairs', 'into', 'a', 'third', 'subframe', 'and', 'a', 'fourth', 'subframe', 'before', 'completing', 'the', 'autofocusing', 'calibrate', 'brightness', 'of', 'the', 'third', 'subframe', 'and', 'the', 'fourth', 'subframe', 'to', 'be', 'identical', 'using', 'a', 'shading', 'algorithm', 'and', 'perform', 'the', 'autofocusing', 'according', 'to', 'the', 'third', 'subframe', 'and', 'the', 'fourth', 'subframe', 'the', 'imaging', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'is', 'configured', 'to', 'calculate', 'the', 'features', 'using', 'at', 'least', 'one', 'of', 'a', 'rule', 'based', 'algorithm', 'and', 'a', 'machine', 'learning', 'algorithm', 'the', 'imaging', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'decoding', 'is', 'decoding', 'qr', 'codes', 'and', 'the', 'is', 'face', 'an', 'operating', 'of', 'an', 'imaging', 'device', 'the', 'imaging', 'device', 'comprising', 'a', 'of', 'phase', 'detection', 'pixel', 'pairs', 'and', 'a', 'of', 'regular', 'pixels', 'the', 'operating', 'comprising', 'turning', 'on', 'the', 'phase', 'detection', 'pixel', 'pairs', 'for', 'autofocusing', 'and', 'outputting', 'autofocused', 'frame', 'after', 'completing', 'the', 'autofocusing', 'dividing', 'the', 'autofocused', 'frame', 'acquired', 'by', 'the', 'phase', 'detection', 'pixel', 'pairs', 'into', 'a', 'first', 'subframe', 'and', 'a', 'second', 'subframe', 'calculating', 'features', 'of', 'at', 'least', 'one', 'of', 'the', 'first', 'subframe', 'and', 'the', 'second', 'subframe', 'wherein', 'the', 'feature', 'comprise', 'module', 'widths', 'of', 'a', 'finder', 'pattern', 'and', 'the', 'finder', 'pattern', 'has', 'a', 'predetermined', 'ratio', 'a', 'harr-like', 'feature', 'or', 'a', 'gabor', 'feature', 'and', 'selectively', 'activating', 'at', 'least', 'a', 'part', 'of', 'the', 'regular', 'pixels', 'according', 'to', 'the', 'features', 'calculated', 'from', 'at', 'least', 'one', 'of', 'the', 'first', 'subframe', 'and', 'the', 'second', 'subframe', 'divided', 'from', 'the', 'autofocused', 'frame', 'the', 'operating', 'as', 'ed', 'in', 'wherein', 'the', 'selectively', 'activating', 'comprises', 'activating', 'a', 'first', 'part', 'of', 'the', 'regular', 'pixels', 'to', 'perform', 'an', 'decoding', 'according', 'to', 'pixel', 'data', 'of', 'the', 'first', 'part', 'of', 'the', 'regular', 'pixels', 'or', 'activating', 'all', 'the', 'regular', 'pixels', 'to', 'perform', 'an', 'according', 'to', 'pixel', 'data', 'of', 'the', 'all', 'regular', 'pixels', 'the', 'operating', 'as', 'ed', 'in', 'wherein', 'pixel', 'data', 'of', 'the', 'phase', 'detection', 'pixel', 'pairs', 'captured', 'in', 'a', 'same', 'frame', 'with', 'the', 'pixel', 'data', 'of', 'the', 'regular', 'pixels', 'is', 'also', 'used', 'in', 'performing', 'the', 'decoding', 'and', 'the', 'the', 'operating', 'as', 'ed', 'in', 'wherein', 'the', 'decoding', 'is', 'decoding', 'qr', 'codes', 'and', 'the', 'is', 'face', 'the', 'operating', 'as', 'ed', 'in', 'wherein', 'the', 'phase', 'detection', 'pixel', 'pairs', 'are', 'partially', 'covered', 'pixels', 'or', 'have', 'a', 'structure', 'of', 'dual', 'pixel', 'an', 'apparatus', 'comprising', 'a', 'first', 'camera', 'module', 'configured', 'to', 'obtain', 'a', 'first', 'of', 'an', 'object', 'with', 'a', 'first', 'field', 'of', 'view', 'a', 'second', 'camera', 'module', 'configured', 'to', 'obtain', 'a', 'second', 'of', 'the', 'object', 'with', 'a', 'second', 'field', 'of', 'view', 'different', 'from', 'the', 'first', 'field', 'of', 'view', 'a', 'first', 'depth', 'map', 'generator', 'configured', 'to', 'generate', 'a', 'first', 'depth', 'map', 'of', 'the', 'first', 'based', 'on', 'the', 'first', 'and', 'the', 'second', 'and', 'a', 'second', 'depth', 'map', 'generator', 'configured', 'to', 'generate', 'a', 'second', 'depth', 'map', 'of', 'the', 'second', 'based', 'on', 'the', 'first', 'the', 'second', 'and', 'the', 'first', 'depth', 'map', 'the', 'apparatus', 'of', 'wherein', 'the', 'first', 'field', 'of', 'view', 'is', 'a', 'narrow', 'angle', 'and', 'the', 'second', 'field', 'of', 'view', 'is', 'a', 'wider', 'angle', 'the', 'apparatus', 'of', 'wherein', 'the', 'second', 'is', 'divided', 'into', 'a', 'primary', 'region', 'and', 'a', 'residual', 'region', 'and', 'the', 'second', 'depth', 'map', 'generator', 'comprises', 'a', 'relationship', 'estimating', 'module', 'configured', 'to', 'estimate', 'a', 'relationship', 'between', 'the', 'primary', 'region', 'and', 'the', 'residual', 'region', 'based', 'on', 'the', 'first', 'and', 'the', 'second', 'and', 'a', 'depth', 'map', 'estimating', 'module', 'configured', 'to', 'estimate', 'a', 'depth', 'map', 'of', 'the', 'residual', 'region', 'based', 'on', 'the', 'estimated', 'relationship', 'and', 'the', 'first', 'depth', 'map', 'the', 'apparatus', 'of', 'wherein', 'at', 'least', 'one', 'of', 'the', 'relationship', 'estimating', 'module', 'and', 'the', 'depth', 'map', 'estimating', 'module', 'performs', 'an', 'estimating', 'operation', 'based', 'on', 'a', 'neural', 'network', 'module', 'the', 'apparatus', 'of', 'further', 'comprising', 'a', 'depth', 'map', 'fusion', 'unit', 'configured', 'to', 'generate', 'a', 'third', 'depth', 'map', 'of', 'the', 'second', 'by', 'performing', 'a', 'fusion', 'operation', 'based', 'on', 'the', 'first', 'depth', 'map', 'and', 'the', 'second', 'depth', 'map', 'the', 'apparatus', 'of', 'wherein', 'the', 'depth', 'map', 'fusion', 'unit', 'comprises', 'a', 'tone', 'mapping', 'module', 'configured', 'to', 'generate', 'a', 'tone-mapped', 'second', 'depth', 'map', 'to', 'correspond', 'to', 'the', 'first', 'depth', 'map', 'by', 'performing', 'a', 'bias', 'removing', 'operation', 'on', 'the', 'second', 'depth', 'map', 'and', 'a', 'fusion', 'module', 'configured', 'to', 'generate', 'the', 'third', 'depth', 'map', 'by', 'fusing', 'the', 'tone-mapped', 'second', 'depth', 'map', 'and', 'the', 'first', 'depth', 'map', 'the', 'apparatus', 'of', 'wherein', 'the', 'depth', 'map', 'fusion', 'unit', 'further', 'comprises', 'a', 'propagating', 'module', 'configured', 'to', 'generate', 'a', 'propagated', 'first', 'depth', 'map', 'in', 'the', 'second', 'by', 'iterated', 'propagating', 'of', 'the', 'first', 'depth', 'map', 'based', 'on', 'the', 'first', 'depth', 'map', 'and', 'the', 'second', 'and', 'the', 'fusion', 'module', 'generates', 'the', 'third', 'depth', 'map', 'by', 'fusing', 'the', 'tone-mapped', 'second', 'depth', 'map', 'and', 'the', 'propagated', 'first', 'depth', 'map', 'the', 'apparatus', 'of', 'wherein', 'the', 'depth', 'map', 'fusion', 'unit', 'further', 'comprises', 'a', 'post-processing', 'module', 'configured', 'to', 'perform', 'a', 'post-processing', 'operation', 'on', 'the', 'third', 'depth', 'map', 'generated', 'by', 'the', 'fusion', 'module', 'to', 'provide', 'the', 'post-processed', 'third', 'depth', 'map', 'the', 'apparatus', 'of', 'wherein', 'the', 'post-processing', 'module', 'performs', 'the', 'post-processing', 'operation', 'by', 'filtering', 'an', 'interface', 'generated', 'in', 'the', 'third', 'depth', 'map', 'in', 'accordance', 'with', 'fusion', 'of', 'the', 'fusion', 'module', 'the', 'apparatus', 'of', 'wherein', 'the', 'post-processing', 'module', 'removes', 'artifacts', 'generated', 'in', 'the', 'third', 'depth', 'map', 'in', 'accordance', 'with', 'fusion', 'of', 'the', 'fusion', 'module', 'the', 'apparatus', 'of', 'wherein', 'the', 'first', 'depth', 'map', 'generator', 'analyses', 'a', 'distance', 'relationship', 'between', 'the', 'first', 'and', 'the', 'second', 'and', 'generates', 'a', 'first', 'depth', 'map', 'of', 'the', 'first', 'based', 'on', 'the', 'distance', 'relationship', 'a', 'of', 'processing', 'an', 'of', 'an', 'electronic', 'apparatus', 'the', 'comprising', 'obtaining', 'a', 'first', 'of', 'an', 'object', 'using', 'a', 'first', 'camera', 'module', 'obtaining', 'a', 'second', 'of', 'the', 'object', 'using', 'a', 'second', 'camera', 'module', 'generating', 'a', 'first', 'depth', 'map', 'of', 'the', 'first', 'based', 'on', 'the', 'first', 'and', 'the', 'second', 'estimating', 'a', 'relationship', 'between', 'a', 'primary', 'region', 'of', 'the', 'second', 'and', 'a', 'residual', 'region', 'of', 'the', 'second', 'based', 'on', 'the', 'first', 'and', 'the', 'second', 'and', 'generating', 'a', 'second', 'depth', 'map', 'of', 'the', 'second', 'based', 'on', 'the', 'estimated', 'relationship', 'between', 'the', 'primary', 'region', 'and', 'the', 'residual', 'region', 'and', 'the', 'first', 'depth', 'map', 'the', 'of', 'wherein', 'the', 'electronic', 'apparatus', 'comprises', 'a', 'first', 'camera', 'module', 'including', 'a', 'first', 'lens', 'having', 'a', 'first', 'field', 'of', 'view', 'and', 'a', 'second', 'camera', 'module', 'including', 'a', 'second', 'lens', 'having', 'a', 'second', 'field', 'of', 'view', 'wider', 'than', 'the', 'first', 'field', 'of', 'view', 'the', 'of', 'wherein', 'the', 'generating', 'of', 'the', 'second', 'depth', 'map', 'comprises', 'estimating', 'a', 'depth', 'map', 'of', 'the', 'residual', 'region', 'based', 'on', 'the', 'estimated', 'relationship', 'between', 'the', 'primary', 'region', 'and', 'the', 'residual', 'region', 'and', 'the', 'first', 'depth', 'map', 'and', 'generating', 'the', 'second', 'depth', 'map', 'based', 'on', 'a', 'depth', 'map', 'of', 'the', 'residual', 'region', 'and', 'the', 'first', 'depth', 'map', 'the', 'of', 'wherein', 'the', 'estimating', 'of', 'the', 'relationship', 'between', 'a', 'primary', 'region', 'of', 'the', 'second', 'is', 'performed', 'using', 'a', 'neural', 'network', 'model', 'the', 'of', 'further', 'comprising', 'performing', 'a', 'pre-processing', 'operation', 'on', 'the', 'second', 'depth', 'map', 'and', 'generating', 'a', 'third', 'depth', 'map', 'of', 'the', 'residual', 'by', 'fusing', 'the', 'second', 'depth', 'map', 'on', 'which', 'the', 'pre-processing', 'operation', 'is', 'performed', 'and', 'the', 'first', 'depth', 'map', 'the', 'of', 'wherein', 'the', 'performing', 'of', 'the', 'pre-processing', 'operation', 'comprises', 'performing', 'a', 'tone', 'mapping', 'operation', 'between', 'a', 'depth', 'map', 'of', 'the', 'primary', 'region', 'and', 'a', 'depth', 'map', 'of', 'the', 'residual', 'region', 'based', 'on', 'the', 'second', 'depth', 'map', 'an', 'operating', 'for', 'an', 'electronic', 'apparatus', 'the', 'electronic', 'apparatus', 'including', 'a', 'first', 'camera', 'module', 'providing', 'a', 'first', 'of', 'an', 'object', 'using', 'a', 'first', 'field', 'of', 'view', 'and', 'a', 'second', 'camera', 'module', 'providing', 'a', 'second', 'of', 'the', 'object', 'using', 'second', 'field', 'of', 'view', 'wider', 'than', 'the', 'first', 'field', 'of', 'view', 'and', 'a', 'generating', 'a', 'depth', 'map', 'of', 'the', 'second', 'based', 'on', 'a', 'primary', 'region', 'of', 'the', 'second', 'and', 'a', 'residual', 'region', 'of', 'the', 'second', 'the', 'operating', 'comprising', 'generating', 'a', 'first', 'depth', 'map', 'of', 'the', 'primary', 'region', 'by', 'estimating', 'a', 'relationship', 'between', 'the', 'first', 'and', 'the', 'second', 'estimating', 'a', 'relationship', 'between', 'the', 'primary', 'region', 'and', 'the', 'residual', 'region', 'based', 'on', 'the', 'first', 'and', 'the', 'second', 'generating', 'a', 'second', 'depth', 'map', 'of', 'the', 'second', 'by', 'estimating', 'a', 'depth', 'map', 'of', 'the', 'second', 'region', 'based', 'on', 'the', 'estimated', 'relationship', 'between', 'the', 'primary', 'region', 'and', 'the', 'residual', 'region', 'and', 'generating', 'a', 'depth', 'map', 'of', 'the', 'second', 'by', 'fusing', 'the', 'first', 'depth', 'map', 'and', 'the', 'second', 'depth', 'map', 'the', 'operation', 'of', 'further', 'comprising', 'executing', 'an', 'application', 'that', 'applies', 'an', 'effect', 'to', 'the', 'second', 'based', 'on', 'a', 'depth', 'map', 'of', 'the', 'residual', 'the', 'operation', 'of', 'wherein', 'the', 'application', 'applies', 'at', 'least', 'one', 'effect', 'of', 'auto-focusing', 'out-focusing', 'forebackground', 'separation', 'face', 'object', 'detection', 'within', 'a', 'frame', 'and', 'augmented', 'reality', 'to', 'the', 'second', 'based', 'on', 'a', 'depth', 'map', 'of', 'the', 'second', 'a', 'payment', 'based', 'on', 'a', 'face', 'comprising', 'acquiring', 'first', 'face', 'information', 'of', 'a', 'target', 'extracting', 'first', 'characteristic', 'information', 'from', 'the', 'first', 'face', 'information', 'wherein', 'the', 'first', 'characteristic', 'information', 'includes', 'head', 'posture', 'information', 'of', 'the', 'target', 'and', 'gaze', 'information', 'of', 'the', 'target', 'determining', 'whether', 'the', 'target', 'has', 'a', 'willingness', 'to', 'pay', 'according', 'to', 'the', 'head', 'posture', 'information', 'of', 'the', 'target', 'and', 'the', 'gaze', 'information', 'of', 'the', 'target', 'including', 'determining', 'whether', 'an', 'angle', 'of', 'rotation', 'in', 'each', 'preset', 'direction', 'is', 'less', 'than', 'an', 'angle', 'threshold', 'wherein', 'the', 'head', 'posture', 'information', 'includes', 'the', 'angle', 'of', 'rotation', 'in', 'each', 'preset', 'direction', 'determining', 'whether', 'a', 'probability', 'value', 'that', 'a', 'gazes', 'at', 'a', 'payment', 'screen', 'is', 'greater', 'than', 'a', 'probability', 'threshold', 'wherein', 'the', 'gaze', 'information', 'includes', 'the', 'probability', 'value', 'that', 'a', 'gazes', 'at', 'a', 'payment', 'screen', 'and', 'in', 'response', 'to', 'determining', 'that', 'the', 'angle', 'of', 'rotation', 'in', 'each', 'preset', 'direction', 'is', 'less', 'than', 'the', 'angle', 'threshold', 'and', 'that', 'the', 'probability', 'value', 'that', 'a', 'gazes', 'at', 'a', 'payment', 'screen', 'is', 'greater', 'than', 'the', 'probability', 'threshold', 'determining', 'that', 'the', 'target', 'has', 'a', 'willingness', 'to', 'pay', 'and', 'in', 'response', 'to', 'determining', 'that', 'the', 'target', 'has', 'a', 'willingness', 'to', 'pay', 'completing', 'a', 'payment', 'operation', 'based', 'on', 'the', 'face', 'the', 'as', 'ed', 'in', 'wherein', 'the', 'completing', 'a', 'payment', 'operation', 'based', 'on', 'the', 'face', 'comprises', 'triggering', 'and', 'performing', 'a', 'payment', 'initiating', 'operation', 'to', 'acquire', 'second', 'face', 'information', 'based', 'on', 'the', 'face', 'determining', 'whether', 'second', 'characteristic', 'information', 'extracted', 'from', 'the', 'second', 'face', 'information', 'indicates', 'that', 'the', 'has', 'a', 'willingness', 'to', 'pay', 'and', 'in', 'response', 'to', 'determining', 'that', 'the', 'second', 'characteristic', 'information', 'indicates', 'that', 'the', 'has', 'a', 'willingness', 'to', 'pay', 'triggering', 'and', 'performing', 'a', 'payment', 'confirmation', 'operation', 'to', 'complete', 'the', 'payment', 'operation', 'based', 'on', 'payment', 'account', 'information', 'corresponding', 'to', 'the', 'target', 'the', 'as', 'ed', 'in', 'wherein', 'the', 'determining', 'whether', 'second', 'characteristic', 'information', 'extracted', 'from', 'the', 'second', 'face', 'information', 'indicates', 'that', 'the', 'has', 'a', 'willingness', 'to', 'pay', 'comprises', 'determining', 'whether', 'a', 'current', 'corresponding', 'to', 'the', 'second', 'face', 'information', 'is', 'consistent', 'with', 'the', 'target', 'and', 'in', 'response', 'to', 'determining', 'that', 'the', 'current', 'is', 'consistent', 'with', 'the', 'target', 'determining', 'whether', 'the', 'target', 'has', 'a', 'willingness', 'to', 'pay', 'according', 'to', 'the', 'second', 'characteristic', 'information', 'extracted', 'from', 'the', 'second', 'face', 'information', 'the', 'as', 'ed', 'in', 'wherein', 'the', 'extracting', 'first', 'characteristic', 'information', 'from', 'the', 'first', 'face', 'information', 'comprises', 'determining', 'the', 'head', 'posture', 'information', 'of', 'the', 'target', 'using', 'a', 'head', 'posture', 'model', 'based', 'on', 'the', 'first', 'face', 'information', 'and', 'determining', 'the', 'gaze', 'information', 'of', 'the', 'target', 'using', 'a', 'gaze', 'information', 'model', 'based', 'on', 'characteristics', 'of', 'an', 'eye', 'region', 'in', 'the', 'first', 'face', 'information', 'the', 'as', 'ed', 'in', 'wherein', 'the', 'head', 'posture', 'model', 'is', 'obtained', 'through', 'training', 'by', 'acquiring', 'a', 'first', 'sample', 'data', 'set', 'wherein', 'the', 'first', 'sample', 'data', 'set', 'includes', 'a', 'of', 'pieces', 'of', 'first', 'sample', 'data', 'and', 'each', 'of', 'the', 'of', 'pieces', 'of', 'first', 'sample', 'data', 'includes', 'a', 'correspondence', 'between', 'a', 'sample', 'face', 'and', 'head', 'posture', 'information', 'determining', 'mean', 'data', 'and', 'variance', 'data', 'of', 'a', 'of', 'sample', 'face', 's', 'for', 'each', 'of', 'the', 'of', 'pieces', 'of', 'first', 'sample', 'data', 'preprocessing', 'the', 'sample', 'face', 'contained', 'in', 'each', 'of', 'the', 'of', 'pieces', 'of', 'first', 'sample', 'data', 'based', 'on', 'the', 'mean', 'data', 'and', 'the', 'variance', 'data', 'to', 'obtain', 'a', 'preprocessed', 'sample', 'face', 'setting', 'the', 'preprocessed', 'sample', 'face', 'and', 'the', 'corresponding', 'head', 'posture', 'information', 'as', 'a', 'first', 'model', 'training', 'sample', 'and', 'performing', 'training', 'using', 'a', 'machine', 'learning', 'and', 'based', 'on', 'a', 'of', 'first', 'model', 'training', 'samples', 'to', 'obtain', 'the', 'head', 'posture', 'model', 'the', 'as', 'ed', 'in', 'wherein', 'the', 'gaze', 'information', 'model', 'is', 'obtained', 'through', 'training', 'by', 'acquiring', 'a', 'second', 'sample', 'data', 'set', 'wherein', 'the', 'second', 'sample', 'data', 'set', 'includes', 'a', 'of', 'pieces', 'of', 'second', 'sample', 'data', 'and', 'each', 'of', 'the', 'of', 'pieces', 'of', 'second', 'sample', 'data', 'includes', 'a', 'correspondence', 'between', 'a', 'sample', 'eye', 'and', 'gaze', 'information', 'determining', 'mean', 'data', 'and', 'variance', 'data', 'of', 'a', 'of', 'sample', 'eye', 's', 'for', 'each', 'of', 'the', 'of', 'pieces', 'of', 'second', 'sample', 'data', 'preprocessing', 'the', 'sample', 'eye', 'contained', 'in', 'each', 'of', 'the', 'of', 'pieces', 'of', 'second', 'sample', 'data', 'based', 'on', 'the', 'mean', 'data', 'and', 'the', 'variance', 'data', 'to', 'obtain', 'a', 'preprocessed', 'sample', 'eye', 'setting', 'the', 'preprocessed', 'sample', 'eye', 'and', 'the', 'corresponding', 'gaze', 'information', 'as', 'a', 'second', 'model', 'training', 'sample', 'and', 'performing', 'training', 'using', 'a', 'machine', 'learning', 'and', 'based', 'on', 'a', 'of', 'second', 'model', 'training', 'samples', 'to', 'obtain', 'the', 'gaze', 'information', 'model', 'the', 'as', 'ed', 'in', 'wherein', 'the', 'angle', 'of', 'rotation', 'in', 'each', 'preset', 'direction', 'comprises', 'a', 'pitch', 'angle', 'a', 'yaw', 'angle', 'and', 'a', 'roll', 'angle', 'wherein', 'the', 'pitch', 'angle', 'refers', 'to', 'an', 'angle', 'of', 'rotation', 'around', 'a', 'x-axis', 'the', 'yaw', 'angle', 'refers', 'to', 'an', 'angle', 'of', 'rotation', 'around', 'a', 'y-axis', 'and', 'the', 'roll', 'angle', 'refers', 'to', 'an', 'angle', 'of', 'rotation', 'around', 'a', 'z-axis', 'a', 'payment', 'device', 'based', 'on', 'a', 'face', 'comprising', 'a', 'and', 'a', 'non-transitory', 'computer-readable', 'storage', 'medium', 'storing', 'instructions', 'executable', 'by', 'the', 'to', 'cause', 'the', 'device', 'to', 'perform', 'operations', 'comprising', 'acquiring', 'first', 'face', 'information', 'of', 'a', 'target', 'extracting', 'first', 'characteristic', 'information', 'from', 'the', 'first', 'face', 'information', 'wherein', 'the', 'first', 'characteristic', 'information', 'includes', 'head', 'posture', 'information', 'of', 'the', 'target', 'and', 'gaze', 'information', 'of', 'the', 'target', 'determining', 'whether', 'the', 'target', 'has', 'a', 'willingness', 'to', 'pay', 'according', 'to', 'the', 'head', 'posture', 'information', 'of', 'the', 'target', 'and', 'the', 'gaze', 'information', 'of', 'the', 'target', 'including', 'determining', 'whether', 'an', 'angle', 'of', 'rotation', 'in', 'each', 'preset', 'direction', 'is', 'less', 'than', 'an', 'angle', 'threshold', 'wherein', 'the', 'head', 'posture', 'information', 'includes', 'the', 'angle', 'of', 'rotation', 'in', 'each', 'preset', 'direction', 'determining', 'whether', 'a', 'probability', 'value', 'that', 'a', 'gazes', 'at', 'a', 'payment', 'screen', 'is', 'greater', 'than', 'a', 'probability', 'threshold', 'wherein', 'the', 'gaze', 'information', 'includes', 'the', 'probability', 'value', 'that', 'a', 'gazes', 'at', 'a', 'payment', 'screen', 'and', 'in', 'response', 'to', 'determining', 'that', 'the', 'angle', 'of', 'rotation', 'in', 'each', 'preset', 'direction', 'is', 'less', 'than', 'the', 'angle', 'threshold', 'and', 'that', 'the', 'probability', 'value', 'that', 'a', 'gazes', 'at', 'a', 'payment', 'screen', 'is', 'greater', 'than', 'the', 'probability', 'threshold', 'determining', 'that', 'the', 'target', 'has', 'a', 'willingness', 'to', 'pay', 'and', 'in', 'response', 'to', 'determining', 'that', 'the', 'target', 'has', 'a', 'willingness', 'to', 'pay', 'completing', 'a', 'payment', 'operation', 'based', 'on', 'the', 'face', 'the', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'completing', 'a', 'payment', 'operation', 'based', 'on', 'the', 'face', 'comprises', 'triggering', 'and', 'performing', 'a', 'payment', 'initiating', 'operation', 'to', 'acquire', 'second', 'face', 'information', 'based', 'on', 'the', 'face', 'determining', 'whether', 'second', 'characteristic', 'information', 'extracted', 'from', 'the', 'second', 'face', 'information', 'indicates', 'that', 'the', 'has', 'a', 'willingness', 'to', 'pay', 'and', 'in', 'response', 'to', 'determining', 'that', 'the', 'second', 'characteristic', 'information', 'indicates', 'that', 'the', 'has', 'a', 'willingness', 'to', 'pay', 'triggering', 'and', 'performing', 'a', 'payment', 'confirmation', 'operation', 'to', 'complete', 'the', 'payment', 'operation', 'based', 'on', 'payment', 'account', 'information', 'corresponding', 'to', 'the', 'target', 'the', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'determining', 'whether', 'second', 'characteristic', 'information', 'extracted', 'from', 'the', 'second', 'face', 'information', 'indicates', 'that', 'the', 'has', 'a', 'willingness', 'to', 'pay', 'comprises', 'determining', 'whether', 'a', 'current', 'corresponding', 'to', 'the', 'second', 'face', 'information', 'is', 'consistent', 'with', 'the', 'target', 'and', 'in', 'response', 'to', 'determining', 'that', 'the', 'current', 'is', 'consistent', 'with', 'the', 'target', 'determining', 'whether', 'the', 'target', 'has', 'a', 'willingness', 'to', 'pay', 'according', 'to', 'the', 'second', 'characteristic', 'information', 'extracted', 'from', 'the', 'second', 'face', 'information', 'the', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'extracting', 'first', 'characteristic', 'information', 'from', 'the', 'first', 'face', 'information', 'comprises', 'determining', 'the', 'head', 'posture', 'information', 'of', 'the', 'target', 'using', 'a', 'head', 'posture', 'model', 'based', 'on', 'the', 'first', 'face', 'information', 'and', 'determining', 'the', 'gaze', 'information', 'of', 'the', 'target', 'using', 'a', 'gaze', 'information', 'model', 'based', 'on', 'characteristics', 'of', 'an', 'eye', 'region', 'in', 'the', 'first', 'face', 'information', 'the', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'head', 'posture', 'model', 'is', 'obtained', 'through', 'training', 'by', 'acquiring', 'a', 'first', 'sample', 'data', 'set', 'wherein', 'the', 'first', 'sample', 'data', 'set', 'includes', 'a', 'of', 'pieces', 'of', 'first', 'sample', 'data', 'and', 'each', 'of', 'the', 'of', 'pieces', 'of', 'first', 'sample', 'data', 'includes', 'a', 'correspondence', 'between', 'a', 'sample', 'face', 'and', 'head', 'posture', 'information', 'determining', 'mean', 'data', 'and', 'variance', 'data', 'of', 'a', 'of', 'sample', 'face', 's', 'for', 'each', 'of', 'the', 'of', 'pieces', 'of', 'first', 'sample', 'data', 'preprocessing', 'the', 'sample', 'face', 'contained', 'in', 'each', 'of', 'the', 'of', 'pieces', 'of', 'first', 'sample', 'data', 'based', 'on', 'the', 'mean', 'data', 'and', 'the', 'variance', 'data', 'to', 'obtain', 'a', 'preprocessed', 'sample', 'face', 'setting', 'the', 'preprocessed', 'sample', 'face', 'and', 'the', 'corresponding', 'head', 'posture', 'information', 'as', 'a', 'first', 'model', 'training', 'sample', 'and', 'performing', 'training', 'using', 'a', 'machine', 'learning', 'and', 'based', 'on', 'a', 'of', 'first', 'model', 'training', 'samples', 'to', 'obtain', 'the', 'head', 'posture', 'model', 'the', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'gaze', 'information', 'model', 'is', 'obtained', 'through', 'training', 'by', 'acquiring', 'a', 'second', 'sample', 'data', 'set', 'wherein', 'the', 'second', 'sample', 'data', 'set', 'includes', 'a', 'of', 'pieces', 'of', 'second', 'sample', 'data', 'and', 'each', 'of', 'the', 'of', 'pieces', 'of', 'second', 'sample', 'data', 'includes', 'a', 'correspondence', 'between', 'a', 'sample', 'eye', 'and', 'gaze', 'information', 'determining', 'mean', 'data', 'and', 'variance', 'data', 'of', 'a', 'of', 'sample', 'eye', 's', 'for', 'each', 'of', 'the', 'of', 'pieces', 'of', 'second', 'sample', 'data', 'preprocessing', 'the', 'sample', 'eye', 'contained', 'in', 'each', 'of', 'the', 'of', 'pieces', 'of', 'second', 'sample', 'data', 'based', 'on', 'the', 'mean', 'data', 'and', 'the', 'variance', 'data', 'to', 'obtain', 'a', 'preprocessed', 'sample', 'eye', 'setting', 'the', 'preprocessed', 'sample', 'eye', 'and', 'the', 'corresponding', 'gaze', 'information', 'as', 'a', 'second', 'model', 'training', 'sample', 'and', 'performing', 'training', 'using', 'a', 'machine', 'learning', 'and', 'on', 'a', 'of', 'second', 'model', 'training', 'samples', 'to', 'obtain', 'the', 'gaze', 'information', 'model', 'the', 'device', 'as', 'ed', 'in', 'wherein', 'the', 'angle', 'of', 'rotation', 'in', 'each', 'preset', 'direction', 'comprises', 'a', 'pitch', 'angle', 'a', 'yaw', 'angle', 'and', 'a', 'roll', 'angle', 'wherein', 'the', 'pitch', 'angle', 'refers', 'to', 'an', 'angle', 'of', 'rotation', 'around', 'a', 'x-axis', 'the', 'yaw', 'angle', 'refers', 'to', 'an', 'angle', 'of', 'rotation', 'around', 'a', 'y-axis', 'and', 'the', 'roll', 'angle', 'refers', 'to', 'an', 'angle', 'of', 'rotation', 'around', 'a', 'z-axis', 'a', 'non-transitory', 'computer-readable', 'storage', 'medium', 'for', 'a', 'payment', 'based', 'on', 'a', 'face', 'configured', 'with', 'instructions', 'executable', 'by', 'one', 'or', 'more', 's', 'to', 'cause', 'the', 'one', 'or', 'more', 's', 'to', 'perform', 'operations', 'comprising', 'acquiring', 'first', 'face', 'information', 'of', 'a', 'target', 'extracting', 'first', 'characteristic', 'information', 'from', 'the', 'first', 'face', 'information', 'wherein', 'the', 'first', 'characteristic', 'information', 'includes', 'head', 'posture', 'information', 'of', 'the', 'target', 'and', 'gaze', 'information', 'of', 'the', 'target', 'determining', 'whether', 'the', 'target', 'has', 'a', 'willingness', 'to', 'pay', 'according', 'to', 'the', 'head', 'posture', 'information', 'of', 'the', 'target', 'and', 'the', 'gaze', 'information', 'of', 'the', 'target', 'including', 'determining', 'whether', 'an', 'angle', 'of', 'rotation', 'in', 'each', 'preset', 'direction', 'is', 'less', 'than', 'an', 'angle', 'threshold', 'wherein', 'the', 'head', 'posture', 'information', 'includes', 'the', 'angle', 'of', 'rotation', 'in', 'each', 'preset', 'direction', 'determining', 'whether', 'a', 'probability', 'value', 'that', 'a', 'gazes', 'at', 'a', 'payment', 'screen', 'is', 'greater', 'than', 'a', 'probability', 'threshold', 'wherein', 'the', 'gaze', 'information', 'includes', 'the', 'probability', 'value', 'that', 'a', 'gazes', 'at', 'a', 'payment', 'screen', 'and', 'in', 'response', 'to', 'determining', 'that', 'the', 'angle', 'of', 'rotation', 'in', 'each', 'preset', 'direction', 'is', 'less', 'than', 'the', 'angle', 'threshold', 'and', 'that', 'the', 'probability', 'value', 'that', 'a', 'gazes', 'at', 'a', 'payment', 'screen', 'is', 'greater', 'than', 'the', 'probability', 'threshold', 'determining', 'that', 'the', 'target', 'has', 'a', 'willingness', 'to', 'pay', 'and', 'in', 'response', 'to', 'determining', 'that', 'the', 'target', 'has', 'a', 'willingness', 'to', 'pay', 'completing', 'a', 'payment', 'operation', 'based', 'on', 'the', 'face', 'the', 'storage', 'medium', 'as', 'ed', 'in', 'wherein', 'the', 'completing', 'a', 'payment', 'operation', 'based', 'on', 'the', 'face', 'comprises', 'triggering', 'and', 'performing', 'a', 'payment', 'initiating', 'operation', 'to', 'acquire', 'second', 'face', 'information', 'based', 'on', 'the', 'face', 'determining', 'whether', 'second', 'characteristic', 'information', 'extracted', 'from', 'the', 'second', 'face', 'information', 'indicates', 'that', 'the', 'has', 'a', 'willingness', 'to', 'pay', 'and', 'in', 'response', 'to', 'determining', 'that', 'the', 'second', 'characteristic', 'information', 'indicates', 'that', 'the', 'has', 'a', 'willingness', 'to', 'pay', 'triggering', 'and', 'performing', 'a', 'payment', 'confirmation', 'operation', 'to', 'complete', 'the', 'payment', 'operation', 'based', 'on', 'payment', 'account', 'information', 'corresponding', 'to', 'the', 'target', 'the', 'storage', 'medium', 'as', 'ed', 'in', 'wherein', 'the', 'determining', 'whether', 'second', 'characteristic', 'information', 'extracted', 'from', 'the', 'second', 'face', 'information', 'indicates', 'that', 'the', 'has', 'a', 'willingness', 'to', 'pay', 'comprises', 'determining', 'whether', 'a', 'current', 'corresponding', 'to', 'the', 'second', 'face', 'information', 'is', 'consistent', 'with', 'the', 'target', 'and', 'in', 'response', 'to', 'determining', 'that', 'the', 'current', 'is', 'consistent', 'with', 'the', 'target', 'determining', 'whether', 'the', 'target', 'has', 'a', 'willingness', 'to', 'pay', 'according', 'to', 'the', 'second', 'characteristic', 'information', 'extracted', 'from', 'the', 'second', 'face', 'information', 'the', 'storage', 'medium', 'as', 'ed', 'in', 'wherein', 'the', 'extracting', 'first', 'characteristic', 'information', 'from', 'the', 'first', 'face', 'information', 'comprises', 'determining', 'the', 'head', 'posture', 'information', 'of', 'the', 'target', 'using', 'a', 'head', 'posture', 'model', 'based', 'on', 'the', 'first', 'face', 'information', 'and', 'determining', 'the', 'gaze', 'information', 'of', 'the', 'target', 'using', 'a', 'gaze', 'information', 'model', 'based', 'on', 'characteristics', 'of', 'an', 'eye', 'region', 'in', 'the', 'first', 'face', 'information', 'the', 'storage', 'medium', 'as', 'ed', 'in', 'wherein', 'the', 'head', 'posture', 'model', 'is', 'obtained', 'through', 'training', 'by', 'acquiring', 'a', 'first', 'sample', 'data', 'set', 'wherein', 'the', 'first', 'sample', 'data', 'set', 'includes', 'a', 'of', 'pieces', 'of', 'first', 'sample', 'data', 'and', 'each', 'of', 'the', 'of', 'pieces', 'of', 'first', 'sample', 'data', 'includes', 'a', 'correspondence', 'between', 'a', 'sample', 'face', 'and', 'head', 'posture', 'information', 'determining', 'mean', 'data', 'and', 'variance', 'data', 'of', 'a', 'of', 'sample', 'face', 's', 'for', 'each', 'of', 'the', 'of', 'pieces', 'of', 'first', 'sample', 'data', 'preprocessing', 'the', 'sample', 'face', 'contained', 'in', 'each', 'of', 'the', 'of', 'pieces', 'of', 'first', 'sample', 'data', 'based', 'on', 'the', 'mean', 'data', 'and', 'the', 'variance', 'data', 'to', 'obtain', 'a', 'preprocessed', 'sample', 'face', 'setting', 'the', 'preprocessed', 'sample', 'face', 'and', 'the', 'corresponding', 'head', 'posture', 'information', 'as', 'a', 'first', 'model', 'training', 'sample', 'and', 'performing', 'training', 'using', 'a', 'machine', 'learning', 'and', 'based', 'on', 'a', 'of', 'first', 'model', 'training', 'samples', 'to', 'obtain', 'the', 'head', 'posture', 'model', 'and', 'wherein', 'the', 'gaze', 'information', 'model', 'is', 'obtained', 'through', 'training', 'by', 'acquiring', 'a', 'second', 'sample', 'data', 'set', 'wherein', 'the', 'second', 'sample', 'data', 'set', 'includes', 'a', 'of', 'pieces', 'of', 'second', 'sample', 'data', 'and', 'each', 'of', 'the', 'of', 'pieces', 'of', 'second', 'sample', 'data', 'includes', 'a', 'correspondence', 'between', 'a', 'sample', 'eye', 'and', 'gaze', 'information', 'determining', 'mean', 'data', 'and', 'variance', 'data', 'of', 'a', 'of', 'sample', 'eye', 's', 'for', 'each', 'of', 'the', 'of', 'pieces', 'of', 'second', 'sample', 'data', 'preprocessing', 'the', 'sample', 'eye', 'contained', 'in', 'each', 'of', 'the', 'of', 'pieces', 'of', 'second', 'sample', 'data', 'based', 'on', 'the', 'mean', 'data', 'and', 'the', 'variance', 'data', 'to', 'obtain', 'a', 'preprocessed', 'sample', 'eye', 'setting', 'the', 'preprocessed', 'sample', 'eye', 'and', 'the', 'corresponding', 'gaze', 'information', 'as', 'a', 'second', 'model', 'training', 'sample', 'and', 'performing', 'training', 'using', 'a', 'machine', 'learning', 'and', 'based', 'on', 'a', 'of', 'second', 'model', 'training', 'samples', 'to', 'obtain', 'the', 'gaze', 'information', 'model', 'the', 'storage', 'medium', 'as', 'ed', 'in', 'wherein', 'the', 'angle', 'of', 'rotation', 'in', 'each', 'preset', 'direction', 'comprises', 'a', 'pitch', 'angle', 'a', 'yaw', 'angle', 'and', 'a', 'roll', 'angle', 'wherein', 'the', 'pitch', 'angle', 'refers', 'to', 'an', 'angle', 'of', 'rotation', 'around', 'a', 'x-axis', 'the', 'yaw', 'angle', 'refers', 'to', 'an', 'angle', 'of', 'rotation', 'around', 'a', 'y-axis', 'and', 'the', 'roll', 'angle', 'refers', 'to', 'an', 'angle', 'of', 'rotation', 'around', 'a', 'z-axis', 'a', 'comprising', 'detecting', 'by', 'a', 'motion', 'detection', 'module', 'a', 'motion', 'by', 'a', 'subject', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'assigning', 'a', 'unique', 'session', 'identification', 'number', 'to', 'the', 'subject', 'detected', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'detecting', 'a', 'facial', 'area', 'of', 'the', 'subject', 'detected', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'generating', 'an', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'assessing', 'a', 'quality', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'determining', 'an', 'identity', 'of', 'the', 'subject', 'based', 'on', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'identifying', 'an', 'intent', 'of', 'the', 'subject', 'and', 'authorizing', 'access', 'to', 'a', 'point', 'of', 'entry', 'based', 'on', 'the', 'determined', 'identity', 'of', 'the', 'subject', 'and', 'based', 'on', 'the', 'intent', 'of', 'the', 'subject', 'the', 'of', 'further', 'comprising', 'determining', 'one', 'or', 'more', 'additional', 'subjects', 'within', 'the', 'predetermined', 'area', 'of', 'view', 'and', 'assigning', 'a', 'unique', 'session', 'identification', 'number', 'to', 'each', 'of', 'the', 'one', 'or', 'more', 'additional', 'subjects', 'detected', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'the', 'of', 'wherein', 'the', 'assessing', 'a', 'quality', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'comprises', 'assessing', 'whether', 'the', 'quality', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'object', 'equates', 'predetermined', 'metric', 'of', 'quality', 'and', 'upon', 'determining', 'that', 'the', 'quality', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'object', 'is', 'inferior', 'to', 'the', 'predetermined', 'metric', 'of', 'quality', 'discarding', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'and', 'generating', 'a', 'second', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'the', 'of', 'further', 'comprising', 'detecting', 'whether', 'the', 'facial', 'area', 'of', 'the', 'subject', 'is', 'photographic', 'and', 'upon', 'detecting', 'that', 'the', 'facial', 'area', 'of', 'the', 'subject', 'is', 'a', 'photographic', 'generating', 'a', 'warning', 'and', 'restrict', 'access', 'to', 'the', 'point', 'of', 'entry', 'the', 'of', 'further', 'comprising', 'conducing', 'an', 'incremental', 'training', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'the', 'of', 'wherein', 'conducing', 'an', 'incremental', 'training', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'comprises', 'capturing', 'a', 'first', 'of', 'the', 'facial', 'area', 'having', 'facial', 'landmarks', 'converting', 'the', 'first', 'of', 'the', 'facial', 'area', 'into', 'a', 'first', 'numeric', 'vector', 'capturing', 'a', 'second', 'of', 'the', 'facial', 'area', 'having', 'facial', 'landmarks', 'converting', 'the', 'second', 'of', 'the', 'facial', 'area', 'into', 'a', 'second', 'numeric', 'vector', 'calculating', 'a', 'weighted', 'mean', 'of', 'the', 'first', 'numeric', 'vector', 'and', 'the', 'second', 'numeric', 'vector', 'wherein', 'the', 'weighted', 'mean', 'represents', 'a', 'change', 'in', 'a', 'facial', 'area', 'and', 'storing', 'the', 'weighted', 'mean', 'in', 'the', 'database', 'the', 'of', 'wherein', 'determining', 'an', 'identity', 'of', 'the', 'subject', 'based', 'on', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'comprises', 'comparing', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'with', 'a', 'of', 's', 'stored', 'in', 'a', 'database', 'and', 'authenticating', 'the', 'subject', 'the', 'of', 'wherein', 'identifying', 'an', 'intent', 'of', 'the', 'subject', 'comprises', 'upon', 'detecting', 'the', 'facial', 'area', 'in', 'a', 'bounding', 'box', 'commencing', 'authentication', 'of', 'the', 'subject', 'calculating', 'a', 'directional', 'vector', 'of', 'a', 'face', 'of', 'the', 'subject', 'determine', 'an', 'intent', 'of', 'the', 'subject', 'to', 'gain', 'access', 'to', 'the', 'point', 'of', 'entry', 'based', 'on', 'the', 'directional', 'vector', 'of', 'the', 'face', 'of', 'the', 'subject', 'granting', 'the', 'access', 'to', 'the', 'point', 'of', 'entry', 'based', 'on', 'authentication', 'of', 'the', 'subject', 'and', 'based', 'on', 'determining', 'the', 'intent', 'of', 'the', 'subject', 'a', 'non-transitory', 'computer', 'readable', 'medium', 'having', 'program', 'instructions', 'stored', 'thereon', 'that', 'in', 'response', 'to', 'execution', 'by', 'a', 'computing', 'device', 'cause', 'the', 'computing', 'device', 'to', 'perform', 'operations', 'comprising', 'detecting', 'a', 'motion', 'by', 'a', 'subject', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'assigning', 'a', 'unique', 'session', 'identification', 'number', 'to', 'the', 'subject', 'detected', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'detecting', 'a', 'facial', 'area', 'of', 'the', 'subject', 'detected', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'generating', 'an', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'assessing', 'a', 'quality', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'determining', 'an', 'identity', 'of', 'the', 'subject', 'based', 'on', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'identifying', 'an', 'intent', 'of', 'the', 'subject', 'and', 'authorizing', 'access', 'to', 'a', 'point', 'of', 'entry', 'based', 'on', 'the', 'determined', 'identity', 'of', 'the', 'subject', 'and', 'based', 'on', 'the', 'intent', 'of', 'the', 'subject', 'the', 'non-transitory', 'computer', 'readable', 'medium', 'of', 'further', 'comprising', 'determining', 'one', 'or', 'more', 'additional', 'subjects', 'within', 'the', 'predetermined', 'area', 'of', 'view', 'and', 'assigning', 'a', 'unique', 'session', 'identification', 'number', 'to', 'each', 'of', 'the', 'one', 'or', 'more', 'additional', 'subjects', 'detected', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'the', 'non-transitory', 'computer', 'readable', 'medium', 'of', 'wherein', 'the', 'assessing', 'a', 'quality', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'comprises', 'assessing', 'whether', 'the', 'quality', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'object', 'equates', 'predetermined', 'metric', 'of', 'quality', 'and', 'upon', 'determining', 'that', 'the', 'quality', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'object', 'is', 'inferior', 'to', 'the', 'predetermined', 'metric', 'of', 'quality', 'discarding', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'and', 'generating', 'a', 'second', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'the', 'non-transitory', 'computer', 'readable', 'medium', 'of', 'further', 'comprising', 'detecting', 'whether', 'the', 'facial', 'area', 'of', 'the', 'subject', 'is', 'photographic', 'and', 'upon', 'detecting', 'that', 'the', 'facial', 'area', 'of', 'the', 'subject', 'is', 'a', 'photographic', 'generating', 'a', 'warning', 'and', 'restrict', 'access', 'to', 'the', 'access', 'point', 'the', 'non-transitory', 'computer', 'readable', 'medium', 'of', 'further', 'comprising', 'conducing', 'an', 'incremental', 'training', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'the', 'non-transitory', 'computer', 'readable', 'medium', 'of', 'wherein', 'conducing', 'an', 'incremental', 'training', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'comprises', 'capturing', 'a', 'first', 'of', 'the', 'facial', 'area', 'having', 'facial', 'landmarks', 'converting', 'the', 'first', 'of', 'the', 'facial', 'area', 'into', 'a', 'first', 'numeric', 'vector', 'capturing', 'a', 'second', 'of', 'the', 'facial', 'area', 'having', 'facial', 'landmarks', 'converting', 'the', 'second', 'of', 'the', 'facial', 'area', 'into', 'a', 'second', 'numeric', 'vector', 'calculating', 'a', 'weighted', 'mean', 'of', 'the', 'first', 'numeric', 'vector', 'and', 'the', 'second', 'numeric', 'vector', 'wherein', 'the', 'weighted', 'mean', 'represents', 'a', 'change', 'in', 'a', 'facial', 'area', 'and', 'storing', 'the', 'weighted', 'mean', 'in', 'the', 'database', 'an', 'apparatus', 'for', 'face', 'comprising', 'a', 'and', 'a', 'memory', 'to', 'store', 'computer', 'program', 'instructions', 'the', 'computer', 'program', 'instructions', 'when', 'executed', 'on', 'the', 'cause', 'the', 'to', 'perform', 'operations', 'comprising', 'detecting', 'a', 'motion', 'by', 'a', 'subject', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'assigning', 'a', 'unique', 'session', 'identification', 'number', 'to', 'the', 'subject', 'detected', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'detecting', 'a', 'facial', 'area', 'of', 'the', 'subject', 'detected', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'generating', 'an', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'assessing', 'a', 'quality', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'determining', 'an', 'identity', 'of', 'the', 'subject', 'based', 'on', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'identifying', 'an', 'intent', 'of', 'the', 'subject', 'and', 'authorizing', 'access', 'to', 'a', 'point', 'of', 'entry', 'based', 'on', 'the', 'determined', 'identity', 'of', 'the', 'subject', 'and', 'based', 'on', 'the', 'intent', 'of', 'the', 'subject', 'the', 'apparatus', 'of', 'further', 'comprising', 'determining', 'one', 'or', 'more', 'additional', 'subjects', 'within', 'the', 'predetermined', 'area', 'of', 'view', 'and', 'assigning', 'a', 'unique', 'session', 'identification', 'number', 'to', 'each', 'of', 'the', 'one', 'or', 'more', 'additional', 'subjects', 'detected', 'within', 'a', 'predetermined', 'area', 'of', 'view', 'the', 'apparatus', 'of', 'wherein', 'the', 'assessing', 'a', 'quality', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'comprises', 'assessing', 'whether', 'the', 'quality', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'object', 'equates', 'predetermined', 'metric', 'of', 'quality', 'and', 'upon', 'determining', 'that', 'the', 'quality', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'object', 'is', 'inferior', 'to', 'the', 'predetermined', 'metric', 'of', 'quality', 'discarding', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'and', 'generating', 'a', 'second', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'the', 'apparatus', 'of', 'further', 'comprising', 'detecting', 'whether', 'the', 'facial', 'area', 'of', 'the', 'subject', 'is', 'photographic', 'and', 'upon', 'detecting', 'that', 'the', 'facial', 'area', 'of', 'the', 'subject', 'is', 'a', 'photographic', 'generating', 'a', 'warning', 'and', 'restrict', 'access', 'to', 'the', 'access', 'point', 'the', 'apparatus', 'of', 'further', 'comprising', 'conducing', 'an', 'incremental', 'training', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'the', 'apparatus', 'of', 'wherein', 'conducing', 'an', 'incremental', 'training', 'of', 'the', 'of', 'the', 'facial', 'area', 'of', 'the', 'subject', 'comprises', 'capturing', 'a', 'first', 'of', 'the', 'facial', 'area', 'having', 'facial', 'landmarks', 'converting', 'the', 'first', 'of', 'the', 'facial', 'area', 'into', 'a', 'first', 'numeric', 'vector', 'capturing', 'a', 'second', 'of', 'the', 'facial', 'area', 'having', 'facial', 'landmarks', 'converting', 'the', 'second', 'of', 'the', 'facial', 'area', 'into', 'a', 'second', 'numeric', 'vector', 'calculating', 'a', 'weighted', 'mean', 'of', 'the', 'first', 'numeric', 'vector', 'and', 'the', 'second', 'numeric', 'vector', 'wherein', 'the', 'weighted', 'mean', 'represents', 'a', 'change', 'in', 'a', 'facial', 'area', 'and', 'storing', 'the', 'weighted', 'mean', 'in', 'the', 'database', 'a', 'robot', 'comprising', 'a', 'body', 'configured', 'to', 'rotate', 'and', 'to', 'tilt', 'a', 'camera', 'coupled', 'to', 'the', 'body', 'and', 'configured', 'to', 'rotate', 'and', 'tilt', 'according', 'to', 'the', 'rotate', 'and', 'the', 'tilt', 'of', 'the', 'body', 'wherein', 'the', 'camera', 'is', 'configured', 'to', 'acquire', 'a', 'video', 'of', 'a', 'space', 'a', 'face', 'unit', 'configured', 'to', 'recognize', 'respective', 'of', 'one', 'or', 'more', 'persons', 'in', 'the', 'video', 'a', 'unit', 'configured', 'to', 'track', 'motion', 'of', 'each', 'of', 'the', 'recognized', 'of', 'the', 'one', 'or', 'more', 'persons', 'and', 'a', 'controller', 'configured', 'to', 'calculate', 'a', 'respective', 'size', 'of', 'each', 'of', 'the', 'of', 'the', 'one', 'or', 'more', 'persons', 'select', 'a', 'first', 'person', 'from', 'among', 'the', 'one', 'or', 'more', 'persons', 'based', 'on', 'the', 'calculated', 'sizes', 'of', 'the', 'and', 'control', 'at', 'least', 'one', 'of', 'a', 'direction', 'of', 'the', 'rotation', 'of', 'the', 'camera', 'an', 'angle', 'of', 'the', 'tilt', 'of', 'the', 'camera', 'and', 'a', 'focal', 'distance', 'of', 'the', 'camera', 'based', 'on', 'the', 'tracked', 'motion', 'of', 'the', 'recognized', 'face', 'of', 'the', 'first', 'person', 'the', 'robot', 'of', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'control', 'the', 'direction', 'of', 'the', 'rotation', 'of', 'the', 'camera', 'and', 'the', 'angle', 'of', 'the', 'tilt', 'of', 'the', 'camera', 'to', 'achieve', 'an', 'particular', 'of', 'the', 'camera', 'relative', 'to', 'the', 'face', 'of', 'the', 'first', 'person', 'and', 'control', 'a', 'focal', 'distance', 'of', 'the', 'camera', 'by', 'comparing', 'respective', 'sizes', 'of', 'the', 'face', 'of', 'the', 'first', 'person', 'before', 'and', 'after', 'motion', 'of', 'the', 'first', 'person', 'the', 'robot', 'of', 'wherein', 'the', 'particular', 'occurs', 'when', 'the', 'camera', 'a', 'general', 'direction', 'of', 'the', 'face', 'of', 'the', 'first', 'person', 'the', 'robot', 'of', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'normalize', 'sizes', 'of', 'the', 'of', 'the', 'one', 'or', 'more', 'persons', 'based', 'on', 'an', 'interocular', 'distance', 'and', 'select', 'the', 'first', 'person', 'based', 'on', 'the', 'normalized', 'sizes', 'of', 'the', 'of', 'the', 'one', 'or', 'more', 'persons', 'the', 'robot', 'of', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'select', 'a', 'person', 'having', 'a', 'largest', 'face', 'size', 'from', 'among', 'the', 'one', 'or', 'more', 'persons', 'as', 'the', 'first', 'person', 'the', 'robot', 'of', 'further', 'comprising', 'a', 'microphone', 'configured', 'to', 'receive', 'a', 'spoken', 'audio', 'that', 'is', 'present', 'in', 'the', 'space', 'wherein', 'the', 'controller', 'is', 'further', 'configured', 'to', 'select', 'the', 'first', 'person', 'further', 'based', 'on', 'the', 'received', 'spoken', 'audio', 'the', 'robot', 'of', 'wherein', 'the', 'controller', 'is', 'further', 'configured', 'to', 'control', 'gain', 'of', 'the', 'microphone', 'by', 'comparing', 'respective', 'sizes', 'of', 'the', 'face', 'of', 'the', 'first', 'person', 'before', 'and', 'after', 'motion', 'of', 'the', 'first', 'person', 'the', 'robot', 'of', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'calculate', 'a', 'position', 'from', 'which', 'the', 'spoken', 'audio', 'is', 'provided', 'and', 'select', 'the', 'first', 'person', 'further', 'based', 'on', 'whether', 'the', 'one', 'or', 'more', 'persons', 'are', 'in', 'the', 'position', 'from', 'which', 'the', 'voice', 'signal', 'is', 'provided', 'the', 'robot', 'of', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'select', 'a', 'second', 'person', 'as', 'the', 'first', 'person', 'from', 'among', 'the', 'one', 'or', 'more', 'persons', 'when', 'the', 'second', 'person', 'is', 'located', 'in', 'the', 'position', 'from', 'which', 'the', 'spoken', 'audio', 'is', 'provided', 'the', 'robot', 'of', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'select', 'a', 'second', 'person', 'having', 'a', 'largest', 'face', 'size', 'as', 'the', 'first', 'person', 'from', 'among', 'the', 'one', 'or', 'more', 'persons', 'when', 'none', 'of', 'the', 'one', 'or', 'more', 'persons', 'is', 'located', 'in', 'the', 'position', 'from', 'which', 'the', 'spoken', 'audio', 'is', 'provided', 'the', 'robot', 'of', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'select', 'a', 'second', 'person', 'having', 'a', 'largest', 'face', 'size', 'as', 'the', 'first', 'person', 'from', 'among', 'the', 'one', 'or', 'more', 'persons', 'when', 'a', 'of', 'persons', 'from', 'among', 'the', 'one', 'or', 'more', 'persons', 'are', 'located', 'in', 'the', 'position', 'from', 'which', 'the', 'spoken', 'audio', 'is', 'provided', 'the', 'robot', 'of', 'further', 'comprising', 'a', 'speaker', 'wherein', 'the', 'controller', 'is', 'configured', 'to', 'control', 'volume', 'of', 'the', 'speaker', 'by', 'comparing', 'respective', 'sizes', 'of', 'the', 'face', 'of', 'the', 'first', 'person', 'before', 'and', 'after', 'motion', 'of', 'the', 'first', 'person', 'the', 'robot', 'of', 'wherein', 'the', 'body', 'is', 'further', 'configured', 'to', 'rotate', 'in', 'a', 'lateral', 'direction', 'and', 'to', 'tilt', 'in', 'an', 'vertical', 'direction', 'an', 'comprising', 'a', 'camera', 'coupled', 'to', 'the', 'body', 'and', 'configured', 'to', 'rotate', 'and', 'to', 'tilt', 'wherein', 'the', 'camera', 'is', 'configured', 'to', 'acquire', 'a', 'video', 'of', 'a', 'space', 'within', 'which', 'one', 'or', 'more', 'persons', 'are', 'positioned', 'and', 'a', 'configured', 'to', 'recognize', 'respective', 'of', 'the', 'one', 'or', 'more', 'persons', 'in', 'the', 'video', 'track', 'motion', 'of', 'each', 'of', 'the', 'recognized', 'of', 'the', 'one', 'or', 'more', 'persons', 'calculate', 'a', 'respective', 'size', 'of', 'each', 'of', 'the', 'of', 'the', 'one', 'or', 'more', 'persons', 'select', 'a', 'first', 'person', 'from', 'among', 'the', 'one', 'or', 'more', 'persons', 'based', 'on', 'the', 'calculated', 'sizes', 'of', 'the', 'and', 'control', 'at', 'least', 'one', 'of', 'a', 'direction', 'of', 'the', 'rotation', 'of', 'the', 'camera', 'an', 'angle', 'of', 'the', 'tilt', 'of', 'the', 'camera', 'and', 'a', 'focal', 'distance', 'of', 'the', 'camera', 'based', 'on', 'the', 'tracked', 'motion', 'of', 'the', 'recognized', 'face', 'of', 'the', 'first', 'person', 'a', 'comprising', 'acquiring', 'by', 'a', 'camera', 'a', 'video', 'of', 'a', 'space', 'within', 'which', 'one', 'or', 'more', 'persons', 'are', 'positioned', 'respective', 'of', 'the', 'one', 'or', 'more', 'persons', 'in', 'the', 'video', 'motion', 'of', 'each', 'of', 'the', 'recognized', 'of', 'the', 'one', 'or', 'more', 'persons', 'calculating', 'a', 'respective', 'size', 'of', 'each', 'of', 'the', 'of', 'the', 'one', 'or', 'more', 'persons', 'selecting', 'a', 'first', 'person', 'from', 'among', 'the', 'one', 'or', 'more', 'persons', 'based', 'on', 'the', 'calculated', 'sizes', 'of', 'the', 'and', 'controlling', 'at', 'least', 'one', 'of', 'a', 'direction', 'of', 'rotation', 'of', 'the', 'camera', 'an', 'angle', 'of', 'tilt', 'of', 'the', 'camera', 'and', 'a', 'focal', 'distance', 'of', 'the', 'camera', 'based', 'on', 'the', 'tracked', 'motion', 'of', 'the', 'recognized', 'face', 'of', 'the', 'first', 'person', 'a', 'of', 'inferring', 'topics', 'from', 'a', 'multimodal', 'file', 'the', 'comprising', 'receiving', 'a', 'multimodal', 'file', 'extracting', 'a', 'set', 'of', 'entities', 'from', 'the', 'multimodal', 'file', 'linking', 'the', 'set', 'of', 'entities', 'to', 'produce', 'a', 'set', 'of', 'linked', 'entities', 'obtaining', 'reference', 'information', 'for', 'the', 'set', 'of', 'entities', 'based', 'at', 'least', 'on', 'the', 'reference', 'information', 'generating', 'a', 'graph', 'of', 'the', 'set', 'of', 'linked', 'entities', 'the', 'graph', 'comprising', 'nodes', 'and', 'edges', 'based', 'at', 'least', 'on', 'the', 'nodes', 'and', 'edges', 'of', 'the', 'graph', 'determining', 'clusters', 'in', 'the', 'graph', 'based', 'at', 'least', 'on', 'the', 'clusters', 'in', 'the', 'graph', 'identifying', 'topic', 'candidates', 'extracting', 'features', 'from', 'the', 'clusters', 'in', 'the', 'graph', 'based', 'at', 'least', 'on', 'the', 'extracted', 'features', 'selecting', 'at', 'least', 'one', 'topicid', 'from', 'among', 'the', 'topic', 'candidates', 'to', 'represent', 'at', 'least', 'one', 'cluster', 'and', 'indexing', 'the', 'multimodal', 'file', 'with', 'the', 'at', 'least', 'one', 'topicid', 'the', 'of', 'wherein', 'the', 'multimodal', 'file', 'comprises', 'a', 'video', 'portion', 'and', 'an', 'audio', 'portion', 'and', 'wherein', 'extracting', 'a', 'set', 'of', 'entities', 'from', 'the', 'multimodal', 'file', 'comprises', 'detecting', 'objects', 'in', 'the', 'video', 'portion', 'of', 'the', 'multimodal', 'file', 'and', 'detecting', 'text', 'in', 'the', 'audio', 'portion', 'of', 'the', 'multimodal', 'file', 'the', 'of', 'wherein', 'detecting', 'objects', 'comprises', 'performing', 'face', 'the', 'of', 'wherein', 'detecting', 'text', 'comprises', 'performing', 'a', 'speech', 'to', 'text', 'process', 'the', 'of', 'further', 'comprising', 'identifying', 'a', 'language', 'used', 'in', 'the', 'audio', 'portion', 'of', 'the', 'multimodal', 'file', 'and', 'wherein', 'performing', 'a', 'speech', 'to', 'text', 'process', 'comprises', 'performing', 'a', 'speech', 'to', 'text', 'process', 'in', 'the', 'identified', 'language', 'the', 'of', 'further', 'comprising', 'translating', 'the', 'detected', 'text', 'the', 'of', 'further', 'comprising', 'determining', 'significant', 'clusters', 'and', 'insignificant', 'clusters', 'in', 'the', 'determined', 'clusters', 'and', 'wherein', 'extracting', 'features', 'from', 'the', 'clusters', 'in', 'the', 'graph', 'comprises', 'extracting', 'features', 'from', 'the', 'significant', 'clusters', 'in', 'the', 'graph', 'the', 'of', 'wherein', 'extracting', 'features', 'from', 'the', 'clusters', 'in', 'the', 'graph', 'comprises', 'at', 'least', 'one', 'process', 'selected', 'from', 'the', 'list', 'consisting', 'of', 'determining', 'a', 'graph', 'diameter', 'and', 'determining', 'a', 'jaccard', 'coefficient', 'the', 'of', 'wherein', 'selecting', 'at', 'least', 'one', 'topicid', 'to', 'represent', 'at', 'least', 'one', 'cluster', 'comprises', 'based', 'at', 'least', 'on', 'the', 'extracted', 'features', 'mapping', 'topic', 'candidates', 'into', 'a', 'probability', 'interval', 'and', 'based', 'at', 'least', 'on', 'the', 'mapping', 'ranking', 'topic', 'candidates', 'within', 'the', 'at', 'least', 'one', 'cluster', 'and', 'selecting', 'the', 'at', 'least', 'one', 'topicid', 'based', 'at', 'least', 'on', 'the', 'ranking', 'the', 'of', 'further', 'comprising', 'translating', 'the', 'at', 'least', 'one', 'topicid', 'and', 'wherein', 'indexing', 'the', 'multimodal', 'file', 'with', 'the', 'at', 'least', 'one', 'topicid', 'comprises', 'indexing', 'the', 'multimodal', 'file', 'with', 'the', 'at', 'least', 'one', 'translated', 'topicid', 'a', 'system', 'for', 'inferring', 'topics', 'from', 'a', 'multimodal', 'file', 'the', 'system', 'comprising', 'an', 'entity', 'extraction', 'component', 'comprising', 'an', 'object', 'detection', 'component', 'and', 'a', 'speech', 'to', 'text', 'component', 'operative', 'to', 'extract', 'a', 'set', 'of', 'entities', 'from', 'a', 'multimodal', 'file', 'comprising', 'a', 'video', 'portion', 'and', 'an', 'audio', 'portion', 'an', 'entity', 'linking', 'component', 'operative', 'to', 'link', 'the', 'extracted', 'set', 'of', 'entities', 'to', 'produce', 'a', 'set', 'of', 'linked', 'entities', 'an', 'information', 'retrieval', 'component', 'operative', 'to', 'obtain', 'reference', 'information', 'for', 'the', 'extracted', 'set', 'of', 'entities', 'a', 'graphing', 'and', 'analysis', 'component', 'operative', 'to', 'generate', 'a', 'graph', 'of', 'the', 'set', 'of', 'linked', 'entities', 'the', 'graph', 'comprising', 'nodes', 'and', 'edges', 'based', 'at', 'least', 'on', 'the', 'nodes', 'and', 'edges', 'of', 'the', 'graph', 'determine', 'clusters', 'in', 'the', 'graph', 'based', 'at', 'least', 'on', 'the', 'clusters', 'in', 'the', 'graph', 'identify', 'topic', 'candidates', 'and', 'extract', 'features', 'from', 'the', 'clusters', 'in', 'the', 'graph', 'a', 'topicid', 'selection', 'component', 'operative', 'to', 'rank', 'the', 'topic', 'candidates', 'within', 'at', 'least', 'one', 'cluster', 'and', 'based', 'at', 'least', 'on', 'the', 'ranking', 'select', 'at', 'least', 'one', 'topicid', 'from', 'among', 'the', 'topic', 'candidates', 'to', 'represent', 'at', 'least', 'one', 'cluster', 'and', 'a', 'video', 'indexer', 'operative', 'to', 'index', 'the', 'multimodal', 'file', 'with', 'the', 'at', 'least', 'one', 'topicid', 'the', 'system', 'of', 'wherein', 'the', 'object', 'detection', 'component', 'is', 'operative', 'to', 'perform', 'face', 'the', 'system', 'of', 'wherein', 'the', 'speech', 'to', 'text', 'component', 'is', 'operative', 'to', 'extract', 'entity', 'information', 'in', 'at', 'least', 'two', 'different', 'languages', 'one', 'or', 'more', 'computer', 'storage', 'devices', 'having', 'computer-executable', 'instructions', 'stored', 'thereon', 'for', 'inferring', 'topics', 'from', 'a', 'multimodal', 'file', 'which', 'on', 'execution', 'by', 'a', 'computer', 'cause', 'the', 'computer', 'to', 'perform', 'operations', 'comprising', 'receiving', 'a', 'multimodal', 'file', 'comprising', 'a', 'video', 'portion', 'and', 'an', 'audio', 'portion', 'extracting', 'a', 'set', 'of', 'entities', 'from', 'the', 'multimodal', 'file', 'wherein', 'extracting', 'a', 'set', 'of', 'entities', 'from', 'the', 'multimodal', 'file', 'comprises', 'detecting', 'objects', 'in', 'the', 'video', 'portion', 'of', 'the', 'multimodal', 'file', 'with', 'face', 'detecting', 'text', 'in', 'the', 'audio', 'portion', 'of', 'the', 'multimodal', 'file', 'with', 'a', 'speech', 'to', 'text', 'process', 'and', 'disambiguating', 'among', 'a', 'set', 'of', 'detected', 'entity', 'names', 'linking', 'the', 'set', 'of', 'entities', 'to', 'produce', 'a', 'set', 'of', 'linked', 'entities', 'obtaining', 'reference', 'information', 'for', 'the', 'set', 'of', 'entities', 'based', 'at', 'least', 'on', 'the', 'reference', 'information', 'generating', 'a', 'graph', 'of', 'the', 'set', 'of', 'linked', 'entities', 'the', 'graph', 'comprising', 'nodes', 'and', 'edges', 'based', 'at', 'least', 'on', 'the', 'nodes', 'and', 'edges', 'of', 'the', 'graph', 'determining', 'clusters', 'in', 'the', 'graph', 'determining', 'significant', 'clusters', 'and', 'insignificant', 'clusters', 'in', 'the', 'determined', 'clusters', 'based', 'at', 'least', 'on', 'the', 'significant', 'clusters', 'in', 'the', 'graph', 'identifying', 'topic', 'candidates', 'extracting', 'features', 'from', 'the', 'significant', 'clusters', 'in', 'the', 'graph', 'based', 'at', 'least', 'on', 'the', 'extracted', 'features', 'mapping', 'the', 'topic', 'candidates', 'into', 'a', 'probability', 'interval', 'based', 'at', 'least', 'on', 'the', 'mapping', 'ranking', 'the', 'topic', 'candidates', 'within', 'at', 'least', 'one', 'significant', 'cluster', 'based', 'on', 'the', 'ranking', 'selecting', 'at', 'least', 'one', 'topicid', 'from', 'among', 'the', 'topic', 'candidates', 'to', 'represent', 'the', 'at', 'least', 'one', 'significant', 'cluster', 'and', 'indexing', 'the', 'multimodal', 'file', 'with', 'the', 'at', 'least', 'one', 'topicid', 'the', 'one', 'or', 'more', 'computer', 'storage', 'devices', 'of', 'wherein', 'the', 'operations', 'further', 'comprise', 'identifying', 'a', 'language', 'used', 'in', 'the', 'audio', 'portion', 'of', 'the', 'multimodal', 'file', 'and', 'detecting', 'text', 'in', 'the', 'audio', 'portion', 'of', 'the', 'multimodal', 'file', 'with', 'a', 'speech', 'to', 'text', 'process', 'comprises', 'performing', 'a', 'speech', 'to', 'text', 'process', 'in', 'the', 'identified', 'language权利要求', '、', '一种人脸识别方法其特征在于包括', '通过第一摄像头获取第一人脸图像', '提取所述第一人脸图像的第一人脸特征', '将所述第一人脸特征与预先存储的第二人脸特征进行对比获得参考相似度所述第', '二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得所述第二摄像头与所述第', '一摄像头属于不同类型的摄像头', '根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否对应相同人。', '、', '根据权利要求', '所述的方法其特征在于', '所述第一摄像头为热成像摄像头所述第二摄像头为可见光摄像头', '或者所述第一摄像头为可见光摄像头所述第一摄像头为热成像摄像头。', '、', '根据权利要求', '或', '所述的方法其特征在于所述根据所述参考相似度确定所', '述第一人脸特征与所述第二人脸特征是否对应相同人包括', '根据所述参考相似度、', '参考误报率以及相似度阈值确定所述第一人脸特征与所述第二', '人脸特征是否对应相同人其中不同的误报率对应不同的相似度阈值。', '、', '根据权利要求', '或', '所述的方法其特征在于所述根据所述参考相似度确定所', '述第一人脸特征与所述第二人脸特征是否对应相同人包括', '根据所述参考相似度以及阈值信息确定归一化后的参考相似度', '根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否对', '应相同人。', '、', '根据权利要求', '-任一项所述的方法其特征在于所述提取所述第一人脸图像的', '第_人脸特征包括', '将所述第一人脸图像输入预先训练完成的神经网络通过所述神经网络输出所述第一', '人脸图像的第一人脸特征其中所述神经网络基于第一类型图像样本和第二类型图像样', '本训练得到所述第一类型图像样本和所述第二类型图像样本由不同类型的摄像头拍摄得', '到且所述第一类型图像样本和所述第二类型图像样本中包括人脸。', '、', '根据权利要求', '所述的方法其特征在于所述神经网络基于所述第一类型图像', '样本、', '所述第二类型图像样本和混合类型图像样本训练得到所述混合类型图像样本由所', '述第一类型图像样本和所述第二类型图像样本配对而得。', '、', '根据权利要求', '-任一项所述的方法其特征在于所述第一摄像头包括车载摄像', '头所述通过第一摄像头获取第一人脸图像包括', '通过所述车载摄像头获取所述第一人脸图像所述第一人脸图像包括车辆的用车人的', '人脸图像。', '、', '根据权利要求', '所述的方法其特征在于所述用车人包括驾驶所述车辆的人、', '乘坐所述车辆的人、', '对所述车辆进行修理的人、', '给所述车辆加油的人以及控制所述车辆的', '人中的一项或多项。', '、', '根据权利要求', '所述的方法其特征在于所述用车人包括驾驶所述车辆的人', '所述通过所述车载摄像头获取所述第一人脸图像包括', '在接收到触发指令的情况下通过所述车载摄像头获取所述第一人脸图像', '或者在所述车辆运行时通过所述车载摄像头获取所述第一人脸图像', '或者在所述车辆的运行速度达到参考速度的情况下通过所述车载摄像头获取所述', '第一人脸图像。', '、', '根据权利要求', '-任一项所述的方法其特征在于所述第二人脸图像为对所述', '用车人进行人脸注册的图像所述将所述第一人脸特征与预先存储的第二人脸特征进行对', '比之前所述方法还包括', '通过所述第二摄像头获取所述第二人脸图像', '提取所述第二人脸图像的第二人脸特征', '保存所述第二人脸图像的第二人脸特征。', '、', '一种神经网络训练方法其特征在于包括', '获取第一类型图像样本和第二类型图像样本所述第一类型图像样本和所述第二类型', '图像样本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述第二类型图像样', '本中包括人脸', '根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。', '、', '根据权利要求', '所述的方法其特征在于所述根据所述第一类型图像样本和所', '述第二类型图像样本训练神经网络包括', '将所述第一类型图像样本和所述第二类型图像样本配对得到所述第一类型图像样本', '和所述第二类型图像样本的混合类型图像样本', '根据所述第一类型图像样本、', '所述第二类型图像样本和所述混合类型图像样本训练', '所述神经网络。', '、', '根据权利要求', '所述的方法其特征在于所述根据所述第一类型图像样本、', '所述第二类型图像样本和所述混合类型图像样本训练所述神经网络包括', '通过所述神经网络获取所述第一类型图像样本的人脸预测结果、', '所述第二类型图像样', '本的人脸预测结果和所述混合类型图像样本的人脸预测结果', '根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、', '所述第二类型图', '像样本的人脸预测结果和人脸标注结果之间的差异、', '以及所述混合类型图像样本的人脸预', '测结果和人脸标注结果的差异训练所述神经网络。', '、', '根据权利要求', '所述的方法其特征在于所述神经网络中包括第一分类器、', '第二分类器和混合分类器所述通过所述神经网络获取所述第一类型图像样本的人脸预测', '结果、', '所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果', '包括', '将所述第一类型图像样本的人脸特征输入至所述第一分类器中得到所述第一类型图', '像样本的人脸预测结果', '将所述第二类型图像样本的人脸特征输入至所述第二分类器中得到所述第二类型图', '像样本的人脸预测结果', '将所述混合类型图像样本的人脸特征输入至所述混合分类器中得到所述混合类型图', '像样本的人脸预测结果。', '、', '根据权利要求', '所述的方法其特征在于所述方法还包括', '在训练完成的所述神经网络中去除所述第一分类器、', '所述第二分类器和所述混合分类', '器得到用于进行人脸识别的神经网络。', '、', '一种人脸识别装置其特征在于包括', '第一获取单元用于通过第一摄像头获取第一人脸图像', '第一提取单元用于提取所述第一人脸图像的第一人脸特征', '对比单元用于将所述第一人脸特征与预先存储的第二人脸特征进行对比获得参考', '相似度所述第二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得所述第二', '摄像头与所述第一摄像头属于不同类型的摄像头', '确定单元用于根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否', '对应相同人。', '、', '根据权利要求', '所述的装置其特征在于', '所述第一摄像头为热成像摄像头所述第二摄像头为可见光摄像头', '或者所述第一摄像头为可见光摄像头所述第一摄像头为热成像摄像头。', '、', '根据权利要求', '或', '所述的装置其特征在于', '所述确定单元具体用于根据所述参考相似度、', '参考误报率以及相似度阈值确定所述', '第一人脸特征与所述第二人脸特征是否对应相同人其中不同的误报率对应不同的相似', '度阈值。', '、', '根据权利要求', '或', '所述的装置其特征在于', '所述确定单元具体用于根据所述参考相似度以及阈值信息确定归一化后的参考相似', '度以及根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否', '对应相同人。', '、', '根据权利要求', '-任_项所述的装置其特征在于', '所述第一提取单元具体用于将所述第一人脸图像输入预先训练完成的神经网络通', '过所述神经网络输出所述第一人脸图像的第一人脸特征其中所述神经网络基于第一类', '型图像样本和第二类型图像样本训练得到所述第一类型图像样本和所述第二类型图像样', '本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述第二类型图像样本中包', '括人脸。', '、', '根据权利要求', '所述的装置其特征在于所述神经网络基于所述第一类型图', '像样本、', '所述第二类型图像样本和混合类型图像样本训练得到所述混合类型图像样本由', '所述第一类型图像样本和所述第二类型图像样本配对而得。', '、', '根据权利要求', '-任一项所述的装置其特征在于所述第一摄像头包括车载', '摄像头', '所述第一获取单元具体用于通过所述车载摄像头获取所述第一人脸图像所述第一', '人脸图像包括车辆的用车人的人脸图像。', '、', '根据权利要求', '所述的装置其特征在于所述用车人包括驾驶所述车辆的人、', '乘坐所述车辆的人、', '对所述车辆进行修理的人、', '给所述车辆加油的人以及控制所述车辆的', '人中的一项或多项。', '、', '根据权利要求', '所述的装置其特征在于所述用车人包括驾驶所述车辆的人', '所述第一获取单元具体用于在接收到触发指令的情况下通过所述车载摄像头获取所述', '第一人脸图像', '或者所述第一获取单元具体用于在所述车辆运行时通过所述车载摄像头获取所', '述第', '_人脸图像', '或者所述第一获取单元具体用于在所述车辆的运行速度达到参考速度的情况下', '通过所述车载摄像头获取所述第一人脸图像。', '、', '根据权利要求', '-任一项所述的装置其特征在于所述第二人脸图像为对所', '述用车人进行人脸注册的图像所述装置还包括', '第二获取单元用于通过所述第二摄像头获取所述第二人脸图像', '第二提取单元用于提取所述第二人脸图像的第二人脸特征', '保存单元用于保存所述第二人脸图像的第二人脸特征。', '、', '一种神经网络训练装置其特征在于包括', '获取单元用于获取第一类型图像样本和第二类型图像样本所述第一类型图像样本', '和所述第二类型图像样本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述', '第二类型图像样本中包括人脸', '训练单元用于根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。', '、', '根据权利要求', '所述的装置其特征在于所述训练单元包括', '配对子单元用于将所述第一类型图像样本和所述第二类型图像样本配对得到所述', '第一类型图像样本和所述第二类型图像样本的混合类型图像样本', '训练子单元用于根据所述第一类型图像样本、', '所述第二类型图像样本和所述混合类', '型图像样本训练所述神经网络。', '、', '根据权利要求', '所述的装置其特征在于', '所述训练子单元具体用于通过所述神经网络获取所述第一类型图像样本的人脸预测', '结果、', '所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果', '以及根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、', '所述第二类型图', '像样本的人脸预测结果和人脸标注结果之间的差异、', '以及所述混合类型图像样本的人脸预', '测结果和人脸标注结果的差异训练所述神经网络。', '、', '根据权利要求', '所述的装置其特征在于所述神经网络中包括第一分类器、', '第二分类器和混合分类器', '所述训练子单元具体用于将所述第一类型图像样本的人脸特征输入至所述第一分类', '器中得到所述第一类型图像样本的人脸预测结果以及将所述第二类型图像样本的人脸', '特征输入至所述第二分类器中得到所述第二类型图像样本的人脸预测结果以及将所述', '混合类型图像样本的人脸特征输入至所述混合分类器中得到所述混合类型图像样本的人', '脸预测结果。', '、', '根据权利要求', '所述的装置其特征在于所述装置还包括', '神经网络应用单元用于在训练完成的所述神经网络中去除所述第一分类器、', '所述第', '二分类器和所述混合分类器得到用于进行人脸识别的神经网络。', '、', '一种电子设备其特征在于包括处理器和存储器所述处理器和所述存储器耦', '合其中所述存储器用于存储程序指令所述程序指令被所述处理器执行时使所述处', '理器执行权利要求', '-任一项所述的方法和或使所述处理器执行权利要求', '-任一', '项所述的方法。', '、', '一种计算机可读存储介质其特征在于所述计算机可读存储介质中存储有计算', '机程序所述计算机程序包括程序指令所述程序指令当被处理器执行时使所述处理器', '执行权利要求', '-任一项所述的方法和或使所述处理器执行权利要求', '-任一项所', '述的方法。', 'a', 'system', 'for', 'alerting', 'on', 'vision', 'impairment', 'said', 'system', 'comprising', 'a', 'processing', 'unit', 'configured', 'and', 'operable', 'for', 'receiving', 'scene', 'data', 'being', 'indicative', 'of', 'a', 'scene', 'of', 'at', 'least', 'one', 'consumer', 'in', 'an', 'environment', 'identifying', 'in', 'the', 'scene', 'data', 'a', 'certain', 'consumer', 'identifying', 'an', 'event', 'being', 'indicative', 'of', 'a', 'behavioral', 'compensation', 'for', 'vision', 'impairment', 'and', 'upon', 'identification', 'of', 'such', 'an', 'event', 'sending', 'a', 'notification', 'relating', 'to', 'the', 'vision', 'impairment', 'the', 'system', 'of', 'further', 'comprising', 'at', 'least', 'one', 'sensing', 'unit', 'configured', 'and', 'operable', 'for', 'detecting', 'the', 'scene', 'data', 'the', 'system', 'of', 'wherein', 'said', 'at', 'least', 'one', 'sensing', 'unit', 'comprises', 'at', 'least', 'one', 'of', 'at', 'least', 'one', 'imaging', 'unit', 'configured', 'and', 'operable', 'for', 'capturing', 'at', 'least', 'one', 'of', 'at', 'least', 'a', 'portion', 'of', 'a', 'consumer', \"'s\", 'body', 'at', 'least', 'one', 'motion', 'detector', 'configured', 'and', 'operable', 'for', 'detecting', 'consumer', 'data', 'being', 'indicative', 'of', 'a', 'motion', 'of', 'a', 'consumer', 'or', 'at', 'least', 'one', 'eye', 'tracker', 'configured', 'and', 'operable', 'for', 'eye', 'motion', 'of', 'a', 'consumer', 'the', 'system', 'of', 'wherein', 'the', 'at', 'least', 'one', 'imaging', 'unit', 'comprises', 'a', 'of', 'cameras', 'placed', 'at', 'different', 'heights', 'the', 'system', 'of', 'any', 'one', 'of', 's', 'to', 'wherein', 'said', 'sensing', 'unit', 'is', 'accommodated', 'in', 'an', 'optical', 'or', 'digital', 'eyewear', 'frame', 'display', 'the', 'system', 'of', 'any', 'one', 'of', 's', 'to', 'wherein', 'said', 'processing', 'unit', 'is', 'configured', 'and', 'operable', 'for', 'identifying', 'a', 'consumer', \"'s\", 'condition', 'said', 'consumer', \"'s\", 'condition', 'comprising', 'consumer', 'data', 'being', 'indicative', 'of', 'the', 'consumer', \"'s\", 'position', 'and', 'location', 'relative', 'to', 'at', 'least', 'one', 'object', 'in', 'the', 'consumer', \"'s\", 'environment', 'said', 'consumer', 'data', 'comprises', 'at', 'least', 'one', 'of', 'a', 'consumer', \"'s\", 'face', 'eyewear', 'posture', 'position', 'sound', 'or', 'motion', 'the', 'system', 'of', 'any', 'one', 'of', 's', 'to', 'wherein', 'said', 'event', 'comprises', 'at', 'least', 'one', 'position', 'and', 'of', 'head', 'increase', 'or', 'decrease', 'of', 'viewing', 'distance', 'between', 'the', 'consumer', 'and', 'viewed', 'object', 'and', 'changing', 'the', 'position', 'of', 'eyeglasses', 'worn', 'by', 'the', 'consumer', 'the', 'system', 'of', 'any', 'one', 'of', 's', 'to', 'wherein', 'said', 'event', 'is', 'identified', 'by', 'identifying', 's', 'having', 'an', 'feature', 'being', 'indicative', 'of', 'behavioral', 'compensation', 'performing', 'a', 'bruckner', 'test', 'performing', 'a', 'hirschberg', 'test', 'and', 'measuring', 'blink', 'count', 'frequency', 'the', 'system', 'of', 'wherein', 'the', 'feature', 'being', 'indicative', 'of', 'behavioral', 'compensation', 'comprises', 'squinting', 'head', 'certain', 'distances', 'between', 'an', 'object', 'and', 'consumer', \"'s\", 'eyes', 'certain', 'position', 'of', 'eyeglasses', 'on', 'the', 'consumer', \"'s\", 'face', 'strabismus', 'cataracts', 'and', 'reflections', 'from', 'the', 'eye', 'the', 'system', 'of', 'any', 'one', 'of', 's', 'to', 'wherein', 'the', 'notification', 'includes', 'at', 'least', 'one', 'of', 'the', 'data', 'indicative', 'of', 'the', 'identified', 'event', 'data', 'indicative', 'of', 'the', 'identified', 'consumer', 'ophthalmologic', 'recommendations', 'based', 'on', 'the', 'identified', 'event', 'or', 'lack', 'of', 'events', 'or', 'an', 'appointment', 'for', 'a', 'vision', 'test', 'the', 'system', 'of', 'any', 'one', 'of', 's', 'to', 'wherein', 'said', 'processing', 'unit', 'comprises', 'a', 'memory', 'for', 'storing', 'at', 'least', 'one', 'of', 'a', 'reference', 'data', 'indicative', 'of', 'behavioral', 'compensation', 'for', 'vision', 'impairment', 'data', 'indicative', 'of', 'the', 'notification', 'or', 'data', 'indicative', 'of', 'a', 'follow-up', 'of', 'the', 'notification', 'the', 'system', 'of', 'wherein', 'said', 'processing', 'unit', 'is', 'configured', 'for', 'at', 'least', 'one', 'of', 'identifying', 'the', 'event', 'upon', 'comparison', 'between', 'the', 'detected', 'data', 'and', 'the', 'reference', 'data', 'or', 'determining', 'a', 'probability', 'for', 'a', 'vision', 'impairment', 'of', 'the', 'consumer', 'based', 'on', 'the', 'comparison', 'the', 'system', 'of', 'any', 'one', 'of', 's', 'to', 'wherein', 'said', 'processing', 'unit', 'comprises', 'a', 'communication', 'interface', 'being', 'configured', 'for', 'sending', 'the', 'notification', 'to', 'at', 'least', 'one', 'of', 'the', 'identified', 'consumer', 'or', 'a', 'third', 'party', 'the', 'system', 'of', 'any', 'one', 'of', 's', 'to', 'wherein', 'said', 'processing', 'unit', 'is', 'configured', 'for', 'providing', 'a', 'frame', 'recommendation', 'the', 'system', 'of', 'any', 'one', 'of', 's', 'to', 'wherein', 'said', 'memory', 'is', 'configured', 'for', 'storing', 'a', 'database', 'including', 'a', 'multiplicity', 'of', 'data', 'sets', 'related', 'to', 'a', 'of', 'spectacle', 'frame', 'models', 'and', 'sizes', 'the', 'system', 'according', 'to', 'or', 'wherein', 'said', 'processing', 'unit', 'is', 'configured', 'and', 'operable', 'to', 'correlate', 'between', 'frames', 'parameters', 'and', 'ophthalmic', 'prescriptions', 'the', 'system', 'according', 'to', 'any', 'of', 's', 'to', 'wherein', 'said', 'processing', 'unit', 'is', 'configured', 'and', 'operable', 'to', 'correlate', 'between', 'frames', 'parameters', 'and', 'facial', 'features', 'the', 'system', 'according', 'to', 'any', 'of', 's', 'to', 'wherein', 'said', 'processing', 'unit', 'is', 'configured', 'and', 'operable', 'to', 'correlate', 'between', 'frames', 'parameters', 'and', 'eyewear', 'preferences', 'the', 'system', 'according', 'to', 'any', 'of', 's', 'to', 'comprising', 'a', 'server', 'and', 'at', 'least', 'one', 'computer', 'entity', 'linked', 'to', 'the', 'server', 'via', 'a', 'network', 'wherein', 'said', 'network', 'is', 'configured', 'to', 'receive', 'and', 'respond', 'to', 'requests', 'sent', 'across', 'the', 'network', 'transmitting', 'one', 'or', 'more', 'modules', 'of', 'computer', 'executable', 'program', 'instructions', 'and', 'displayable', 'data', 'to', 'the', 'network', 'connected', 'computer', 'platform', 'in', 'response', 'to', 'a', 'request', 'wherein', 'said', 'modules', 'include', 'modules', 'configured', 'to', 'receive', 'and', 'transmit', 'information', 'transmitting', 'a', 'frame', 'recommendation', 'and', 'an', 'optical', 'lens', 'option', 'recommendation', 'based', 'on', 'received', 'information', 'for', 'display', 'by', 'the', 'network', 'connected', 'computer', 'platform', 'a', 'computer', 'program', 'instructions', 'stored', 'in', 'the', 'local', 'storage', 'that', 'when', 'executed', 'by', 'a', 'processing', 'unit', 'cause', 'the', 'processing', 'unit', 'to', 'receive', 'data', 'being', 'indicative', 'of', 'a', 'scene', 'of', 'at', 'least', 'one', 'consumer', 'in', 'an', 'environment', 'identify', 'in', 'the', 'data', 'a', 'certain', 'consumer', 'identify', 'an', 'event', 'being', 'indicative', 'of', 'a', 'behavioral', 'compensation', 'for', 'vision', 'impairment', 'and', 'upon', 'identification', 'of', 'such', 'an', 'event', 'send', 'a', 'notification', 'relating', 'to', 'the', 'vision', 'impairment', 'a', 'computer', 'program', 'product', 'stored', 'on', 'a', 'tangible', 'computer', 'readable', 'medium', 'comprising', 'a', 'library', 'of', 'software', 'modules', 'which', 'cause', 'a', 'computer', 'executing', 'them', 'to', 'prompt', 'for', 'information', 'pertinent', 'to', 'at', 'least', 'one', 'of', 'an', 'eyeglasses', 'recommendation', 'and', 'an', 'optical', 'lens', 'option', 'recommendation', 'to', 'store', 'said', 'information', 'or', 'to', 'display', 'eyewear', 'recommendations', 'the', 'computer', 'program', 'product', 'of', 'wherein', 'said', 'library', 'further', 'comprises', 'a', 'module', 'for', 'frame', 'selection', 'point', 'of', 'sales', 'and', 'advertising', 'a', 'computer', 'platform', 'for', 'facilitating', 'eye', 'glasses', 'marketing', 'or', 'selection', 'comprising', 'a', 'camera', 'a', 'configured', 'to', 'execute', 'computer', 'program', 'instructions', 'to', 'cause', 'the', 'to', 'take', 'an', 'of', 'a', 'consumer', 'identify', 'in', 'the', 'a', 'certain', 'consumer', 'identify', 'an', 'event', 'being', 'indicative', 'of', 'a', 'behavioral', 'compensation', 'for', 'vision', 'impairment', 'and', 'upon', 'identification', 'of', 'such', 'an', 'event', 'sending', 'a', 'notification', 'relating', 'to', 'the', 'vision', 'impairment', 'local', 'storage', 'for', 'executable', 'instructions', 'for', 'carrying', 'out', 'storage', 'of', 'information', 'a', 'for', 'alerting', 'on', 'vision', 'impairment', 'said', 'comprising', 'identifying', 'a', 'certain', 'individual', 'in', 'scene', 'data', 'being', 'indicative', 'of', 'a', 'scene', 'of', 'at', 'least', 'one', 'consumer', 'in', 'an', 'environment', 'identifying', 'an', 'event', 'being', 'indicative', 'of', 'a', 'behavioral', 'compensation', 'for', 'vision', 'impairment', 'and', 'upon', 'identification', 'of', 'such', 'an', 'event', 'sending', 'a', 'notification', 'on', 'the', 'vision', 'impairment', 'the', 'of', 'further', 'comprising', 'detecting', 'data', 'being', 'indicative', 'of', 'a', 'scene', 'of', 'at', 'least', 'one', 'consumer', 'in', 'a', 'retail', 'environment', 'the', 'of', 'wherein', 'detecting', 'the', 'data', 'being', 'indicative', 'of', 'at', 'least', 'one', 'consumer', 'comprises', 'at', 'least', 'one', 'of', 'capturing', 'at', 'least', 'one', 'of', 'at', 'least', 'one', 'consumer', 'detecting', 'data', 'being', 'indicative', 'of', 'a', 'motion', 'of', 'a', 'consumer', 'or', 'an', 'eye', 'motion', 'of', 'a', 'consumer', 'the', 'of', 'wherein', 'capturing', 'at', 'least', 'one', 'of', 'at', 'least', 'one', 'consumer', 'comprises', 'continuously', 'recording', 'a', 'scene', 'the', 'of', 'any', 'one', 'of', 's', 'to', 'further', 'comprising', 'identifying', 'in', 'the', 'data', 'the', 'consumer', \"'\", 's', 'condition', 'including', 'data', 'being', 'indicative', 'of', 'the', 'consumer', \"'s\", 'position', 'and', 'location', 'relative', 'to', 'the', 'consumer', \"'s\", 'environment', 'said', 'data', 'comprising', 'at', 'least', 'one', 'of', 'the', 'consumer', \"'s\", 'face', 'posture', 'position', 'sound', 'or', 'motion', 'the', 'of', 'any', 'one', 'of', 's', 'to', 'wherein', 'said', 'event', 'comprises', 'at', 'least', 'one', 'of', 'position', 'and', 'of', 'head', 'increase', 'or', 'decrease', 'of', 'viewing', 'distance', 'between', 'the', 'consumer', 'and', 'viewed', 'object', 'or', 'changing', 'the', 'position', 'of', 'eyeglasses', 'worn', 'by', 'the', 'consumer', 'the', 'of', 'any', 'one', 'of', 's', 'to', 'wherein', 'identifying', 'of', 'the', 'event', 'comprises', 'identifying', 's', 'having', 'an', 'feature', 'being', 'indicative', 'of', 'behavioral', 'compensation', 'performing', 'a', 'bruckner', 'test', 'performing', 'a', 'hirschberg', 'test', 'and', 'measuring', 'blink', 'countfrequency', 'the', 'of', 'wherein', 'the', 'feature', 'being', 'indicative', 'of', 'behavioral', 'compensation', 'comprises', 'squinting', 'head', 'certain', 'distances', 'between', 'an', 'object', 'and', 'a', 'consumer', \"'s\", 'eyes', 'certain', 'position', 'of', 'eyeglasses', 'on', 'the', 'consumer', \"'s\", 'face', 'strabismus', 'cataracts', 'and', 'reflections', 'from', 'the', 'eye', 'the', 'of', 'any', 'one', 'of', 's', 'to', 'wherein', 'identifying', 'in', 'the', 'at', 'least', 'one', 'a', 'consumer', 'in', 'a', 'retail', 'environment', 'comprising', 'at', 'least', 'one', 'of', 'receiving', 'data', 'characterizing', 'the', 'retail', 'environment', 'or', 'performing', 'face', 'the', 'of', 'any', 'one', 'of', 's', 'to', 'wherein', 'sending', 'a', 'notification', 'comprising', 'sending', 'the', 'notification', 'to', 'at', 'least', 'one', 'of', 'the', 'identified', 'consumer', 'or', 'a', 'third', 'party', 'the', 'of', 'any', 'one', 'of', 's', 'to', 'wherein', 'the', 'notification', 'includes', 'at', 'least', 'one', 'of', 'the', 'data', 'indicative', 'of', 'the', 'identified', 'event', 'data', 'indicative', 'of', 'the', 'identified', 'consumer', 'ophthalmologic', 'recommendations', 'based', 'on', 'the', 'identified', 'event', 'or', 'lack', 'of', 'events', 'and', 'an', 'appointment', 'for', 'a', 'vision', 'test', 'the', 'of', 'any', 'one', 'of', 's', 'to', 'further', 'comprising', 'storing', 'at', 'least', 'one', 'of', 'a', 'reference', 'data', 'indicative', 'of', 'behavioral', 'compensation', 'for', 'vision', 'impairment', 'data', 'indicative', 'of', 'the', 'notification', 'or', 'data', 'indicative', 'of', 'a', 'follow-up', 'of', 'the', 'notification', 'the', 'of', 'further', 'comprising', 'identifying', 'the', 'event', 'upon', 'comparison', 'between', 'the', 'detected', 'data', 'and', 'the', 'reference', 'data', 'and', 'determining', 'a', 'probability', 'for', 'a', 'vision', 'impairment', 'of', 'the', 'consumer', 'based', 'on', 'the', 'comparison', 'a', 'computer', 'program', 'intended', 'to', 'be', 'stored', 'in', 'a', 'memory', 'of', 'a', 'unit', 'of', 'a', 'computer', 'system', 'or', 'in', 'a', 'removable', 'memory', 'medium', 'adapted', 'to', 'cooperate', 'with', 'a', 'reader', 'of', 'the', 'unit', 'comprising', 'instructions', 'for', 'implementing', 'the', 'according', 'to', 'any', 'of', 's', 'to']\n"
     ]
    }
   ],
   "source": [
    "# the output is a list, where each element is a token of the original text\n",
    "tokenized_text_a = nltk.word_tokenize(lowera_text)\n",
    "print(tokenized_text_a)\n",
    "\n",
    "tokenized_text_c = nltk.word_tokenize(lowerc_text)\n",
    "print(tokenized_text_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e73d5",
   "metadata": {},
   "source": [
    "### StopWords removal\n",
    "We remove the stopwords from the text. The language we are using is english, so we remove the english stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "998af632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7147530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['electronic', 'apparatus', 'including', 'capturing', 'storage', 'operation', 'method', 'thereof', 'provided', 'capturing', 'captures', 'storage', 'records', 'modules', 'coupled', 'capturing', 'storage', 'configured', 'configure', 'capturing', 'capture', 'head', 'perform', 'obtain', 'detect', 'facial', 'landmarks', 'within', 'estimate', 'head', 'posture', 'angle', 'according', 'facial', 'landmarks', 'calculate', 'gaze', 'position', 'gazes', 'screen', 'according', 'head', 'posture', 'angle', 'rotation', 'reference', 'angle', 'predetermined', 'calibration', 'positions', 'configure', 'screen', 'display', 'corresponding', 'visual', 'effect', 'according', 'gaze', 'positionthe', 'present', 'disclosure', 'provides', 'product', 'thereof', 'adopts', 'fusion', 'method', 'perform', 'machine', 'learning', 'computations', 'technical', 'effects', 'present', 'disclosure', 'include', 'fewer', 'computations', 'less', 'power', 'consumptiona', 'method', 'detecting', 'body', 'information', 'passengers', 'vehicle', 'based', 'humans', \"'\", 'status', 'provided', 'method', 'includes', 'steps', 'passenger', 'body', 'information-detecting', 'inputting', 'interior', 'vehicle', 'face', 'network', 'detect', 'faces', 'passengers', 'output', 'passenger', 'feature', 'information', 'inputting', 'interior', 'body', 'network', 'detect', 'bodies', 'output', 'body-part', 'length', 'information', 'b', 'retrieving', 'specific', 'height', 'mapping', 'information', 'referring', 'height', 'mapping', 'table', 'ratios', 'segment', 'body', 'portions', 'human', 'groups', 'heights', 'per', 'human', 'groups', 'acquiring', 'specific', 'height', 'specific', 'passenger', 'retrieving', 'specific', 'weight', 'mapping', 'information', 'weight', 'mapping', 'table', 'correlations', 'heights', 'weights', 'per', 'human', 'groups', 'acquiring', 'weight', 'specific', 'passenger', 'referring', 'specific', 'heighttechniques', 'related', 'improved', 'video', 'coding', 'based', 'face', 'detection', 'region', 'extraction', 'tracking', 'discussed', 'techniques', 'may', 'include', 'performing', 'facial', 'search', 'video', 'frame', 'determine', 'candidate', 'video', 'frame', 'testing', 'candidate', 'based', 'skin', 'tone', 'information', 'determine', 'valid', 'invalid', 'rejecting', 'invalid', 'encoding', 'video', 'frame', 'based', 'valid', 'generate', 'coded', 'bitstreama', 'method', 'managing', 'smart', 'database', 'stores', 'facial', 'face', 'provided', 'method', 'includes', 'steps', 'managing', 'counting', 'specific', 'facial', 'corresponding', 'specific', 'person', 'smart', 'database', 'new', 'facial', 'continuously', 'stored', 'determining', 'whether', 'first', 'counted', 'value', 'representing', 'count', 'specific', 'facial', 'satisfies', 'first', 'set', 'value', 'b', 'first', 'counted', 'value', 'satisfies', 'first', 'set', 'value', 'inputting', 'specific', 'facial', 'neural', 'aggregation', 'network', 'generate', 'quality', 'scores', 'specific', 'facial', 'aggregation', 'specific', 'facial', 'second', 'counted', 'value', 'representing', 'count', 'specific', 'quality', 'scores', 'among', 'quality', 'scores', 'highest', 'counting', 'thereof', 'satisfies', 'second', 'set', 'value', 'deleting', 'part', 'specific', 'facial', 'corresponding', 'uncounted', 'quality', 'scores', 'smart', 'databasea', 'capable', 'determining', 'algorithms', 'applied', 'regions', 'interest', 'within', 'digital', 'representations', 'presented', 'preprocessing', 'module', 'utilizes', 'one', 'feature', 'identification', 'algorithms', 'determine', 'regions', 'interest', 'based', 'feature', 'density', 'preprocessing', 'modules', 'leverages', 'feature', 'density', 'signature', 'region', 'determine', 'diverse', 'modules', 'operate', 'region', 'interest', 'specific', 'embodiment', 'focuses', 'structured', 'documents', 'also', 'presented', 'disclosed', 'approach', 'enhanced', 'addition', 'object', 'classifier', 'classifies', 'types', 'objects', 'found', 'regions', 'interestdisclosed', 'mobile', 'terminal', 'mobile', 'terminal', 'may', 'include', 'front', 'camera', 'obtaining', 'face', 'glance', 'sensor', 'tilted', 'certain', 'angle', 'disposed', 'adjacent', 'front', 'camera', 'obtain', 'metadata', 'face', 'controller', 'obtaining', 'distance', 'glance', 'sensor', 'front', 'camera', 'distance', 'enabling', 'area', 'overlap', 'region', 'first', 'region', 'representing', 'range', 'photographable', 'front', 'camera', 'overlaps', 'second', 'region', 'representing', 'range', 'photographable', 'glance', 'sensor', 'maximumthis', 'disclosure', 'provides', 'methods', 'apparatus', 'including', 'computer', 'programs', 'encoded', 'computer', 'storage', 'media', 'intelligent', 'routing', 'notifications', 'related', 'media', 'programming', 'one', 'aspect', 'smart', 'television', 'tv', 'implemented', 'track', \"'s\", 'tv', 'watching', 'behavior', 'anticipate', 'programming', 'based', 'behavior', 'aspects', 'smart', 'tv', 'implemented', 'detect', \"'s\", 'presence', 'based', 'detection', 'automatically', 'change', 'tv', 'channel', 'media', 'programming', 'analyzed', 'desirable', 'aspects', 'smart', 'tv', 'implemented', 'transmit', 'notification', 'instructions', 'electronic', 'within', 'network', 'attempt', 'alert', 'upcoming', 'media', 'programming', 'additionally', 'smart', 'tv', 'implemented', 'transmit', 'detection', 'instructions', 'electronic', 'within', 'network', 'whereby', 'electronic', 'attempt', 'detect', \"'s\", 'presence', 'voice', 'configured', 'output', 'test', 'depth+multi-spectral', 'including', 'pixels', 'pixel', 'corresponds', 'one', 'sensors', 'sensor', 'array', 'camera', 'includes', 'least', 'depth', 'value', 'spectral', 'value', 'spectral', 'light', 'sub-band', 'spectral', 'illuminators', 'camera', 'face', 'machine', 'previously', 'trained', 'set', 'labeled', 'training', 'depth+multi-spectral', 'structure', 'test', 'depth+multi-spectral', 'face', 'machine', 'configured', 'output', 'confidence', 'value', 'indicating', 'likelihood', 'test', 'depth+multi-spectral', 'includes', 'faceembodiments', 'present', 'disclosure', 'relate', 'processing', 'method', 'apparatus', 'electronic', 'method', 'includes', 'acquiring', 'photo', 'album', 'obtained', 'face', 'clustering', 'collecting', 'face', 'information', 'respective', 'photo', 'album', 'acquiring', 'face', 'parameter', 'according', 'face', 'information', 'selecting', 'cover', 'according', 'face', 'parameter', 'taking', 'face-region', 'cover', 'setting', 'face-region', 'cover', 'photo', 'albumtechniques', 'described', 'herein', 'provide', 'location-based', 'access', 'control', 'secured', 'resources', 'generally', 'described', 'configurations', 'disclosed', 'herein', 'enable', 'dynamically', 'modify', 'access', 'secured', 'resources', 'based', 'one', 'location-related', 'actions', 'example', 'techniques', 'disclosed', 'herein', 'enable', 'computing', 'control', 'access', 'resources', 'computing', 'display', 'secured', 'locations', 'secured', 'data', 'configurations', 'techniques', 'disclosed', 'herein', 'enable', 'controlled', 'access', 'secured', 'resources', 'based', 'least', 'part', 'invitation', 'associated', 'location', 'positioning', 'data', 'indicating', 'location', 'one', 'embodiment', 'provides', 'method', 'comprising', 'receiving', 'piece', 'content', 'salient', 'moments', 'data', 'piece', 'content', 'method', 'comprises', 'based', 'salient', 'moments', 'data', 'determining', 'first', 'path', 'viewport', 'piece', 'content', 'method', 'comprises', 'displaying', 'viewport', 'display', 'movement', 'viewport', 'based', 'first', 'path', 'playback', 'piece', 'content', 'method', 'comprises', 'generating', 'augmentation', 'salient', 'moment', 'occurring', 'piece', 'content', 'presenting', 'augmentation', 'viewport', 'portion', 'playback', 'augmentation', 'comprises', 'interactive', 'hint', 'guiding', 'viewport', 'salient', 'momenta', 'computer-implemented', 'method', 'computer', 'program', 'product', 'provided', 'facial', 'method', 'includes', 'receiving', 'method', 'also', 'includes', 'extracting', 'feature', 'extractor', 'utilizing', 'convolutional', 'neural', 'network', 'cnn', 'enlarged', 'intra-class', 'variance', 'long-tail', 'classes', 'feature', 'vectors', 'method', 'additionally', 'includes', 'generating', 'feature', 'generator', 'discriminative', 'feature', 'vectors', 'feature', 'vectors', 'method', 'includes', 'classifying', 'utilizing', 'fully', 'connected', 'classifier', 'identity', 'discriminative', 'feature', 'vector', 'method', 'also', 'includes', 'control', 'operation', '-based', 'machine', 'react', 'accordance', 'identitysome', 'embodiments', 'invention', 'provide', 'efficient', 'expressive', 'machine-trained', 'networks', 'performing', 'machine', 'learning', 'machine-trained', 'mt', 'networks', 'embodiments', 'use', 'novel', 'processing', 'nodes', 'novel', 'activation', 'functions', 'allow', 'mt', 'network', 'efficiently', 'define', 'fewer', 'processing', 'node', 'layers', 'complex', 'mathematical', 'expression', 'solves', 'particular', 'problem', 'eg', 'face', 'speech', 'etc', 'embodiments', 'activation', 'function', 'eg', 'cup', 'function', 'used', 'numerous', 'processing', 'nodes', 'mt', 'network', 'machine', 'learning', 'activation', 'function', 'configured', 'differently', 'different', 'processing', 'nodes', 'different', 'nodes', 'emulate', 'implement', 'two', 'different', 'functions', 'eg', 'two', 'boolean', 'logical', 'operators', 'xor', 'activation', 'function', 'embodiments', 'periodic', 'function', 'configured', 'implement', 'different', 'functions', 'eg', 'different', 'sinusoidal', 'functionsmethods', 'may', 'provide', 'facial', 'least', 'one', 'input', 'utilizing', 'hierarchical', 'feature', 'learning', 'pair-wise', 'receptive', 'field', 'theory', 'may', 'used', 'input', 'generate', 'pre-processed', 'multi-channel', 'channels', 'pre-processed', 'may', 'activated', 'based', 'amount', 'feature', 'rich', 'details', 'within', 'channels', 'similarly', 'local', 'patches', 'may', 'activated', 'based', 'discriminant', 'within', 'local', 'patches', 'may', 'extracted', 'local', 'patches', 'discriminant', 'may', 'selected', 'order', 'perform', 'feature', 'matching', 'pair', 'sets', 'may', 'utilize', 'patch', 'feature', 'pooling', 'pair-wise', 'matching', 'large-scale', 'training', 'order', 'quickly', 'accurately', 'perform', 'facial', 'low', 'cost', 'memory', 'computationa', 'method', 'controlling', 'terminal', 'provided', 'terminal', 'includes', 'capturing', 'apparatus', 'least', 'one', 'acquired', 'capturing', 'apparatus', 'motion', 'parameter', 'terminal', 'obtained', 'processing', 'acquired', 'controlled', 'performed', 'based', 'motion', 'parameter', 'equal', 'less', 'preset', 'parameter', 'threshold', 'skipped', 'based', 'motion', 'parameter', 'greater', 'preset', 'parameter', 'thresholda', 'drive-through', 'order', 'processing', 'method', 'apparatus', 'disclosed', 'drive-through', 'order', 'processing', 'method', 'includes', 'receiving', 'customer', 'information', 'detected', 'vision', 'providing', 'product', 'information', 'based', 'customer', 'information', 'processing', 'product', 'order', 'customer', 'according', 'present', 'disclosure', 'possible', 'rapidly', 'process', 'order', 'using', 'customer', 'information', 'based', 'customer', 'using', 'artificial', 'intelligence', 'ai', 'model', 'machine', 'learning', 'g', 'networkan', 'processing', 'method', 'performed', 'computing', 'includes', 'identifying', 'using', 'face', 'one', 'faces', 'face', 'corresponding', 'respective', 'person', 'captured', 'first', 'identified', 'face', 'extracting', 'set', 'profile', 'parameters', 'corresponding', 'person', 'first', 'selecting', 'tiles', 'first', 'tile', 'matches', 'face', 'corresponding', 'person', 'first', 'accordance', 'predefined', 'correspondence', 'set', 'profile', 'parameters', 'corresponding', 'person', 'set', 'pre-stored', 'description', 'parameters', 'first', 'tile', 'generating', 'second', 'covering', 'faces', 'respective', 'persons', 'first', 'corresponding', 'first', 'tiles', 'sharing', 'first', 'second', 'predefined', 'order', 'via', 'group', 'chat', 'sessionin', 'one', 'embodiment', 'artificial', 'reality', 'determines', 'performance', 'metric', 'eye', 'tracking', 'first', 'performance', 'threshold', 'eye', 'tracking', 'associated', 'head-mounted', 'display', 'worn', 'artificial', 'reality', 'receives', 'first', 'inputs', 'associated', 'body', 'determines', 'region', 'looking', 'within', 'field', 'view', 'head-mounted', 'display', 'based', 'received', 'first', 'inputs', 'determines', 'vergence', 'distance', 'based', 'least', 'first', 'inputs', 'associated', 'body', 'region', 'looking', 'locations', 'one', 'objects', 'scene', 'displayed', 'head-mounted', 'display', 'adjusts', 'one', 'configurations', 'head-mounted', 'display', 'based', 'determined', 'vergence', 'distance', 'computer-implemented', 'method', 'provided', '-based', 'self-guided', 'object', 'detection', 'method', 'includes', 'receiving', 'set', 'respective', 'grid', 'thereon', 'labeled', 'regarding', 'respective', 'object', 'detected', 'using', 'grid', 'level', 'label', 'data', 'method', 'includes', 'training', 'grid-based', 'object', 'detector', 'using', 'grid', 'level', 'label', 'data', 'method', 'also', 'includes', 'determining', 'respective', 'bounding', 'box', 'respective', 'object', 'applying', 'local', 'segmentation', 'method', 'additionally', 'includes', 'training', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'joint', 'object', 'localization', 'object', 'using', 'respective', 'bounding', 'box', 'respective', 'object', 'input', 'rcnna', 'method', 'face', 'comprising', 'multiple', 'phases', 'implemented', 'parallel', 'architecture', 'first', 'phase', 'normalization', 'phase', 'whereby', 'captured', 'normalized', 'size', 'orientation', 'illumination', 'stored', 'preexisting', 'database', 'second', 'phase', 'feature', 'extractiondistance', 'matrix', 'phase', 'distance', 'matrix', 'generated', 'captured', 'coarse', 'phase', 'generated', 'distance', 'matrix', 'compared', 'distance', 'matrices', 'database', 'using', 'euclidean', 'distance', 'matches', 'create', 'candidate', 'lists', 'detailed', 'phase', 'multiple', 'face', 'algorithms', 'applied', 'candidate', 'lists', 'produce', 'final', 'result', 'distance', 'matrices', 'normalized', 'database', 'may', 'broken', 'parallel', 'lists', 'parallelization', 'feature', 'extractiondistance', 'matrix', 'phase', 'candidate', 'lists', 'may', 'also', 'grouped', 'according', 'dissimilarity', 'algorithm', 'parallel', 'processing', 'detailed', 'phasean', 'imaging', 'including', 'pixel', 'matrix', 'provided', 'pixel', 'matrix', 'includes', 'phase', 'detection', 'pixels', 'regular', 'pixels', 'performs', 'autofocusing', 'according', 'pixel', 'data', 'phase', 'detection', 'pixels', 'determines', 'operating', 'resolution', 'regular', 'pixels', 'according', 'autofocused', 'pixel', 'data', 'phase', 'detection', 'pixels', 'wherein', 'phase', 'detection', 'pixels', 'always-on', 'pixels', 'regular', 'pixels', 'selectively', 'turned', 'autofocusing', 'accomplishedan', 'apparatus', 'includes', 'first', 'camera', 'module', 'providing', 'first', 'object', 'first', 'field', 'view', 'second', 'camera', 'module', 'providing', 'second', 'object', 'second', 'field', 'view', 'different', 'first', 'field', 'view', 'first', 'depth', 'map', 'generator', 'generates', 'first', 'depth', 'map', 'first', 'based', 'first', 'second', 'second', 'depth', 'map', 'generator', 'generates', 'second', 'depth', 'map', 'second', 'based', 'first', 'second', 'first', 'depth', 'mapmethods', 'apparatus', 'including', 'computer', 'programs', 'encoded', 'computer', 'storage', 'media', 'payment', 'based', 'face', 'provided', 'one', 'methods', 'includes', 'acquiring', 'first', 'face', 'information', 'target', 'extracting', 'first', 'characteristic', 'information', 'first', 'face', 'information', 'wherein', 'first', 'characteristic', 'information', 'includes', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'determining', 'whether', 'target', 'willingness', 'pay', 'according', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'including', 'determining', 'whether', 'angle', 'rotation', 'preset', 'direction', 'less', 'angle', 'threshold', 'whether', 'probability', 'value', 'gazes', 'payment', 'screen', 'greater', 'probability', 'threshold', 'response', 'determining', 'target', 'willingness', 'pay', 'completing', 'payment', 'operation', 'based', 'face', 'novel', 'method', 'apparatus', 'face', 'authentication', 'disclosed', 'disclosed', 'method', 'comprises', 'detecting', 'motion', 'subject', 'within', 'predetermined', 'area', 'view', 'assigning', 'unique', 'session', 'identification', 'number', 'subject', 'detected', 'within', 'predetermined', 'area', 'view', 'detecting', 'facial', 'area', 'subject', 'detected', 'within', 'predetermined', 'area', 'view', 'generating', 'facial', 'area', 'subject', 'assessing', 'quality', 'facial', 'area', 'subject', 'conducing', 'incremental', 'training', 'facial', 'area', 'subject', 'determining', 'identity', 'subject', 'based', 'facial', 'area', 'subject', 'identifying', 'intent', 'subject', 'authorizing', 'access', 'point', 'entry', 'based', 'determined', 'identity', 'subject', 'based', 'intent', 'subjectdisclosed', 'herein', 'robot', 'electronic', 'acquiring', 'video', 'method', 'acquiring', 'video', 'using', 'robot', 'robot', 'includes', 'camera', 'configured', 'rotate', 'lateral', 'direction', 'tilt', 'vertical', 'direction', 'controls', 'least', 'one', 'direction', 'rotation', 'camera', 'angle', 'tilt', 'camera', 'focal', 'distance', 'camera', 'tracking', 'video', 'acquired', 'cameras', 'methods', 'disclosed', 'inferring', 'topics', 'file', 'containing', 'audio', 'video', 'example', 'multimodal', 'multimedia', 'file', 'order', 'facilitate', 'video', 'indexing', 'set', 'entities', 'extracted', 'file', 'linked', 'produce', 'graph', 'reference', 'information', 'also', 'obtained', 'set', 'entities', 'entities', 'may', 'drawn', 'example', 'wikipedia', 'categories', 'large', 'ontological', 'data', 'sources', 'analysis', 'graph', 'using', 'unsupervised', 'learning', 'permits', 'determining', 'clusters', 'graph', 'extracting', 'clusters', 'possibly', 'using', 'supervised', 'learning', 'provides', 'selection', 'topic', 'identifiers', 'topic', 'identifiers', 'used', 'indexing', 'filea', 'face', 'method', 'neural', 'network', 'training', 'method', 'apparatus', 'electronic', 'method', 'comprises', 'obtaining', 'first', 'face', 'means', 'first', 'camera', 'extracting', 'first', 'face', 'feature', 'first', 'face', 'comparing', 'first', 'face', 'feature', 'pre-stored', 'second', 'face', 'feature', 'obtain', 'reference', 'similarity', 'second', 'face', 'feature', 'obtained', 'extracting', 'feature', 'second', 'face', 'obtained', 'second', 'camera', 'second', 'camera', 'first', 'camera', 'different', 'types', 'cameras', 'determining', 'according', 'reference', 'similarity', 'whether', 'first', 'face', 'feature', 'second', 'face', 'feature', 'correspond', 'person', 'present', 'invention', 'discloses', 'technique', 'alerting', 'vision', 'impairment', 'comprises', 'processing', 'unit', 'configured', 'operable', 'receiving', 'scene', 'data', 'indicative', 'scene', 'least', 'one', 'consumer', 'environment', 'identifying', 'scene', 'data', 'certain', 'consumer', 'identifying', 'event', 'indicative', 'behavioral', 'compensation', 'vision', 'impairment', 'upon', 'identification', 'event', 'sending', 'notification', 'relating', 'vision', 'impairment']\n"
     ]
    }
   ],
   "source": [
    "# we prepare a empty list, which will contain the words after the stopwords removal\n",
    "tokenized_vector_a = []\n",
    "\n",
    "# we iterate into the list of tokens obtained through the tokenization\n",
    "for token in tokenized_text_a:\n",
    "    # if a token is not a stopword, we insert it in the list\n",
    "    if token not in stopwords_en:\n",
    "        tokenized_vector_a.append(token)\n",
    "\n",
    "# the output is a list of all the tokens of the original text excluding the stopwords\n",
    "print(tokenized_vector_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cac860e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['configured', 'make', 'screen', 'display', 'frames', 'comprising', 'capturing', 'device', 'storage', 'device', 'storing', 'modules', 'coupled', 'capturing', 'device', 'storage', 'device', 'configured', 'execute', 'modules', 'storage', 'device', 'configure', 'screen', 'display', 'marker', 'objects', 'predetermined', 'positions', 'configure', 'capturing', 'device', 'capture', 'first', 'head', 'looking', 'predetermined', 'positions', 'perform', 'first', 'face', 'operations', 'first', 'head', 'obtain', 'first', 'face', 'regions', 'corresponding', 'predetermined', 'positions', 'detect', 'first', 'facial', 'landmarks', 'corresponding', 'first', 'face', 'regions', 'calculate', 'rotation', 'reference', 'angles', 'looking', 'predetermined', 'positions', 'according', 'first', 'facial', 'landmarks', 'configure', 'capturing', 'device', 'capture', 'second', 'head', 'perform', 'second', 'head', 'obtain', 'second', 'face', 'region', 'detect', 'second', 'facial', 'landmarks', 'within', 'second', 'face', 'region', 'estimate', 'head', 'posture', 'angle', 'according', 'second', 'facial', 'landmarks', 'calculate', 'gaze', 'position', 'screen', 'according', 'head', 'posture', 'angle', 'rotation', 'reference', 'angles', 'predetermined', 'positions', 'configure', 'screen', 'display', 'corresponding', 'visual', 'effect', 'according', 'gaze', 'position', 'according', 'wherein', 'gaze', 'position', 'comprises', 'first', 'coordinate', 'value', 'first', 'axial', 'direction', 'second', 'coordinate', 'value', 'second', 'axial', 'direction', 'according', 'wherein', 'head', 'posture', 'angles', 'comprise', 'head', 'pitch', 'angle', 'head', 'yaw', 'angle', 'rotation', 'reference', 'angles', 'comprise', 'first', 'pitch', 'angle', 'second', 'pitch', 'angle', 'first', 'yaw', 'angle', 'second', 'yaw', 'angle', 'corresponding', 'predetermined', 'positions', 'according', 'wherein', 'performs', 'interpolation', 'operation', 'extrapolation', 'operation', 'according', 'first', 'yaw', 'angle', 'second', 'yaw', 'angle', 'first', 'position', 'corresponding', 'first', 'yaw', 'angle', 'among', 'predetermined', 'positions', 'second', 'position', 'corresponding', 'second', 'yaw', 'angle', 'among', 'predetermined', 'positions', 'head', 'yaw', 'angle', 'thereby', 'obtaining', 'first', 'coordinate', 'value', 'gaze', 'position', 'performs', 'interpolation', 'operation', 'extrapolation', 'operation', 'according', 'first', 'pitch', 'angle', 'second', 'pitch', 'angle', 'third', 'position', 'corresponding', 'first', 'pitch', 'angle', 'among', 'predetermined', 'positions', 'fourth', 'position', 'corresponding', 'second', 'pitch', 'angle', 'among', 'predetermined', 'positions', 'head', 'pitch', 'angle', 'thereby', 'obtaining', 'second', 'coordinate', 'value', 'gaze', 'position', 'according', 'wherein', 'calculates', 'first', 'viewing', 'distances', 'screen', 'according', 'first', 'facial', 'landmarks', 'estimates', 'second', 'viewing', 'distance', 'screen', 'according', 'second', 'facial', 'landmarks', 'adjusts', 'rotation', 'reference', 'angles', 'gaze', 'position', 'according', 'second', 'viewing', 'distance', 'first', 'viewing', 'distances', 'according', 'wherein', 'maps', 'two-dimensional', 'position', 'coordinates', 'second', 'facial', 'landmarks', 'plane', 'coordinate', 'system', 'three-dimensional', 'position', 'coordinates', 'three-dimensional', 'coordinate', 'system', 'estimates', 'head', 'posture', 'angle', 'according', 'three-dimensional', 'position', 'coordinates', 'second', 'facial', 'landmarks', 'according', 'wherein', 'second', 'head', 'comprises', 'wearable', 'device', 'second', 'facial', 'landmarks', 'comprise', 'third', 'facial', 'landmarks', 'covered', 'wearable', 'device', 'according', 'wherein', 'second', 'head', 'comprises', 'wearable', 'device', 'second', 'facial', 'landmarks', 'comprise', 'one', 'simulated', 'landmarks', 'marked', 'wearable', 'device', 'operating', 'adapted', 'comprising', 'capturing', 'device', 'making', 'screen', 'display', 'frames', 'comprising', 'configuring', 'screen', 'display', 'marker', 'objects', 'predetermined', 'positions', 'configuring', 'capturing', 'device', 'capture', 'first', 'head', 'looking', 'predetermined', 'positions', 'performing', 'first', 'face', 'operations', 'first', 'head', 'obtain', 'first', 'face', 'regions', 'corresponding', 'predetermined', 'positions', 'detecting', 'first', 'facial', 'landmarks', 'corresponding', 'first', 'face', 'regions', 'calculating', 'rotation', 'reference', 'angles', 'looking', 'predetermined', 'positions', 'according', 'first', 'facial', 'landmarks', 'configuring', 'capturing', 'device', 'capture', 'second', 'head', 'performing', 'second', 'head', 'obtain', 'second', 'face', 'region', 'detecting', 'second', 'facial', 'landmarks', 'within', 'second', 'face', 'region', 'estimating', 'head', 'posture', 'angle', 'according', 'second', 'facial', 'landmarks', 'calculating', 'gaze', 'position', 'screen', 'according', 'head', 'posture', 'angle', 'rotation', 'reference', 'angles', 'predetermined', 'positions', 'configuring', 'screen', 'display', 'corresponding', 'visual', 'effect', 'according', 'gaze', 'position', 'operation', 'according', 'wherein', 'gaze', 'position', 'comprises', 'first', 'coordinate', 'value', 'first', 'axial', 'direction', 'second', 'coordinate', 'value', 'second', 'axial', 'direction', 'operation', 'according', 'wherein', 'head', 'posture', 'angles', 'comprise', 'head', 'pitch', 'angle', 'head', 'yaw', 'angle', 'rotation', 'reference', 'angles', 'comprise', 'first', 'pitch', 'angle', 'second', 'pitch', 'angle', 'first', 'yaw', 'angle', 'second', 'yaw', 'angle', 'corresponding', 'predetermined', 'positions', 'operation', 'according', 'wherein', 'step', 'calculating', 'gaze', 'position', 'screen', 'according', 'head', 'posture', 'angle', 'rotation', 'reference', 'angles', 'predetermined', 'positions', 'comprises', 'performing', 'interpolation', 'operation', 'extrapolation', 'operation', 'according', 'first', 'yaw', 'angle', 'second', 'yaw', 'angle', 'first', 'position', 'corresponding', 'first', 'yaw', 'angle', 'among', 'predetermined', 'positions', 'second', 'position', 'corresponding', 'second', 'yaw', 'angle', 'among', 'predetermined', 'positions', 'head', 'yaw', 'angle', 'thereby', 'obtaining', 'first', 'coordinate', 'value', 'gaze', 'position', 'performing', 'interpolation', 'operation', 'extrapolation', 'operation', 'according', 'first', 'pitch', 'angle', 'second', 'pitch', 'angle', 'third', 'position', 'corresponding', 'first', 'pitch', 'angle', 'among', 'predetermined', 'positions', 'fourth', 'position', 'corresponding', 'second', 'pitch', 'angle', 'among', 'predetermined', 'positions', 'head', 'pitch', 'angle', 'thereby', 'obtaining', 'second', 'coordinate', 'value', 'gaze', 'position', 'operation', 'according', 'wherein', 'comprises', 'calculating', 'first', 'viewing', 'distances', 'screen', 'according', 'first', 'facial', 'landmarks', 'estimating', 'second', 'viewing', 'distance', 'screen', 'according', 'second', 'facial', 'landmarks', 'adjusting', 'rotation', 'reference', 'angles', 'gaze', 'position', 'according', 'second', 'viewing', 'distance', 'first', 'viewing', 'distances', 'operation', 'according', 'wherein', 'comprises', 'mapping', 'two-dimensional', 'position', 'coordinates', 'second', 'facial', 'landmarks', 'plane', 'coordinate', 'system', 'three-dimensional', 'position', 'coordinates', 'three-dimensional', 'coordinate', 'system', 'estimating', 'head', 'posture', 'angle', 'according', 'three-dimensional', 'position', 'coordinates', 'second', 'facial', 'landmarks', 'operation', 'according', 'wherein', 'second', 'head', 'comprises', 'wearable', 'device', 'second', 'facial', 'landmarks', 'comprise', 'third', 'facial', 'landmarks', 'covered', 'wearable', 'device', 'operation', 'according', 'wherein', 'second', 'head', 'comprises', 'wearable', 'device', 'second', 'facial', 'landmarks', 'comprise', 'one', 'simulated', 'landmarks', 'marked', 'wearable', 'device', 'computation', 'applied', 'computing', 'system', 'wherein', 'computing', 'system', 'comprises', 'control', 'unit', 'computation', 'group', 'general', 'storage', 'unit', 'wherein', 'control', 'unit', 'comprises', 'first', 'memory', 'decoding', 'logic', 'controller', 'wherein', 'computation', 'group', 'comprises', 'group', 'controller', 'computing', 'units', 'general', 'storage', 'unit', 'configured', 'store', 'data', 'computation', 'comprises', 'receiving', 'controller', 'first', 'level', 'instruction', 'sequence', 'partitioning', 'decoding', 'logic', 'first', 'level', 'instruction', 'sequence', 'second', 'level', 'instruction', 'sequences', 'creating', 'controller', 'threads', 'second', 'level', 'instruction', 'sequences', 'allocating', 'controller', 'independent', 'register', 'well', 'configuring', 'independent', 'addressing', 'function', 'thread', 'threads', 'wherein', 'integer', 'greater', 'equal', 'obtaining', 'group', 'controller', 'computation', 'types', 'second', 'level', 'instruction', 'sequences', 'obtaining', 'corresponding', 'fusion', 'computation', 'manner', 'computation', 'types', 'according', 'computation', 'types', 'adopting', 'computing', 'units', 'fusion', 'computation', 'manner', 'call', 'threads', 'performing', 'computations', 'second', 'level', 'instruction', 'sequences', 'obtain', 'final', 'result', 'wherein', 'obtaining', 'group', 'controller', 'computation', 'types', 'second', 'level', 'instruction', 'sequences', 'obtaining', 'corresponding', 'fusion', 'computation', 'manner', 'computation', 'types', 'according', 'computation', 'types', 'adopting', 'computing', 'units', 'fusion', 'computation', 'manner', 'call', 'threads', 'performing', 'computations', 'second', 'instruction', 'sequences', 'obtain', 'final', 'result', 'computation', 'types', 'represent', 'computation', 'operations', 'type', 'group', 'controller', 'calls', 'combined', 'computation', 'manner', 'single', 'instruction', 'multiple', 'data', 'type', 'combination', 'single', 'instruction', 'multiple', 'threads', 'uses', 'threads', 'perform', 'combined', 'computation', 'manner', 'obtain', 'final', 'result', 'includes', 'partitioning', 'decoding', 'logic', 'threads', 'n', 'wraps', 'allocating', 'computing', 'units', 'converting', 'group', 'controller', 'second', 'instruction', 'sequences', 'second', 'control', 'signals', 'sending', 'second', 'control', 'signals', 'computing', 'units', 'calling', 'computing', 'units', 'wraps', 'allocated', 'computing', 'units', 'second', 'control', 'signals', 'fetch', 'corresponding', 'data', 'according', 'independent', 'addressing', 'function', 'performing', 'computing', 'units', 'computations', 'data', 'obtain', 'intermediate', 'results', 'splicing', 'intermediate', 'results', 'obtain', 'final', 'result', 'wherein', 'obtaining', 'group', 'controller', 'computation', 'types', 'second', 'level', 'instruction', 'sequences', 'obtaining', 'corresponding', 'fusion', 'computation', 'manner', 'computation', 'types', 'according', 'computation', 'types', 'adopting', 'computing', 'units', 'fusion', 'computation', 'manner', 'call', 'threads', 'performing', 'computations', 'second', 'instruction', 'sequences', 'obtain', 'final', 'result', 'computation', 'types', 'represent', 'computation', 'operations', 'different', 'types', 'group', 'controller', 'calls', 'simultaneous', 'multi-threading', 'threads', 'perform', 'computations', 'obtain', 'final', 'result', 'includes', 'partitioning', 'decoding', 'logic', 'threads', 'n', 'wraps', 'converting', 'second', 'instruction', 'sequences', 'second', 'control', 'signals', 'obtaining', 'group', 'controller', 'computation', 'types', 'supported', 'computing', 'units', 'allocating', 'controller', 'n', 'wraps', 'second', 'control', 'signals', 'corresponding', 'computing', 'units', 'support', 'computation', 'types', 'wraps', 'second', 'control', 'signals', 'calling', 'computing', 'units', 'wraps', 'allocated', 'computing', 'units', 'second', 'control', 'signals', 'fetching', 'computing', 'units', 'corresponding', 'data', 'performing', 'computing', 'units', 'computations', 'data', 'obtain', 'intermediate', 'results', 'splicing', 'intermediate', 'results', 'obtain', 'final', 'result', 'comprising', 'wrap', 'wraps', 'blocked', 'adding', 'wrap', 'waiting', 'queue', 'data', 'wrap', 'already', 'fetched', 'adding', 'wrap', 'preparation', 'queue', 'wherein', 'preparation', 'queue', 'queue', 'wrap', 'scheduled', 'executing', 'located', 'computing', 'resource', 'idle', 'wherein', 'first', 'level', 'instruction', 'sequence', 'includes', 'long', 'instruction', 'second', 'level', 'instruction', 'sequence', 'includes', 'instruction', 'sequence', 'wherein', 'computing', 'system', 'includes', 'tree', 'module', 'wherein', 'tree', 'module', 'includes', 'root', 'port', 'branch', 'ports', 'wherein', 'root', 'port', 'tree', 'module', 'connected', 'group', 'controller', 'branch', 'ports', 'tree', 'module', 'connected', 'computing', 'unit', 'computing', 'units', 'respectively', 'tree', 'module', 'configured', 'forward', 'data', 'blocks', 'wraps', 'instruction', 'sequences', 'group', 'controller', 'computing', 'units', 'wherein', 'tree', 'module', 'n-ary', 'tree', 'wherein', 'n', 'integer', 'greater', 'equal', 'wherein', 'computing', 'system', 'includes', 'branch', 'processing', 'circuit', 'wherein', 'branch', 'processing', 'circuit', 'connected', 'group', 'controller', 'computing', 'units', 'branch', 'processing', 'circuit', 'configured', 'forward', 'data', 'wraps', 'instruction', 'sequences', 'group', 'controller', 'computing', 'units', 'computing', 'system', 'comprising', 'control', 'unit', 'computation', 'group', 'general', 'storage', 'unit', 'wherein', 'control', 'unit', 'includes', 'first', 'memory', 'decoding', 'logic', 'controller', 'computation', 'group', 'includes', 'group', 'controller', 'computing', 'units', 'general', 'storage', 'unit', 'configured', 'store', 'data', 'controller', 'configured', 'receive', 'first', 'level', 'instruction', 'sequence', 'control', 'first', 'memory', 'decoding', 'logic', 'decoding', 'logic', 'configured', 'partition', 'first', 'level', 'instruction', 'sequence', 'second', 'level', 'instruction', 'sequences', 'controller', 'configured', 'create', 'threads', 'second', 'level', 'instruction', 'sequences', 'allocate', 'independent', 'register', 'configure', 'independent', 'addressing', 'function', 'thread', 'threads', 'integer', 'greater', 'equal', 'controller', 'configured', 'convert', 'second', 'instruction', 'sequences', 'control', 'signals', 'sending', 'group', 'controller', 'group', 'controller', 'configured', 'receive', 'control', 'signals', 'obtain', 'computational', 'types', 'control', 'signals', 'divide', 'threads', 'n', 'wraps', 'allocate', 'n', 'wraps', 'control', 'signals', 'computing', 'units', 'according', 'computational', 'types', 'computing', 'units', 'configured', 'fetch', 'data', 'general', 'storage', 'unit', 'allocated', 'wraps', 'control', 'signals', 'perform', 'computations', 'obtain', 'intermediate', 'result', 'group', 'controller', 'configured', 'splice', 'intermediate', 'results', 'obtain', 'final', 'computation', 'result', 'computing', 'system', 'wherein', 'computing', 'units', 'includes', 'addition', 'computing', 'unit', 'multiplication', 'computing', 'unit', 'activation', 'computing', 'unit', 'dedicated', 'computing', 'unit', 'computing', 'system', 'wherein', 'dedicated', 'computing', 'unit', 'includes', 'face', 'computing', 'unit', 'graphics', 'computing', 'unit', 'fingerprint', 'computing', 'unit', 'neural', 'network', 'computing', 'unit', 'computing', 'system', 'wherein', 'group', 'controller', 'configured', 'computation', 'types', 'control', 'signals', 'graphics', 'computations', 'fingerprint', 'identification', 'face', 'neural', 'network', 'operations', 'allocate', 'control', 'signals', 'face', 'computing', 'unit', 'graphics', 'computing', 'unit', 'fingerprint', 'computing', 'unit', 'neural', 'network', 'computing', 'unit', 'respectively', 'computing', 'system', 'wherein', 'first', 'level', 'instruction', 'sequence', 'includes', 'long', 'instruction', 'second', 'level', 'instruction', 'sequence', 'includes', 'instruction', 'sequence', 'computing', 'system', 'comprising', 'tree', 'module', 'wherein', 'tree', 'module', 'includes', 'root', 'port', 'branch', 'ports', 'wherein', 'root', 'port', 'tree', 'module', 'connected', 'group', 'controller', 'branch', 'ports', 'tree', 'module', 'connected', 'computing', 'unit', 'computing', 'units', 'respectively', 'tree', 'module', 'configured', 'forward', 'data', 'blocks', 'wraps', 'instruction', 'sequences', 'group', 'controller', 'computing', 'units', 'computing', 'system', 'wherein', 'tree', 'module', 'n-ary', 'tree', 'wherein', 'n', 'integer', 'greater', 'equal', 'computing', 'system', 'wherein', 'computing', 'system', 'includes', 'branch', 'processing', 'circuit', 'branch', 'processing', 'circuit', 'connected', 'group', 'controller', 'computing', 'units', 'branch', 'processing', 'circuit', 'configured', 'forward', 'data', 'wraps', 'instruction', 'sequences', 'group', 'controller', 'computing', 'units', 'computer', 'program', 'product', 'comprising', 'non-instant', 'computer', 'readable', 'storage', 'medium', 'wherein', 'computer', 'program', 'stored', 'non-instant', 'computer', 'readable', 'storage', 'medium', 'computer', 'program', 'capable', 'causing', 'computer', 'perform', '-', 'operations', 'detecting', 'body', 'information', 'one', 'passengers', 'vehicle', 'based', 'humans', \"'\", 'status', 'comprising', 'steps', 'least', 'one', 'interior', 'interior', 'vehicle', 'acquired', 'passenger', 'body', 'information-detecting', 'device', 'performing', 'process', 'inputting', 'interior', 'face', 'network', 'thereby', 'allow', 'face', 'network', 'detect', 'passengers', 'interior', 'thus', 'output', 'multiple', 'pieces', 'passenger', 'feature', 'information', 'corresponding', 'detected', 'ii', 'process', 'inputting', 'interior', 'body', 'network', 'thereby', 'allow', 'body', 'network', 'detect', 'bodies', 'passengers', 'interior', 'thus', 'output', 'body-part', 'length', 'information', 'detected', 'bodies', 'b', 'passenger', 'body', 'information-detecting', 'device', 'performing', 'process', 'retrieving', 'specific', 'height', 'mapping', 'information', 'corresponding', 'specific', 'passenger', 'feature', 'information', 'specific', 'passenger', 'height', 'mapping', 'table', 'stores', 'height', 'mapping', 'information', 'representing', 'respective', 'one', 'predetermined', 'ratios', 'one', 'segment', 'body', 'portions', 'human', 'groups', 'heights', 'per', 'human', 'groups', 'process', 'acquiring', 'specific', 'height', 'specific', 'passenger', 'specific', 'height', 'mapping', 'information', 'referring', 'specific', 'body-part', 'length', 'information', 'specific', 'passenger', 'process', 'retrieving', 'specific', 'weight', 'mapping', 'information', 'corresponding', 'specific', 'passenger', 'feature', 'information', 'weight', 'mapping', 'table', 'stores', 'multiple', 'pieces', 'weight', 'mapping', 'information', 'representing', 'predetermined', 'correlations', 'heights', 'weights', 'per', 'human', 'groups', 'process', 'acquiring', 'weight', 'specific', 'passenger', 'specific', 'weight', 'mapping', 'information', 'referring', 'specific', 'height', 'specific', 'passenger', 'wherein', 'step', 'passenger', 'body', 'information-detecting', 'device', 'performs', 'process', 'inputting', 'interior', 'body', 'network', 'thereby', 'allow', 'body', 'network', 'output', 'one', 'one', 'channels', 'corresponding', 'interior', 'via', 'feature', 'extraction', 'network', 'ii', 'generate', 'least', 'one', 'keypoint', 'heatmap', 'least', 'one', 'part', 'affinity', 'field', 'one', 'channels', 'corresponding', 'via', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'iii', 'extract', 'keypoints', 'keypoint', 'heatmap', 'via', 'keypoint', 'detector', 'group', 'extracted', 'keypoints', 'referring', 'part', 'affinity', 'field', 'thus', 'generate', 'body', 'parts', 'per', 'passengers', 'result', 'allow', 'body', 'network', 'output', 'multiple', 'pieces', 'body-part', 'length', 'information', 'passengers', 'referring', 'body', 'parts', 'per', 'passengers', 'wherein', 'feature', 'extraction', 'network', 'includes', 'least', 'one', 'convolutional', 'layer', 'applies', 'least', 'one', 'convolution', 'operation', 'interior', 'thereby', 'output', 'wherein', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'includes', 'one', 'fully', 'convolutional', 'network', '×', 'convolutional', 'layer', 'applies', 'fully-convolution', 'operation', '×', 'convolution', 'operation', 'thereby', 'generate', 'keypoint', 'heatmap', 'part', 'affinity', 'field', 'wherein', 'keypoint', 'detector', 'connects', 'referring', 'part', 'affinity', 'field', 'pairs', 'respectively', 'highest', 'mutual', 'connection', 'probabilities', 'connected', 'among', 'extracted', 'keypoints', 'thereby', 'group', 'extracted', 'keypoints', 'wherein', 'feature', 'extraction', 'network', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'learned', 'learning', 'device', 'performing', 'process', 'inputting', 'least', 'one', 'training', 'including', 'one', 'objects', 'training', 'feature', 'extraction', 'network', 'thereby', 'allow', 'feature', 'extraction', 'network', 'generate', 'one', 'training', 'one', 'channels', 'applying', 'least', 'one', 'convolutional', 'operation', 'training', 'ii', 'process', 'inputting', 'training', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'thereby', 'allow', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'generate', 'one', 'keypoint', 'heatmaps', 'training', 'one', 'part', 'affinity', 'fields', 'training', 'one', 'channels', 'training', 'iii', 'process', 'inputting', 'keypoint', 'heatmaps', 'training', 'part', 'affinity', 'fields', 'training', 'keypoint', 'detector', 'thereby', 'allow', 'keypoint', 'detector', 'extract', 'keypoints', 'training', 'keypoint', 'heatmaps', 'training', 'process', 'grouping', 'extracted', 'keypoints', 'training', 'referring', 'part', 'affinity', 'fields', 'training', 'thereby', 'detect', 'keypoints', 'per', 'objects', 'training', 'iv', 'process', 'allowing', 'loss', 'layer', 'calculate', 'one', 'losses', 'referring', 'keypoints', 'per', 'objects', 'training', 'corresponding', 'ground', 'truths', 'thereby', 'adjust', 'one', 'parameters', 'feature', 'extraction', 'network', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'losses', 'minimized', 'backpropagation', 'using', 'losses', 'wherein', 'step', 'passenger', 'body', 'information-detecting', 'device', 'performs', 'process', 'inputting', 'interior', 'face', 'network', 'thereby', 'allow', 'face', 'network', 'detect', 'passengers', 'located', 'interior', 'via', 'face', 'detector', 'output', 'multiple', 'pieces', 'passenger', 'feature', 'information', 'facial', 'via', 'facial', 'feature', 'classifier', 'wherein', 'step', 'passenger', 'body', 'information-detecting', 'device', 'performs', 'process', 'inputting', 'interior', 'face', 'network', 'thereby', 'allow', 'face', 'network', 'apply', 'least', 'one', 'convolution', 'operation', 'interior', 'thus', 'output', 'least', 'one', 'feature', 'map', 'corresponding', 'interior', 'via', 'least', 'one', 'convolutional', 'layer', 'ii', 'output', 'one', 'proposal', 'boxes', 'passengers', 'estimated', 'located', 'feature', 'map', 'via', 'region', 'proposal', 'network', 'iii', 'apply', 'pooling', 'operation', 'one', 'regions', 'corresponding', 'proposal', 'boxes', 'feature', 'map', 'thus', 'output', 'least', 'one', 'feature', 'vector', 'via', 'pooling', 'layer', 'iv', 'apply', 'fully-connected', 'operation', 'feature', 'vector', 'thus', 'output', 'multiple', 'pieces', 'passenger', 'feature', 'information', 'corresponding', 'passengers', 'corresponding', 'proposal', 'boxes', 'via', 'fully', 'connected', 'layer', 'wherein', 'multiple', 'pieces', 'passenger', 'feature', 'information', 'include', 'ages', 'genders', 'races', 'corresponding', 'passengers', 'passenger', 'body', 'information-detecting', 'device', 'detecting', 'body', 'information', 'one', 'passengers', 'vehicle', 'based', 'humans', \"'\", 'status', 'comprising', 'least', 'one', 'memory', 'stores', 'instructions', 'least', 'one', 'configured', 'execute', 'instructions', 'perform', 'support', 'another', 'device', 'perform', 'least', 'one', 'interior', 'interior', 'vehicle', 'acquired', 'process', 'inputting', 'interior', 'face', 'network', 'thereby', 'allow', 'face', 'network', 'detect', 'passengers', 'interior', 'thus', 'output', 'multiple', 'pieces', 'passenger', 'feature', 'information', 'corresponding', 'detected', 'ii', 'process', 'inputting', 'interior', 'body', 'network', 'thereby', 'allow', 'body', 'network', 'detect', 'bodies', 'passengers', 'interior', 'thus', 'output', 'body-part', 'length', 'information', 'detected', 'bodies', 'ii', 'process', 'retrieving', 'specific', 'height', 'mapping', 'information', 'corresponding', 'specific', 'passenger', 'feature', 'information', 'specific', 'passenger', 'height', 'mapping', 'table', 'stores', 'height', 'mapping', 'information', 'representing', 'respective', 'one', 'predetermined', 'ratios', 'one', 'segment', 'body', 'portions', 'human', 'groups', 'heights', 'per', 'human', 'groups', 'process', 'acquiring', 'specific', 'height', 'specific', 'passenger', 'specific', 'height', 'mapping', 'information', 'referring', 'specific', 'body-part', 'length', 'information', 'specific', 'passenger', 'process', 'retrieving', 'specific', 'weight', 'mapping', 'information', 'corresponding', 'specific', 'passenger', 'feature', 'information', 'weight', 'mapping', 'table', 'stores', 'multiple', 'pieces', 'weight', 'mapping', 'information', 'representing', 'predetermined', 'correlations', 'heights', 'weights', 'per', 'human', 'groups', 'process', 'acquiring', 'weight', 'specific', 'passenger', 'specific', 'weight', 'mapping', 'information', 'referring', 'specific', 'height', 'specific', 'passenger', 'passenger', 'body', 'information-detecting', 'device', 'wherein', 'process', 'performs', 'process', 'inputting', 'interior', 'body', 'network', 'thereby', 'allow', 'body', 'network', 'output', 'one', 'one', 'channels', 'corresponding', 'interior', 'via', 'feature', 'extraction', 'network', 'ii', 'generate', 'least', 'one', 'keypoint', 'heatmap', 'least', 'one', 'part', 'affinity', 'field', 'one', 'channels', 'corresponding', 'via', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'iii', 'extract', 'keypoints', 'keypoint', 'heatmap', 'via', 'keypoint', 'detector', 'group', 'extracted', 'keypoints', 'referring', 'part', 'affinity', 'field', 'thus', 'generate', 'body', 'parts', 'per', 'passengers', 'result', 'allow', 'body', 'network', 'output', 'multiple', 'pieces', 'body-part', 'length', 'information', 'passengers', 'referring', 'body', 'parts', 'per', 'passengers', 'passenger', 'body', 'information-detecting', 'device', 'wherein', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'includes', 'one', 'fully', 'convolutional', 'network', '×', 'convolutional', 'layer', 'applies', 'fully-convolution', 'operation', '×', 'convolution', 'operation', 'thereby', 'generate', 'keypoint', 'heatmap', 'part', 'affinity', 'field', 'passenger', 'body', 'information-detecting', 'device', 'wherein', 'keypoint', 'detector', 'connects', 'referring', 'part', 'affinity', 'field', 'pairs', 'respectively', 'highest', 'mutual', 'connection', 'probabilities', 'connected', 'among', 'extracted', 'keypoints', 'thereby', 'group', 'extracted', 'keypoints', 'passenger', 'body', 'information-detecting', 'device', 'wherein', 'feature', 'extraction', 'network', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'learned', 'learning', 'device', 'performing', 'process', 'inputting', 'least', 'one', 'training', 'including', 'one', 'objects', 'training', 'feature', 'extraction', 'network', 'thereby', 'allow', 'feature', 'extraction', 'network', 'generate', 'one', 'training', 'one', 'channels', 'applying', 'least', 'one', 'convolutional', 'operation', 'training', 'ii', 'process', 'inputting', 'training', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'thereby', 'allow', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'generate', 'one', 'keypoint', 'heatmaps', 'training', 'one', 'part', 'affinity', 'fields', 'training', 'one', 'channels', 'training', 'iii', 'process', 'inputting', 'keypoint', 'heatmaps', 'training', 'part', 'affinity', 'fields', 'training', 'keypoint', 'detector', 'thereby', 'allow', 'keypoint', 'detector', 'extract', 'keypoints', 'training', 'keypoint', 'heatmaps', 'training', 'process', 'grouping', 'extracted', 'keypoints', 'training', 'referring', 'part', 'affinity', 'fields', 'training', 'thereby', 'detect', 'keypoints', 'per', 'objects', 'training', 'iv', 'process', 'allowing', 'loss', 'layer', 'calculate', 'one', 'losses', 'referring', 'keypoints', 'per', 'objects', 'training', 'corresponding', 'ground', 'truths', 'thereby', 'adjust', 'one', 'parameters', 'feature', 'extraction', 'network', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'losses', 'minimized', 'backpropagation', 'using', 'losses', 'passenger', 'body', 'information-detecting', 'device', 'wherein', 'process', 'performs', 'process', 'inputting', 'interior', 'face', 'network', 'thereby', 'allow', 'face', 'network', 'apply', 'least', 'one', 'convolution', 'operation', 'interior', 'thus', 'output', 'least', 'one', 'feature', 'map', 'corresponding', 'interior', 'via', 'least', 'one', 'convolutional', 'layer', 'ii', 'output', 'one', 'proposal', 'boxes', 'passengers', 'estimated', 'located', 'feature', 'map', 'via', 'region', 'proposal', 'network', 'iii', 'apply', 'pooling', 'operation', 'one', 'regions', 'corresponding', 'proposal', 'boxes', 'feature', 'map', 'thus', 'output', 'least', 'one', 'feature', 'vector', 'via', 'pooling', 'layer', 'iv', 'apply', 'fully-connected', 'operation', 'feature', 'vector', 'thus', 'output', 'multiple', 'pieces', 'passenger', 'feature', 'information', 'corresponding', 'passengers', 'corresponding', 'proposal', 'boxes', 'via', 'fully', 'connected', 'layer', 'computer', 'implemented', 'performing', 'video', 'coding', 'based', 'face', 'detection', 'comprising', 'receiving', 'video', 'frame', 'comprising', 'one', 'video', 'frames', 'video', 'sequence', 'determining', 'video', 'frame', 'key', 'frame', 'video', 'sequence', 'performing', 'response', 'video', 'frame', 'key', 'frame', 'video', 'sequence', 'multi-stage', 'facial', 'search', 'video', 'frame', 'based', 'predetermined', 'feature', 'templates', 'predetermined', 'number', 'stages', 'determine', 'first', 'candidate', 'face', 'region', 'second', 'candidate', 'face', 'region', 'video', 'frame', 'testing', 'first', 'second', 'candidate', 'face', 'regions', 'based', 'skin', 'tone', 'information', 'determine', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'second', 'candidate', 'face', 'region', 'invalid', 'face', 'region', 'rejecting', 'second', 'candidate', 'face', 'region', 'outputting', 'first', 'candidate', 'face', 'region', 'encoding', 'video', 'frame', 'based', 'least', 'part', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'generate', 'coded', 'bitstream', 'wherein', 'skin', 'tone', 'information', 'comprises', 'skin', 'probability', 'map', 'wherein', 'said', 'testing', 'first', 'second', 'candidate', 'face', 'regions', 'based', 'skin', 'tone', 'information', 'performed', 'response', 'video', 'frame', 'key', 'frame', 'video', 'sequence', 'wherein', 'first', 'candidate', 'face', 'region', 'comprises', 'rectangular', 'region', 'comprising', 'determining', 'free', 'form', 'shape', 'face', 'region', 'corresponding', 'first', 'candidate', 'face', 'region', 'wherein', 'free', 'form', 'shape', 'face', 'region', 'least', 'one', 'pixel', 'accuracy', 'small', 'block', 'pixels', 'accuracy', 'wherein', 'determining', 'free', 'form', 'shape', 'face', 'region', 'comprises', 'generating', 'enhanced', 'skip', 'probability', 'map', 'corresponding', 'first', 'candidate', 'face', 'region', 'binarizing', 'enhanced', 'skip', 'probability', 'map', 'overlaying', 'binarized', 'enhanced', 'skip', 'probability', 'map', 'least', 'portion', 'video', 'frame', 'provide', 'free', 'form', 'shape', 'face', 'region', 'wherein', 'second', 'video', 'frame', 'comprises', 'non-key', 'frame', 'video', 'sequence', 'comprising', 'performing', 'face', 'detection', 'second', 'video', 'frame', 'video', 'sequence', 'based', 'free', 'form', 'shape', 'face', 'region', 'comprising', 'second', 'free', 'form', 'shape', 'face', 'region', 'second', 'video', 'frame', 'based', 'free', 'form', 'shape', 'face', 'region', 'video', 'frame', 'wherein', 'second', 'free', 'form', 'shape', 'face', 'region', 'comprises', 'determining', 'location', 'second', 'valid', 'face', 'region', 'second', 'video', 'frame', 'based', 'displacement', 'offset', 'respect', 'first', 'candidate', 'face', 'region', 'comprising', 'determining', 'displacement', 'offset', 'based', 'offset', 'centroid', 'bounding', 'box', 'around', 'skin', 'enhanced', 'region', 'corresponding', 'first', 'candidate', 'face', 'region', 'second', 'centroid', 'second', 'bounding', 'box', 'around', 'second', 'skin', 'enhanced', 'region', 'second', 'video', 'frame', 'wherein', 'encoding', 'video', 'frame', 'based', 'least', 'part', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'comprises', 'least', 'one', 'reducing', 'quantization', 'parameter', 'corresponding', 'first', 'candidate', 'face', 'region', 'adjusting', 'lambda', 'value', 'first', 'candidate', 'face', 'region', 'disabling', 'skip', 'coding', 'first', 'candidate', 'face', 'region', 'wherein', 'bitstream', 'comprises', 'least', 'one', 'hadvanced', 'video', 'coding', 'avc', 'compliant', 'bitstream', 'hhigh', 'efficiency', 'video', 'coding', 'hevc', 'compliant', 'bitstream', 'vp', 'compliant', 'bitstream', 'vp', 'compliant', 'bitstream', 'alliance', 'open', 'media', 'aom', 'av', 'compliant', 'bitstream', 'computer', 'implemented', 'performing', 'face', 'detection', 'comprising', 'receiving', 'video', 'frame', 'sequence', 'video', 'frames', 'performing', 'multi-stage', 'facial', 'search', 'video', 'frame', 'based', 'predetermined', 'feature', 'templates', 'predetermined', 'number', 'stages', 'determine', 'first', 'candidate', 'face', 'region', 'second', 'candidate', 'face', 'region', 'video', 'frame', 'testing', 'first', 'second', 'candidate', 'face', 'regions', 'based', 'skin', 'tone', 'information', 'determine', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'second', 'candidate', 'face', 'region', 'invalid', 'face', 'region', 'rejecting', 'second', 'candidate', 'face', 'region', 'outputting', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'processing', 'providing', 'index', 'indicative', 'person', 'present', 'video', 'frame', 'based', 'valid', 'face', 'region', 'wherein', 'sequence', 'video', 'frames', 'comprises', 'sequence', 'surveillance', 'video', 'frames', 'comprising', 'performing', 'face', 'surveillance', 'video', 'frames', 'based', 'valid', 'face', 'region', 'wherein', 'sequence', 'video', 'frames', 'comprises', 'sequence', 'decoded', 'video', 'frames', 'comprising', 'adding', 'marker', 'corresponding', 'received', 'video', 'frame', 'perform', 'face', 'received', 'video', 'frame', 'based', 'valid', 'face', 'region', 'wherein', 'sequence', 'video', 'frames', 'received', 'device', 'login', 'attempt', 'comprising', 'performing', 'face', 'based', 'valid', 'face', 'region', 'allowing', 'access', 'device', 'secured', 'face', 'recognized', 'wherein', 'sequence', 'video', 'frames', 'comprises', 'sequence', 'videoconferencing', 'frames', 'comprising', 'encoding', 'video', 'frame', 'based', 'least', 'part', 'valid', 'face', 'region', 'generate', 'coded', 'bitstream', 'wherein', 'encoding', 'video', 'frame', 'comprises', 'encoding', 'background', 'region', 'video', 'frame', 'bitstream', 'comprising', 'encoding', 'video', 'frame', 'based', 'least', 'part', 'valid', 'face', 'region', 'generate', 'coded', 'bitstream', 'wherein', 'encoding', 'video', 'frame', 'comprises', 'including', 'metadata', 'corresponding', 'valid', 'face', 'region', 'bitstream', 'comprising', 'decoding', 'coded', 'bitstream', 'generate', 'decoded', 'video', 'frame', 'determine', 'metadata', 'corresponding', 'valid', 'face', 'region', 'bitstream', 'comprising', 'least', 'one', 'replacing', 'valid', 'face', 'region', 'based', 'decoded', 'metadata', 'cropping', 'displaying', 'data', 'corresponding', 'valid', 'face', 'region', 'based', 'decoded', 'metadata', 'indexing', 'decoded', 'video', 'frame', 'based', 'decoded', 'metadata', 'system', 'performing', 'video', 'coding', 'based', 'face', 'detection', 'comprising', 'memory', 'configured', 'store', 'video', 'frame', 'comprising', 'one', 'video', 'frames', 'video', 'sequence', 'coupled', 'memory', 'receive', 'video', 'frame', 'determine', 'video', 'frame', 'key', 'frame', 'video', 'sequence', 'perform', 'response', 'video', 'frame', 'key', 'frame', 'video', 'sequence', 'multi-stage', 'facial', 'search', 'video', 'frame', 'based', 'predetermined', 'feature', 'templates', 'predetermined', 'number', 'stages', 'determine', 'first', 'candidate', 'face', 'region', 'second', 'candidate', 'face', 'region', 'video', 'frame', 'test', 'first', 'second', 'candidate', 'face', 'regions', 'based', 'skin', 'tone', 'information', 'determine', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'second', 'candidate', 'face', 'region', 'invalid', 'face', 'region', 'reject', 'second', 'candidate', 'face', 'region', 'outputting', 'first', 'candidate', 'face', 'region', 'encode', 'video', 'frame', 'based', 'least', 'part', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'generate', 'coded', 'bitstream', 'system', 'wherein', 'skin', 'tone', 'information', 'comprises', 'skin', 'probability', 'map', 'system', 'wherein', 'first', 'candidate', 'face', 'region', 'comprises', 'rectangular', 'region', 'determine', 'free', 'form', 'shape', 'face', 'region', 'corresponding', 'first', 'candidate', 'face', 'region', 'wherein', 'free', 'form', 'shape', 'face', 'region', 'least', 'one', 'pixel', 'accuracy', 'small', 'block', 'pixels', 'accuracy', 'system', 'wherein', 'determine', 'free', 'form', 'shape', 'face', 'region', 'comprises', 'generate', 'enhanced', 'skip', 'probability', 'map', 'corresponding', 'first', 'candidate', 'face', 'region', 'binarize', 'enhanced', 'skip', 'probability', 'map', 'overlay', 'binarized', 'enhanced', 'skip', 'probability', 'map', 'least', 'portion', 'video', 'frame', 'provide', 'free', 'form', 'shape', 'face', 'region', 'system', 'wherein', 'second', 'video', 'frame', 'comprises', 'non-key', 'frame', 'video', 'sequence', 'perform', 'face', 'detection', 'second', 'video', 'frame', 'video', 'sequence', 'based', 'free', 'form', 'shape', 'face', 'region', 'system', 'wherein', 'track', 'second', 'free', 'form', 'shape', 'face', 'region', 'second', 'video', 'frame', 'based', 'free', 'form', 'shape', 'face', 'region', 'video', 'frame', 'system', 'wherein', 'encode', 'video', 'frame', 'based', 'least', 'part', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'comprises', 'reduce', 'quantization', 'parameter', 'corresponding', 'first', 'candidate', 'face', 'region', 'adjust', 'lambda', 'value', 'first', 'candidate', 'face', 'region', 'disable', 'skip', 'coding', 'first', 'candidate', 'face', 'region', 'least', 'one', 'non-transitory', 'machine', 'readable', 'medium', 'comprising', 'instructions', 'response', 'executed', 'device', 'cause', 'device', 'perform', 'video', 'coding', 'based', 'face', 'detection', 'receiving', 'video', 'frame', 'comprising', 'one', 'video', 'frames', 'video', 'sequence', 'determining', 'video', 'frame', 'key', 'frame', 'video', 'sequence', 'performing', 'response', 'video', 'frame', 'key', 'frame', 'video', 'sequence', 'multi-stage', 'facial', 'search', 'video', 'frame', 'based', 'predetermined', 'feature', 'templates', 'predetermined', 'number', 'stages', 'determine', 'first', 'candidate', 'face', 'region', 'second', 'candidate', 'face', 'region', 'video', 'frame', 'testing', 'first', 'second', 'candidate', 'face', 'regions', 'based', 'skin', 'tone', 'information', 'determine', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'second', 'candidate', 'face', 'region', 'invalid', 'face', 'region', 'rejecting', 'second', 'candidate', 'face', 'region', 'outputting', 'first', 'candidate', 'face', 'region', 'encoding', 'video', 'frame', 'based', 'least', 'part', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'generate', 'coded', 'bitstream', 'non-transitory', 'machine', 'readable', 'medium', 'wherein', 'skin', 'tone', 'information', 'comprises', 'skin', 'probability', 'map', 'non-transitory', 'machine', 'readable', 'medium', 'wherein', 'first', 'candidate', 'face', 'region', 'comprises', 'rectangular', 'region', 'machine', 'readable', 'medium', 'comprising', 'instructions', 'response', 'executed', 'device', 'cause', 'device', 'perform', 'video', 'coding', 'based', 'face', 'detection', 'determining', 'free', 'form', 'shape', 'face', 'region', 'corresponding', 'first', 'candidate', 'face', 'region', 'wherein', 'free', 'form', 'shape', 'face', 'region', 'least', 'one', 'pixel', 'accuracy', 'small', 'block', 'pixels', 'accuracy', 'non-transitory', 'machine', 'readable', 'medium', 'wherein', 'determining', 'free', 'form', 'shape', 'face', 'region', 'comprises', 'generating', 'enhanced', 'skip', 'probability', 'map', 'corresponding', 'first', 'candidate', 'face', 'region', 'binarizing', 'enhanced', 'skip', 'probability', 'map', 'overlaying', 'binarized', 'enhanced', 'skip', 'probability', 'map', 'least', 'portion', 'video', 'frame', 'provide', 'free', 'form', 'shape', 'face', 'region', 'non-transitory', 'machine', 'readable', 'medium', 'wherein', 'second', 'video', 'frame', 'comprises', 'non-key', 'frame', 'video', 'sequence', 'machine', 'readable', 'medium', 'comprising', 'instructions', 'response', 'executed', 'device', 'cause', 'device', 'perform', 'video', 'coding', 'based', 'face', 'detection', 'performing', 'face', 'detection', 'second', 'video', 'frame', 'video', 'sequence', 'based', 'free', 'form', 'shape', 'face', 'region', 'non-transitory', 'machine', 'readable', 'medium', 'machine', 'readable', 'medium', 'comprising', 'instructions', 'response', 'executed', 'device', 'cause', 'device', 'perform', 'video', 'coding', 'based', 'face', 'detection', 'second', 'free', 'form', 'shape', 'face', 'region', 'second', 'video', 'frame', 'based', 'free', 'form', 'shape', 'face', 'region', 'video', 'frame', 'non-transitory', 'machine', 'readable', 'medium', 'wherein', 'encoding', 'video', 'frame', 'based', 'least', 'part', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'comprises', 'least', 'one', 'reducing', 'quantization', 'parameter', 'corresponding', 'first', 'candidate', 'face', 'region', 'adjusting', 'lambda', 'value', 'first', 'candidate', 'face', 'region', 'disabling', 'skip', 'coding', 'first', 'candidate', 'face', 'region', 'managing', 'smart', 'database', 'stores', 'facial', 'face', 'comprising', 'steps', 'managing', 'device', 'performing', 'process', 'counting', 'one', 'specific', 'facial', 'corresponding', 'least', 'one', 'specific', 'person', 'stored', 'smart', 'database', 'new', 'facial', 'face', 'continuously', 'stored', 'process', 'determining', 'whether', 'first', 'counted', 'value', 'representing', 'count', 'specific', 'facial', 'satisfies', 'preset', 'first', 'set', 'value', 'b', 'first', 'counted', 'value', 'determined', 'satisfying', 'first', 'set', 'value', 'managing', 'device', 'performing', 'process', 'inputting', 'specific', 'facial', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'neural', 'aggregation', 'network', 'generate', 'quality', 'scores', 'specific', 'facial', 'aggregation', 'specific', 'facial', 'process', 'sorting', 'quality', 'scores', 'corresponding', 'specific', 'facial', 'descending', 'order', 'quality', 'scores', 'process', 'counting', 'sorted', 'specific', 'facial', 'descending', 'order', 'second', 'counted', 'value', 'represents', 'number', 'counted', 'part', 'specific', 'facial', 'becomes', 'equal', 'preset', 'second', 'set', 'value', 'process', 'deleting', 'uncounted', 'part', 'specific', 'facial', 'smart', 'database', 'comprising', 'step', 'c', 'managing', 'device', 'performing', 'process', 'generating', 'least', 'one', 'optimal', 'feature', 'weighted', 'summation', 'one', 'features', 'specific', 'facial', 'using', 'counted', 'part', 'quality', 'scores', 'process', 'setting', 'optimal', 'feature', 'representative', 'face', 'corresponding', 'specific', 'person', 'wherein', 'step', 'b', 'managing', 'device', 'performs', 'process', 'inputting', 'specific', 'facial', 'cnn', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'cnn', 'generate', 'one', 'features', 'corresponding', 'specific', 'facial', 'process', 'inputting', 'least', 'one', 'feature', 'vector', 'features', 'embedded', 'aggregation', 'module', 'including', 'least', 'two', 'attention', 'blocks', 'thereby', 'allow', 'aggregation', 'module', 'generate', 'quality', 'scores', 'features', 'wherein', 'step', 'b', 'managing', 'device', 'performs', 'process', 'matching', 'i-', 'one', 'features', 'corresponding', 'specific', 'facial', 'stored', 'smart', 'database', 'i-', 'quality', 'scores', 'ii', 'specific', 'person', 'process', 'storing', 'matched', 'features', 'matched', 'quality', 'scores', 'smart', 'database', 'comprising', 'step', 'managing', 'device', 'performing', 'one', 'process', 'learning', 'face', 'system', 'using', 'specific', 'facial', 'corresponding', 'specific', 'person', 'stored', 'smart', 'database', 'ii', 'process', 'transmitting', 'specific', 'facial', 'corresponding', 'specific', 'person', 'learning', 'device', 'corresponding', 'face', 'system', 'thereby', 'allow', 'learning', 'device', 'learn', 'face', 'system', 'using', 'specific', 'facial', 'wherein', 'neural', 'aggregation', 'network', 'learned', 'learning', 'device', 'repeating', 'process', 'inputting', 'multiple', 'facial', 'training', 'corresponding', 'set', 'single', 'face', 'video', 'single', 'face', 'cnn', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'cnn', 'generate', 'one', 'features', 'training', 'applying', 'least', 'one', 'convolution', 'operation', 'facial', 'training', 'ii', 'process', 'inputting', 'least', 'one', 'feature', 'vector', 'training', 'features', 'training', 'embedded', 'aggregation', 'module', 'including', 'least', 'two', 'attention', 'blocks', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'aggregation', 'module', 'generate', 'quality', 'scores', 'training', 'features', 'training', 'aggregation', 'features', 'training', 'using', 'one', 'attention', 'parameters', 'learned', 'previous', 'iteration', 'iii', 'process', 'outputting', 'least', 'one', 'optimal', 'feature', 'training', 'weighted', 'summation', 'features', 'training', 'using', 'quality', 'scores', 'training', 'iv', 'process', 'updating', 'attention', 'parameters', 'learned', 'previous', 'iteration', 'least', 'two', 'attention', 'blocks', 'one', 'losses', 'minimized', 'outputted', 'loss', 'layer', 'referring', 'optimal', 'feature', 'training', 'corresponding', 'ground', 'truth', 'managing', 'device', 'managing', 'smart', 'database', 'stores', 'facial', 'face', 'comprising', 'least', 'one', 'memory', 'stores', 'instructions', 'least', 'one', 'configured', 'execute', 'instructions', 'perform', 'support', 'another', 'device', 'perform', 'process', 'counting', 'one', 'specific', 'facial', 'corresponding', 'least', 'one', 'specific', 'person', 'stored', 'smart', 'database', 'new', 'facial', 'face', 'continuously', 'stored', 'process', 'determining', 'whether', 'first', 'counted', 'value', 'representing', 'count', 'specific', 'facial', 'satisfies', 'preset', 'first', 'set', 'value', 'ii', 'first', 'counted', 'value', 'determined', 'satisfying', 'first', 'set', 'value', 'process', 'inputting', 'specific', 'facial', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'neural', 'aggregation', 'network', 'generate', 'quality', 'scores', 'specific', 'facial', 'aggregation', 'specific', 'facial', 'process', 'sorting', 'quality', 'scores', 'corresponding', 'specific', 'facial', 'descending', 'order', 'quality', 'scores', 'process', 'counting', 'sorted', 'specific', 'facial', 'descending', 'order', 'second', 'counted', 'value', 'represents', 'number', 'counted', 'part', 'specific', 'facial', 'becomes', 'equal', 'preset', 'second', 'set', 'value', 'process', 'deleting', 'uncounted', 'part', 'specific', 'facial', 'smart', 'database', 'managing', 'device', 'wherein', 'performs', 'iii', 'process', 'generating', 'least', 'one', 'optimal', 'feature', 'weighted', 'summation', 'one', 'features', 'specific', 'facial', 'using', 'counted', 'part', 'quality', 'scores', 'process', 'setting', 'optimal', 'feature', 'representative', 'face', 'corresponding', 'specific', 'person', 'managing', 'device', 'wherein', 'process', 'ii', 'performs', 'process', 'inputting', 'specific', 'facial', 'cnn', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'cnn', 'generate', 'one', 'features', 'corresponding', 'specific', 'facial', 'process', 'inputting', 'least', 'one', 'feature', 'vector', 'features', 'embedded', 'aggregation', 'module', 'including', 'least', 'two', 'attention', 'blocks', 'thereby', 'allow', 'aggregation', 'module', 'generate', 'quality', 'scores', 'features', 'managing', 'device', 'wherein', 'process', 'ii', 'performs', 'process', 'matching', 'i-', 'one', 'features', 'corresponding', 'specific', 'facial', 'stored', 'smart', 'database', 'i-', 'quality', 'scores', 'ii', 'specific', 'person', 'process', 'storing', 'matched', 'features', 'matched', 'quality', 'scores', 'smart', 'database', 'managing', 'device', 'wherein', 'performs', 'iv', 'one', 'process', 'learning', 'face', 'system', 'using', 'specific', 'facial', 'corresponding', 'specific', 'person', 'stored', 'smart', 'database', 'ii', 'process', 'transmitting', 'specific', 'facial', 'corresponding', 'specific', 'person', 'learning', 'device', 'corresponding', 'face', 'system', 'thereby', 'allow', 'learning', 'device', 'learn', 'face', 'system', 'using', 'specific', 'facial', 'managing', 'device', 'wherein', 'neural', 'aggregation', 'network', 'learned', 'learning', 'device', 'repeating', 'process', 'inputting', 'multiple', 'facial', 'training', 'corresponding', 'set', 'single', 'face', 'video', 'single', 'face', 'cnn', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'cnn', 'generate', 'one', 'features', 'training', 'applying', 'least', 'one', 'convolution', 'operation', 'facial', 'training', 'ii', 'process', 'inputting', 'least', 'one', 'feature', 'vector', 'training', 'features', 'training', 'embedded', 'aggregation', 'module', 'including', 'least', 'two', 'attention', 'blocks', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'aggregation', 'module', 'generate', 'quality', 'scores', 'training', 'features', 'training', 'aggregation', 'features', 'training', 'using', 'one', 'attention', 'parameters', 'learned', 'previous', 'iteration', 'iii', 'process', 'outputting', 'least', 'one', 'optimal', 'feature', 'training', 'weighted', 'summation', 'features', 'training', 'using', 'quality', 'scores', 'training', 'iv', 'process', 'updating', 'attention', 'parameters', 'learned', 'previous', 'iteration', 'least', 'two', 'attention', 'blocks', 'one', 'losses', 'minimized', 'outputted', 'loss', 'layer', 'referring', 'optimal', 'feature', 'training', 'corresponding', 'ground', 'truth', 'object', 'data', 'processing', 'system', 'comprising', 'least', 'one', 'configured', 'execute', 'least', 'one', 'implementation', 'algorithms', 'stored', 'least', 'one', 'non-transitory', 'computer-readable', 'storage', 'medium', 'algorithm', 'feature', 'density', 'selection', 'criteria', 'data', 'preprocessing', 'code', 'executed', 'least', 'one', 'data', 'preprocessing', 'code', 'comprising', 'invariant', 'feature', 'identification', 'algorithm', 'configured', 'obtain', 'digital', 'representation', 'scene', 'scene', 'comprising', 'one', 'textual', 'media', 'generate', 'set', 'invariant', 'features', 'applying', 'invariant', 'feature', 'identification', 'algorithm', 'digital', 'representation', 'cluster', 'set', 'invariant', 'features', 'regions', 'interest', 'digital', 'representation', 'scene', 'region', 'interest', 'region', 'feature', 'density', 'classify', 'region', 'classifier', 'code', 'least', 'one', 'regions', 'interest', 'according', 'object', 'type', 'function', 'attributes', 'derived', 'region', 'feature', 'density', 'digital', 'representation', 'wherein', 'least', 'one', 'classified', 'regions', 'interest', 'corresponds', 'text', 'use', 'classification', 'result', 'corresponding', 'least', 'one', 'regions', 'interest', 'classify', 'another', 'regions', 'interest', 'according', 'object', 'type', 'wherein', 'another', 'regions', 'interest', 'corresponds', 'region', 'interest', 'system', 'wherein', 'preprocessing', 'code', 'based', 'feature', 'density', 'selection', 'criteria', 'determines', 'ocr', 'algorithm', 'applicable', 'text', 'algorithms', 'applicable', 'aspects', 'photographs', 'logos', 'system', 'wherein', 'creates', 'profile', 'camera-equipped', 'smartphone', 'includes', 'information', 'visually', 'impaired', 'causes', 'prioritized', 'execution', 'ocr', 'algorithm', 'text', 'reader', 'program', 'begins', 'reading', 'text', 'quickly', 'possible', 'system', 'comprising', 'audio', 'tactile', 'feedback', 'mechanism', 'helps', 'position', 'smart', 'phone', 'relative', 'text', 'system', 'comprising', '``', 'hold', 'still', \"''\", 'audio', 'feedback', 'signal', 'sent', 'text', 'center', 'captured', 'scene', 'system', 'wherein', 'digital', 'representation', 'comprises', 'least', 'one', 'following', 'types', 'digital', 'data', 'data', 'video', 'data', 'audio', 'data', 'system', 'wherein', 'invariant', 'feature', 'identification', 'algorithm', 'comprises', 'least', 'one', 'following', 'feature', 'identification', 'algorithms', 'fast', 'sift', 'freak', 'brisk', 'harris', 'daisy', 'mser', 'system', 'wherein', 'invariant', 'feature', 'identification', 'algorithm', 'includes', 'least', 'one', 'following', 'edge', 'detection', 'algorithm', 'corner', 'detection', 'algorithm', 'saliency', 'map', 'algorithm', 'curve', 'detection', 'algorithm', 'texton', 'identification', 'algorithm', 'wavelets', 'algorithm', 'system', 'wherein', 'least', 'one', 'region', 'interest', 'represents', 'least', 'one', 'physical', 'object', 'scene', 'system', 'wherein', 'least', 'one', 'region', 'interest', 'represents', 'least', 'one', 'textual', 'media', 'scene', 'system', 'wherein', 'region', 'interest', 'represents', 'document', 'textual', 'media', 'system', 'wherein', 'region', 'interest', 'represents', 'financial', 'document', 'system', 'wherein', 'region', 'interest', 'represents', 'structured', 'document', 'system', 'wherein', 'least', 'one', 'implementation', 'algorithms', 'includes', 'least', 'one', 'following', 'template', 'driven', 'algorithm', 'face', 'algorithm', 'optical', 'character', 'algorithm', 'speech', 'algorithm', 'object', 'algorithm', 'system', 'wherein', 'data', 'preprocessing', 'code', 'configured', 'assign', 'region', 'interest', 'least', 'one', 'algorithm', 'function', 'scene', 'context', 'derived', 'digital', 'representation', 'system', 'wherein', 'scene', 'context', 'includes', 'least', 'one', 'following', 'types', 'data', 'location', 'position', 'time', 'identity', 'news', 'event', 'medical', 'event', 'promotion', 'system', 'comprising', 'mobile', 'device', 'comprising', 'least', 'one', 'implementation', 'algorithms', 'data', 'preprocessing', 'code', 'system', 'wherein', 'mobile', 'device', 'comprises', 'least', 'one', 'following', 'smart', 'phone', 'tablet', 'wearable', 'glass', 'toy', 'vehicle', 'computer', 'phablet', 'system', 'comprising', 'network-accessible', 'server', 'device', 'comprising', 'least', 'one', 'implementation', 'algorithms', 'data', 'preprocessing', 'code', 'system', 'wherein', 'object', 'type', 'includes', 'least', 'one', 'following', 'face', 'animal', 'vehicle', 'document', 'plant', 'building', 'appliance', 'clothing', 'body', 'part', 'toy', 'object', 'data', 'processing', 'system', 'comprising', 'least', 'one', 'configured', 'execute', 'least', 'one', 'implementation', 'algorithms', 'stored', 'least', 'one', 'non-transitory', 'computer-readable', 'storage', 'medium', 'algorithm', 'feature', 'density', 'selection', 'criteria', 'data', 'preprocessing', 'code', 'executed', 'least', 'one', 'data', 'preprocessing', 'code', 'comprising', 'invariant', 'feature', 'identification', 'algorithm', 'configured', 'obtain', 'digital', 'representation', 'scene', 'scene', 'comprising', 'one', 'textual', 'media', 'generate', 'set', 'invariant', 'features', 'applying', 'invariant', 'feature', 'identification', 'algorithm', 'digital', 'representation', 'cluster', 'set', 'invariant', 'features', 'regions', 'interest', 'digital', 'representation', 'scene', 'region', 'interest', 'region', 'feature', 'density', 'classify', 'region', 'classifier', 'code', 'least', 'one', 'regions', 'interest', 'according', 'object', 'type', 'function', 'attributes', 'derived', 'region', 'feature', 'density', 'digital', 'representation', 'wherein', 'least', 'one', 'classified', 'regions', 'interest', 'corresponds', 'text', 'use', 'classification', 'result', 'corresponding', 'least', 'one', 'regions', 'interest', 'classify', 'another', 'regions', 'interest', 'according', 'object', 'type', 'wherein', 'another', 'regions', 'interest', 'corresponds', 'region', 'interest', 'assign', 'region', 'interest', 'least', 'one', 'algorithm', 'least', 'one', 'implementation', 'diverse', 'algorithms', 'function', 'region', 'feature', 'density', 'region', 'interest', 'feature', 'density', 'selection', 'criteria', 'least', 'one', 'implementation', 'diverse', 'algorithms', 'configure', 'assigned', 'algorithms', 'process', 'respective', 'regions', 'interest', 'wherein', 'preprocessing', 'code', 'based', 'feature', 'density', 'selection', 'criteria', 'determines', 'ocr', 'algorithm', 'applicable', 'text', 'algorithms', 'applicable', 'aspects', 'photographs', 'logos', 'device', 'comprising', 'least', 'one', 'configured', 'execute', 'least', 'one', 'implementation', 'algorithms', 'stored', 'least', 'one', 'non-transitory', 'computer-readable', 'storage', 'medium', 'algorithm', 'feature', 'density', 'selection', 'criteria', 'data', 'preprocessing', 'code', 'executed', 'least', 'one', 'data', 'preprocessing', 'code', 'comprising', 'invariant', 'feature', 'identification', 'algorithm', 'configured', 'obtain', 'digital', 'representation', 'scene', 'scene', 'comprising', 'one', 'textual', 'media', 'generate', 'set', 'invariant', 'features', 'applying', 'invariant', 'feature', 'identification', 'algorithm', 'digital', 'representation', 'cluster', 'set', 'invariant', 'features', 'regions', 'interest', 'digital', 'representation', 'scene', 'region', 'interest', 'region', 'feature', 'density', 'classify', 'region', 'classifier', 'code', 'least', 'one', 'regions', 'interest', 'according', 'object', 'type', 'function', 'attributes', 'derived', 'region', 'feature', 'density', 'digital', 'representation', 'wherein', 'least', 'one', 'classified', 'regions', 'interest', 'corresponds', 'text', 'use', 'classification', 'result', 'corresponding', 'least', 'one', 'regions', 'interest', 'classify', 'another', 'regions', 'interest', 'according', 'object', 'type', 'wherein', 'another', 'regions', 'interest', 'corresponds', 'region', 'interest', 'mobile', 'terminal', 'comprising', 'front', 'camera', 'configured', 'obtain', 'two-dimensional', 'face', 'glance', 'sensor', 'tilted', 'certain', 'angle', 'disposed', 'adjacent', 'front', 'camera', 'obtain', 'metadata', 'face', 'controller', 'obtaining', 'distance', 'glance', 'sensor', 'front', 'camera', 'distance', 'enabling', 'area', 'overlap', 'region', 'first', 'region', 'representing', 'range', 'photographable', 'front', 'camera', 'overlaps', 'second', 'region', 'representing', 'range', 'photographable', 'glance', 'sensor', 'maximum', 'mobile', 'terminal', 'wherein', 'controller', 'configured', 'obtain', 'distance', 'enabling', 'area', 'overlap', 'region', 'maximum', 'glance', 'sensor', 'front', 'camera', 'varying', 'tilting', 'angle', 'glance', 'sensor', 'mobile', 'terminal', 'wherein', 'controller', 'configured', 'set', 'distance', 'enabling', 'area', 'overlap', 'region', 'maximum', 'glance', 'sensor', 'front', 'camera', 'tilting', 'angle', 'glance', 'sensor', 'optimal', 'disposition', 'location', 'glance', 'sensor', 'mobile', 'terminal', 'wherein', 'controller', 'configured', 'set', 'disposition', 'location', 'front', 'camera', 'original', 'point', 'calculates', 'coordinates', 'first', 'triangle', 'representing', 'first', 'region', 'based', 'field', 'view', 'front', 'camera', 'maximum', 'photographing', 'distance', 'front', 'camera', 'mobile', 'terminal', 'wherein', 'controller', 'configured', 'calculate', 'coordinates', 'second', 'triangle', 'representing', 'second', 'region', 'based', 'field', 'view', 'glance', 'sensor', 'maximum', 'photographing', 'distance', 'glance', 'sensor', 'distance', 'front', 'camera', 'glance', 'sensor', 'tilting', 'angle', 'glance', 'sensor', 'mobile', 'terminal', 'wherein', 'glance', 'sensor', 'tilted', 'controller', 'configured', 'calculate', 'coordinates', 'third', 'triangle', 'representing', 'third', 'region', 'photographable', 'glance', 'sensor', 'controller', 'configured', 'rotation-convert', 'coordinates', 'third', 'triangle', 'based', 'tilting', 'angle', 'glance', 'sensor', 'calculate', 'coordinates', 'second', 'triangle', 'mobile', 'terminal', 'wherein', 'controller', 'configured', 'calculate', 'coordinates', 'overlap', 'region', 'based', 'coordinates', 'first', 'triangle', 'coordinates', 'second', 'triangle', 'calculates', 'area', 'overlap', 'region', 'based', 'coordinates', 'overlap', 'region', 'mobile', 'terminal', 'wherein', 'controller', 'configured', 'generate', 'three-dimensional', 'face', 'information', 'based', 'face', 'obtained', 'front', 'camera', 'metadata', 'obtained', 'glance', 'sensor', 'mobile', 'terminal', 'wherein', 'metadata', 'comprises', 'one', 'angle', 'face', 'size', 'face', 'location', 'face', 'mobile', 'terminal', 'wherein', 'angle', 'face', 'comprises', 'angle', 'face', 'rotated', 'one', 'pitch', 'axis', 'roll', 'axis', 'yaw', 'axis', 'mobile', 'terminal', 'comprising', 'memory', 'storing', 'generated', 'face', 'information', 'wherein', 'controller', 'configured', 'performs', 'authentication', 'process', 'comparing', 'stored', 'face', 'information', 'face', 'information', 'obtained', 'authentication', 'mobile', 'terminal', 'wherein', 'glance', 'sensor', 'controlled', 'permanently', 'activated', 'low', 'power', 'obtain', 'front', 'metadata', 'front', 'mobile', 'terminal', 'wherein', 'front', 'camera', 'glance', 'sensor', 'disposed', 'line', 'upper', 'end', 'mobile', 'terminal', 'mobile', 'terminal', 'wherein', 'glance', 'sensor', 'tilted', 'one', 'direction', 'direction', 'direction', 'left', 'direction', 'right', 'direction', 'mobile', 'terminal', 'wherein', 'metadata', 'data', 'changed', 'mobile', 'terminal', 'tilted', 'external', 'physical', 'force', 'comprising', 'receiving', 'smart', 'television', 'tv', 'indication', 'upcoming', 'media', 'programming', 'wherein', 'upcoming', 'media', 'programming', 'based', 'profile', 'identifying', 'one', 'devices', 'communication', 'smart', 'tv', 'one', 'devices', 'including', 'least', 'one', 'microphone', 'camera', 'instructing', 'least', 'one', 'identified', 'device', 'detect', 'audio', 'signals', 'using', 'respective', 'microphone', 'detect', 'visual', 'signals', 'using', 'respective', 'camera', 'selecting', 'least', 'one', 'device', 'one', 'devices', 'based', 'detected', 'audio', 'signal', 'detected', 'visual', 'signal', 'providing', 'instructions', 'selected', 'device', 'output', 'notification', 'related', 'upcoming', 'media', 'programming', 'wherein', 'upcoming', 'media', 'programming', 'one', 'live', 'television', 'program', 'recorded', 'television', 'program', 'broadcast', 'television', 'program', 'application-provided', 'program', 'wherein', 'selecting', 'first', 'device', 'based', 'detected', 'audio', 'signal', 'includes', 'voice', 'comprising', 'determining', 'distance', 'recognized', 'voice', 'wherein', 'selecting', 'first', 'device', 'based', 'determined', 'distance', 'wherein', 'selecting', 'first', 'device', 'based', 'detected', 'visual', 'signals', 'includes', 'face', 'wherein', 'face', 'includes', 'face', 'technique', 'comprising', 'presenting', 'smart', 'tv', 'upcoming', 'media', 'programming', 'favorite', 'channel', 'list', 'comprising', 'obtaining', 'media', 'programming', 'viewing', 'data', 'wherein', 'media', 'programming', 'viewing', 'data', 'includes', 'least', 'one', 'historical', 'time', 'historical', 'date', 'one', 'media', 'programs', 'viewed', 'obtaining', 'least', 'one', 'current', 'time', 'current', 'date', 'processing', 'media', 'programming', 'viewing', 'data', 'determine', 'probability', 'one', 'media', 'programs', 'viewed', 'based', 'least', 'one', 'current', 'time', 'current', 'date', 'presenting', 'favorite', 'channel', 'list', 'based', 'determined', 'probability', 'one', 'media', 'programs', 'viewed', 'wherein', 'processing', 'media', 'programming', 'viewing', 'data', 'includes', 'employing', 'neural', 'network', 'model', 'wherein', 'employing', 'neural', 'network', 'model', 'comprises', 'determining', 'duration', 'one', 'media', 'programs', 'viewed', 'least', 'one', 'historical', 'time', 'historical', 'date', 'setting', 'threshold', 'time', 'duration', 'comparing', 'determined', 'duration', 'threshold', 'time', 'duration', 'filtering', 'one', 'media', 'programs', 'viewed', 'threshold', 'time', 'duration', 'smart', 'television', 'tv', 'comprising', 'network', 'interface', 'non-transitory', 'computer-readable', 'medium', 'communication', 'network', 'interface', 'non-transitory', 'computer-readable', 'medium', 'capable', 'executing', '-executable', 'program', 'code', 'stored', 'non-transitory', 'computer-readable', 'medium', 'cause', 'smart', 'tv', 'receive', 'indication', 'upcoming', 'media', 'programming', 'wherein', 'upcoming', 'media', 'programming', 'based', 'profile', 'identify', 'one', 'devices', 'communication', 'smart', 'tv', 'one', 'devices', 'including', 'least', 'one', 'microphone', 'camera', 'instruct', 'least', 'one', 'identified', 'device', 'detect', 'audio', 'signals', 'using', 'respective', 'microphone', 'detect', 'visual', 'signals', 'using', 'respective', 'camera', 'select', 'least', 'one', 'device', 'one', 'devices', 'based', 'detected', 'audio', 'signal', 'detected', 'visual', 'signal', 'provide', 'instructions', 'selected', 'device', 'output', 'notification', 'related', 'upcoming', 'media', 'programming', 'smart', 'tv', 'wherein', 'selecting', 'first', 'device', 'based', 'detected', 'audio', 'signal', 'includes', 'voice', 'smart', 'tv', 'wherein', 'capable', 'executing', '-executable', 'program', 'code', 'determine', 'distance', 'recognized', 'voice', 'wherein', 'selecting', 'first', 'device', 'based', 'determined', 'distance', 'smart', 'tv', 'wherein', 'selecting', 'first', 'device', 'based', 'detected', 'visual', 'signals', 'includes', 'detecting', 'presence', 'smart', 'tv', 'wherein', 'detecting', 'presence', 'includes', 'employing', 'one', 'camera', 'microphone', 'fingerprint', 'sensor', 'associated', 'least', 'one', 'smart', 'tv', 'mobile', 'device', 'smartphone', 'laptop', 'computer', 'tablet', 'device', 'wearable', 'device', 'internet', 'things', 'iot', 'device', 'internet', 'everything', 'ioe', 'device', 'iot', 'hub', 'ioe', 'hub', 'smart', 'television', 'tv', 'comprising', 'means', 'receiving', 'indication', 'upcoming', 'media', 'programming', 'wherein', 'upcoming', 'media', 'programming', 'based', 'profile', 'means', 'identifying', 'one', 'devices', 'communication', 'smart', 'tv', 'one', 'devices', 'including', 'least', 'one', 'microphone', 'camera', 'means', 'instructing', 'least', 'one', 'identified', 'device', 'detect', 'audio', 'signals', 'using', 'respective', 'microphone', 'detect', 'visual', 'signals', 'using', 'respective', 'camera', 'means', 'selecting', 'least', 'one', 'device', 'one', 'devices', 'based', 'detected', 'audio', 'signal', 'detected', 'visual', 'signal', 'means', 'providing', 'instructions', 'selected', 'device', 'output', 'notification', 'related', 'upcoming', 'media', 'programming', 'smart', 'tv', 'wherein', 'one', 'devices', 'includes', 'least', 'one', 'mobile', 'device', 'smartphone', 'laptop', 'computer', 'tablet', 'device', 'wearable', 'device', 'internet', 'things', 'iot', 'device', 'internet', 'everything', 'ioe', 'device', 'iot', 'hub', 'ioe', 'hub', 'another', 'smart', 'tv', 'smart', 'tv', 'wherein', 'upcoming', 'media', 'programming', 'one', 'live', 'television', 'program', 'recorded', 'television', 'program', 'broadcast', 'television', 'program', 'application-provided', 'program', 'smart', 'tv', 'wherein', 'notification', 'includes', 'least', 'one', 'push', 'message', 'sms', 'message', 'waysms', 'message', 'audio', 'alert', 'audio', 'message', 'email', 'message', 'smart', 'tv', 'comprising', 'presenting', 'upcoming', 'media', 'programming', 'favorite', 'channel', 'list', 'smart', 'tv', 'comprising', 'means', 'obtaining', 'media', 'programming', 'viewing', 'data', 'wherein', 'media', 'programming', 'viewing', 'data', 'includes', 'least', 'one', 'historical', 'time', 'historical', 'date', 'one', 'media', 'programs', 'viewed', 'smart', 'tv', 'means', 'obtaining', 'least', 'one', 'current', 'time', 'current', 'date', 'means', 'processing', 'media', 'programming', 'viewing', 'data', 'determine', 'probability', 'one', 'media', 'programs', 'viewed', 'smart', 'tv', 'based', 'least', 'one', 'current', 'time', 'current', 'date', 'means', 'presenting', 'favorite', 'channel', 'list', 'based', 'determined', 'probability', 'one', 'media', 'programs', 'viewed', 'smart', 'tv', 'wherein', 'means', 'processing', 'media', 'programming', 'viewing', 'data', 'includes', 'employing', 'neural', 'network', 'model', 'smart', 'tv', 'wherein', 'employing', 'neural', 'network', 'model', 'comprises', 'determining', 'duration', 'one', 'media', 'programs', 'viewed', 'smart', 'tv', 'least', 'one', 'historical', 'time', 'historical', 'date', 'setting', 'threshold', 'time', 'duration', 'comparing', 'determined', 'duration', 'threshold', 'time', 'duration', 'filtering', 'one', 'media', 'programs', 'viewed', 'threshold', 'time', 'duration', 'smart', 'tv', 'comprising', 'means', 'adjusting', 'least', 'one', 'volume', 'brightness', 'smart', 'tv', 'wherein', 'adjusting', 'based', 'least', 'one', 'historical', 'time', 'historical', 'date', 'smart', 'tv', 'comprising', 'means', 'restricting', 'access', 'one', 'media', 'programs', 'non-transitory', 'computer-readable', 'medium', 'comprising', '-executable', 'program', 'code', 'configured', 'cause', 'smart', 'television', 'tv', 'receive', 'indication', 'upcoming', 'media', 'programming', 'wherein', 'upcoming', 'media', 'programming', 'based', 'profile', 'identify', 'one', 'devices', 'communication', 'smart', 'tv', 'one', 'devices', 'including', 'least', 'one', 'microphone', 'camera', 'instruct', 'least', 'one', 'identified', 'device', 'detect', 'audio', 'signals', 'using', 'respective', 'microphone', 'detect', 'visual', 'signals', 'using', 'respective', 'camera', 'select', 'least', 'one', 'device', 'one', 'devices', 'based', 'detected', 'audio', 'signal', 'detected', 'visual', 'signal', 'provide', 'instructions', 'selected', 'device', 'output', 'notification', 'related', 'upcoming', 'media', 'programming', 'non-transitory', 'computer-readable', 'medium', 'wherein', 'selecting', 'first', 'device', 'based', 'detected', 'audio', 'signal', 'includes', 'voice', 'non-transitory', 'computer-readable', 'medium', 'wherein', 'capable', 'executing', '-executable', 'program', 'code', 'determine', 'distance', 'recognized', 'voice', 'wherein', 'selecting', 'first', 'device', 'based', 'determined', 'distance', 'non-transitory', 'computer-readable', 'medium', 'wherein', 'selecting', 'first', 'device', 'based', 'detected', 'visual', 'signals', 'includes', 'face', 'non-transitory', 'computer-readable', 'medium', 'wherein', 'face', 'includes', 'face', 'technique', 'camera', 'comprising', 'sensor', 'array', 'including', 'sensors', 'infrared', 'ir', 'illuminator', 'configured', 'emit', 'active', 'ir', 'light', 'ir', 'light', 'sub-band', 'spectral', 'illuminators', 'spectral', 'illuminator', 'configured', 'emit', 'active', 'spectral', 'light', 'different', 'spectral', 'light', 'sub-band', 'depth', 'controller', 'machine', 'configured', 'determine', 'depth', 'value', 'sensors', 'based', 'active', 'ir', 'light', 'spectral', 'controller', 'machine', 'configured', 'sensors', 'determine', 'spectral', 'value', 'spectral', 'light', 'sub-band', 'spectral', 'illuminators', 'output', 'machine', 'configured', 'output', 'test', 'depth+multi-spectral', 'including', 'pixels', 'pixel', 'corresponding', 'one', 'sensors', 'sensor', 'array', 'including', 'least', 'depth', 'value', 'spectral', 'value', 'spectral', 'light', 'sub-band', 'spectral', 'illuminators', 'face', 'machine', 'previously', 'trained', 'set', 'labeled', 'training', 'depth+multi-spectral', 'structure', 'test', 'depth+multi-spectral', 'face', 'machine', 'configured', 'output', 'confidence', 'value', 'indicating', 'likelihood', 'test', 'depth+multi-spectral', 'includes', 'face', 'camera', 'wherein', 'spectral', 'value', 'calculated', 'based', 'depth', 'value', 'determined', 'sensor', 'corresponds', 'pixel', 'camera', 'wherein', 'face', 'machine', 'configured', 'use', 'convolutional', 'neural', 'network', 'determine', 'confidence', 'value', 'camera', 'wherein', 'face', 'machine', 'includes', 'input', 'nodes', 'wherein', 'input', 'node', 'configured', 'receive', 'pixel', 'value', 'array', 'corresponding', 'different', 'pixel', 'pixels', 'test', 'depth+multi-spectral', 'wherein', 'pixel', 'value', 'array', 'includes', 'depth', 'value', 'multi-spectral', 'values', 'pixel', 'camera', 'wherein', 'multi-spectral', 'values', 'pixel', 'include', 'three', 'spectral', 'values', 'camera', 'wherein', 'output', 'machine', 'configured', 'output', 'surface', 'normal', 'pixel', 'test', 'depth+multi-spectral', 'wherein', 'pixel', 'value', 'array', 'includes', 'surface', 'normal', 'camera', 'wherein', 'output', 'machine', 'configured', 'output', 'curvature', 'pixel', 'test', 'depth+multi-spectral', 'wherein', 'pixel', 'value', 'array', 'includes', 'curvature', 'camera', 'wherein', 'face', 'machine', 'configured', 'use', 'models', 'determine', 'confidence', 'value', 'wherein', 'models', 'includes', 'channel-specific', 'models', 'wherein', 'channel-specific', 'model', 'configured', 'process', 'different', 'pixel', 'parameter', 'pixels', 'test', 'depth+multi-spectral', 'wherein', 'channel-specific', 'model', 'includes', 'input', 'nodes', 'wherein', 'channel-specific', 'model', 'input', 'node', 'configured', 'receive', 'pixel', 'parameter', 'value', 'different', 'pixel', 'pixels', 'test', 'depth+multi-spectral', 'camera', 'wherein', 'face', 'machine', 'configured', 'use', 'statistical', 'model', 'determine', 'confidence', 'value', 'camera', 'wherein', 'statistical', 'model', 'includes', 'nearest', 'neighbor', 'algorithm', 'camera', 'wherein', 'statistical', 'model', 'includes', 'support', 'vector', 'machine', 'camera', 'wherein', 'face', 'machine', 'configured', 'output', 'location', 'test', 'depth+multi-spectral', 'bounding', 'box', 'around', 'recognized', 'face', 'camera', 'wherein', 'face', 'machine', 'configured', 'output', 'location', 'test', 'depth+multi-spectral', 'identified', 'two-dimensional', 'facial', 'feature', 'recognized', 'face', 'camera', 'wherein', 'face', 'machine', 'configured', 'output', 'location', 'test', 'depth+multi-spectral', 'identified', 'three-dimensional', 'facial', 'feature', 'recognized', 'face', 'camera', 'wherein', 'face', 'machine', 'configured', 'output', 'location', 'test', 'depth+multi-spectral', 'identified', 'spectral', 'feature', 'recognized', 'face', 'camera', 'wherein', 'face', 'machine', 'configured', 'output', 'pixel', 'test', 'depth+multi-spectral', 'confidence', 'value', 'indicating', 'likelihood', 'pixel', 'included', 'face', 'camera', 'wherein', 'face', 'machine', 'configured', 'output', 'identity', 'face', 'recognized', 'test', 'depth+multi-spectral', 'camera', 'wherein', 'sensors', 'sensor', 'array', 'differential', 'sensors', 'wherein', 'spectral', 'value', 'determined', 'based', 'depth', 'value', 'differential', 'measurement', 'differential', 'sensor', 'camera', 'comprising', 'sensor', 'array', 'including', 'sensors', 'infrared', 'ir', 'illuminator', 'configured', 'emit', 'active', 'ir', 'light', 'ir', 'light', 'sub-band', 'spectral', 'illuminators', 'spectral', 'illuminator', 'configured', 'emit', 'active', 'spectral', 'light', 'different', 'spectral', 'light', 'sub-band', 'depth', 'controller', 'machine', 'configured', 'determine', 'depth', 'value', 'sensors', 'based', 'active', 'ir', 'light', 'spectral', 'controller', 'machine', 'configured', 'sensors', 'determine', 'spectral', 'value', 'spectral', 'light', 'sub-band', 'spectral', 'illuminators', 'wherein', 'spectral', 'value', 'calculated', 'based', 'depth', 'value', 'determined', 'sensor', 'corresponds', 'pixel', 'output', 'machine', 'configured', 'output', 'test', 'depth+multi-spectral', 'including', 'pixels', 'pixel', 'corresponding', 'one', 'sensors', 'sensor', 'array', 'including', 'least', 'depth', 'value', 'spectral', 'value', 'spectral', 'light', 'sub-band', 'spectral', 'illuminators', 'face', 'machine', 'including', 'convolutional', 'neural', 'network', 'previously', 'trained', 'set', 'labeled', 'training', 'depth+multi-spectral', 'structure', 'test', 'depth+multi-spectral', 'face', 'machine', 'configured', 'output', 'confidence', 'value', 'indicating', 'likelihood', 'test', 'depth+multi-spectral', 'includes', 'face', 'processing', 'comprising', 'acquiring', 'photo', 'album', 'obtained', 'face', 'clustering', 'collecting', 'face', 'information', 'respective', 'photo', 'album', 'acquiring', 'face', 'parameter', 'according', 'face', 'information', 'selecting', 'cover', 'according', 'face', 'parameter', 'taking', 'face-region', 'cover', 'setting', 'face-region', 'cover', 'photo', 'album', 'wherein', 'selecting', 'cover', 'according', 'face', 'parameter', 'comprises', 'performing', 'calculation', 'face', 'parameter', 'preset', 'way', 'obtain', 'cover', 'score', 'selecting', 'highest', 'cover', 'score', 'cover', 'wherein', 'selecting', 'highest', 'cover', 'score', 'cover', 'comprises', 'acquiring', 'source', 'selecting', 'highest', 'cover', 'score', 'coming', 'preset', 'source', 'cover', 'according', 'wherein', 'selecting', 'highest', 'cover', 'score', 'cover', 'comprises', 'acquiring', 'number', 'contained', 'determining', 'single-person', 'according', 'number', 'selecting', 'single-person', 'highest', 'cover', 'score', 'cover', 'according', 'wherein', 'selecting', 'highest', 'cover', 'score', 'cover', 'comprises', 'single-person', 'photo', 'album', 'determining', 'including', 'two', 'photo', 'album', 'selecting', 'highest', 'cover', 'score', 'including', 'two', 'cover', 'according', 'wherein', 'face', 'information', 'comprises', 'face', 'feature', 'points', 'face', 'parameter', 'comprises', 'face', 'turning', 'angle', 'acquiring', 'face', 'parameter', 'according', 'face', 'information', 'comprises', 'acquiring', 'coordinate', 'values', 'face', 'feature', 'points', 'determining', 'distances', 'angles', 'face', 'feature', 'points', 'determining', 'face', 'turning', 'angle', 'according', 'distances', 'angles', 'according', 'wherein', 'face', 'parameter', 'comprises', 'face', 'ratio', 'acquiring', 'face', 'parameter', 'according', 'face', 'information', 'comprises', 'determining', 'face', 'region', 'according', 'face', 'information', 'calculating', 'ratio', 'area', 'face', 'region', 'area', 'obtain', 'face', 'ratio', 'according', 'wherein', 'calculating', 'face', 'ratio', 'comprises', 'one', 'face', 'subtracting', 'area', 'occupied', 'face', 'corresponding', 'photo', 'album', 'face', 'region', 'obtain', 'remaining', 'area', 'calculating', 'ratio', 'remaining', 'area', 'area', 'obtain', 'face', 'ratio', 'according', 'wherein', 'collecting', 'face', 'information', 'respective', 'photo', 'album', 'comprises', 'acquiring', 'identifications', 'photo', 'album', 'extracting', 'face', 'information', 'corresponding', 'identifications', 'face', 'database', 'face', 'database', 'stored', 'face', 'results', 'face', 'results', 'including', 'face', 'information', 'processing', 'apparatus', 'comprising', 'memory', 'configured', 'store', 'instructions', 'executable', 'wherein', 'configured', 'run', 'program', 'corresponding', 'instructions', 'reading', 'instructions', 'stored', 'memory', 'perform', 'acquiring', 'photo', 'album', 'obtained', 'face', 'clustering', 'collecting', 'face', 'information', 'photo', 'album', 'acquiring', 'face', 'parameter', 'according', 'face', 'information', 'selecting', 'cover', 'according', 'face', 'parameter', 'taking', 'face-region', 'cover', 'setting', 'face-region', 'cover', 'photo', 'album', 'wherein', 'configured', 'perform', 'calculation', 'face', 'parameter', 'preset', 'way', 'obtain', 'cover', 'score', 'select', 'highest', 'cover', 'score', 'cover', 'wherein', 'configured', 'acquire', 'source', 'select', 'highest', 'cover', 'score', 'coming', 'preset', 'source', 'cover', 'apparatus', 'according', 'wherein', 'configured', 'acquire', 'number', 'contained', 'determine', 'single-person', 'according', 'number', 'select', 'single-person', 'highest', 'cover', 'score', 'cover', 'apparatus', 'according', 'wherein', 'configured', 'single-person', 'photo', 'album', 'determine', 'including', 'two', 'photo', 'album', 'select', 'highest', 'cover', 'score', 'including', 'two', 'cover', 'apparatus', 'according', 'wherein', 'face', 'information', 'comprises', 'face', 'feature', 'points', 'face', 'parameter', 'comprises', 'face', 'turning', 'angle', 'configured', 'acquire', 'coordinate', 'values', 'face', 'feature', 'points', 'determine', 'distances', 'angles', 'face', 'feature', 'points', 'determine', 'face', 'turning', 'angle', 'according', 'distances', 'angles', 'apparatus', 'according', 'wherein', 'face', 'parameter', 'comprises', 'face', 'ratio', 'configured', 'determine', 'face', 'region', 'according', 'face', 'information', 'calculate', 'ratio', 'area', 'face', 'region', 'area', 'obtain', 'face', 'ratio', 'apparatus', 'according', 'wherein', 'configured', 'one', 'face', 'subtract', 'area', 'occupied', 'face', 'corresponding', 'photo', 'album', 'face', 'region', 'obtain', 'remaining', 'area', 'calculate', 'ratio', 'remaining', 'area', 'area', 'obtain', 'face', 'ratio', 'apparatus', 'according', 'wherein', 'configured', 'acquire', 'identifications', 'photo', 'album', 'extract', 'face', 'information', 'corresponding', 'identifications', 'face', 'database', 'face', 'database', 'stored', 'face', 'results', 'face', 'results', 'including', 'face', 'information', 'comprising', 'memory', 'display', 'screen', 'input', 'device', 'connected', 'via', 'system', 'bus', 'wherein', 'memory', 'stored', 'computer', 'programs', 'executed', 'cause', 'implement', 'processing', 'processing', 'comprising', 'acquiring', 'photo', 'album', 'obtained', 'face', 'clustering', 'collecting', 'face', 'information', 'respective', 'photo', 'album', 'acquiring', 'face', 'parameter', 'according', 'face', 'information', 'selecting', 'cover', 'according', 'face', 'parameter', 'taking', 'face-region', 'cover', 'setting', 'face-region', 'cover', 'photo', 'album', 'wherein', 'selecting', 'cover', 'according', 'face', 'parameter', 'comprises', 'performing', 'calculation', 'face', 'parameter', 'preset', 'way', 'obtain', 'cover', 'score', 'selecting', 'highest', 'cover', 'score', 'cover', 'wherein', 'selecting', 'highest', 'cover', 'score', 'cover', 'comprises', 'acquiring', 'source', 'selecting', 'highest', 'cover', 'score', 'coming', 'preset', 'source', 'cover', 'according', 'wherein', 'comprises', 'least', 'one', 'mobile', 'phone', 'tablet', 'computer', 'personal', 'digital', 'assistant', 'wearable', 'device', 'computer-implemented', 'comprising', 'receiving', 'computing', 'device', 'meeting', 'invitation', 'identifying', 'location', 'least', 'one', 'invitee', 'meeting', 'invitation', 'configured', 'provide', 'least', 'one', 'invitee', 'physical', 'access', 'location', 'wherein', 'meeting', 'invitation', 'causes', 'system', 'control', 'pathway', 'allowing', 'physical', 'access', 'location', 'providing', 'based', 'meeting', 'invitation', 'least', 'one', 'invitee', 'physical', 'access', 'location', 'controlling', 'pathway', 'allowing', 'least', 'one', 'invitee', 'physically', 'access', 'location', 'pathway', 'response', 'positioning', 'data', 'indicating', 'least', 'one', 'invitee', 'predetermined', 'location', 'near', 'location', 'wherein', 'positioning', 'data', 'based', 'part', 'face', 'camera', 'system', 'identifying', 'least', 'one', 'invitee', 'receiving', 'positioning', 'data', 'face', 'camera', 'system', 'identifying', 'least', 'one', 'invitee', 'wherein', 'positioning', 'data', 'indicates', 'pattern', 'movement', 'least', 'one', 'invitee', 'determining', 'pattern', 'movement', 'indicates', 'least', 'one', 'invitee', 'exited', 'location', 'revoking', 'physical', 'access', 'location', 'identified', 'meeting', 'invitation', 'controlling', 'pathway', 'restrict', 'least', 'one', 'invitee', 'identified', 'meeting', 'invitation', 'physical', 'access', 'location', 'pathway', 'response', 'determining', 'pattern', 'movement', 'indicates', 'least', 'one', 'invitee', 'exited', 'location', 'computer-implemented', 'wherein', 'determining', 'least', 'one', 'invitee', 'exited', 'location', 'comprises', 'determining', 'least', 'one', 'invitee', 'passed', 'egress', 'associated', 'location', 'predetermined', 'direction', 'computer-implemented', 'wherein', 'determining', 'least', 'one', 'invitee', 'exited', 'location', 'comprises', 'determining', 'least', 'one', 'invitee', 'moved', 'area', 'predetermined', 'direction', 'computer-implemented', 'wherein', 'positioning', 'data', 'indicates', 'second', 'pattern', 'movement', 'least', 'one', 'invitee', 'wherein', 'access', 'secured', 'data', 'associated', 'location', 'provided', 'response', 'detecting', 'second', 'pattern', 'movement', 'computer-implemented', 'comprising', 'collating', 'secured', 'data', 'public', 'data', 'generate', 'resource', 'data', 'communicating', 'resource', 'data', 'client', 'computing', 'device', 'associated', 'least', 'one', 'invitee', 'access', 'location', 'provided', 'computer-implemented', 'wherein', 'positioning', 'data', 'indicates', 'least', 'one', 'invitee', 'predetermined', 'location', 'least', 'one', 'invitee', 'passes', 'predetermined', 'location', 'computer-implemented', 'wherein', 'positioning', 'data', 'indicates', 'least', 'one', 'invitee', 'predetermined', 'location', 'least', 'one', 'invitee', 'passes', 'predetermined', 'location', 'near', 'location', 'predetermined', 'direction', 'system', 'comprising', 'memory', 'communication', 'memory', 'computer-readable', 'instructions', 'stored', 'thereupon', 'executed', 'cause', 'receive', 'meeting', 'invitation', 'indicating', 'location', 'identity', 'meeting', 'invitation', 'configured', 'provide', 'least', 'one', 'invitee', 'physical', 'access', 'location', 'wherein', 'meeting', 'invitation', 'causes', 'system', 'control', 'pathway', 'allowing', 'physical', 'access', 'location', 'provide', 'least', 'one', 'invitee', 'associated', 'identity', 'access', 'location', 'controlling', 'pathway', 'allowing', 'least', 'one', 'invitee', 'physically', 'access', 'location', 'pathway', 'response', 'positioning', 'data', 'indicating', 'least', 'one', 'invitee', 'predetermined', 'location', 'near', 'location', 'wherein', 'positioning', 'data', 'based', 'part', 'face', 'camera', 'system', 'identifying', 'least', 'one', 'invitee', 'receive', 'positioning', 'data', 'face', 'camera', 'system', 'identifying', 'least', 'one', 'invitee', 'wherein', 'positioning', 'data', 'indicates', 'pattern', 'movement', 'least', 'one', 'invitee', 'determine', 'pattern', 'movement', 'indicates', 'least', 'one', 'invitee', 'exited', 'location', 'revoke', 'physical', 'access', 'location', 'identified', 'meeting', 'invitation', 'controlling', 'pathway', 'restrict', 'least', 'one', 'invitee', 'identified', 'meeting', 'invitation', 'physical', 'access', 'location', 'pathway', 'response', 'determining', 'pattern', 'movement', 'indicates', 'least', 'one', 'invitee', 'exited', 'location', 'system', 'wherein', 'determining', 'least', 'one', 'invitee', 'exited', 'location', 'comprises', 'determining', 'least', 'one', 'invitee', 'passed', 'egress', 'associated', 'location', 'system', 'wherein', 'determining', 'least', 'one', 'invitee', 'exited', 'location', 'comprises', 'determining', 'least', 'one', 'invitee', 'moved', 'area', 'predetermined', 'direction', 'system', 'wherein', 'positioning', 'data', 'indicates', 'second', 'pattern', 'movement', 'least', 'one', 'invitee', 'wherein', 'access', 'secured', 'data', 'associated', 'location', 'provided', 'response', 'detecting', 'second', 'pattern', 'movement', 'system', 'wherein', 'instructions', 'cause', 'collate', 'secured', 'data', 'public', 'data', 'generate', 'resource', 'data', 'communicate', 'resource', 'data', 'client', 'computing', 'device', 'associated', 'least', 'one', 'invitee', 'access', 'location', 'provided', 'non-transitory', 'computer-readable', 'storage', 'medium', 'computer-executable', 'instructions', 'stored', 'thereupon', 'executed', 'one', 'computing', 'device', 'cause', 'one', 'computing', 'device', 'receive', 'meeting', 'invitation', 'indicating', 'location', 'identity', 'meeting', 'invitation', 'configured', 'provide', 'least', 'one', 'invitee', 'physical', 'access', 'location', 'wherein', 'meeting', 'invitation', 'causes', 'system', 'control', 'pathway', 'allowing', 'physical', 'access', 'location', 'provide', 'least', 'one', 'invitee', 'associated', 'identity', 'access', 'location', 'controlling', 'pathway', 'allowing', 'least', 'one', 'invitee', 'physically', 'access', 'location', 'pathway', 'response', 'positioning', 'data', 'indicating', 'least', 'one', 'invitee', 'predetermined', 'location', 'near', 'location', 'wherein', 'positioning', 'data', 'based', 'part', 'face', 'camera', 'system', 'identifying', 'least', 'one', 'invitee', 'receive', 'positioning', 'data', 'face', 'camera', 'system', 'identifying', 'least', 'one', 'invitee', 'wherein', 'positioning', 'data', 'indicates', 'pattern', 'movement', 'least', 'one', 'invitee', 'determine', 'pattern', 'movement', 'indicates', 'least', 'one', 'invitee', 'exited', 'location', 'revoke', 'physical', 'access', 'location', 'identified', 'meeting', 'invitation', 'controlling', 'pathway', 'restrict', 'least', 'one', 'invitee', 'identified', 'meeting', 'invitation', 'physical', 'access', 'location', 'pathway', 'response', 'determining', 'pattern', 'movement', 'indicates', 'least', 'one', 'invitee', 'exited', 'location', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'determining', 'least', 'one', 'invitee', 'exited', 'location', 'comprises', 'determining', 'least', 'one', 'invitee', 'passed', 'egress', 'associated', 'location', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'positioning', 'data', 'indicates', 'second', 'pattern', 'movement', 'least', 'one', 'invitee', 'wherein', 'access', 'secured', 'data', 'associated', 'location', 'provided', 'response', 'detecting', 'second', 'pattern', 'movement', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'instructions', 'cause', 'one', 'collate', 'secured', 'data', 'public', 'data', 'generate', 'resource', 'data', 'communicate', 'resource', 'data', 'client', 'computing', 'device', 'associated', 'least', 'one', 'invitee', 'access', 'location', 'provided', 'comprising', 'receiving', 'piece', 'content', 'salient', 'data', 'piece', 'content', 'based', 'salient', 'data', 'determining', 'first', 'path', 'viewport', 'piece', 'content', 'wherein', 'first', 'path', 'viewport', 'includes', 'different', 'salient', 'events', 'occurring', 'piece', 'content', 'different', 'times', 'playback', 'piece', 'content', 'providing', 'viewport', 'display', 'device', 'wherein', 'movement', 'viewport', 'based', 'first', 'path', 'viewport', 'salient', 'data', 'playback', 'detecting', 'additional', 'salient', 'event', 'piece', 'content', 'included', 'first', 'path', 'viewport', 'providing', 'indication', 'additional', 'salient', 'event', 'viewport', 'playback', 'wherein', 'salient', 'data', 'identifies', 'salient', 'event', 'piece', 'content', 'salient', 'data', 'indicates', 'salient', 'event', 'piece', 'content', 'corresponding', 'point', 'location', 'salient', 'event', 'piece', 'content', 'corresponding', 'time', 'salient', 'event', 'occurs', 'playback', 'wherein', 'salient', 'data', 'indicates', 'salient', 'event', 'piece', 'content', 'corresponding', 'type', 'salient', 'event', 'corresponding', 'strength', 'value', 'salient', 'event', 'wherein', 'first', 'path', 'viewport', 'controls', 'movement', 'viewport', 'put', 'different', 'salient', 'events', 'view', 'viewport', 'different', 'times', 'playback', 'comprising', 'detecting', 'one', 'salient', 'events', 'piece', 'content', 'based', 'least', 'one', 'following', 'visual', 'data', 'piece', 'content', 'audio', 'data', 'piece', 'content', 'content', 'consumption', 'experience', 'data', 'piece', 'content', 'wherein', 'salient', 'data', 'indicative', 'salient', 'event', 'detected', 'comprising', 'detecting', 'one', 'salient', 'events', 'piece', 'content', 'based', 'least', 'one', 'following', 'face', 'facial', 'emotion', 'object', 'motion', 'metadata', 'piece', 'content', 'wherein', 'salient', 'data', 'indicative', 'salient', 'event', 'detected', 'comprising', 'detecting', 'interaction', 'indication', 'wherein', 'indication', 'comprises', 'interactive', 'hint', 'response', 'detecting', 'interaction', 'adapting', 'first', 'path', 'viewport', 'second', 'path', 'viewport', 'based', 'interaction', 'wherein', 'second', 'path', 'viewport', 'includes', 'additional', 'salient', 'event', 'providing', 'updated', 'viewport', 'piece', 'content', 'display', 'device', 'wherein', 'movement', 'updated', 'viewport', 'based', 'second', 'path', 'viewport', 'salient', 'data', 'playback', 'second', 'path', 'viewport', 'controls', 'movement', 'updated', 'viewport', 'put', 'additional', 'salient', 'event', 'view', 'updated', 'viewport', 'comprising', 'changing', 'weight', 'assigned', 'additional', 'salient', 'event', 'one', 'salient', 'events', 'piece', 'content', 'type', 'additional', 'salient', 'event', 'wherein', 'second', 'path', 'viewport', 'includes', 'one', 'salient', 'events', 'piece', 'content', 'type', 'additional', 'salient', 'event', 'system', 'comprising', 'least', 'one', 'non-transitory', '-readable', 'memory', 'device', 'storing', 'instructions', 'executed', 'least', 'one', 'causes', 'least', 'one', 'perform', 'operations', 'including', 'receiving', 'piece', 'content', 'salient', 'data', 'piece', 'content', 'based', 'salient', 'data', 'determining', 'first', 'path', 'viewport', 'piece', 'content', 'wherein', 'first', 'path', 'viewport', 'includes', 'different', 'salient', 'events', 'occurring', 'piece', 'content', 'different', 'times', 'playback', 'piece', 'content', 'providing', 'viewport', 'display', 'device', 'wherein', 'movement', 'viewport', 'based', 'first', 'path', 'viewport', 'salient', 'data', 'playback', 'detecting', 'additional', 'salient', 'event', 'piece', 'content', 'included', 'first', 'path', 'viewport', 'providing', 'indication', 'additional', 'salient', 'event', 'viewport', 'playback', 'system', 'wherein', 'salient', 'data', 'identifies', 'salient', 'event', 'piece', 'content', 'salient', 'data', 'indicates', 'salient', 'event', 'piece', 'content', 'corresponding', 'point', 'location', 'salient', 'event', 'piece', 'content', 'corresponding', 'time', 'salient', 'event', 'occurs', 'playback', 'system', 'wherein', 'salient', 'data', 'indicates', 'salient', 'event', 'piece', 'content', 'corresponding', 'type', 'salient', 'event', 'corresponding', 'strength', 'value', 'salient', 'event', 'system', 'wherein', 'salient', 'data', 'generated', 'offline', 'server', 'system', 'operations', 'comprising', 'detecting', 'one', 'salient', 'events', 'piece', 'content', 'based', 'least', 'one', 'following', 'visual', 'data', 'piece', 'content', 'audio', 'data', 'piece', 'content', 'content', 'consumption', 'experience', 'data', 'piece', 'content', 'wherein', 'salient', 'data', 'indicative', 'salient', 'event', 'detected', 'system', 'operations', 'comprising', 'detecting', 'one', 'salient', 'events', 'piece', 'content', 'based', 'least', 'one', 'following', 'face', 'facial', 'emotion', 'object', 'motion', 'metadata', 'piece', 'content', 'wherein', 'salient', 'data', 'indicative', 'salient', 'event', 'detected', 'system', 'operations', 'comprising', 'detecting', 'interaction', 'indication', 'wherein', 'indication', 'comprises', 'interactive', 'hint', 'response', 'detecting', 'interaction', 'adapting', 'first', 'path', 'viewport', 'second', 'path', 'viewport', 'based', 'interaction', 'wherein', 'second', 'path', 'viewport', 'includes', 'additional', 'salient', 'event', 'providing', 'updated', 'viewport', 'piece', 'content', 'display', 'device', 'wherein', 'movement', 'updated', 'viewport', 'based', 'second', 'path', 'viewport', 'salient', 'data', 'playback', 'second', 'path', 'viewport', 'controls', 'movement', 'updated', 'viewport', 'put', 'additional', 'salient', 'event', 'view', 'updated', 'viewport', 'system', 'operations', 'comprising', 'changing', 'weight', 'assigned', 'additional', 'salient', 'event', 'one', 'salient', 'events', 'piece', 'content', 'type', 'additional', 'salient', 'event', 'system', 'wherein', 'second', 'path', 'viewport', 'includes', 'one', 'salient', 'events', 'piece', 'content', 'type', 'additional', 'salient', 'event', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'including', 'instructions', 'perform', 'comprising', 'receiving', 'piece', 'content', 'salient', 'data', 'piece', 'content', 'based', 'salient', 'data', 'determining', 'first', 'path', 'viewport', 'piece', 'content', 'wherein', 'first', 'path', 'viewport', 'includes', 'different', 'salient', 'events', 'occurring', 'piece', 'content', 'different', 'times', 'playback', 'piece', 'content', 'providing', 'viewport', 'display', 'device', 'wherein', 'movement', 'viewport', 'based', 'first', 'path', 'viewport', 'salient', 'data', 'playback', 'detecting', 'additional', 'salient', 'event', 'piece', 'content', 'included', 'first', 'path', 'viewport', 'providing', 'indication', 'additional', 'salient', 'event', 'viewport', 'playback', 'computer', 'readable', 'storage', 'medium', 'comprising', 'detecting', 'interaction', 'indication', 'wherein', 'indication', 'comprises', 'interactive', 'hint', 'response', 'detecting', 'interaction', 'adapting', 'first', 'path', 'viewport', 'second', 'path', 'viewport', 'based', 'interaction', 'wherein', 'second', 'path', 'viewport', 'includes', 'additional', 'salient', 'event', 'providing', 'updated', 'viewport', 'piece', 'content', 'display', 'device', 'wherein', 'movement', 'updated', 'viewport', 'based', 'second', 'path', 'viewport', 'salient', 'data', 'playback', 'second', 'path', 'viewport', 'controls', 'movement', 'updated', 'viewport', 'put', 'additional', 'salient', 'event', 'view', 'updated', 'viewport', 'mobile', 'device', 'facial', 'mobile', 'device', 'comprising', 'one', 'cameras', 'device', 'memory', 'coupled', 'device', 'processing', 'system', 'programmed', 'receive', 'one', 'cameras', 'extract', 'feature', 'extractor', 'utilizing', 'convolutional', 'neural', 'network', 'cnn', 'enlarged', 'intra-class', 'variance', 'long-tail', 'classes', 'feature', 'vectors', 'generate', 'feature', 'generator', 'discriminative', 'feature', 'vectors', 'feature', 'vectors', 'classify', 'fully', 'connected', 'classifier', 'identity', 'discriminative', 'feature', 'vectors', 'control', 'operation', 'mobile', 'device', 'react', 'accordance', 'identity', 'mobile', 'device', 'recited', 'includes', 'communication', 'system', 'mobile', 'device', 'recited', 'wherein', 'operation', 'tags', 'video', 'identity', 'uploads', 'video', 'social', 'media', 'mobile', 'device', 'recited', 'wherein', 'operation', 'tags', 'video', 'identity', 'sends', 'video', 'mobile', 'device', 'recited', 'wherein', 'mobile', 'device', 'smart', 'phone', 'mobile', 'device', 'recited', 'wherein', 'mobile', 'device', 'body', 'cam', 'mobile', 'device', 'recited', 'programmed', 'train', 'feature', 'extractor', 'feature', 'generator', 'fully', 'connected', 'classifier', 'alternative', 'bi-stage', 'strategy', 'mobile', 'device', 'recited', 'wherein', 'feature', 'extractor', 'shares', 'covariance', 'matrices', 'across', 'classes', 'transfer', 'intra-class', 'variance', 'regular', 'classes', 'long-tail', 'classes', 'mobile', 'device', 'recited', 'wherein', 'feature', 'generator', 'optimizes', 'softmax', 'loss', 'joint', 'regularization', 'weights', 'features', 'magnitude', 'inner', 'product', 'weights', 'features', 'mobile', 'device', 'recited', 'wherein', 'feature', 'extractor', 'averages', 'feature', 'vector', 'flipped', 'feature', 'vector', 'flipped', 'feature', 'vector', 'generated', 'horizontally', 'flipped', 'frame', 'one', 'mobile', 'device', 'recited', 'wherein', 'selected', 'group', 'consisting', 'video', 'frame', 'video', 'mobile', 'device', 'recited', 'wherein', 'communication', 'system', 'connects', 'remote', 'server', 'includes', 'facial', 'network', 'mobile', 'device', 'recited', 'wherein', 'one', 'stage', 'alternative', 'bi-stage', 'strategy', 'fixes', 'feature', 'extractor', 'applies', 'feature', 'generator', 'generate', 'new', 'transferred', 'features', 'diverse', 'violate', 'decision', 'boundary', 'mobile', 'device', 'recited', 'wherein', 'one', 'stage', 'alternative', 'bi-stage', 'strategy', 'fixes', 'fully', 'connected', 'classifier', 'updates', 'feature', 'extractor', 'feature', 'generator', 'computer', 'program', 'product', 'mobile', 'device', 'facial', 'computer', 'program', 'product', 'comprising', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'program', 'instructions', 'embodied', 'therewith', 'program', 'instructions', 'executable', 'computer', 'cause', 'computer', 'perform', 'comprising', 'receiving', 'device', 'extracting', 'device', 'feature', 'extractor', 'utilizing', 'convolutional', 'neural', 'network', 'cnn', 'enlarged', 'intra-class', 'variance', 'long-tail', 'classes', 'feature', 'vectors', 'generating', 'device', 'feature', 'generator', 'discriminative', 'feature', 'vectors', 'feature', 'vectors', 'classifying', 'device', 'utilizing', 'fully', 'connected', 'classifier', 'identity', 'discriminative', 'feature', 'vector', 'controlling', 'operation', 'mobile', 'device', 'react', 'accordance', 'identity', 'computer-implemented', 'facial', 'mobile', 'device', 'comprising', 'receiving', 'device', 'extracting', 'device', 'feature', 'extractor', 'utilizing', 'convolutional', 'neural', 'network', 'cnn', 'enlarged', 'intra-class', 'variance', 'long-tail', 'classes', 'feature', 'vectors', 'generating', 'device', 'feature', 'generator', 'discriminative', 'feature', 'vectors', 'feature', 'vectors', 'classifying', 'device', 'utilizing', 'fully', 'connected', 'classifier', 'identity', 'discriminative', 'feature', 'vector', 'controlling', 'operation', 'mobile', 'device', 'react', 'accordance', 'identity', 'computer-implemented', 'recited', 'wherein', 'controlling', 'includes', 'tagging', 'video', 'identity', 'uploading', 'video', 'social', 'media', 'computer-implemented', 'recited', 'wherein', 'controlling', 'includes', 'tagging', 'video', 'identity', 'sending', 'video', 'computer-implemented', 'recited', 'wherein', 'extracting', 'includes', 'sharing', 'covariance', 'matrices', 'across', 'classes', 'transfer', 'intra-class', 'variance', 'regular', 'classes', 'long-tail', 'classes', 'computing', 'device', 'comprising', 'non-transitory', 'machine', 'readable', 'medium', 'storing', 'machine', 'trained', 'mt', 'network', 'comprising', 'layers', 'processing', 'nodes', 'processing', 'node', 'configured', 'compute', 'first', 'output', 'value', 'combining', 'set', 'output', 'values', 'set', 'processing', 'nodes', 'use', 'piecewise', 'linear', 'cup', 'function', 'compute', 'second', 'output', 'value', 'first', 'output', 'value', 'processing', 'node', 'wherein', 'piecewise', 'linear', 'cup', 'function', 'prior', 'training', 'mt', 'network', 'comprises', 'least', 'first', 'linear', 'section', 'first', 'slope', 'followed', 'ii', 'second', 'linear', 'section', 'negative', 'second', 'slope', 'followed', 'iii', 'third', 'linear', 'section', 'negative', 'third', 'slope', 'different', 'second', 'slope', 'followed', 'iv', 'fourth', 'linear', 'section', 'positive', 'fourth', 'slope', 'followed', 'v', 'fifth', 'linear', 'section', 'positive', 'fifth', 'slope', 'different', 'fourth', 'slope', 'followed', 'vi', 'sixth', 'linear', 'section', 'sixth', 'slope', 'wherein', 'piecewise', 'linear', 'cup', 'function', 'symmetric', 'vertical', 'axis', 'third', 'fourth', 'linear', 'sections', 'prior', 'training', 'mt', 'network', 'content', 'capturing', 'circuit', 'capturing', 'content', 'processing', 'mt', 'network', 'set', 'processing', 'units', 'executing', 'processing', 'nodes', 'process', 'content', 'captured', 'content', 'capturing', 'circuit', 'wherein', 'training', 'set', 'parameters', 'define', 'piecewise', 'linear', 'cup', 'function', 'node', 'first', 'second', 'pluralities', 'processing', 'nodes', 'processing', 'node', 'first', 'processing', 'nodes', 'configured', 'emulate', 'boolean', 'operator', 'output', 'value', 'processing', 'node', 'range', 'associated', '``', \"''\", 'value', 'set', 'inputs', 'processing', 'node', 'set', 'values', 'range', 'associated', '``', \"''\", 'ii', 'processing', 'node', 'second', 'processing', 'nodes', 'configured', 'emulate', 'boolean', 'xnor', 'operator', 'output', 'value', 'processing', 'node', 'range', 'associated', '``', \"''\", 'set', 'inputs', 'node', 'set', 'values', 'range', 'associated', '``', \"''\", 'b', 'set', 'inputs', 'node', 'set', 'values', 'range', 'associated', '``', \"''\", 'value', 'computing', 'device', 'wherein', 'third', 'linear', 'section', 'piecewise', 'linear', 'cup', 'function', 'first', 'processing', 'node', 'mt', 'network', 'different', 'slope', 'third', 'linear', 'section', 'second', 'processing', 'node', 'mt', 'network', 'computing', 'device', 'wherein', 'length', 'third', 'section', 'piecewise', 'linear', 'cup', 'function', 'first', 'processing', 'node', 'mt', 'network', 'different', 'length', 'third', 'section', 'piecewise', 'linear', 'cup', 'function', 'second', 'processing', 'node', 'mt', 'network', 'computing', 'device', 'wherein', 'sets', 'parameters', 'trained', 'part', 'back', 'propagating', 'module', 'back', 'propagating', 'errors', 'output', 'values', 'later', 'layers', 'processing', 'nodes', 'earlier', 'layers', 'processing', 'nodes', 'adjusting', 'set', 'parameters', 'define', 'piecewise', 'linear', 'cup', 'functions', 'earlier', 'layers', 'processing', 'nodes', 'computing', 'device', 'wherein', 'processing', 'node', 'uses', 'linear', 'function', 'defined', 'set', 'parameters', 'compute', 'first', 'output', 'value', 'processing', 'node', 'wherein', 'back', 'propagating', 'module', 'back', 'propagates', 'errors', 'output', 'values', 'later', 'layers', 'processing', 'nodes', 'earlier', 'layers', 'processing', 'nodes', 'adjusting', 'set', 'parameters', 'define', 'linear', 'functions', 'earlier', 'layers', 'processing', 'nodes', 'computing', 'device', 'wherein', 'first', 'processing', 'nodes', 'emulate', 'boolean', 'operator', 'second', 'processing', 'nodes', 'emulate', 'boolean', 'xnor', 'operator', 'enable', 'mt', 'network', 'implement', 'mathematical', 'problems', 'computing', 'device', 'wherein', 'processing', 'node', 'layers', 'processing', 'nodes', 'receive', 'input', 'values', 'output', 'values', 'processing', 'nodes', 'set', 'prior', 'layers', 'computing', 'device', 'wherein', 'processing', 'node', 'uses', 'linear', 'function', 'compute', 'first', 'output', 'value', 'processing', 'node', 'wherein', 'processing', 'node', \"'s\", 'piecewise', 'linear', 'cup', 'function', 'defined', 'along', 'first', 'second', 'axes', 'first', 'axis', 'defining', 'range', 'output', 'values', 'processing', 'node', \"'s\", 'linear', 'function', 'second', 'axis', 'defining', 'range', 'output', 'values', 'produced', 'piecewise', 'linear', 'cup', 'function', 'range', 'output', 'values', 'processing', 'node', \"'s\", 'linear', 'function', 'computing', 'device', 'comprising', 'content', 'output', 'circuit', 'presenting', 'output', 'based', 'processing', 'content', 'mt', 'network', 'computing', 'device', 'wherein', 'captured', 'content', 'one', 'audio', 'segment', 'wherein', 'presented', 'output', 'output', 'display', 'display', 'screen', 'computing', 'device', 'audio', 'presentation', 'output', 'speaker', 'computing', 'device', 'computing', 'device', 'wherein', 'computing', 'device', 'mobile', 'device', 'computing', 'device', 'wherein', 'mt', 'network', 'mt', 'neural', 'network', 'processing', 'nodes', 'mt', 'neurons', 'computing', 'device', 'wherein', 'set', 'parameters', 'configured', 'training', 'processing', 'nodes', 'comprise', 'least', 'one', 'negative', 'second', 'third', 'slopes', 'second', 'third', 'linear', 'sections', 'positive', 'fourth', 'fifth', 'slopes', 'fourth', 'fifth', 'linear', 'sections', 'first', 'intercept', 'second', 'linear', 'section', 'second', 'intercept', 'fifth', 'linear', 'section', 'set', 'lengths', 'least', 'second', 'third', 'fourth', 'fifth', 'sections', 'computing', 'device', 'wherein', 'trained', 'set', 'parameters', 'define', 'piecewise', 'linear', 'cup', 'function', 'node', 'comprise', 'output', 'values', 'computing', 'device', 'wherein', 'first', 'sixth', 'slopes', 'zerowe', 'system', 'comprising', 'memory', 'device', 'store', 'input', 'including', 'input', 'interface', 'receive', 'input', 'pre-', 'model', 'input', 'yield', 'multi-channel', 'feature', 'extractor', 'extract', 'set', 'features', 'based', 'multi-channel', 'feature', 'selector', 'select', 'one', 'features', 'set', 'features', 'multi-channel', 'wherein', 'one', 'features', 'selected', 'based', 'ability', 'differentiate', 'features', 'feature', 'matcher', 'match', 'one', 'features', 'learned', 'feature', 'set', 'similarity', 'detector', 'determine', 'whether', 'one', 'features', 'meet', 'pre-defined', 'similarity', 'threshold', 'system', 'wherein', 'pre-', 'activate', 'one', 'channels', 'multi-channel', 'yield', 'one', 'activated', 'channels', 'system', 'wherein', 'one', 'activated', 'channels', 'determined', 'based', 'ability', 'differentiate', 'features', 'system', 'wherein', 'pre-', 'activate', 'one', 'local', 'patches', 'one', 'activated', 'channels', 'system', 'wherein', 'one', 'local', 'patches', 'determined', 'based', 'ability', 'differentiate', 'features', 'system', 'wherein', 'feature', 'matcher', 'utilize', 'large-scale', 'data', 'learning', 'process', 'perform', 'feature', 'matching', 'apparatus', 'comprising', 'input', 'interface', 'receive', 'input', 'pre-', 'model', 'input', 'yield', 'multi-channel', 'feature', 'extractor', 'extract', 'set', 'features', 'based', 'multi-channel', 'feature', 'selector', 'select', 'one', 'features', 'set', 'features', 'multi-channel', 'wherein', 'one', 'features', 'selected', 'based', 'ability', 'differentiate', 'features', 'feature', 'matcher', 'match', 'one', 'features', 'learned', 'feature', 'set', 'similarity', 'detector', 'determine', 'whether', 'one', 'features', 'meet', 'pre-defined', 'similarity', 'threshold', 'apparatus', 'wherein', 'pre-', 'activate', 'one', 'channels', 'multi-channel', 'yield', 'one', 'activated', 'channels', 'apparatus', 'wherein', 'one', 'activated', 'channels', 'determined', 'based', 'ability', 'differentiate', 'features', 'apparatus', 'wherein', 'pre-', 'activate', 'one', 'local', 'patches', 'one', 'activated', 'channels', 'apparatus', 'wherein', 'one', 'local', 'patches', 'determined', 'based', 'ability', 'differentiate', 'features', 'apparatus', 'wherein', 'feature', 'matcher', 'utilize', 'large-scale', 'data', 'learning', 'process', 'perform', 'feature', 'matching', 'comprising', 'modeling', 'input', 'yield', 'multi-channel', 'extracting', 'set', 'features', 'based', 'multi-channel', 'selecting', 'one', 'features', 'set', 'features', 'multi-channel', 'wherein', 'one', 'features', 'selected', 'based', 'ability', 'differentiate', 'features', 'matching', 'one', 'features', 'learned', 'feature', 'set', 'determining', 'whether', 'one', 'features', 'meet', 'pre-defined', 'similarity', 'threshold', 'wherein', 'modeling', 'input', 'include', 'activating', 'one', 'channels', 'multi-channel', 'yield', 'one', 'activated', 'channels', 'wherein', 'one', 'activated', 'channels', 'determined', 'based', 'ability', 'differentiate', 'features', 'wherein', 'extracting', 'features', 'input', 'include', 'activating', 'one', 'local', 'patches', 'one', 'activated', 'channels', 'wherein', 'one', 'local', 'patches', 'determined', 'based', 'ability', 'differentiate', 'features', 'wherein', 'feature', 'matcher', 'utilizes', 'large-scale', 'data', 'learning', 'process', 'perform', 'feature', 'matching', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'comprising', 'set', 'instructions', 'executed', 'computing', 'device', 'cause', 'computing', 'device', 'model', 'input', 'yield', 'multi-channel', 'extract', 'set', 'features', 'based', 'multi-channel', 'select', 'one', 'features', 'set', 'features', 'multi-channel', 'wherein', 'features', 'selected', 'based', 'ability', 'differentiate', 'features', 'match', 'one', 'features', 'learned', 'feature', 'set', 'determine', 'whether', 'one', 'features', 'meet', 'pre-defined', 'similarity', 'threshold', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'wherein', 'instructions', 'executed', 'cause', 'computing', 'device', 'activate', 'one', 'channels', 'multi-channel', 'yield', 'one', 'activated', 'channels', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'wherein', 'instructions', 'executed', 'cause', 'computing', 'device', 'determine', 'one', 'activated', 'channels', 'based', 'ability', 'differentiate', 'features', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'wherein', 'extracting', 'features', 'input', 'include', 'activating', 'one', 'local', 'patches', 'one', 'activated', 'channels', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'wherein', 'one', 'local', 'patches', 'determined', 'based', 'ability', 'differentiate', 'features', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'wherein', 'feature', 'matcher', 'utilize', 'large-scale', 'data', 'learning', 'process', 'perform', 'feature', 'matching', 'apparatus', 'comprising', 'means', 'modeling', 'input', 'yield', 'multi-channel', 'means', 'extracting', 'set', 'features', 'based', 'multi-channel', 'means', 'selecting', 'one', 'features', 'set', 'features', 'multi-channel', 'wherein', 'one', 'features', 'selected', 'based', 'ability', 'differentiate', 'features', 'means', 'matching', 'one', 'features', 'learned', 'feature', 'set', 'means', 'determining', 'whether', 'one', 'features', 'meet', 'pre-defined', 'similarity', 'threshold', 'controlling', 'terminal', 'terminal', 'comprising', 'capturing', 'apparatus', 'least', 'one', 'comprising', 'acquiring', 'capturing', 'apparatus', 'obtaining', 'least', 'one', 'motion', 'parameter', 'terminal', 'motion', 'parameter', 'comprising', 'least', 'one', 'motion', 'frequency', 'motion', 'time', 'two', 'parameters', 'among', 'acceleration', 'angular', 'velocity', 'motion', 'amplitude', 'motion', 'frequency', 'motion', 'time', 'transmitting', 'least', 'one', 'parameter', 'threshold', 'obtaining', 'request', 'data', 'management', 'server', 'parameter', 'threshold', 'obtaining', 'request', 'comprising', 'configuration', 'information', 'terminal', 'receiving', 'corresponding', 'preset', 'thresholds', 'correspond', 'configuration', 'information', 'response', 'parameter', 'threshold', 'obtaining', 'request', 'comparing', 'two', 'parameters', 'corresponding', 'preset', 'thresholds', 'controlling', 'least', 'one', 'perform', 'processing', 'acquired', 'based', 'least', 'one', 'two', 'parameters', 'motion', 'parameter', 'greater', 'corresponding', 'preset', 'threshold', 'based', 'two', 'parameters', 'motion', 'parameter', 'respectively', 'greater', 'corresponding', 'preset', 'thresholds', 'wherein', 'acquiring', 'comprises', 'acquiring', 'real', 'time', 'obtaining', 'comprises', 'obtaining', 'motion', 'parameter', 'terminal', 'real', 'time', 'comprising', 'response', 'least', 'one', 'two', 'parameters', 'motion', 'parameter', 'greater', 'corresponding', 'preset', 'threshold', 'obtaining', 'motion', 'parameter', 'terminal', 'response', 'two', 'parameters', 'motion', 'parameter', 'obtained', 'latest', 'time', 'less', 'equal', 'corresponding', 'preset', 'thresholds', 'performing', 'processing', 'acquired', 'latest', 'time', 'according', 'wherein', 'acquiring', 'comprises', 'controlling', 'least', 'one', 'turn', 'capturing', 'apparatus', 'based', 'face', 'instruction', 'acquiring', 'capturing', 'apparatus', 'face', 'capturing', 'apparatus', 'turned', 'according', 'wherein', 'controlling', 'perform', 'processing', 'comprises', 'skipping', 'performing', 'face', 'acquired', 'face', 'based', 'least', 'one', 'two', 'parameters', 'motion', 'parameter', 'greater', 'corresponding', 'preset', 'threshold', 'based', 'two', 'parameters', 'motion', 'parameter', 'respectively', 'greater', 'corresponding', 'preset', 'thresholds', 'according', 'wherein', 'obtaining', 'comprises', 'least', 'one', 'obtaining', 'acceleration', 'terminal', 'using', 'acceleration', 'sensor', 'obtaining', 'angular', 'velocity', 'terminal', 'using', 'gyro', 'sensor', 'according', 'wherein', 'transmitting', 'comprises', 'transmitting', 'parameter', 'threshold', 'obtaining', 'request', 'data', 'management', 'server', 'according', 'preset', 'time', 'period', 'according', 'comprising', 'generating', 'prompt', 'information', 'based', 'least', 'one', 'two', 'parameters', 'motion', 'parameter', 'greater', 'corresponding', 'preset', 'threshold', 'prompt', 'information', 'used', 'prompting', 'terminal', 'stop', 'moving', 'according', 'wherein', 'motion', 'parameter', 'comprises', 'motion', 'frequency', 'motion', 'time', 'terminal', 'comprising', 'capturing', 'apparatus', 'least', 'one', 'memory', 'configured', 'store', 'program', 'code', 'least', 'one', 'configured', 'access', 'least', 'one', 'memory', 'operate', 'according', 'program', 'code', 'program', 'code', 'comprising', 'motion', 'parameter', 'obtaining', 'code', 'configured', 'cause', 'least', 'one', 'acquire', 'using', 'capturing', 'apparatus', 'obtain', 'motion', 'parameter', 'terminal', 'motion', 'parameter', 'comprising', 'least', 'one', 'motion', 'frequency', 'motion', 'time', 'two', 'parameters', 'among', 'acceleration', 'angular', 'velocity', 'motion', 'amplitude', 'motion', 'frequency', 'motion', 'time', 'request', 'transmitting', 'code', 'configured', 'cause', 'least', 'one', 'transmit', 'parameter', 'threshold', 'obtaining', 'request', 'data', 'management', 'server', 'parameter', 'threshold', 'obtaining', 'request', 'comprising', 'configuration', 'information', 'terminal', 'parameter', 'threshold', 'receiving', 'code', 'configured', 'cause', 'least', 'one', 'receive', 'corresponding', 'preset', 'thresholds', 'correspond', 'configuration', 'information', 'response', 'parameter', 'threshold', 'obtaining', 'request', 'comparing', 'code', 'configured', 'cause', 'least', 'one', 'compare', 'two', 'parameters', 'corresponding', 'preset', 'thresholds', 'control', 'code', 'configured', 'cause', 'least', 'one', 'perform', 'processing', 'acquired', 'based', 'least', 'one', 'two', 'parameters', 'motion', 'parameter', 'greater', 'corresponding', 'preset', 'threshold', 'based', 'two', 'parameters', 'motion', 'parameter', 'respectively', 'greater', 'corresponding', 'preset', 'thresholds', 'wherein', 'motion', 'parameter', 'obtaining', 'code', 'causes', 'least', 'one', 'acquire', 'real', 'time', 'obtain', 'motion', 'parameter', 'terminal', 'real', 'time', 'response', 'least', 'one', 'two', 'parameters', 'motion', 'parameter', 'greater', 'corresponding', 'preset', 'threshold', 'obtain', 'motion', 'parameter', 'terminal', 'wherein', 'control', 'code', 'causes', 'least', 'one', 'response', 'two', 'parameters', 'motion', 'parameter', 'obtained', 'latest', 'time', 'less', 'equal', 'corresponding', 'preset', 'thresholds', 'perform', 'processing', 'acquired', 'latest', 'time', 'terminal', 'according', 'wherein', 'program', 'code', 'comprises', 'face', 'instruction', 'receiving', 'code', 'configured', 'cause', 'least', 'one', 'receive', 'face', 'instruction', 'wherein', 'motion', 'parameter', 'obtaining', 'code', 'causes', 'least', 'one', 'control', 'according', 'face', 'instruction', 'capturing', 'apparatus', 'turn', 'acquire', 'face', 'using', 'capturing', 'apparatus', 'capturing', 'apparatus', 'turned', 'wherein', 'control', 'code', 'causes', 'least', 'one', 'skip', 'performing', 'face', 'acquired', 'face', 'based', 'least', 'one', 'two', 'parameters', 'motion', 'parameter', 'greater', 'corresponding', 'preset', 'threshold', 'based', 'two', 'parameters', 'motion', 'parameter', 'respectively', 'greater', 'corresponding', 'preset', 'thresholds', 'terminal', 'according', 'wherein', 'request', 'transmitting', 'code', 'causes', 'least', 'one', 'transmit', 'parameter', 'threshold', 'obtaining', 'request', 'data', 'management', 'server', 'according', 'preset', 'time', 'period', 'terminal', 'according', 'wherein', 'program', 'code', 'comprises', 'prompt', 'information', 'generation', 'code', 'configured', 'cause', 'least', 'one', 'generate', 'prompt', 'information', 'based', 'least', 'one', 'two', 'parameters', 'motion', 'parameter', 'greater', 'corresponding', 'preset', 'threshold', 'prompt', 'information', 'used', 'prompting', 'terminal', 'stop', 'moving', 'terminal', 'according', 'wherein', 'motion', 'parameter', 'comprises', 'motion', 'frequency', 'motion', 'time', 'non-transitory', 'computer-readable', 'storage', 'medium', 'storing', 'machine', 'instruction', 'executed', 'one', 'causes', 'one', 'perform', 'obtaining', 'acquired', 'capturing', 'apparatus', 'obtaining', 'motion', 'parameter', 'terminal', 'terminal', 'comprising', 'capturing', 'apparatus', 'motion', 'parameter', 'comprising', 'least', 'one', 'motion', 'frequency', 'motion', 'time', 'two', 'parameters', 'among', 'acceleration', 'angular', 'velocity', 'motion', 'amplitude', 'motion', 'frequency', 'motion', 'time', 'transmitting', 'parameter', 'threshold', 'obtaining', 'request', 'data', 'management', 'server', 'parameter', 'threshold', 'obtaining', 'request', 'comprising', 'configuration', 'information', 'terminal', 'receiving', 'corresponding', 'preset', 'thresholds', 'correspond', 'configuration', 'information', 'response', 'parameter', 'threshold', 'obtaining', 'request', 'comparing', 'two', 'parameters', 'corresponding', 'preset', 'thresholds', 'controlling', 'perform', 'processing', 'acquired', 'based', 'least', 'one', 'two', 'parameters', 'motion', 'parameter', 'greater', 'corresponding', 'preset', 'threshold', 'based', 'two', 'parameters', 'motion', 'parameter', 'respectively', 'greater', 'corresponding', 'preset', 'thresholds', 'wherein', 'acquiring', 'comprises', 'acquiring', 'real', 'time', 'obtaining', 'comprises', 'obtaining', 'motion', 'parameter', 'terminal', 'real', 'time', 'comprising', 'response', 'least', 'one', 'two', 'parameters', 'motion', 'parameter', 'greater', 'corresponding', 'preset', 'threshold', 'obtaining', 'motion', 'parameter', 'terminal', 'response', 'two', 'parameters', 'motion', 'parameter', 'obtained', 'latest', 'time', 'less', 'equal', 'corresponding', 'preset', 'thresholds', 'performing', 'processing', 'acquired', 'latest', 'time', 'non-transitory', 'computer-readable', 'storage', 'medium', 'according', 'wherein', 'acquired', 'face', 'processing', 'comprises', 'performing', 'face', 'non-transitory', 'computer-readable', 'storage', 'medium', 'according', 'wherein', 'obtaining', 'motion', 'parameter', 'comprises', 'least', 'one', 'obtaining', 'acceleration', 'terminal', 'using', 'acceleration', 'sensor', 'obtaining', 'angular', 'velocity', 'terminal', 'using', 'gyro', 'sensor', 'non-transitory', 'computer-readable', 'storage', 'medium', 'according', 'wherein', 'motion', 'parameter', 'comprises', 'motion', 'frequency', 'motion', 'time', 'processing', 'drive-through', 'order', 'comprising', 'receiving', 'customer', 'information', 'detected', 'vision', 'providing', 'product', 'information', 'customer', 'based', 'customer', 'information', 'processing', 'product', 'order', 'customer', 'according', 'wherein', 'receiving', 'customer', 'information', 'comprises', 'least', 'one', 'receiving', 'customer', 'information', 'associated', 'vehicle', 'information', 'detected', 'vehicle', 'receiving', 'customer', 'information', 'associated', 'identification', 'information', 'detected', 'face', 'according', 'comprising', 'determining', 'whether', 'customer', 'pre-order', 'customer', 'based', 'customer', 'information', 'wherein', 'customer', 'determined', 'pre-order', 'customer', 'providing', 'product', 'information', 'based', 'customer', 'information', 'comprises', 'providing', 'pre-order', 'information', 'using', 'least', 'one', 'audio', 'video', 'processing', 'product', 'order', 'customer', 'comprises', 'providing', 'information', 'promptly', 'guiding', 'vehicle', 'pickup', 'stand', 'using', 'least', 'one', 'audio', 'video', 'providing', 'information', 'additional', 'order', 'available', 'according', 'wherein', 'product', 'information', 'based', 'customer', 'information', 'comprises', 'recently', 'ordered', 'product', 'component', 'frequently', 'ordered', 'product', 'component', 'order', 'history', 'customer', 'information', 'according', 'wherein', 'receiving', 'customer', 'information', 'comprises', 'receiving', 'information', 'age', 'gender', 'passenger', 'detected', 'face', 'providing', 'product', 'information', 'customer', 'based', 'customer', 'information', 'comprises', 'providing', 'recommended', 'menu', 'information', 'differentiated', 'according', 'age', 'gender', 'according', 'wherein', 'processing', 'product', 'order', 'customer', 'comprises', 'determining', 'product', 'component', 'past', 'order', 'history', 'component', 'modified', 'product', 'component', 'product', 'order', 'according', 'wherein', 'processing', 'product', 'order', 'customer', 'comprises', 'paying', 'product', 'price', 'according', 'biometrics-based', 'authentication', 'communication', 'system', 'vehicle', 'mobile', 'terminal', 'according', 'wherein', 'processing', 'product', 'order', 'customer', 'comprises', 'issuing', 'payment', 'number', 'divided', 'payment', 'performing', 'divided', 'payments', 'according', 'payment', 'requests', 'mobile', 'terminals', 'payment', 'numbers', 'inputted', 'according', 'wherein', 'processing', 'product', 'order', 'customer', 'comprises', 'accumulating', 'mileage', 'account', 'corresponding', 'mobile', 'terminal', 'undergoing', 'payment', 'according', 'wherein', 'processing', 'product', 'order', 'customer', 'comprises', 'suggesting', 'takeout', 'packaging', 'according', 'temperature', 'product', 'atmospheric', 'temperature', 'weather', 'vehicle', 'type', 'apparatus', 'configured', 'process', 'drive-through', 'order', 'apparatus', 'comprising', 'transceiver', 'configured', 'receive', 'customer', 'information', 'detected', 'vision', 'digital', 'signage', 'configured', 'provide', 'product', 'information', 'customer', 'based', 'customer', 'information', 'configured', 'process', 'product', 'order', 'customer', 'apparatus', 'according', 'wherein', 'transceiver', 'receives', 'least', 'one', 'customer', 'information', 'associated', 'vehicle', 'information', 'detected', 'vehicle', 'customer', 'information', 'associated', 'identification', 'information', 'detected', 'face', 'apparatus', 'according', 'wherein', 'configured', 'determine', 'whether', 'customer', 'pre-order', 'customer', 'based', 'customer', 'information', 'customer', 'determined', 'pre-order', 'customer', 'perform', 'control', 'operation', 'provide', 'pre-order', 'information', 'control', 'digital', 'signage', 'output', 'information', 'promptly', 'guiding', 'vehicle', 'pickup', 'stand', 'provide', 'information', 'additional', 'order', 'available', 'apparatus', 'according', 'wherein', 'product', 'information', 'based', 'customer', 'information', 'comprises', 'recently', 'ordered', 'product', 'component', 'frequently', 'ordered', 'product', 'component', 'order', 'history', 'customer', 'information', 'apparatus', 'according', 'wherein', 'transceiver', 'configured', 'receive', 'information', 'age', 'gender', 'passenger', 'detected', 'face', 'configured', 'control', 'digital', 'signage', 'provide', 'recommended', 'menu', 'information', 'differentiated', 'according', 'age', 'gender', 'apparatus', 'according', 'wherein', 'configured', 'determine', 'product', 'component', 'past', 'order', 'history', 'component', 'modified', 'product', 'component', 'product', 'order', 'apparatus', 'according', 'wherein', 'configured', 'pay', 'product', 'price', 'according', 'biometrics-based', 'authentication', 'communication', 'system', 'vehicle', 'mobile', 'terminal', 'apparatus', 'according', 'wherein', 'configured', 'issue', 'payment', 'number', 'divided', 'payment', 'perform', 'divided', 'payments', 'according', 'requests', 'mobile', 'terminals', 'payment', 'numbers', 'inputted', 'apparatus', 'according', 'wherein', 'configured', 'accumulate', 'mileage', 'account', 'corresponding', 'mobile', 'terminal', 'undergoing', 'payment', 'apparatus', 'according', 'wherein', 'configured', 'control', 'digital', 'signage', 'suggest', 'takeout', 'packaging', 'according', 'temperature', 'product', 'atmospheric', 'temperature', 'weather', 'vehicle', 'type', 'information', 'processing', 'performed', 'computing', 'device', 'one', 'memory', 'storing', 'programs', 'executed', 'one', 'comprising', 'identifying', 'using', 'face', 'one', 'face', 'corresponding', 'respective', 'person', 'captured', 'first', 'identified', 'face', 'extracting', 'set', 'profile', 'parameters', 'corresponding', 'person', 'first', 'selecting', 'tiles', 'first', 'tile', 'matches', 'face', 'corresponding', 'person', 'first', 'accordance', 'predefined', 'correspondence', 'set', 'profile', 'parameters', 'corresponding', 'person', 'set', 'pre-stored', 'description', 'parameters', 'first', 'tile', 'generating', 'second', 'covering', 'respective', 'persons', 'first', 'corresponding', 'first', 'tiles', 'sharing', 'first', 'second', 'predefined', 'order', 'via', 'group', 'chat', 'session', 'wherein', 'first', 'second', 'displayed', 'group', 'chat', 'session', 'one', 'time', 'one', 'two', 'replaced', 'two', 'periodically', 'wherein', 'extracting', 'set', 'profile', 'parameters', 'corresponding', 'person', 'first', 'includes', 'determining', 'one', 'descriptive', 'labels', 'corresponding', 'identified', 'face', 'corresponding', 'person', 'using', 'first', 'machine', 'learning', 'model', 'wherein', 'first', 'machine', 'learning', 'model', 'trained', 'facial', 'corresponding', 'descriptive', 'labels', 'wherein', 'extracting', 'set', 'profile', 'parameters', 'corresponding', 'person', 'first', 'includes', 'determining', 'identity', 'corresponding', 'person', 'based', 'identified', 'face', 'corresponding', 'person', 'locating', 'respective', 'profile', 'information', 'first', 'person', 'based', 'determined', 'identity', 'corresponding', 'person', 'using', 'one', 'characteristics', 'respective', 'profile', 'information', 'first', 'person', 'set', 'profile', 'parameters', 'corresponding', 'identified', 'face', 'corresponding', 'person', 'wherein', 'least', 'first', 'one', 'first', 'tiles', 'dynamic', 'tile', 'least', 'second', 'one', 'first', 'tiles', 'static', 'tile', 'including', 'receiving', 'comments', 'different', 'group', 'chat', 'session', 'comment', 'including', 'descriptive', 'term', 'respective', 'person', 'identified', 'first', 'choosing', 'descriptive', 'label', 'respective', 'person', 'according', 'comments', 'updating', 'second', 'adding', 'descriptive', 'label', 'adjacent', 'first', 'tile', 'respective', 'person', 'computing', 'device', 'information', 'processing', 'comprising', 'one', 'memory', 'storing', 'instructions', 'executed', 'one', 'cause', 'perform', 'operations', 'comprising', 'identifying', 'using', 'face', 'one', 'face', 'corresponding', 'respective', 'person', 'captured', 'first', 'identified', 'face', 'extracting', 'set', 'profile', 'parameters', 'corresponding', 'person', 'first', 'selecting', 'tiles', 'first', 'tile', 'matches', 'face', 'corresponding', 'person', 'first', 'accordance', 'predefined', 'correspondence', 'set', 'profile', 'parameters', 'corresponding', 'person', 'set', 'pre-stored', 'description', 'parameters', 'first', 'tile', 'generating', 'second', 'covering', 'respective', 'persons', 'first', 'corresponding', 'first', 'tiles', 'sharing', 'first', 'second', 'predefined', 'order', 'via', 'group', 'chat', 'session', 'computing', 'device', 'wherein', 'first', 'second', 'displayed', 'group', 'chat', 'session', 'one', 'time', 'one', 'two', 'replaced', 'two', 'periodically', 'computing', 'device', 'wherein', 'extracting', 'set', 'profile', 'parameters', 'corresponding', 'person', 'first', 'includes', 'determining', 'one', 'descriptive', 'labels', 'corresponding', 'identified', 'face', 'corresponding', 'person', 'using', 'first', 'machine', 'learning', 'model', 'wherein', 'first', 'machine', 'learning', 'model', 'trained', 'facial', 'corresponding', 'descriptive', 'labels', 'computing', 'device', 'wherein', 'extracting', 'set', 'profile', 'parameters', 'corresponding', 'person', 'first', 'includes', 'determining', 'identity', 'corresponding', 'person', 'based', 'identified', 'face', 'corresponding', 'person', 'locating', 'respective', 'profile', 'information', 'first', 'person', 'based', 'determined', 'identity', 'corresponding', 'person', 'using', 'one', 'characteristics', 'respective', 'profile', 'information', 'first', 'person', 'set', 'profile', 'parameters', 'corresponding', 'identified', 'face', 'corresponding', 'person', 'computing', 'device', 'wherein', 'least', 'first', 'one', 'first', 'tiles', 'dynamic', 'tile', 'least', 'second', 'one', 'first', 'tiles', 'static', 'tile', 'computing', 'device', 'wherein', 'operations', 'include', 'receiving', 'comments', 'different', 'group', 'chat', 'session', 'comment', 'including', 'descriptive', 'term', 'respective', 'person', 'identified', 'first', 'choosing', 'descriptive', 'label', 'respective', 'person', 'according', 'comments', 'updating', 'second', 'adding', 'descriptive', 'label', 'adjacent', 'first', 'tile', 'respective', 'person', 'non-transitory', 'computer-readable', 'storage', 'medium', 'storing', 'instructions', 'executed', 'computing', 'device', 'one', 'cause', 'computing', 'device', 'perform', 'operations', 'comprising', 'identifying', 'using', 'face', 'one', 'face', 'corresponding', 'respective', 'person', 'captured', 'first', 'identified', 'face', 'extracting', 'set', 'profile', 'parameters', 'corresponding', 'person', 'first', 'selecting', 'tiles', 'first', 'tile', 'matches', 'face', 'corresponding', 'person', 'first', 'accordance', 'predefined', 'correspondence', 'set', 'profile', 'parameters', 'corresponding', 'person', 'set', 'pre-stored', 'description', 'parameters', 'first', 'tile', 'generating', 'second', 'covering', 'respective', 'persons', 'first', 'corresponding', 'first', 'tiles', 'sharing', 'first', 'second', 'predefined', 'order', 'via', 'group', 'chat', 'session', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'first', 'second', 'displayed', 'group', 'chat', 'session', 'one', 'time', 'one', 'two', 'replaced', 'two', 'periodically', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'extracting', 'set', 'profile', 'parameters', 'corresponding', 'person', 'first', 'includes', 'determining', 'one', 'descriptive', 'labels', 'corresponding', 'identified', 'face', 'corresponding', 'person', 'using', 'first', 'machine', 'learning', 'model', 'wherein', 'first', 'machine', 'learning', 'model', 'trained', 'facial', 'corresponding', 'descriptive', 'labels', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'extracting', 'set', 'profile', 'parameters', 'corresponding', 'person', 'first', 'includes', 'determining', 'identity', 'corresponding', 'person', 'based', 'identified', 'face', 'corresponding', 'person', 'locating', 'respective', 'profile', 'information', 'first', 'person', 'based', 'determined', 'identity', 'corresponding', 'person', 'using', 'one', 'characteristics', 'respective', 'profile', 'information', 'first', 'person', 'set', 'profile', 'parameters', 'corresponding', 'identified', 'face', 'corresponding', 'person', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'least', 'first', 'one', 'first', 'tiles', 'dynamic', 'tile', 'least', 'second', 'one', 'first', 'tiles', 'static', 'tile', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'operations', 'include', 'receiving', 'comments', 'different', 'group', 'chat', 'session', 'comment', 'including', 'descriptive', 'term', 'respective', 'person', 'identified', 'first', 'choosing', 'descriptive', 'label', 'respective', 'person', 'according', 'comments', 'updating', 'second', 'adding', 'descriptive', 'label', 'adjacent', 'first', 'tile', 'respective', 'person', 'comprising', 'computing', 'system', 'determining', 'performance', 'metric', 'eye', 'system', 'first', 'performance', 'threshold', 'wherein', 'eye', 'system', 'associated', 'head-mounted', 'display', 'worn', 'based', 'determination', 'performance', 'metric', 'eye', 'system', 'first', 'performance', 'threshold', 'computer', 'system', 'performing', 'receiving', 'one', 'first', 'inputs', 'associated', 'body', 'estimating', 'region', 'looking', 'within', 'field', 'view', 'head-mounted', 'display', 'based', 'received', 'one', 'first', 'inputs', 'associated', 'body', 'determining', 'vergence', 'distance', 'based', 'least', 'one', 'first', 'inputs', 'associated', 'body', 'estimated', 'region', 'looking', 'locations', 'one', 'objects', 'scene', 'displayed', 'head-mounted', 'display', 'adjusting', 'one', 'configurations', 'head-mounted', 'display', 'based', 'determined', 'vergence', 'distance', 'wherein', 'one', 'configurations', 'head-mounted', 'display', 'comprise', 'one', 'rendering', 'position', 'display', 'screen', 'position', 'optics', 'block', 'comprising', 'determining', 'performance', 'metric', 'eye', 'system', 'second', 'performance', 'threshold', 'receiving', 'eye', 'data', 'eye', 'system', 'determining', 'vergence', 'distance', 'based', 'eye', 'data', 'one', 'first', 'inputs', 'associated', 'body', 'comprising', 'receiving', 'one', 'second', 'inputs', 'associated', 'one', 'displaying', 'elements', 'scene', 'displayed', 'head-mounted', 'display', 'determining', 'vergence', 'distance', 'based', 'least', 'eye', 'data', 'one', 'first', 'inputs', 'associated', 'body', 'one', 'second', 'inputs', 'associated', 'one', 'displaying', 'elements', 'scene', 'comprising', 'feeding', 'one', 'first', 'inputs', 'associated', 'body', 'fusion', 'algorithm', 'wherein', 'fusion', 'algorithm', 'assigns', 'weight', 'score', 'input', 'one', 'first', 'inputs', 'determining', 'vergence', 'distance', 'using', 'fusion', 'algorithm', 'based', 'one', 'first', 'inputs', 'associated', 'body', 'determining', 'z-depth', 'display', 'screen', 'confidence', 'score', 'based', 'one', 'first', 'inputs', 'associated', 'body', 'comprising', 'comparing', 'confidence', 'score', 'confidence', 'level', 'threshold', 'response', 'determination', 'confidence', 'score', 'confidence', 'level', 'threshold', 'feeding', 'one', 'second', 'inputs', 'associated', 'one', 'displaying', 'elements', 'scene', 'fusion', 'algorithm', 'determining', 'z-depth', 'display', 'screen', 'using', 'fusion', 'algorithm', 'based', 'one', 'first', 'inputs', 'associated', 'body', 'one', 'second', 'inputs', 'associated', 'one', 'displaying', 'elements', 'scene', 'comparing', 'comparing', 'fusion', 'algorithm', 'confidence', 'scores', 'associated', 'combinations', 'inputs', 'determining', 'fusion', 'algorithm', 'z-depth', 'display', 'screen', 'based', 'combination', 'inputs', 'associated', 'highest', 'confidence', 'score', 'wherein', 'z-depth', 'confidence', 'score', 'determined', 'fusion', 'algorithm', 'using', 'piecewise', 'comparison', 'one', 'first', 'inputs', 'one', 'second', 'inputs', 'wherein', 'z-depth', 'confidence', 'score', 'determined', 'based', 'correlation', 'two', 'inputs', 'one', 'first', 'inputs', 'one', 'second', 'inputs', 'wherein', 'fusion', 'algorithm', 'comprises', 'machine', 'learning', 'ml', 'algorithm', 'wherein', 'machine', 'learning', 'ml', 'algorithm', 'determines', 'combination', 'first', 'inputs', 'fed', 'fusion', 'algorithm', 'wherein', 'one', 'first', 'inputs', 'associated', 'body', 'comprise', 'one', 'hand', 'position', 'hand', 'direction', 'hand', 'movement', 'hand', 'gesture', 'head', 'position', 'head', 'direction', 'head', 'movement', 'head', 'gesture', 'gaze', 'angle', 'rea', 'body', 'gesture', 'body', 'posture', 'body', 'movement', 'behavior', 'weighted', 'combination', 'one', 'related', 'parameters', 'wherein', 'one', 'first', 'inputs', 'associated', 'body', 'received', 'one', 'controller', 'sensor', 'camera', 'microphone', 'accelerometer', 'headset', 'worn', 'mobile', 'device', 'wherein', 'one', 'second', 'inputs', 'associated', 'one', 'displaying', 'elements', 'comprise', 'one', 'z-buffer', 'value', 'associated', 'displaying', 'element', 'displaying', 'element', 'marked', 'developer', 'analysis', 'result', 'shape', 'displaying', 'element', 'face', 'result', 'object', 'result', 'person', 'identified', 'displaying', 'content', 'object', 'identified', 'displaying', 'content', 'correlation', 'two', 'displaying', 'elements', 'weighted', 'combination', 'one', 'second', 'inputs', 'comprising', 'determining', 'performance', 'metric', 'eye', 'system', 'second', 'performance', 'threshold', 'receiving', 'one', 'second', 'inputs', 'associated', 'one', 'displaying', 'elements', 'scene', 'displayed', 'head-mounted', 'display', 'determining', 'vergence', 'distance', 'based', 'least', 'one', 'first', 'inputs', 'associated', 'body', 'one', 'second', 'inputs', 'associated', 'one', 'displaying', 'elements', 'wherein', 'determining', 'performance', 'metric', 'eye', 'system', 'second', 'performance', 'threshold', 'comprises', 'determining', 'eye', 'system', 'exist', 'fails', 'provide', 'eye', 'data', 'wherein', 'performance', 'metric', 'eye', 'system', 'comprises', 'one', 'accuracy', 'parameter', 'eye', 'system', 'precision', 'parameter', 'eye', 'system', 'value', 'parameter', 'eye', 'system', 'detectability', 'pupil', 'metric', 'based', 'one', 'parameters', 'associated', 'parameter', 'change', 'parameter', 'changing', 'trend', 'data', 'availability', 'weighted', 'combination', 'one', 'performance', 'related', 'parameters', 'wherein', 'one', 'parameters', 'associated', 'comprise', 'one', 'eye', 'distance', 'pupil', 'position', 'pupil', 'status', 'correlation', 'two', 'pupils', 'head', 'size', 'position', 'headset', 'worn', 'angle', 'headset', 'worn', 'direction', 'headset', 'worn', 'alignment', 'eyes', 'weighted', 'combination', 'one', 'related', 'parameters', 'associated', 'wherein', 'first', 'performance', 'threshold', 'comprises', 'one', 'pre-determined', 'value', 'pre-determined', 'range', 'state', 'data', 'changing', 'speed', 'data', 'trend', 'data', 'change', 'one', 'non-transitory', 'computer-readable', 'storage', 'media', 'embodying', 'software', 'operable', 'executed', 'computing', 'system', 'determine', 'performance', 'metric', 'eye', 'system', 'first', 'performance', 'threshold', 'wherein', 'eye', 'system', 'associated', 'head-mounted', 'display', 'worn', 'based', 'determination', 'performance', 'metric', 'eye', 'system', 'first', 'performance', 'threshold', 'media', 'embodying', 'software', 'operable', 'executed', 'computing', 'system', 'receive', 'one', 'first', 'inputs', 'associated', 'body', 'estimate', 'region', 'looking', 'within', 'field', 'view', 'head-mounted', 'display', 'based', 'received', 'one', 'first', 'inputs', 'associated', 'body', 'determine', 'vergence', 'distance', 'based', 'least', 'one', 'first', 'inputs', 'associated', 'body', 'estimated', 'region', 'looking', 'locations', 'one', 'objects', 'scene', 'displayed', 'head-mounted', 'display', 'adjust', 'one', 'configurations', 'head-mounted', 'display', 'based', 'determined', 'vergence', 'distance', 'system', 'comprising', 'one', 'non-transitory', 'computer-readable', 'storage', 'media', 'embodying', 'instructions', 'one', 'coupled', 'storage', 'media', 'operable', 'execute', 'instructions', 'determine', 'performance', 'metric', 'eye', 'system', 'first', 'performance', 'threshold', 'wherein', 'eye', 'system', 'associated', 'head-mounted', 'display', 'worn', 'based', 'determination', 'performance', 'metric', 'eye', 'system', 'first', 'performance', 'threshold', 'system', 'configured', 'receive', 'one', 'first', 'inputs', 'associated', 'body', 'estimate', 'region', 'looking', 'within', 'field', 'view', 'head-mounted', 'display', 'based', 'received', 'one', 'first', 'inputs', 'associated', 'body', 'determine', 'vergence', 'distance', 'based', 'least', 'one', 'first', 'inputs', 'associated', 'body', 'estimated', 'region', 'looking', 'locations', 'one', 'objects', 'scene', 'displayed', 'head-mounted', 'display', 'adjust', 'one', 'configurations', 'head-mounted', 'display', 'based', 'determined', 'vergence', 'distance', 'computer-implemented', '-based', 'self-guided', 'object', 'detection', 'comprising', 'receiving', 'device', 'set', 'respective', 'grid', 'thereon', 'labeled', 'regarding', 'respective', 'object', 'detected', 'using', 'grid', 'level', 'label', 'data', 'training', 'device', 'grid-based', 'object', 'detector', 'using', 'grid', 'level', 'label', 'data', 'determining', 'device', 'respective', 'bounding', 'box', 'respective', 'object', 'applying', 'local', 'segmentation', 'training', 'device', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'joint', 'object', 'localization', 'object', 'classification', 'using', 'respective', 'bounding', 'box', 'respective', 'object', 'input', 'rcnn', 'computer-implemented', 'comprising', 'performing', 'action', 'responsive', 'object', 'localization', 'object', 'classification', 'respective', 'new', 'object', 'new', 'rcnn', 'applied', 'computer-implemented', 'wherein', 'action', 'comprises', 'autonomously', 'controlling', 'motor', 'vehicle', 'avoid', 'collision', 'new', 'object', 'responsive', 'object', 'localization', 'object', 'classification', 'respective', 'new', 'object', 'computer-implemented', 'wherein', 'local', 'segmentation', 'performed', 'using', 'self-similarity', 'search', 'template', 'matching', 'provide', 'respective', 'bounding', 'box', 'around', 'respective', 'object', 'set', 'computer-implemented', 'wherein', 'local', 'segmentation', 'applied', 'segment', 'respective', 'target', 'region', 'therein', 'computer-implemented', 'wherein', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'forms', 'model', 'object', 'training', 'stage', 'detect', 'objects', 'new', 'inference', 'stage', 'computer-implemented', 'wherein', 'performed', 'system', 'selected', 'group', 'consisting', 'surveillance', 'system', 'face', 'detection', 'system', 'face', 'system', 'cancer', 'detection', 'system', 'object', 'system', 'advanced', 'driver-assistance', 'system', 'computer', 'program', 'product', '-based', 'self-guided', 'object', 'detection', 'computer', 'program', 'product', 'comprising', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'program', 'instructions', 'embodied', 'therewith', 'program', 'instructions', 'executable', 'computer', 'cause', 'computer', 'perform', 'comprising', 'receiving', 'device', 'set', 'respective', 'grid', 'thereon', 'labeled', 'regarding', 'respective', 'object', 'detected', 'using', 'grid', 'level', 'label', 'data', 'training', 'device', 'grid-based', 'object', 'detector', 'using', 'grid', 'level', 'label', 'data', 'determining', 'device', 'respective', 'bounding', 'box', 'respective', 'object', 'applying', 'local', 'segmentation', 'training', 'device', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'joint', 'object', 'localization', 'object', 'classification', 'using', 'respective', 'bounding', 'box', 'respective', 'object', 'input', 'rcnn', 'computer', 'program', 'product', 'wherein', 'comprises', 'performing', 'action', 'responsive', 'object', 'localization', 'object', 'classification', 'respective', 'new', 'object', 'new', 'rcnn', 'applied', 'computer', 'program', 'product', 'wherein', 'action', 'comprises', 'autonomously', 'controlling', 'motor', 'vehicle', 'avoid', 'collision', 'new', 'object', 'responsive', 'object', 'localization', 'object', 'classification', 'respective', 'new', 'object', 'computer', 'program', 'product', 'wherein', 'local', 'segmentation', 'performed', 'using', 'self-similarity', 'search', 'template', 'matching', 'provide', 'respective', 'bounding', 'box', 'around', 'respective', 'object', 'set', 'computer', 'program', 'product', 'wherein', 'local', 'segmentation', 'applied', 'segment', 'respective', 'target', 'region', 'therein', 'computer', 'program', 'product', 'wherein', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'forms', 'model', 'object', 'training', 'stage', 'detect', 'objects', 'new', 'inference', 'stage', 'computer', 'program', 'product', 'wherein', 'performed', 'system', 'selected', 'group', 'consisting', 'surveillance', 'system', 'face', 'detection', 'system', 'face', 'system', 'cancer', 'detection', 'system', 'object', 'system', 'advanced', 'driver-assistance', 'system', 'computer', 'processing', 'system', '-based', 'self-guided', 'object', 'detection', 'comprising', 'memory', 'device', 'storing', 'program', 'code', 'device', 'running', 'program', 'code', 'receive', 'set', 'respective', 'grid', 'thereon', 'labeled', 'regarding', 'respective', 'object', 'detected', 'using', 'grid', 'level', 'label', 'data', 'train', 'grid-based', 'object', 'detector', 'using', 'grid', 'level', 'label', 'data', 'determine', 'respective', 'bounding', 'box', 'respective', 'object', 'applying', 'local', 'segmentation', 'train', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'joint', 'object', 'localization', 'object', 'classification', 'using', 'respective', 'bounding', 'box', 'respective', 'object', 'input', 'rcnn', 'computer', 'processing', 'system', 'wherein', 'device', 'runs', 'program', 'code', 'perform', 'action', 'responsive', 'object', 'localization', 'object', 'classification', 'respective', 'new', 'object', 'new', 'rcnn', 'applied', 'computer', 'processing', 'system', 'wherein', 'action', 'comprises', 'autonomously', 'controlling', 'motor', 'vehicle', 'avoid', 'collision', 'new', 'object', 'responsive', 'object', 'localization', 'object', 'classification', 'respective', 'new', 'object', 'computer', 'processing', 'system', 'wherein', 'local', 'segmentation', 'performed', 'using', 'self-similarity', 'search', 'template', 'matching', 'provide', 'respective', 'bounding', 'box', 'around', 'respective', 'object', 'set', 'computer', 'processing', 'system', 'wherein', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'forms', 'model', 'object', 'training', 'stage', 'detect', 'objects', 'new', 'inference', 'stage', 'computer', 'processing', 'system', 'wherein', 'computer', 'processing', 'system', 'comprised', 'system', 'selected', 'group', 'consisting', 'surveillance', 'system', 'face', 'detection', 'system', 'face', 'system', 'cancer', 'detection', 'system', 'object', 'system', 'advanced', 'driver-assistance', 'system', 'scalable', 'parallel', 'cloud-based', 'face', 'utilizing', 'database', 'normalized', 'stored', 'comprising', 'capturing', 'using', 'camera', 'detecting', 'face', 'captured', 'normalizing', 'detected', 'facial', 'match', 'normalized', 'stored', 'identifying', 'facial', 'features', 'normalized', 'detected', 'facial', 'generating', 'facial', 'metrics', 'facial', 'features', 'calculating', 'euclidean', 'distances', 'facial', 'metrics', 'normalized', 'detected', 'facial', 'corresponding', 'facial', 'metrics', 'stored', 'comparing', 'euclidean', 'distance', 'predetermined', 'threshold', 'responsive', 'euclidean', 'distance', 'comparison', 'producing', 'reduced', 'candidate', 'list', 'best', 'possible', 'matches', 'normalized', 'stored', 'comparing', 'parallel', 'normalized', 'detected', 'facial', 'normalized', 'stored', 'reduced', 'candidate', 'list', 'utilizing', 'face', 'algorithms', 'parallel', 'processing', 'system', 'uses', 'different', 'face', 'algorithm', 'responsive', 'comparison', 'producing', 'best', 'match', 'results', 'parallel', 'subset', 'reduced', 'candidate', 'list', 'selecting', 'final', 'match', 'best', 'match', 'results', 'using', 'deep', 'learning', 'neural', 'network', 'face', 'algorithm', 'trained', 'outputs', 'individual', 'face', 'algorithms', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'detecting', 'face', 'captured', 'comprises', 'utilizing', 'opencv', 'detect', 'face', 'captured', 'extracting', 'location', 'eyes', 'tip', 'nose', 'face', 'determining', 'distance', 'eyes', 'cropping', 'face', 'captured', 'width', 'height', 'cropped', 'face', 'function', 'distance', 'eyes', 'rotating', 'face', 'angle', 'rotation', 'function', 'distance', 'eyes', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'width', 'cropped', 'face', 'times', 'distance', 'eyes', 'height', 'cropped', 'face', 'times', 'distance', 'eyes', 'angle', 'rotation', 'angle', 'formed', 'straight', 'line', 'joining', 'eyes', 'x-axis', 'face', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'rotating', 'face', 'comprises', 'rotating', 'face', 'provide', 'frontal', 'face', 'pattern', 'scalable', 'parallel', 'cloud-based', 'face', 'comprising', 'step', 'proportionally', 'rescaling', 'cropped', 'rotated', 'scalable', 'parallel', 'cloud-based', 'face', 'proportional', 'rescaling', 'yields', 'cropped', 'rotated', 'size', '=', 'pixels', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'facial', 'features', 'identified', 'normalized', 'detected', 'facial', 'comprise', 'pair', 'eyes', 'tip', 'nose', 'mouth', 'center', 'mouth', 'chin', 'area', 'comprising', 'bottom', 'top', 'left', 'landmark', 'top', 'right', 'landmark', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'generating', 'facial', 'metrics', 'comprises', 'calculating', 'distance', 'pair', 'eyes', 'distance', 'eyes', 'tip', 'nose', 'distance', 'equal', 'width', 'mouth', 'distance', 'tip', 'nose', 'center', 'mouth', 'distance', 'bottom', 'chin', 'center', 'mouth', 'distance', 'top', 'left', 'landmark', 'chin', 'tip', 'nose', 'distance', 'top', 'right', 'landmark', 'chin', 'tip', 'nose', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'performing', 'euclidean', 'distance', 'match', 'comprises', 'partitioning', 'normalized', 'stored', 'substantially', 'equal', 'subsets', 'performing', 'euclidean', 'distance', 'match', 'facial', 'metrics', 'normalized', 'detected', 'facial', 'corresponding', 'facial', 'metrics', 'stored', 'subsets', 'normalized', 'stored', 'separate', 'parallel', 'processing', 'system', 'generate', 'euclidean', 'distance', 'stored', 'subset', 'comparing', 'euclidean', 'distance', 'predetermined', 'threshold', 'separate', 'responsive', 'euclidean', 'distance', 'comparison', 'producing', 'reduced', 'candidate', 'list', 'best', 'possible', 'matches', 'normalized', 'stored', 'subset', 'combining', 'reduced', 'candidate', 'lists', 'subset', 'produce', 'single', 'reduced', 'candidate', 'list', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'face', 'algorithms', 'utilized', 'comparing', 'parallel', 'normalized', 'detected', 'facial', 'normalized', 'stored', 'reduced', 'candidate', 'list', 'consists', 'face', 'algorithms', 'selected', 'group', 'consisting', 'principle', 'component', 'analysis', 'pca-based', 'algorithms', 'linear', 'discriminant', 'analysis', 'lda', 'algorithms', 'independent', 'component', 'analysis', 'ica', 'algorithms', 'kernel-based', 'algorithms', 'feature-based', 'techniques', 'algorithms', 'based', 'neural', 'networks', 'algorithms', 'based', 'transforms', 'model-based', 'face', 'algorithms', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'pca-based', 'algorithms', 'include', 'eigen', 'face', 'detection', 'lda', 'algorithms', 'include', 'fisher', 'face', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'comparing', 'parallel', 'captured', 'normalized', 'stored', 'reduced', 'candidate', 'list', 'comprises', 'partitioning', 'reduced', 'candidate', 'list', 'substantially', 'equal', 'subsets', 'processing', 'subset', 'different', 'parallel', 'processing', 'system', 'uses', 'unique', 'face', 'algorithm', 'produce', 'best', 'match', 'results', 'using', 'reduce', 'function', 'mapreduce', 'program', 'combine', 'best', 'match', 'results', 'subsets', 'produce', 'single', 'set', 'best', 'match', 'results', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'partitioning', 'reduced', 'candidate', 'list', 'comprises', 'selecting', 'comprising', 'subset', 'optimizing', 'variance', 'according', 'following', 'equation', 'n', 'number', 'rows', 'columns', 'face', 'vector', 'n', 'number', 'groups', 'σij', 'standard', 'deviation', 'dimension', 'group', 'j', 'face', 'vector', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'selecting', 'comprising', 'subset', 'optimizing', 'variance', 'according', 'following', 'equation', 'dμi', 'μj', 'euclidean', 'distance', 'mean', 'group', 'mean', 'group', 'j', 'face', 'vector', 'l', 'number', 'group', 'levels', 'scalable', 'parallel', 'cloud-based', 'face', 'selecting', 'final', 'match', 'best', 'match', 'results', 'utilizing', 'deep', 'learning', 'neural', 'network', 'face', 'algorithm', 'comprises', 'utilizing', 'either', 'adaboost', 'machine-learning', 'algorithm', 'neural', 'networks', 'machine-learning', 'model', 'scalable', 'parallel', 'cloud-based', 'face', 'normalizing', 'detected', 'facial', 'match', 'normalized', 'stored', 'includes', 'normalizing', 'detected', 'facial', 'size', 'illumination', 'normalized', 'stored', 'non-transitory', 'computer-readable', 'medium', 'containing', 'executable', 'program', 'instructions', 'causing', 'computer', 'perform', 'face', 'comprising', 'detecting', 'face', 'captured', 'camera', 'normalizing', 'detected', 'facial', 'match', 'normalized', 'stored', 'identifying', 'facial', 'features', 'normalized', 'detected', 'facial', 'generating', 'facial', 'metrics', 'facial', 'features', 'calculating', 'euclidean', 'distances', 'facial', 'metrics', 'normalized', 'detected', 'facial', 'corresponding', 'facial', 'metrics', 'stored', 'comparing', 'euclidean', 'distance', 'predetermined', 'threshold', 'responsive', 'euclidean', 'distance', 'comparison', 'producing', 'reduced', 'candidate', 'list', 'best', 'possible', 'matches', 'normalized', 'stored', 'comparing', 'parallel', 'captured', 'normalized', 'stored', 'reduced', 'candidate', 'list', 'utilizing', 'face', 'algorithms', 'parallel', 'processing', 'system', 'uses', 'different', 'face', 'algorithm', 'responsive', 'comparison', 'producing', 'best', 'match', 'results', 'parallel', 'subset', 'reduced', 'candidate', 'list', 'selecting', 'final', 'match', 'best', 'match', 'results', 'using', 'deep', 'learning', 'neural', 'network', 'face', 'algorithm', 'trained', 'outputs', 'individual', 'face', 'algorithms', 'non-transitory', 'computer-readable', 'medium', 'containing', 'executable', 'program', 'instructions', 'wherein', 'face', 'algorithms', 'utilized', 'comparing', 'parallel', 'normalized', 'detected', 'facial', 'normalized', 'stored', 'reduced', 'candidate', 'list', 'consists', 'face', 'algorithms', 'selected', 'group', 'consisting', 'principle', 'component', 'analysis', 'pca-based', 'algorithms', 'linear', 'discriminant', 'analysis', 'lda', 'algorithms', 'independent', 'component', 'analysis', 'ica', 'algorithms', 'kernel-based', 'algorithms', 'feature-based', 'techniques', 'algorithms', 'based', 'neural', 'networks', 'algorithms', 'based', 'transforms', 'model-based', 'face', 'algorithms', 'non-transitory', 'computer-readable', 'medium', 'containing', 'executable', 'program', 'instructions', 'wherein', 'pca-based', 'algorithms', 'include', 'eigen', 'face', 'detection', 'lda', 'algorithms', 'include', 'fisher', 'face', 'non-transitory', 'computer-readable', 'medium', 'containing', 'executable', 'program', 'instructions', 'selecting', 'final', 'match', 'best', 'match', 'results', 'utilizing', 'deep', 'learning', 'neural', 'network', 'face', 'algorithm', 'comprises', 'utilizing', 'either', 'adaboost', 'machine-learning', 'algorithm', 'neural', 'networks', 'machine-learning', 'model', 'imaging', 'device', 'comprising', 'condensing', 'lens', 'sensor', 'configured', 'detect', 'light', 'passing', 'condensing', 'lens', 'comprising', 'pixel', 'matrix', 'wherein', 'pixel', 'matrix', 'comprises', 'phase', 'detection', 'pixel', 'pairs', 'regular', 'pixels', 'configured', 'turn', 'phase', 'detection', 'pixel', 'pairs', 'autofocusing', 'output', 'autofocused', 'pixel', 'data', 'completing', 'autofocusing', 'divide', 'autofocused', 'pixel', 'data', 'first', 'subframe', 'second', 'subframe', 'calculate', 'features', 'least', 'one', 'first', 'subframe', 'second', 'subframe', 'wherein', 'features', 'comprise', 'module', 'widths', 'finder', 'pattern', 'finder', 'pattern', 'predetermined', 'ratio', 'harr-like', 'feature', 'gabor', 'feature', 'determine', 'operating', 'resolution', 'regular', 'pixels', 'according', 'features', 'calculated', 'least', 'one', 'first', 'subframe', 'second', 'subframe', 'divided', 'autofocused', 'pixel', 'data', 'imaging', 'device', 'ed', 'wherein', 'phase', 'detection', 'pixel', 'pairs', 'comprises', 'first', 'pixel', 'second', 'pixel', 'cover', 'layer', 'covering', 'upon', 'first', 'region', 'first', 'pixel', 'upon', 'second', 'region', 'second', 'pixel', 'wherein', 'first', 'region', 'second', 'region', 'mirror', 'symmetrical', 'microlens', 'aligned', 'least', 'one', 'first', 'pixel', 'second', 'pixel', 'imaging', 'device', 'ed', 'wherein', 'first', 'region', 'second', 'region', '%', '%', 'area', 'single', 'pixel', 'imaging', 'device', 'ed', 'wherein', 'configured', 'perform', 'autofocusing', 'using', 'dual', 'pixel', 'autofocus', 'technique', 'according', 'pixel', 'data', 'phase', 'detection', 'pixel', 'pairs', 'completing', 'autofocusing', 'imaging', 'device', 'ed', 'wherein', 'configured', 'divide', 'pixel', 'data', 'phase', 'detection', 'pixel', 'pairs', 'third', 'subframe', 'fourth', 'subframe', 'completing', 'autofocusing', 'perform', 'autofocusing', 'according', 'third', 'subframe', 'fourth', 'subframe', 'imaging', 'device', 'ed', 'wherein', 'configured', 'calibrate', 'brightness', 'third', 'subframe', 'fourth', 'subframe', 'identical', 'using', 'shading', 'algorithm', 'imaging', 'device', 'ed', 'wherein', 'operating', 'resolution', 'selected', 'first', 'resolution', 'smaller', 'number', 'regular', 'pixels', 'second', 'resolution', 'larger', 'first', 'resolution', 'imaging', 'device', 'ed', 'wherein', 'regular', 'pixels', 'turned', 'autofocusing', 'imaging', 'device', 'ed', 'wherein', 'number', 'phase', 'detection', 'pixel', 'pairs', 'smaller', 'regular', 'pixels', 'imaging', 'device', 'comprising', 'condensing', 'lens', 'sensor', 'configured', 'detect', 'light', 'passing', 'condensing', 'lens', 'comprising', 'pixel', 'matrix', 'wherein', 'pixel', 'matrix', 'comprises', 'phase', 'detection', 'pixel', 'pairs', 'regular', 'pixels', 'configured', 'turn', 'phase', 'detection', 'pixel', 'pairs', 'autofocusing', 'output', 'autofocused', 'pixel', 'data', 'completing', 'autofocusing', 'divide', 'autofocused', 'pixel', 'data', 'first', 'subframe', 'second', 'subframe', 'calculate', 'features', 'least', 'one', 'first', 'subframe', 'second', 'subframe', 'wherein', 'features', 'comprise', 'module', 'widths', 'finder', 'pattern', 'finder', 'pattern', 'predetermined', 'ratio', 'harr-like', 'feature', 'gabor', 'feature', 'select', 'decoding', 'using', 'pixel', 'data', 'regular', 'pixels', 'according', 'features', 'calculated', 'least', 'one', 'first', 'subframe', 'second', 'subframe', 'divided', 'autofocused', 'pixel', 'data', 'imaging', 'device', 'ed', 'wherein', 'phase', 'detection', 'pixel', 'pairs', 'comprises', 'first', 'pixel', 'second', 'pixel', 'cover', 'layer', 'covering', 'upon', 'first', 'region', 'first', 'pixel', 'upon', 'second', 'region', 'second', 'pixel', 'wherein', 'first', 'region', 'second', 'region', 'mirror', 'symmetrical', 'microlens', 'aligned', 'least', 'one', 'first', 'pixel', 'second', 'pixel', 'imaging', 'device', 'ed', 'wherein', 'configured', 'perform', 'autofocusing', 'using', 'dual', 'pixel', 'autofocus', 'technique', 'according', 'pixel', 'data', 'phase', 'detection', 'pixel', 'pairs', 'completing', 'autofocusing', 'imaging', 'device', 'ed', 'wherein', 'configured', 'divide', 'pixel', 'data', 'phase', 'detection', 'pixel', 'pairs', 'third', 'subframe', 'fourth', 'subframe', 'completing', 'autofocusing', 'calibrate', 'brightness', 'third', 'subframe', 'fourth', 'subframe', 'identical', 'using', 'shading', 'algorithm', 'perform', 'autofocusing', 'according', 'third', 'subframe', 'fourth', 'subframe', 'imaging', 'device', 'ed', 'wherein', 'configured', 'calculate', 'features', 'using', 'least', 'one', 'rule', 'based', 'algorithm', 'machine', 'learning', 'algorithm', 'imaging', 'device', 'ed', 'wherein', 'decoding', 'decoding', 'qr', 'codes', 'face', 'operating', 'imaging', 'device', 'imaging', 'device', 'comprising', 'phase', 'detection', 'pixel', 'pairs', 'regular', 'pixels', 'operating', 'comprising', 'turning', 'phase', 'detection', 'pixel', 'pairs', 'autofocusing', 'outputting', 'autofocused', 'frame', 'completing', 'autofocusing', 'dividing', 'autofocused', 'frame', 'acquired', 'phase', 'detection', 'pixel', 'pairs', 'first', 'subframe', 'second', 'subframe', 'calculating', 'features', 'least', 'one', 'first', 'subframe', 'second', 'subframe', 'wherein', 'feature', 'comprise', 'module', 'widths', 'finder', 'pattern', 'finder', 'pattern', 'predetermined', 'ratio', 'harr-like', 'feature', 'gabor', 'feature', 'selectively', 'activating', 'least', 'part', 'regular', 'pixels', 'according', 'features', 'calculated', 'least', 'one', 'first', 'subframe', 'second', 'subframe', 'divided', 'autofocused', 'frame', 'operating', 'ed', 'wherein', 'selectively', 'activating', 'comprises', 'activating', 'first', 'part', 'regular', 'pixels', 'perform', 'decoding', 'according', 'pixel', 'data', 'first', 'part', 'regular', 'pixels', 'activating', 'regular', 'pixels', 'perform', 'according', 'pixel', 'data', 'regular', 'pixels', 'operating', 'ed', 'wherein', 'pixel', 'data', 'phase', 'detection', 'pixel', 'pairs', 'captured', 'frame', 'pixel', 'data', 'regular', 'pixels', 'also', 'used', 'performing', 'decoding', 'operating', 'ed', 'wherein', 'decoding', 'decoding', 'qr', 'codes', 'face', 'operating', 'ed', 'wherein', 'phase', 'detection', 'pixel', 'pairs', 'partially', 'covered', 'pixels', 'structure', 'dual', 'pixel', 'apparatus', 'comprising', 'first', 'camera', 'module', 'configured', 'obtain', 'first', 'object', 'first', 'field', 'view', 'second', 'camera', 'module', 'configured', 'obtain', 'second', 'object', 'second', 'field', 'view', 'different', 'first', 'field', 'view', 'first', 'depth', 'map', 'generator', 'configured', 'generate', 'first', 'depth', 'map', 'first', 'based', 'first', 'second', 'second', 'depth', 'map', 'generator', 'configured', 'generate', 'second', 'depth', 'map', 'second', 'based', 'first', 'second', 'first', 'depth', 'map', 'apparatus', 'wherein', 'first', 'field', 'view', 'narrow', 'angle', 'second', 'field', 'view', 'wider', 'angle', 'apparatus', 'wherein', 'second', 'divided', 'primary', 'region', 'residual', 'region', 'second', 'depth', 'map', 'generator', 'comprises', 'relationship', 'estimating', 'module', 'configured', 'estimate', 'relationship', 'primary', 'region', 'residual', 'region', 'based', 'first', 'second', 'depth', 'map', 'estimating', 'module', 'configured', 'estimate', 'depth', 'map', 'residual', 'region', 'based', 'estimated', 'relationship', 'first', 'depth', 'map', 'apparatus', 'wherein', 'least', 'one', 'relationship', 'estimating', 'module', 'depth', 'map', 'estimating', 'module', 'performs', 'estimating', 'operation', 'based', 'neural', 'network', 'module', 'apparatus', 'comprising', 'depth', 'map', 'fusion', 'unit', 'configured', 'generate', 'third', 'depth', 'map', 'second', 'performing', 'fusion', 'operation', 'based', 'first', 'depth', 'map', 'second', 'depth', 'map', 'apparatus', 'wherein', 'depth', 'map', 'fusion', 'unit', 'comprises', 'tone', 'mapping', 'module', 'configured', 'generate', 'tone-mapped', 'second', 'depth', 'map', 'correspond', 'first', 'depth', 'map', 'performing', 'bias', 'removing', 'operation', 'second', 'depth', 'map', 'fusion', 'module', 'configured', 'generate', 'third', 'depth', 'map', 'fusing', 'tone-mapped', 'second', 'depth', 'map', 'first', 'depth', 'map', 'apparatus', 'wherein', 'depth', 'map', 'fusion', 'unit', 'comprises', 'propagating', 'module', 'configured', 'generate', 'propagated', 'first', 'depth', 'map', 'second', 'iterated', 'propagating', 'first', 'depth', 'map', 'based', 'first', 'depth', 'map', 'second', 'fusion', 'module', 'generates', 'third', 'depth', 'map', 'fusing', 'tone-mapped', 'second', 'depth', 'map', 'propagated', 'first', 'depth', 'map', 'apparatus', 'wherein', 'depth', 'map', 'fusion', 'unit', 'comprises', 'post-processing', 'module', 'configured', 'perform', 'post-processing', 'operation', 'third', 'depth', 'map', 'generated', 'fusion', 'module', 'provide', 'post-processed', 'third', 'depth', 'map', 'apparatus', 'wherein', 'post-processing', 'module', 'performs', 'post-processing', 'operation', 'filtering', 'interface', 'generated', 'third', 'depth', 'map', 'accordance', 'fusion', 'fusion', 'module', 'apparatus', 'wherein', 'post-processing', 'module', 'removes', 'artifacts', 'generated', 'third', 'depth', 'map', 'accordance', 'fusion', 'fusion', 'module', 'apparatus', 'wherein', 'first', 'depth', 'map', 'generator', 'analyses', 'distance', 'relationship', 'first', 'second', 'generates', 'first', 'depth', 'map', 'first', 'based', 'distance', 'relationship', 'processing', 'electronic', 'apparatus', 'comprising', 'obtaining', 'first', 'object', 'using', 'first', 'camera', 'module', 'obtaining', 'second', 'object', 'using', 'second', 'camera', 'module', 'generating', 'first', 'depth', 'map', 'first', 'based', 'first', 'second', 'estimating', 'relationship', 'primary', 'region', 'second', 'residual', 'region', 'second', 'based', 'first', 'second', 'generating', 'second', 'depth', 'map', 'second', 'based', 'estimated', 'relationship', 'primary', 'region', 'residual', 'region', 'first', 'depth', 'map', 'wherein', 'electronic', 'apparatus', 'comprises', 'first', 'camera', 'module', 'including', 'first', 'lens', 'first', 'field', 'view', 'second', 'camera', 'module', 'including', 'second', 'lens', 'second', 'field', 'view', 'wider', 'first', 'field', 'view', 'wherein', 'generating', 'second', 'depth', 'map', 'comprises', 'estimating', 'depth', 'map', 'residual', 'region', 'based', 'estimated', 'relationship', 'primary', 'region', 'residual', 'region', 'first', 'depth', 'map', 'generating', 'second', 'depth', 'map', 'based', 'depth', 'map', 'residual', 'region', 'first', 'depth', 'map', 'wherein', 'estimating', 'relationship', 'primary', 'region', 'second', 'performed', 'using', 'neural', 'network', 'model', 'comprising', 'performing', 'pre-processing', 'operation', 'second', 'depth', 'map', 'generating', 'third', 'depth', 'map', 'residual', 'fusing', 'second', 'depth', 'map', 'pre-processing', 'operation', 'performed', 'first', 'depth', 'map', 'wherein', 'performing', 'pre-processing', 'operation', 'comprises', 'performing', 'tone', 'mapping', 'operation', 'depth', 'map', 'primary', 'region', 'depth', 'map', 'residual', 'region', 'based', 'second', 'depth', 'map', 'operating', 'electronic', 'apparatus', 'electronic', 'apparatus', 'including', 'first', 'camera', 'module', 'providing', 'first', 'object', 'using', 'first', 'field', 'view', 'second', 'camera', 'module', 'providing', 'second', 'object', 'using', 'second', 'field', 'view', 'wider', 'first', 'field', 'view', 'generating', 'depth', 'map', 'second', 'based', 'primary', 'region', 'second', 'residual', 'region', 'second', 'operating', 'comprising', 'generating', 'first', 'depth', 'map', 'primary', 'region', 'estimating', 'relationship', 'first', 'second', 'estimating', 'relationship', 'primary', 'region', 'residual', 'region', 'based', 'first', 'second', 'generating', 'second', 'depth', 'map', 'second', 'estimating', 'depth', 'map', 'second', 'region', 'based', 'estimated', 'relationship', 'primary', 'region', 'residual', 'region', 'generating', 'depth', 'map', 'second', 'fusing', 'first', 'depth', 'map', 'second', 'depth', 'map', 'operation', 'comprising', 'executing', 'application', 'applies', 'effect', 'second', 'based', 'depth', 'map', 'residual', 'operation', 'wherein', 'application', 'applies', 'least', 'one', 'effect', 'auto-focusing', 'out-focusing', 'forebackground', 'separation', 'face', 'object', 'detection', 'within', 'frame', 'augmented', 'reality', 'second', 'based', 'depth', 'map', 'second', 'payment', 'based', 'face', 'comprising', 'acquiring', 'first', 'face', 'information', 'target', 'extracting', 'first', 'characteristic', 'information', 'first', 'face', 'information', 'wherein', 'first', 'characteristic', 'information', 'includes', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'determining', 'whether', 'target', 'willingness', 'pay', 'according', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'including', 'determining', 'whether', 'angle', 'rotation', 'preset', 'direction', 'less', 'angle', 'threshold', 'wherein', 'head', 'posture', 'information', 'includes', 'angle', 'rotation', 'preset', 'direction', 'determining', 'whether', 'probability', 'value', 'gazes', 'payment', 'screen', 'greater', 'probability', 'threshold', 'wherein', 'gaze', 'information', 'includes', 'probability', 'value', 'gazes', 'payment', 'screen', 'response', 'determining', 'angle', 'rotation', 'preset', 'direction', 'less', 'angle', 'threshold', 'probability', 'value', 'gazes', 'payment', 'screen', 'greater', 'probability', 'threshold', 'determining', 'target', 'willingness', 'pay', 'response', 'determining', 'target', 'willingness', 'pay', 'completing', 'payment', 'operation', 'based', 'face', 'ed', 'wherein', 'completing', 'payment', 'operation', 'based', 'face', 'comprises', 'triggering', 'performing', 'payment', 'initiating', 'operation', 'acquire', 'second', 'face', 'information', 'based', 'face', 'determining', 'whether', 'second', 'characteristic', 'information', 'extracted', 'second', 'face', 'information', 'indicates', 'willingness', 'pay', 'response', 'determining', 'second', 'characteristic', 'information', 'indicates', 'willingness', 'pay', 'triggering', 'performing', 'payment', 'confirmation', 'operation', 'complete', 'payment', 'operation', 'based', 'payment', 'account', 'information', 'corresponding', 'target', 'ed', 'wherein', 'determining', 'whether', 'second', 'characteristic', 'information', 'extracted', 'second', 'face', 'information', 'indicates', 'willingness', 'pay', 'comprises', 'determining', 'whether', 'current', 'corresponding', 'second', 'face', 'information', 'consistent', 'target', 'response', 'determining', 'current', 'consistent', 'target', 'determining', 'whether', 'target', 'willingness', 'pay', 'according', 'second', 'characteristic', 'information', 'extracted', 'second', 'face', 'information', 'ed', 'wherein', 'extracting', 'first', 'characteristic', 'information', 'first', 'face', 'information', 'comprises', 'determining', 'head', 'posture', 'information', 'target', 'using', 'head', 'posture', 'model', 'based', 'first', 'face', 'information', 'determining', 'gaze', 'information', 'target', 'using', 'gaze', 'information', 'model', 'based', 'characteristics', 'eye', 'region', 'first', 'face', 'information', 'ed', 'wherein', 'head', 'posture', 'model', 'obtained', 'training', 'acquiring', 'first', 'sample', 'data', 'set', 'wherein', 'first', 'sample', 'data', 'set', 'includes', 'pieces', 'first', 'sample', 'data', 'pieces', 'first', 'sample', 'data', 'includes', 'correspondence', 'sample', 'face', 'head', 'posture', 'information', 'determining', 'mean', 'data', 'variance', 'data', 'sample', 'face', 'pieces', 'first', 'sample', 'data', 'preprocessing', 'sample', 'face', 'contained', 'pieces', 'first', 'sample', 'data', 'based', 'mean', 'data', 'variance', 'data', 'obtain', 'preprocessed', 'sample', 'face', 'setting', 'preprocessed', 'sample', 'face', 'corresponding', 'head', 'posture', 'information', 'first', 'model', 'training', 'sample', 'performing', 'training', 'using', 'machine', 'learning', 'based', 'first', 'model', 'training', 'samples', 'obtain', 'head', 'posture', 'model', 'ed', 'wherein', 'gaze', 'information', 'model', 'obtained', 'training', 'acquiring', 'second', 'sample', 'data', 'set', 'wherein', 'second', 'sample', 'data', 'set', 'includes', 'pieces', 'second', 'sample', 'data', 'pieces', 'second', 'sample', 'data', 'includes', 'correspondence', 'sample', 'eye', 'gaze', 'information', 'determining', 'mean', 'data', 'variance', 'data', 'sample', 'eye', 'pieces', 'second', 'sample', 'data', 'preprocessing', 'sample', 'eye', 'contained', 'pieces', 'second', 'sample', 'data', 'based', 'mean', 'data', 'variance', 'data', 'obtain', 'preprocessed', 'sample', 'eye', 'setting', 'preprocessed', 'sample', 'eye', 'corresponding', 'gaze', 'information', 'second', 'model', 'training', 'sample', 'performing', 'training', 'using', 'machine', 'learning', 'based', 'second', 'model', 'training', 'samples', 'obtain', 'gaze', 'information', 'model', 'ed', 'wherein', 'angle', 'rotation', 'preset', 'direction', 'comprises', 'pitch', 'angle', 'yaw', 'angle', 'roll', 'angle', 'wherein', 'pitch', 'angle', 'refers', 'angle', 'rotation', 'around', 'x-axis', 'yaw', 'angle', 'refers', 'angle', 'rotation', 'around', 'y-axis', 'roll', 'angle', 'refers', 'angle', 'rotation', 'around', 'z-axis', 'payment', 'device', 'based', 'face', 'comprising', 'non-transitory', 'computer-readable', 'storage', 'medium', 'storing', 'instructions', 'executable', 'cause', 'device', 'perform', 'operations', 'comprising', 'acquiring', 'first', 'face', 'information', 'target', 'extracting', 'first', 'characteristic', 'information', 'first', 'face', 'information', 'wherein', 'first', 'characteristic', 'information', 'includes', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'determining', 'whether', 'target', 'willingness', 'pay', 'according', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'including', 'determining', 'whether', 'angle', 'rotation', 'preset', 'direction', 'less', 'angle', 'threshold', 'wherein', 'head', 'posture', 'information', 'includes', 'angle', 'rotation', 'preset', 'direction', 'determining', 'whether', 'probability', 'value', 'gazes', 'payment', 'screen', 'greater', 'probability', 'threshold', 'wherein', 'gaze', 'information', 'includes', 'probability', 'value', 'gazes', 'payment', 'screen', 'response', 'determining', 'angle', 'rotation', 'preset', 'direction', 'less', 'angle', 'threshold', 'probability', 'value', 'gazes', 'payment', 'screen', 'greater', 'probability', 'threshold', 'determining', 'target', 'willingness', 'pay', 'response', 'determining', 'target', 'willingness', 'pay', 'completing', 'payment', 'operation', 'based', 'face', 'device', 'ed', 'wherein', 'completing', 'payment', 'operation', 'based', 'face', 'comprises', 'triggering', 'performing', 'payment', 'initiating', 'operation', 'acquire', 'second', 'face', 'information', 'based', 'face', 'determining', 'whether', 'second', 'characteristic', 'information', 'extracted', 'second', 'face', 'information', 'indicates', 'willingness', 'pay', 'response', 'determining', 'second', 'characteristic', 'information', 'indicates', 'willingness', 'pay', 'triggering', 'performing', 'payment', 'confirmation', 'operation', 'complete', 'payment', 'operation', 'based', 'payment', 'account', 'information', 'corresponding', 'target', 'device', 'ed', 'wherein', 'determining', 'whether', 'second', 'characteristic', 'information', 'extracted', 'second', 'face', 'information', 'indicates', 'willingness', 'pay', 'comprises', 'determining', 'whether', 'current', 'corresponding', 'second', 'face', 'information', 'consistent', 'target', 'response', 'determining', 'current', 'consistent', 'target', 'determining', 'whether', 'target', 'willingness', 'pay', 'according', 'second', 'characteristic', 'information', 'extracted', 'second', 'face', 'information', 'device', 'ed', 'wherein', 'extracting', 'first', 'characteristic', 'information', 'first', 'face', 'information', 'comprises', 'determining', 'head', 'posture', 'information', 'target', 'using', 'head', 'posture', 'model', 'based', 'first', 'face', 'information', 'determining', 'gaze', 'information', 'target', 'using', 'gaze', 'information', 'model', 'based', 'characteristics', 'eye', 'region', 'first', 'face', 'information', 'device', 'ed', 'wherein', 'head', 'posture', 'model', 'obtained', 'training', 'acquiring', 'first', 'sample', 'data', 'set', 'wherein', 'first', 'sample', 'data', 'set', 'includes', 'pieces', 'first', 'sample', 'data', 'pieces', 'first', 'sample', 'data', 'includes', 'correspondence', 'sample', 'face', 'head', 'posture', 'information', 'determining', 'mean', 'data', 'variance', 'data', 'sample', 'face', 'pieces', 'first', 'sample', 'data', 'preprocessing', 'sample', 'face', 'contained', 'pieces', 'first', 'sample', 'data', 'based', 'mean', 'data', 'variance', 'data', 'obtain', 'preprocessed', 'sample', 'face', 'setting', 'preprocessed', 'sample', 'face', 'corresponding', 'head', 'posture', 'information', 'first', 'model', 'training', 'sample', 'performing', 'training', 'using', 'machine', 'learning', 'based', 'first', 'model', 'training', 'samples', 'obtain', 'head', 'posture', 'model', 'device', 'ed', 'wherein', 'gaze', 'information', 'model', 'obtained', 'training', 'acquiring', 'second', 'sample', 'data', 'set', 'wherein', 'second', 'sample', 'data', 'set', 'includes', 'pieces', 'second', 'sample', 'data', 'pieces', 'second', 'sample', 'data', 'includes', 'correspondence', 'sample', 'eye', 'gaze', 'information', 'determining', 'mean', 'data', 'variance', 'data', 'sample', 'eye', 'pieces', 'second', 'sample', 'data', 'preprocessing', 'sample', 'eye', 'contained', 'pieces', 'second', 'sample', 'data', 'based', 'mean', 'data', 'variance', 'data', 'obtain', 'preprocessed', 'sample', 'eye', 'setting', 'preprocessed', 'sample', 'eye', 'corresponding', 'gaze', 'information', 'second', 'model', 'training', 'sample', 'performing', 'training', 'using', 'machine', 'learning', 'second', 'model', 'training', 'samples', 'obtain', 'gaze', 'information', 'model', 'device', 'ed', 'wherein', 'angle', 'rotation', 'preset', 'direction', 'comprises', 'pitch', 'angle', 'yaw', 'angle', 'roll', 'angle', 'wherein', 'pitch', 'angle', 'refers', 'angle', 'rotation', 'around', 'x-axis', 'yaw', 'angle', 'refers', 'angle', 'rotation', 'around', 'y-axis', 'roll', 'angle', 'refers', 'angle', 'rotation', 'around', 'z-axis', 'non-transitory', 'computer-readable', 'storage', 'medium', 'payment', 'based', 'face', 'configured', 'instructions', 'executable', 'one', 'cause', 'one', 'perform', 'operations', 'comprising', 'acquiring', 'first', 'face', 'information', 'target', 'extracting', 'first', 'characteristic', 'information', 'first', 'face', 'information', 'wherein', 'first', 'characteristic', 'information', 'includes', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'determining', 'whether', 'target', 'willingness', 'pay', 'according', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'including', 'determining', 'whether', 'angle', 'rotation', 'preset', 'direction', 'less', 'angle', 'threshold', 'wherein', 'head', 'posture', 'information', 'includes', 'angle', 'rotation', 'preset', 'direction', 'determining', 'whether', 'probability', 'value', 'gazes', 'payment', 'screen', 'greater', 'probability', 'threshold', 'wherein', 'gaze', 'information', 'includes', 'probability', 'value', 'gazes', 'payment', 'screen', 'response', 'determining', 'angle', 'rotation', 'preset', 'direction', 'less', 'angle', 'threshold', 'probability', 'value', 'gazes', 'payment', 'screen', 'greater', 'probability', 'threshold', 'determining', 'target', 'willingness', 'pay', 'response', 'determining', 'target', 'willingness', 'pay', 'completing', 'payment', 'operation', 'based', 'face', 'storage', 'medium', 'ed', 'wherein', 'completing', 'payment', 'operation', 'based', 'face', 'comprises', 'triggering', 'performing', 'payment', 'initiating', 'operation', 'acquire', 'second', 'face', 'information', 'based', 'face', 'determining', 'whether', 'second', 'characteristic', 'information', 'extracted', 'second', 'face', 'information', 'indicates', 'willingness', 'pay', 'response', 'determining', 'second', 'characteristic', 'information', 'indicates', 'willingness', 'pay', 'triggering', 'performing', 'payment', 'confirmation', 'operation', 'complete', 'payment', 'operation', 'based', 'payment', 'account', 'information', 'corresponding', 'target', 'storage', 'medium', 'ed', 'wherein', 'determining', 'whether', 'second', 'characteristic', 'information', 'extracted', 'second', 'face', 'information', 'indicates', 'willingness', 'pay', 'comprises', 'determining', 'whether', 'current', 'corresponding', 'second', 'face', 'information', 'consistent', 'target', 'response', 'determining', 'current', 'consistent', 'target', 'determining', 'whether', 'target', 'willingness', 'pay', 'according', 'second', 'characteristic', 'information', 'extracted', 'second', 'face', 'information', 'storage', 'medium', 'ed', 'wherein', 'extracting', 'first', 'characteristic', 'information', 'first', 'face', 'information', 'comprises', 'determining', 'head', 'posture', 'information', 'target', 'using', 'head', 'posture', 'model', 'based', 'first', 'face', 'information', 'determining', 'gaze', 'information', 'target', 'using', 'gaze', 'information', 'model', 'based', 'characteristics', 'eye', 'region', 'first', 'face', 'information', 'storage', 'medium', 'ed', 'wherein', 'head', 'posture', 'model', 'obtained', 'training', 'acquiring', 'first', 'sample', 'data', 'set', 'wherein', 'first', 'sample', 'data', 'set', 'includes', 'pieces', 'first', 'sample', 'data', 'pieces', 'first', 'sample', 'data', 'includes', 'correspondence', 'sample', 'face', 'head', 'posture', 'information', 'determining', 'mean', 'data', 'variance', 'data', 'sample', 'face', 'pieces', 'first', 'sample', 'data', 'preprocessing', 'sample', 'face', 'contained', 'pieces', 'first', 'sample', 'data', 'based', 'mean', 'data', 'variance', 'data', 'obtain', 'preprocessed', 'sample', 'face', 'setting', 'preprocessed', 'sample', 'face', 'corresponding', 'head', 'posture', 'information', 'first', 'model', 'training', 'sample', 'performing', 'training', 'using', 'machine', 'learning', 'based', 'first', 'model', 'training', 'samples', 'obtain', 'head', 'posture', 'model', 'wherein', 'gaze', 'information', 'model', 'obtained', 'training', 'acquiring', 'second', 'sample', 'data', 'set', 'wherein', 'second', 'sample', 'data', 'set', 'includes', 'pieces', 'second', 'sample', 'data', 'pieces', 'second', 'sample', 'data', 'includes', 'correspondence', 'sample', 'eye', 'gaze', 'information', 'determining', 'mean', 'data', 'variance', 'data', 'sample', 'eye', 'pieces', 'second', 'sample', 'data', 'preprocessing', 'sample', 'eye', 'contained', 'pieces', 'second', 'sample', 'data', 'based', 'mean', 'data', 'variance', 'data', 'obtain', 'preprocessed', 'sample', 'eye', 'setting', 'preprocessed', 'sample', 'eye', 'corresponding', 'gaze', 'information', 'second', 'model', 'training', 'sample', 'performing', 'training', 'using', 'machine', 'learning', 'based', 'second', 'model', 'training', 'samples', 'obtain', 'gaze', 'information', 'model', 'storage', 'medium', 'ed', 'wherein', 'angle', 'rotation', 'preset', 'direction', 'comprises', 'pitch', 'angle', 'yaw', 'angle', 'roll', 'angle', 'wherein', 'pitch', 'angle', 'refers', 'angle', 'rotation', 'around', 'x-axis', 'yaw', 'angle', 'refers', 'angle', 'rotation', 'around', 'y-axis', 'roll', 'angle', 'refers', 'angle', 'rotation', 'around', 'z-axis', 'comprising', 'detecting', 'motion', 'detection', 'module', 'motion', 'subject', 'within', 'predetermined', 'area', 'view', 'assigning', 'unique', 'session', 'identification', 'number', 'subject', 'detected', 'within', 'predetermined', 'area', 'view', 'detecting', 'facial', 'area', 'subject', 'detected', 'within', 'predetermined', 'area', 'view', 'generating', 'facial', 'area', 'subject', 'assessing', 'quality', 'facial', 'area', 'subject', 'determining', 'identity', 'subject', 'based', 'facial', 'area', 'subject', 'identifying', 'intent', 'subject', 'authorizing', 'access', 'point', 'entry', 'based', 'determined', 'identity', 'subject', 'based', 'intent', 'subject', 'comprising', 'determining', 'one', 'additional', 'subjects', 'within', 'predetermined', 'area', 'view', 'assigning', 'unique', 'session', 'identification', 'number', 'one', 'additional', 'subjects', 'detected', 'within', 'predetermined', 'area', 'view', 'wherein', 'assessing', 'quality', 'facial', 'area', 'subject', 'comprises', 'assessing', 'whether', 'quality', 'facial', 'area', 'object', 'equates', 'predetermined', 'metric', 'quality', 'upon', 'determining', 'quality', 'facial', 'area', 'object', 'inferior', 'predetermined', 'metric', 'quality', 'discarding', 'facial', 'area', 'subject', 'generating', 'second', 'facial', 'area', 'subject', 'comprising', 'detecting', 'whether', 'facial', 'area', 'subject', 'photographic', 'upon', 'detecting', 'facial', 'area', 'subject', 'photographic', 'generating', 'warning', 'restrict', 'access', 'point', 'entry', 'comprising', 'conducing', 'incremental', 'training', 'facial', 'area', 'subject', 'wherein', 'conducing', 'incremental', 'training', 'facial', 'area', 'subject', 'comprises', 'capturing', 'first', 'facial', 'area', 'facial', 'landmarks', 'converting', 'first', 'facial', 'area', 'first', 'numeric', 'vector', 'capturing', 'second', 'facial', 'area', 'facial', 'landmarks', 'converting', 'second', 'facial', 'area', 'second', 'numeric', 'vector', 'calculating', 'weighted', 'mean', 'first', 'numeric', 'vector', 'second', 'numeric', 'vector', 'wherein', 'weighted', 'mean', 'represents', 'change', 'facial', 'area', 'storing', 'weighted', 'mean', 'database', 'wherein', 'determining', 'identity', 'subject', 'based', 'facial', 'area', 'subject', 'comprises', 'comparing', 'facial', 'area', 'subject', 'stored', 'database', 'authenticating', 'subject', 'wherein', 'identifying', 'intent', 'subject', 'comprises', 'upon', 'detecting', 'facial', 'area', 'bounding', 'box', 'commencing', 'authentication', 'subject', 'calculating', 'directional', 'vector', 'face', 'subject', 'determine', 'intent', 'subject', 'gain', 'access', 'point', 'entry', 'based', 'directional', 'vector', 'face', 'subject', 'granting', 'access', 'point', 'entry', 'based', 'authentication', 'subject', 'based', 'determining', 'intent', 'subject', 'non-transitory', 'computer', 'readable', 'medium', 'program', 'instructions', 'stored', 'thereon', 'response', 'execution', 'computing', 'device', 'cause', 'computing', 'device', 'perform', 'operations', 'comprising', 'detecting', 'motion', 'subject', 'within', 'predetermined', 'area', 'view', 'assigning', 'unique', 'session', 'identification', 'number', 'subject', 'detected', 'within', 'predetermined', 'area', 'view', 'detecting', 'facial', 'area', 'subject', 'detected', 'within', 'predetermined', 'area', 'view', 'generating', 'facial', 'area', 'subject', 'assessing', 'quality', 'facial', 'area', 'subject', 'determining', 'identity', 'subject', 'based', 'facial', 'area', 'subject', 'identifying', 'intent', 'subject', 'authorizing', 'access', 'point', 'entry', 'based', 'determined', 'identity', 'subject', 'based', 'intent', 'subject', 'non-transitory', 'computer', 'readable', 'medium', 'comprising', 'determining', 'one', 'additional', 'subjects', 'within', 'predetermined', 'area', 'view', 'assigning', 'unique', 'session', 'identification', 'number', 'one', 'additional', 'subjects', 'detected', 'within', 'predetermined', 'area', 'view', 'non-transitory', 'computer', 'readable', 'medium', 'wherein', 'assessing', 'quality', 'facial', 'area', 'subject', 'comprises', 'assessing', 'whether', 'quality', 'facial', 'area', 'object', 'equates', 'predetermined', 'metric', 'quality', 'upon', 'determining', 'quality', 'facial', 'area', 'object', 'inferior', 'predetermined', 'metric', 'quality', 'discarding', 'facial', 'area', 'subject', 'generating', 'second', 'facial', 'area', 'subject', 'non-transitory', 'computer', 'readable', 'medium', 'comprising', 'detecting', 'whether', 'facial', 'area', 'subject', 'photographic', 'upon', 'detecting', 'facial', 'area', 'subject', 'photographic', 'generating', 'warning', 'restrict', 'access', 'access', 'point', 'non-transitory', 'computer', 'readable', 'medium', 'comprising', 'conducing', 'incremental', 'training', 'facial', 'area', 'subject', 'non-transitory', 'computer', 'readable', 'medium', 'wherein', 'conducing', 'incremental', 'training', 'facial', 'area', 'subject', 'comprises', 'capturing', 'first', 'facial', 'area', 'facial', 'landmarks', 'converting', 'first', 'facial', 'area', 'first', 'numeric', 'vector', 'capturing', 'second', 'facial', 'area', 'facial', 'landmarks', 'converting', 'second', 'facial', 'area', 'second', 'numeric', 'vector', 'calculating', 'weighted', 'mean', 'first', 'numeric', 'vector', 'second', 'numeric', 'vector', 'wherein', 'weighted', 'mean', 'represents', 'change', 'facial', 'area', 'storing', 'weighted', 'mean', 'database', 'apparatus', 'face', 'comprising', 'memory', 'store', 'computer', 'program', 'instructions', 'computer', 'program', 'instructions', 'executed', 'cause', 'perform', 'operations', 'comprising', 'detecting', 'motion', 'subject', 'within', 'predetermined', 'area', 'view', 'assigning', 'unique', 'session', 'identification', 'number', 'subject', 'detected', 'within', 'predetermined', 'area', 'view', 'detecting', 'facial', 'area', 'subject', 'detected', 'within', 'predetermined', 'area', 'view', 'generating', 'facial', 'area', 'subject', 'assessing', 'quality', 'facial', 'area', 'subject', 'determining', 'identity', 'subject', 'based', 'facial', 'area', 'subject', 'identifying', 'intent', 'subject', 'authorizing', 'access', 'point', 'entry', 'based', 'determined', 'identity', 'subject', 'based', 'intent', 'subject', 'apparatus', 'comprising', 'determining', 'one', 'additional', 'subjects', 'within', 'predetermined', 'area', 'view', 'assigning', 'unique', 'session', 'identification', 'number', 'one', 'additional', 'subjects', 'detected', 'within', 'predetermined', 'area', 'view', 'apparatus', 'wherein', 'assessing', 'quality', 'facial', 'area', 'subject', 'comprises', 'assessing', 'whether', 'quality', 'facial', 'area', 'object', 'equates', 'predetermined', 'metric', 'quality', 'upon', 'determining', 'quality', 'facial', 'area', 'object', 'inferior', 'predetermined', 'metric', 'quality', 'discarding', 'facial', 'area', 'subject', 'generating', 'second', 'facial', 'area', 'subject', 'apparatus', 'comprising', 'detecting', 'whether', 'facial', 'area', 'subject', 'photographic', 'upon', 'detecting', 'facial', 'area', 'subject', 'photographic', 'generating', 'warning', 'restrict', 'access', 'access', 'point', 'apparatus', 'comprising', 'conducing', 'incremental', 'training', 'facial', 'area', 'subject', 'apparatus', 'wherein', 'conducing', 'incremental', 'training', 'facial', 'area', 'subject', 'comprises', 'capturing', 'first', 'facial', 'area', 'facial', 'landmarks', 'converting', 'first', 'facial', 'area', 'first', 'numeric', 'vector', 'capturing', 'second', 'facial', 'area', 'facial', 'landmarks', 'converting', 'second', 'facial', 'area', 'second', 'numeric', 'vector', 'calculating', 'weighted', 'mean', 'first', 'numeric', 'vector', 'second', 'numeric', 'vector', 'wherein', 'weighted', 'mean', 'represents', 'change', 'facial', 'area', 'storing', 'weighted', 'mean', 'database', 'robot', 'comprising', 'body', 'configured', 'rotate', 'tilt', 'camera', 'coupled', 'body', 'configured', 'rotate', 'tilt', 'according', 'rotate', 'tilt', 'body', 'wherein', 'camera', 'configured', 'acquire', 'video', 'space', 'face', 'unit', 'configured', 'recognize', 'respective', 'one', 'persons', 'video', 'unit', 'configured', 'track', 'motion', 'recognized', 'one', 'persons', 'controller', 'configured', 'calculate', 'respective', 'size', 'one', 'persons', 'select', 'first', 'person', 'among', 'one', 'persons', 'based', 'calculated', 'sizes', 'control', 'least', 'one', 'direction', 'rotation', 'camera', 'angle', 'tilt', 'camera', 'focal', 'distance', 'camera', 'based', 'tracked', 'motion', 'recognized', 'face', 'first', 'person', 'robot', 'wherein', 'controller', 'configured', 'control', 'direction', 'rotation', 'camera', 'angle', 'tilt', 'camera', 'achieve', 'particular', 'camera', 'relative', 'face', 'first', 'person', 'control', 'focal', 'distance', 'camera', 'comparing', 'respective', 'sizes', 'face', 'first', 'person', 'motion', 'first', 'person', 'robot', 'wherein', 'particular', 'occurs', 'camera', 'general', 'direction', 'face', 'first', 'person', 'robot', 'wherein', 'controller', 'configured', 'normalize', 'sizes', 'one', 'persons', 'based', 'interocular', 'distance', 'select', 'first', 'person', 'based', 'normalized', 'sizes', 'one', 'persons', 'robot', 'wherein', 'controller', 'configured', 'select', 'person', 'largest', 'face', 'size', 'among', 'one', 'persons', 'first', 'person', 'robot', 'comprising', 'microphone', 'configured', 'receive', 'spoken', 'audio', 'present', 'space', 'wherein', 'controller', 'configured', 'select', 'first', 'person', 'based', 'received', 'spoken', 'audio', 'robot', 'wherein', 'controller', 'configured', 'control', 'gain', 'microphone', 'comparing', 'respective', 'sizes', 'face', 'first', 'person', 'motion', 'first', 'person', 'robot', 'wherein', 'controller', 'configured', 'calculate', 'position', 'spoken', 'audio', 'provided', 'select', 'first', 'person', 'based', 'whether', 'one', 'persons', 'position', 'voice', 'signal', 'provided', 'robot', 'wherein', 'controller', 'configured', 'select', 'second', 'person', 'first', 'person', 'among', 'one', 'persons', 'second', 'person', 'located', 'position', 'spoken', 'audio', 'provided', 'robot', 'wherein', 'controller', 'configured', 'select', 'second', 'person', 'largest', 'face', 'size', 'first', 'person', 'among', 'one', 'persons', 'none', 'one', 'persons', 'located', 'position', 'spoken', 'audio', 'provided', 'robot', 'wherein', 'controller', 'configured', 'select', 'second', 'person', 'largest', 'face', 'size', 'first', 'person', 'among', 'one', 'persons', 'persons', 'among', 'one', 'persons', 'located', 'position', 'spoken', 'audio', 'provided', 'robot', 'comprising', 'speaker', 'wherein', 'controller', 'configured', 'control', 'volume', 'speaker', 'comparing', 'respective', 'sizes', 'face', 'first', 'person', 'motion', 'first', 'person', 'robot', 'wherein', 'body', 'configured', 'rotate', 'lateral', 'direction', 'tilt', 'vertical', 'direction', 'comprising', 'camera', 'coupled', 'body', 'configured', 'rotate', 'tilt', 'wherein', 'camera', 'configured', 'acquire', 'video', 'space', 'within', 'one', 'persons', 'positioned', 'configured', 'recognize', 'respective', 'one', 'persons', 'video', 'track', 'motion', 'recognized', 'one', 'persons', 'calculate', 'respective', 'size', 'one', 'persons', 'select', 'first', 'person', 'among', 'one', 'persons', 'based', 'calculated', 'sizes', 'control', 'least', 'one', 'direction', 'rotation', 'camera', 'angle', 'tilt', 'camera', 'focal', 'distance', 'camera', 'based', 'tracked', 'motion', 'recognized', 'face', 'first', 'person', 'comprising', 'acquiring', 'camera', 'video', 'space', 'within', 'one', 'persons', 'positioned', 'respective', 'one', 'persons', 'video', 'motion', 'recognized', 'one', 'persons', 'calculating', 'respective', 'size', 'one', 'persons', 'selecting', 'first', 'person', 'among', 'one', 'persons', 'based', 'calculated', 'sizes', 'controlling', 'least', 'one', 'direction', 'rotation', 'camera', 'angle', 'tilt', 'camera', 'focal', 'distance', 'camera', 'based', 'tracked', 'motion', 'recognized', 'face', 'first', 'person', 'inferring', 'topics', 'multimodal', 'file', 'comprising', 'receiving', 'multimodal', 'file', 'extracting', 'set', 'entities', 'multimodal', 'file', 'linking', 'set', 'entities', 'produce', 'set', 'linked', 'entities', 'obtaining', 'reference', 'information', 'set', 'entities', 'based', 'least', 'reference', 'information', 'generating', 'graph', 'set', 'linked', 'entities', 'graph', 'comprising', 'nodes', 'edges', 'based', 'least', 'nodes', 'edges', 'graph', 'determining', 'clusters', 'graph', 'based', 'least', 'clusters', 'graph', 'identifying', 'topic', 'candidates', 'extracting', 'features', 'clusters', 'graph', 'based', 'least', 'extracted', 'features', 'selecting', 'least', 'one', 'topicid', 'among', 'topic', 'candidates', 'represent', 'least', 'one', 'cluster', 'indexing', 'multimodal', 'file', 'least', 'one', 'topicid', 'wherein', 'multimodal', 'file', 'comprises', 'video', 'portion', 'audio', 'portion', 'wherein', 'extracting', 'set', 'entities', 'multimodal', 'file', 'comprises', 'detecting', 'objects', 'video', 'portion', 'multimodal', 'file', 'detecting', 'text', 'audio', 'portion', 'multimodal', 'file', 'wherein', 'detecting', 'objects', 'comprises', 'performing', 'face', 'wherein', 'detecting', 'text', 'comprises', 'performing', 'speech', 'text', 'process', 'comprising', 'identifying', 'language', 'used', 'audio', 'portion', 'multimodal', 'file', 'wherein', 'performing', 'speech', 'text', 'process', 'comprises', 'performing', 'speech', 'text', 'process', 'identified', 'language', 'comprising', 'translating', 'detected', 'text', 'comprising', 'determining', 'significant', 'clusters', 'insignificant', 'clusters', 'determined', 'clusters', 'wherein', 'extracting', 'features', 'clusters', 'graph', 'comprises', 'extracting', 'features', 'significant', 'clusters', 'graph', 'wherein', 'extracting', 'features', 'clusters', 'graph', 'comprises', 'least', 'one', 'process', 'selected', 'list', 'consisting', 'determining', 'graph', 'diameter', 'determining', 'jaccard', 'coefficient', 'wherein', 'selecting', 'least', 'one', 'topicid', 'represent', 'least', 'one', 'cluster', 'comprises', 'based', 'least', 'extracted', 'features', 'mapping', 'topic', 'candidates', 'probability', 'interval', 'based', 'least', 'mapping', 'ranking', 'topic', 'candidates', 'within', 'least', 'one', 'cluster', 'selecting', 'least', 'one', 'topicid', 'based', 'least', 'ranking', 'comprising', 'translating', 'least', 'one', 'topicid', 'wherein', 'indexing', 'multimodal', 'file', 'least', 'one', 'topicid', 'comprises', 'indexing', 'multimodal', 'file', 'least', 'one', 'translated', 'topicid', 'system', 'inferring', 'topics', 'multimodal', 'file', 'system', 'comprising', 'entity', 'extraction', 'component', 'comprising', 'object', 'detection', 'component', 'speech', 'text', 'component', 'operative', 'extract', 'set', 'entities', 'multimodal', 'file', 'comprising', 'video', 'portion', 'audio', 'portion', 'entity', 'linking', 'component', 'operative', 'link', 'extracted', 'set', 'entities', 'produce', 'set', 'linked', 'entities', 'information', 'retrieval', 'component', 'operative', 'obtain', 'reference', 'information', 'extracted', 'set', 'entities', 'graphing', 'analysis', 'component', 'operative', 'generate', 'graph', 'set', 'linked', 'entities', 'graph', 'comprising', 'nodes', 'edges', 'based', 'least', 'nodes', 'edges', 'graph', 'determine', 'clusters', 'graph', 'based', 'least', 'clusters', 'graph', 'identify', 'topic', 'candidates', 'extract', 'features', 'clusters', 'graph', 'topicid', 'selection', 'component', 'operative', 'rank', 'topic', 'candidates', 'within', 'least', 'one', 'cluster', 'based', 'least', 'ranking', 'select', 'least', 'one', 'topicid', 'among', 'topic', 'candidates', 'represent', 'least', 'one', 'cluster', 'video', 'indexer', 'operative', 'index', 'multimodal', 'file', 'least', 'one', 'topicid', 'system', 'wherein', 'object', 'detection', 'component', 'operative', 'perform', 'face', 'system', 'wherein', 'speech', 'text', 'component', 'operative', 'extract', 'entity', 'information', 'least', 'two', 'different', 'languages', 'one', 'computer', 'storage', 'devices', 'computer-executable', 'instructions', 'stored', 'thereon', 'inferring', 'topics', 'multimodal', 'file', 'execution', 'computer', 'cause', 'computer', 'perform', 'operations', 'comprising', 'receiving', 'multimodal', 'file', 'comprising', 'video', 'portion', 'audio', 'portion', 'extracting', 'set', 'entities', 'multimodal', 'file', 'wherein', 'extracting', 'set', 'entities', 'multimodal', 'file', 'comprises', 'detecting', 'objects', 'video', 'portion', 'multimodal', 'file', 'face', 'detecting', 'text', 'audio', 'portion', 'multimodal', 'file', 'speech', 'text', 'process', 'disambiguating', 'among', 'set', 'detected', 'entity', 'names', 'linking', 'set', 'entities', 'produce', 'set', 'linked', 'entities', 'obtaining', 'reference', 'information', 'set', 'entities', 'based', 'least', 'reference', 'information', 'generating', 'graph', 'set', 'linked', 'entities', 'graph', 'comprising', 'nodes', 'edges', 'based', 'least', 'nodes', 'edges', 'graph', 'determining', 'clusters', 'graph', 'determining', 'significant', 'clusters', 'insignificant', 'clusters', 'determined', 'clusters', 'based', 'least', 'significant', 'clusters', 'graph', 'identifying', 'topic', 'candidates', 'extracting', 'features', 'significant', 'clusters', 'graph', 'based', 'least', 'extracted', 'features', 'mapping', 'topic', 'candidates', 'probability', 'interval', 'based', 'least', 'mapping', 'ranking', 'topic', 'candidates', 'within', 'least', 'one', 'significant', 'cluster', 'based', 'ranking', 'selecting', 'least', 'one', 'topicid', 'among', 'topic', 'candidates', 'represent', 'least', 'one', 'significant', 'cluster', 'indexing', 'multimodal', 'file', 'least', 'one', 'topicid', 'one', 'computer', 'storage', 'devices', 'wherein', 'operations', 'comprise', 'identifying', 'language', 'used', 'audio', 'portion', 'multimodal', 'file', 'detecting', 'text', 'audio', 'portion', 'multimodal', 'file', 'speech', 'text', 'process', 'comprises', 'performing', 'speech', 'text', 'process', 'identified', 'language权利要求', '、', '一种人脸识别方法其特征在于包括', '通过第一摄像头获取第一人脸图像', '提取所述第一人脸图像的第一人脸特征', '将所述第一人脸特征与预先存储的第二人脸特征进行对比获得参考相似度所述第', '二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得所述第二摄像头与所述第', '一摄像头属于不同类型的摄像头', '根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否对应相同人。', '、', '根据权利要求', '所述的方法其特征在于', '所述第一摄像头为热成像摄像头所述第二摄像头为可见光摄像头', '或者所述第一摄像头为可见光摄像头所述第一摄像头为热成像摄像头。', '、', '根据权利要求', '或', '所述的方法其特征在于所述根据所述参考相似度确定所', '述第一人脸特征与所述第二人脸特征是否对应相同人包括', '根据所述参考相似度、', '参考误报率以及相似度阈值确定所述第一人脸特征与所述第二', '人脸特征是否对应相同人其中不同的误报率对应不同的相似度阈值。', '、', '根据权利要求', '或', '所述的方法其特征在于所述根据所述参考相似度确定所', '述第一人脸特征与所述第二人脸特征是否对应相同人包括', '根据所述参考相似度以及阈值信息确定归一化后的参考相似度', '根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否对', '应相同人。', '、', '根据权利要求', '-任一项所述的方法其特征在于所述提取所述第一人脸图像的', '第_人脸特征包括', '将所述第一人脸图像输入预先训练完成的神经网络通过所述神经网络输出所述第一', '人脸图像的第一人脸特征其中所述神经网络基于第一类型图像样本和第二类型图像样', '本训练得到所述第一类型图像样本和所述第二类型图像样本由不同类型的摄像头拍摄得', '到且所述第一类型图像样本和所述第二类型图像样本中包括人脸。', '、', '根据权利要求', '所述的方法其特征在于所述神经网络基于所述第一类型图像', '样本、', '所述第二类型图像样本和混合类型图像样本训练得到所述混合类型图像样本由所', '述第一类型图像样本和所述第二类型图像样本配对而得。', '、', '根据权利要求', '-任一项所述的方法其特征在于所述第一摄像头包括车载摄像', '头所述通过第一摄像头获取第一人脸图像包括', '通过所述车载摄像头获取所述第一人脸图像所述第一人脸图像包括车辆的用车人的', '人脸图像。', '、', '根据权利要求', '所述的方法其特征在于所述用车人包括驾驶所述车辆的人、', '乘坐所述车辆的人、', '对所述车辆进行修理的人、', '给所述车辆加油的人以及控制所述车辆的', '人中的一项或多项。', '、', '根据权利要求', '所述的方法其特征在于所述用车人包括驾驶所述车辆的人', '所述通过所述车载摄像头获取所述第一人脸图像包括', '在接收到触发指令的情况下通过所述车载摄像头获取所述第一人脸图像', '或者在所述车辆运行时通过所述车载摄像头获取所述第一人脸图像', '或者在所述车辆的运行速度达到参考速度的情况下通过所述车载摄像头获取所述', '第一人脸图像。', '、', '根据权利要求', '-任一项所述的方法其特征在于所述第二人脸图像为对所述', '用车人进行人脸注册的图像所述将所述第一人脸特征与预先存储的第二人脸特征进行对', '比之前所述方法还包括', '通过所述第二摄像头获取所述第二人脸图像', '提取所述第二人脸图像的第二人脸特征', '保存所述第二人脸图像的第二人脸特征。', '、', '一种神经网络训练方法其特征在于包括', '获取第一类型图像样本和第二类型图像样本所述第一类型图像样本和所述第二类型', '图像样本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述第二类型图像样', '本中包括人脸', '根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。', '、', '根据权利要求', '所述的方法其特征在于所述根据所述第一类型图像样本和所', '述第二类型图像样本训练神经网络包括', '将所述第一类型图像样本和所述第二类型图像样本配对得到所述第一类型图像样本', '和所述第二类型图像样本的混合类型图像样本', '根据所述第一类型图像样本、', '所述第二类型图像样本和所述混合类型图像样本训练', '所述神经网络。', '、', '根据权利要求', '所述的方法其特征在于所述根据所述第一类型图像样本、', '所述第二类型图像样本和所述混合类型图像样本训练所述神经网络包括', '通过所述神经网络获取所述第一类型图像样本的人脸预测结果、', '所述第二类型图像样', '本的人脸预测结果和所述混合类型图像样本的人脸预测结果', '根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、', '所述第二类型图', '像样本的人脸预测结果和人脸标注结果之间的差异、', '以及所述混合类型图像样本的人脸预', '测结果和人脸标注结果的差异训练所述神经网络。', '、', '根据权利要求', '所述的方法其特征在于所述神经网络中包括第一分类器、', '第二分类器和混合分类器所述通过所述神经网络获取所述第一类型图像样本的人脸预测', '结果、', '所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果', '包括', '将所述第一类型图像样本的人脸特征输入至所述第一分类器中得到所述第一类型图', '像样本的人脸预测结果', '将所述第二类型图像样本的人脸特征输入至所述第二分类器中得到所述第二类型图', '像样本的人脸预测结果', '将所述混合类型图像样本的人脸特征输入至所述混合分类器中得到所述混合类型图', '像样本的人脸预测结果。', '、', '根据权利要求', '所述的方法其特征在于所述方法还包括', '在训练完成的所述神经网络中去除所述第一分类器、', '所述第二分类器和所述混合分类', '器得到用于进行人脸识别的神经网络。', '、', '一种人脸识别装置其特征在于包括', '第一获取单元用于通过第一摄像头获取第一人脸图像', '第一提取单元用于提取所述第一人脸图像的第一人脸特征', '对比单元用于将所述第一人脸特征与预先存储的第二人脸特征进行对比获得参考', '相似度所述第二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得所述第二', '摄像头与所述第一摄像头属于不同类型的摄像头', '确定单元用于根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否', '对应相同人。', '、', '根据权利要求', '所述的装置其特征在于', '所述第一摄像头为热成像摄像头所述第二摄像头为可见光摄像头', '或者所述第一摄像头为可见光摄像头所述第一摄像头为热成像摄像头。', '、', '根据权利要求', '或', '所述的装置其特征在于', '所述确定单元具体用于根据所述参考相似度、', '参考误报率以及相似度阈值确定所述', '第一人脸特征与所述第二人脸特征是否对应相同人其中不同的误报率对应不同的相似', '度阈值。', '、', '根据权利要求', '或', '所述的装置其特征在于', '所述确定单元具体用于根据所述参考相似度以及阈值信息确定归一化后的参考相似', '度以及根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否', '对应相同人。', '、', '根据权利要求', '-任_项所述的装置其特征在于', '所述第一提取单元具体用于将所述第一人脸图像输入预先训练完成的神经网络通', '过所述神经网络输出所述第一人脸图像的第一人脸特征其中所述神经网络基于第一类', '型图像样本和第二类型图像样本训练得到所述第一类型图像样本和所述第二类型图像样', '本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述第二类型图像样本中包', '括人脸。', '、', '根据权利要求', '所述的装置其特征在于所述神经网络基于所述第一类型图', '像样本、', '所述第二类型图像样本和混合类型图像样本训练得到所述混合类型图像样本由', '所述第一类型图像样本和所述第二类型图像样本配对而得。', '、', '根据权利要求', '-任一项所述的装置其特征在于所述第一摄像头包括车载', '摄像头', '所述第一获取单元具体用于通过所述车载摄像头获取所述第一人脸图像所述第一', '人脸图像包括车辆的用车人的人脸图像。', '、', '根据权利要求', '所述的装置其特征在于所述用车人包括驾驶所述车辆的人、', '乘坐所述车辆的人、', '对所述车辆进行修理的人、', '给所述车辆加油的人以及控制所述车辆的', '人中的一项或多项。', '、', '根据权利要求', '所述的装置其特征在于所述用车人包括驾驶所述车辆的人', '所述第一获取单元具体用于在接收到触发指令的情况下通过所述车载摄像头获取所述', '第一人脸图像', '或者所述第一获取单元具体用于在所述车辆运行时通过所述车载摄像头获取所', '述第', '_人脸图像', '或者所述第一获取单元具体用于在所述车辆的运行速度达到参考速度的情况下', '通过所述车载摄像头获取所述第一人脸图像。', '、', '根据权利要求', '-任一项所述的装置其特征在于所述第二人脸图像为对所', '述用车人进行人脸注册的图像所述装置还包括', '第二获取单元用于通过所述第二摄像头获取所述第二人脸图像', '第二提取单元用于提取所述第二人脸图像的第二人脸特征', '保存单元用于保存所述第二人脸图像的第二人脸特征。', '、', '一种神经网络训练装置其特征在于包括', '获取单元用于获取第一类型图像样本和第二类型图像样本所述第一类型图像样本', '和所述第二类型图像样本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述', '第二类型图像样本中包括人脸', '训练单元用于根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。', '、', '根据权利要求', '所述的装置其特征在于所述训练单元包括', '配对子单元用于将所述第一类型图像样本和所述第二类型图像样本配对得到所述', '第一类型图像样本和所述第二类型图像样本的混合类型图像样本', '训练子单元用于根据所述第一类型图像样本、', '所述第二类型图像样本和所述混合类', '型图像样本训练所述神经网络。', '、', '根据权利要求', '所述的装置其特征在于', '所述训练子单元具体用于通过所述神经网络获取所述第一类型图像样本的人脸预测', '结果、', '所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果', '以及根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、', '所述第二类型图', '像样本的人脸预测结果和人脸标注结果之间的差异、', '以及所述混合类型图像样本的人脸预', '测结果和人脸标注结果的差异训练所述神经网络。', '、', '根据权利要求', '所述的装置其特征在于所述神经网络中包括第一分类器、', '第二分类器和混合分类器', '所述训练子单元具体用于将所述第一类型图像样本的人脸特征输入至所述第一分类', '器中得到所述第一类型图像样本的人脸预测结果以及将所述第二类型图像样本的人脸', '特征输入至所述第二分类器中得到所述第二类型图像样本的人脸预测结果以及将所述', '混合类型图像样本的人脸特征输入至所述混合分类器中得到所述混合类型图像样本的人', '脸预测结果。', '、', '根据权利要求', '所述的装置其特征在于所述装置还包括', '神经网络应用单元用于在训练完成的所述神经网络中去除所述第一分类器、', '所述第', '二分类器和所述混合分类器得到用于进行人脸识别的神经网络。', '、', '一种电子设备其特征在于包括处理器和存储器所述处理器和所述存储器耦', '合其中所述存储器用于存储程序指令所述程序指令被所述处理器执行时使所述处', '理器执行权利要求', '-任一项所述的方法和或使所述处理器执行权利要求', '-任一', '项所述的方法。', '、', '一种计算机可读存储介质其特征在于所述计算机可读存储介质中存储有计算', '机程序所述计算机程序包括程序指令所述程序指令当被处理器执行时使所述处理器', '执行权利要求', '-任一项所述的方法和或使所述处理器执行权利要求', '-任一项所', '述的方法。', 'system', 'alerting', 'vision', 'impairment', 'said', 'system', 'comprising', 'processing', 'unit', 'configured', 'operable', 'receiving', 'scene', 'data', 'indicative', 'scene', 'least', 'one', 'consumer', 'environment', 'identifying', 'scene', 'data', 'certain', 'consumer', 'identifying', 'event', 'indicative', 'behavioral', 'compensation', 'vision', 'impairment', 'upon', 'identification', 'event', 'sending', 'notification', 'relating', 'vision', 'impairment', 'system', 'comprising', 'least', 'one', 'sensing', 'unit', 'configured', 'operable', 'detecting', 'scene', 'data', 'system', 'wherein', 'said', 'least', 'one', 'sensing', 'unit', 'comprises', 'least', 'one', 'least', 'one', 'imaging', 'unit', 'configured', 'operable', 'capturing', 'least', 'one', 'least', 'portion', 'consumer', \"'s\", 'body', 'least', 'one', 'motion', 'detector', 'configured', 'operable', 'detecting', 'consumer', 'data', 'indicative', 'motion', 'consumer', 'least', 'one', 'eye', 'tracker', 'configured', 'operable', 'eye', 'motion', 'consumer', 'system', 'wherein', 'least', 'one', 'imaging', 'unit', 'comprises', 'cameras', 'placed', 'different', 'heights', 'system', 'one', 'wherein', 'said', 'sensing', 'unit', 'accommodated', 'optical', 'digital', 'eyewear', 'frame', 'display', 'system', 'one', 'wherein', 'said', 'processing', 'unit', 'configured', 'operable', 'identifying', 'consumer', \"'s\", 'condition', 'said', 'consumer', \"'s\", 'condition', 'comprising', 'consumer', 'data', 'indicative', 'consumer', \"'s\", 'position', 'location', 'relative', 'least', 'one', 'object', 'consumer', \"'s\", 'environment', 'said', 'consumer', 'data', 'comprises', 'least', 'one', 'consumer', \"'s\", 'face', 'eyewear', 'posture', 'position', 'sound', 'motion', 'system', 'one', 'wherein', 'said', 'event', 'comprises', 'least', 'one', 'position', 'head', 'increase', 'decrease', 'viewing', 'distance', 'consumer', 'viewed', 'object', 'changing', 'position', 'eyeglasses', 'worn', 'consumer', 'system', 'one', 'wherein', 'said', 'event', 'identified', 'identifying', 'feature', 'indicative', 'behavioral', 'compensation', 'performing', 'bruckner', 'test', 'performing', 'hirschberg', 'test', 'measuring', 'blink', 'count', 'frequency', 'system', 'wherein', 'feature', 'indicative', 'behavioral', 'compensation', 'comprises', 'squinting', 'head', 'certain', 'distances', 'object', 'consumer', \"'s\", 'eyes', 'certain', 'position', 'eyeglasses', 'consumer', \"'s\", 'face', 'strabismus', 'cataracts', 'reflections', 'eye', 'system', 'one', 'wherein', 'notification', 'includes', 'least', 'one', 'data', 'indicative', 'identified', 'event', 'data', 'indicative', 'identified', 'consumer', 'ophthalmologic', 'recommendations', 'based', 'identified', 'event', 'lack', 'events', 'appointment', 'vision', 'test', 'system', 'one', 'wherein', 'said', 'processing', 'unit', 'comprises', 'memory', 'storing', 'least', 'one', 'reference', 'data', 'indicative', 'behavioral', 'compensation', 'vision', 'impairment', 'data', 'indicative', 'notification', 'data', 'indicative', 'follow-up', 'notification', 'system', 'wherein', 'said', 'processing', 'unit', 'configured', 'least', 'one', 'identifying', 'event', 'upon', 'comparison', 'detected', 'data', 'reference', 'data', 'determining', 'probability', 'vision', 'impairment', 'consumer', 'based', 'comparison', 'system', 'one', 'wherein', 'said', 'processing', 'unit', 'comprises', 'communication', 'interface', 'configured', 'sending', 'notification', 'least', 'one', 'identified', 'consumer', 'third', 'party', 'system', 'one', 'wherein', 'said', 'processing', 'unit', 'configured', 'providing', 'frame', 'recommendation', 'system', 'one', 'wherein', 'said', 'memory', 'configured', 'storing', 'database', 'including', 'multiplicity', 'data', 'sets', 'related', 'spectacle', 'frame', 'models', 'sizes', 'system', 'according', 'wherein', 'said', 'processing', 'unit', 'configured', 'operable', 'correlate', 'frames', 'parameters', 'ophthalmic', 'prescriptions', 'system', 'according', 'wherein', 'said', 'processing', 'unit', 'configured', 'operable', 'correlate', 'frames', 'parameters', 'facial', 'features', 'system', 'according', 'wherein', 'said', 'processing', 'unit', 'configured', 'operable', 'correlate', 'frames', 'parameters', 'eyewear', 'preferences', 'system', 'according', 'comprising', 'server', 'least', 'one', 'computer', 'entity', 'linked', 'server', 'via', 'network', 'wherein', 'said', 'network', 'configured', 'receive', 'respond', 'requests', 'sent', 'across', 'network', 'transmitting', 'one', 'modules', 'computer', 'executable', 'program', 'instructions', 'displayable', 'data', 'network', 'connected', 'computer', 'platform', 'response', 'request', 'wherein', 'said', 'modules', 'include', 'modules', 'configured', 'receive', 'transmit', 'information', 'transmitting', 'frame', 'recommendation', 'optical', 'lens', 'option', 'recommendation', 'based', 'received', 'information', 'display', 'network', 'connected', 'computer', 'platform', 'computer', 'program', 'instructions', 'stored', 'local', 'storage', 'executed', 'processing', 'unit', 'cause', 'processing', 'unit', 'receive', 'data', 'indicative', 'scene', 'least', 'one', 'consumer', 'environment', 'identify', 'data', 'certain', 'consumer', 'identify', 'event', 'indicative', 'behavioral', 'compensation', 'vision', 'impairment', 'upon', 'identification', 'event', 'send', 'notification', 'relating', 'vision', 'impairment', 'computer', 'program', 'product', 'stored', 'tangible', 'computer', 'readable', 'medium', 'comprising', 'library', 'software', 'modules', 'cause', 'computer', 'executing', 'prompt', 'information', 'pertinent', 'least', 'one', 'eyeglasses', 'recommendation', 'optical', 'lens', 'option', 'recommendation', 'store', 'said', 'information', 'display', 'eyewear', 'recommendations', 'computer', 'program', 'product', 'wherein', 'said', 'library', 'comprises', 'module', 'frame', 'selection', 'point', 'sales', 'advertising', 'computer', 'platform', 'facilitating', 'eye', 'glasses', 'marketing', 'selection', 'comprising', 'camera', 'configured', 'execute', 'computer', 'program', 'instructions', 'cause', 'take', 'consumer', 'identify', 'certain', 'consumer', 'identify', 'event', 'indicative', 'behavioral', 'compensation', 'vision', 'impairment', 'upon', 'identification', 'event', 'sending', 'notification', 'relating', 'vision', 'impairment', 'local', 'storage', 'executable', 'instructions', 'carrying', 'storage', 'information', 'alerting', 'vision', 'impairment', 'said', 'comprising', 'identifying', 'certain', 'individual', 'scene', 'data', 'indicative', 'scene', 'least', 'one', 'consumer', 'environment', 'identifying', 'event', 'indicative', 'behavioral', 'compensation', 'vision', 'impairment', 'upon', 'identification', 'event', 'sending', 'notification', 'vision', 'impairment', 'comprising', 'detecting', 'data', 'indicative', 'scene', 'least', 'one', 'consumer', 'retail', 'environment', 'wherein', 'detecting', 'data', 'indicative', 'least', 'one', 'consumer', 'comprises', 'least', 'one', 'capturing', 'least', 'one', 'least', 'one', 'consumer', 'detecting', 'data', 'indicative', 'motion', 'consumer', 'eye', 'motion', 'consumer', 'wherein', 'capturing', 'least', 'one', 'least', 'one', 'consumer', 'comprises', 'continuously', 'recording', 'scene', 'one', 'comprising', 'identifying', 'data', 'consumer', \"'\", 'condition', 'including', 'data', 'indicative', 'consumer', \"'s\", 'position', 'location', 'relative', 'consumer', \"'s\", 'environment', 'said', 'data', 'comprising', 'least', 'one', 'consumer', \"'s\", 'face', 'posture', 'position', 'sound', 'motion', 'one', 'wherein', 'said', 'event', 'comprises', 'least', 'one', 'position', 'head', 'increase', 'decrease', 'viewing', 'distance', 'consumer', 'viewed', 'object', 'changing', 'position', 'eyeglasses', 'worn', 'consumer', 'one', 'wherein', 'identifying', 'event', 'comprises', 'identifying', 'feature', 'indicative', 'behavioral', 'compensation', 'performing', 'bruckner', 'test', 'performing', 'hirschberg', 'test', 'measuring', 'blink', 'countfrequency', 'wherein', 'feature', 'indicative', 'behavioral', 'compensation', 'comprises', 'squinting', 'head', 'certain', 'distances', 'object', 'consumer', \"'s\", 'eyes', 'certain', 'position', 'eyeglasses', 'consumer', \"'s\", 'face', 'strabismus', 'cataracts', 'reflections', 'eye', 'one', 'wherein', 'identifying', 'least', 'one', 'consumer', 'retail', 'environment', 'comprising', 'least', 'one', 'receiving', 'data', 'characterizing', 'retail', 'environment', 'performing', 'face', 'one', 'wherein', 'sending', 'notification', 'comprising', 'sending', 'notification', 'least', 'one', 'identified', 'consumer', 'third', 'party', 'one', 'wherein', 'notification', 'includes', 'least', 'one', 'data', 'indicative', 'identified', 'event', 'data', 'indicative', 'identified', 'consumer', 'ophthalmologic', 'recommendations', 'based', 'identified', 'event', 'lack', 'events', 'appointment', 'vision', 'test', 'one', 'comprising', 'storing', 'least', 'one', 'reference', 'data', 'indicative', 'behavioral', 'compensation', 'vision', 'impairment', 'data', 'indicative', 'notification', 'data', 'indicative', 'follow-up', 'notification', 'comprising', 'identifying', 'event', 'upon', 'comparison', 'detected', 'data', 'reference', 'data', 'determining', 'probability', 'vision', 'impairment', 'consumer', 'based', 'comparison', 'computer', 'program', 'intended', 'stored', 'memory', 'unit', 'computer', 'system', 'removable', 'memory', 'medium', 'adapted', 'cooperate', 'reader', 'unit', 'comprising', 'instructions', 'implementing', 'according']\n"
     ]
    }
   ],
   "source": [
    "# we prepare a empty list, which will contain the words after the stopwords removal\n",
    "tokenized_vector_c = []\n",
    "\n",
    "# we iterate into the list of tokens obtained through the tokenization\n",
    "for token in tokenized_text_c:\n",
    "    # if a token is not a stopword, we insert it in the list\n",
    "    if token not in stopwords_en:\n",
    "        tokenized_vector_c.append(token)\n",
    "\n",
    "# the output is a list of all the tokens of the original text excluding the stopwords\n",
    "print(tokenized_vector_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d538983",
   "metadata": {},
   "source": [
    "### POS Analysis\n",
    "We now do the POS analysis: we use the pos tagging to assign each word to its pos tag, then we clean and simplify the pos text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce410d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('electronic', 'JJ'), ('apparatus', 'NN'), ('including', 'VBG'), ('capturing', 'VBG'), ('storage', 'NN'), ('operation', 'NN'), ('method', 'NN'), ('thereof', 'NN'), ('provided', 'VBD'), ('capturing', 'VBG'), ('captures', 'NNS'), ('storage', 'NN'), ('records', 'NNS'), ('modules', 'NNS'), ('coupled', 'VBD'), ('capturing', 'VBG'), ('storage', 'NN'), ('configured', 'VBD'), ('configure', 'NN'), ('capturing', 'VBG'), ('capture', 'NN'), ('head', 'NN'), ('perform', 'NN'), ('obtain', 'VB'), ('detect', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('within', 'IN'), ('estimate', 'JJ'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'NN'), ('according', 'VBG'), ('facial', 'JJ'), ('landmarks', 'NN'), ('calculate', 'NN'), ('gaze', 'NN'), ('position', 'NN'), ('gazes', 'VBZ'), ('screen', 'JJ'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'JJ'), ('rotation', 'NN'), ('reference', 'NN'), ('angle', 'NN'), ('predetermined', 'VBD'), ('calibration', 'NN'), ('positions', 'NNS'), ('configure', 'VBP'), ('screen', 'JJ'), ('display', 'NN'), ('corresponding', 'VBG'), ('visual', 'JJ'), ('effect', 'NN'), ('according', 'VBG'), ('gaze', 'NN'), ('positionthe', 'NN'), ('present', 'JJ'), ('disclosure', 'NN'), ('provides', 'VBZ'), ('product', 'NN'), ('thereof', 'NN'), ('adopts', 'NNS'), ('fusion', 'VBP'), ('method', 'JJ'), ('perform', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('computations', 'NNS'), ('technical', 'JJ'), ('effects', 'NNS'), ('present', 'JJ'), ('disclosure', 'NN'), ('include', 'VBP'), ('fewer', 'JJR'), ('computations', 'NNS'), ('less', 'RBR'), ('power', 'NN'), ('consumptiona', 'NN'), ('method', 'NN'), ('detecting', 'VBG'), ('body', 'NN'), ('information', 'NN'), ('passengers', 'NNS'), ('vehicle', 'NN'), ('based', 'VBN'), ('humans', 'NNS'), (\"'\", 'POS'), ('status', 'NN'), ('provided', 'VBD'), ('method', 'NN'), ('includes', 'VBZ'), ('steps', 'NNS'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('inputting', 'JJ'), ('interior', 'JJ'), ('vehicle', 'NN'), ('face', 'NN'), ('network', 'NN'), ('detect', 'JJ'), ('faces', 'VBZ'), ('passengers', 'NNS'), ('output', 'NN'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('inputting', 'VBG'), ('interior', 'NN'), ('body', 'NN'), ('network', 'NN'), ('detect', 'JJ'), ('bodies', 'NNS'), ('output', 'NN'), ('body-part', 'JJ'), ('length', 'NN'), ('information', 'NN'), ('b', 'NN'), ('retrieving', 'VBG'), ('specific', 'JJ'), ('height', 'JJ'), ('mapping', 'NN'), ('information', 'NN'), ('referring', 'VBG'), ('height', 'NN'), ('mapping', 'VBG'), ('table', 'JJ'), ('ratios', 'NNS'), ('segment', 'NN'), ('body', 'NN'), ('portions', 'NNS'), ('human', 'JJ'), ('groups', 'NNS'), ('heights', 'NNS'), ('per', 'IN'), ('human', 'JJ'), ('groups', 'NNS'), ('acquiring', 'VBG'), ('specific', 'JJ'), ('height', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('retrieving', 'VBG'), ('specific', 'JJ'), ('weight', 'NN'), ('mapping', 'VBG'), ('information', 'NN'), ('weight', 'NN'), ('mapping', 'NN'), ('table', 'JJ'), ('correlations', 'NNS'), ('heights', 'NNS'), ('weights', 'NNS'), ('per', 'IN'), ('human', 'JJ'), ('groups', 'NNS'), ('acquiring', 'VBG'), ('weight', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('referring', 'VBG'), ('specific', 'JJ'), ('heighttechniques', 'NNS'), ('related', 'VBN'), ('improved', 'JJ'), ('video', 'NN'), ('coding', 'VBG'), ('based', 'VBN'), ('face', 'NN'), ('detection', 'NN'), ('region', 'NN'), ('extraction', 'NN'), ('tracking', 'VBG'), ('discussed', 'VBN'), ('techniques', 'NNS'), ('may', 'MD'), ('include', 'VB'), ('performing', 'VBG'), ('facial', 'JJ'), ('search', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('determine', 'JJ'), ('candidate', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('testing', 'VBG'), ('candidate', 'NN'), ('based', 'VBN'), ('skin', 'JJ'), ('tone', 'NN'), ('information', 'NN'), ('determine', 'NN'), ('valid', 'JJ'), ('invalid', 'JJ'), ('rejecting', 'NN'), ('invalid', 'JJ'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('valid', 'JJ'), ('generate', 'NN'), ('coded', 'VBD'), ('bitstreama', 'NN'), ('method', 'NN'), ('managing', 'VBG'), ('smart', 'JJ'), ('database', 'NN'), ('stores', 'NNS'), ('facial', 'JJ'), ('face', 'NN'), ('provided', 'VBD'), ('method', 'NN'), ('includes', 'VBZ'), ('steps', 'NNS'), ('managing', 'VBG'), ('counting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('specific', 'JJ'), ('person', 'NN'), ('smart', 'JJ'), ('database', 'NN'), ('new', 'JJ'), ('facial', 'JJ'), ('continuously', 'RB'), ('stored', 'VBN'), ('determining', 'VBG'), ('whether', 'IN'), ('first', 'JJ'), ('counted', 'VBN'), ('value', 'NN'), ('representing', 'VBG'), ('count', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('satisfies', 'NNS'), ('first', 'RB'), ('set', 'VBN'), ('value', 'NN'), ('b', 'NN'), ('first', 'RB'), ('counted', 'VBD'), ('value', 'NN'), ('satisfies', 'NNS'), ('first', 'RB'), ('set', 'VBN'), ('value', 'NN'), ('inputting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('generate', 'VBP'), ('quality', 'NN'), ('scores', 'NNS'), ('specific', 'JJ'), ('facial', 'JJ'), ('aggregation', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('second', 'NN'), ('counted', 'VBD'), ('value', 'NN'), ('representing', 'VBG'), ('count', 'NN'), ('specific', 'JJ'), ('quality', 'NN'), ('scores', 'NNS'), ('among', 'IN'), ('quality', 'NN'), ('scores', 'NNS'), ('highest', 'JJS'), ('counting', 'VBG'), ('thereof', 'JJ'), ('satisfies', 'NNS'), ('second', 'VBP'), ('set', 'VBN'), ('value', 'NN'), ('deleting', 'VBG'), ('part', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('uncounted', 'JJ'), ('quality', 'NN'), ('scores', 'NNS'), ('smart', 'VBP'), ('databasea', 'NN'), ('capable', 'JJ'), ('determining', 'VBG'), ('algorithms', 'NN'), ('applied', 'VBD'), ('regions', 'NNS'), ('interest', 'NN'), ('within', 'IN'), ('digital', 'JJ'), ('representations', 'NNS'), ('presented', 'VBD'), ('preprocessing', 'VBG'), ('module', 'NN'), ('utilizes', 'IN'), ('one', 'CD'), ('feature', 'NN'), ('identification', 'NN'), ('algorithms', 'FW'), ('determine', 'NN'), ('regions', 'NNS'), ('interest', 'NN'), ('based', 'VBN'), ('feature', 'NN'), ('density', 'NN'), ('preprocessing', 'VBG'), ('modules', 'NNS'), ('leverages', 'VBZ'), ('feature', 'VBP'), ('density', 'NN'), ('signature', 'NN'), ('region', 'NN'), ('determine', 'NN'), ('diverse', 'NN'), ('modules', 'NNS'), ('operate', 'VBP'), ('region', 'NN'), ('interest', 'NN'), ('specific', 'JJ'), ('embodiment', 'NN'), ('focuses', 'NNS'), ('structured', 'VBD'), ('documents', 'NNS'), ('also', 'RB'), ('presented', 'VBD'), ('disclosed', 'JJ'), ('approach', 'NN'), ('enhanced', 'VBD'), ('addition', 'NN'), ('object', 'NN'), ('classifier', 'NN'), ('classifies', 'NNS'), ('types', 'VBZ'), ('objects', 'NNS'), ('found', 'VBN'), ('regions', 'NNS'), ('interestdisclosed', 'VBN'), ('mobile', 'JJ'), ('terminal', 'NN'), ('mobile', 'JJ'), ('terminal', 'NN'), ('may', 'MD'), ('include', 'VB'), ('front', 'JJ'), ('camera', 'NN'), ('obtaining', 'VBG'), ('face', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('tilted', 'VBD'), ('certain', 'JJ'), ('angle', 'NN'), ('disposed', 'VBD'), ('adjacent', 'JJ'), ('front', 'JJ'), ('camera', 'NN'), ('obtain', 'VB'), ('metadata', 'JJ'), ('face', 'NN'), ('controller', 'NN'), ('obtaining', 'VBG'), ('distance', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('front', 'NN'), ('camera', 'NN'), ('distance', 'NN'), ('enabling', 'VBG'), ('area', 'NN'), ('overlap', 'IN'), ('region', 'NN'), ('first', 'JJ'), ('region', 'NN'), ('representing', 'VBG'), ('range', 'NN'), ('photographable', 'JJ'), ('front', 'NN'), ('camera', 'NN'), ('overlaps', 'VBZ'), ('second', 'JJ'), ('region', 'NN'), ('representing', 'VBG'), ('range', 'NN'), ('photographable', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('maximumthis', 'NN'), ('disclosure', 'NN'), ('provides', 'VBZ'), ('methods', 'NNS'), ('apparatus', 'RB'), ('including', 'VBG'), ('computer', 'NN'), ('programs', 'NNS'), ('encoded', 'VBD'), ('computer', 'NN'), ('storage', 'NN'), ('media', 'NNS'), ('intelligent', 'JJ'), ('routing', 'VBG'), ('notifications', 'NNS'), ('related', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('one', 'CD'), ('aspect', 'JJ'), ('smart', 'JJ'), ('television', 'NN'), ('tv', 'NN'), ('implemented', 'VBD'), ('track', 'NN'), (\"'s\", 'POS'), ('tv', 'NN'), ('watching', 'VBG'), ('behavior', 'JJ'), ('anticipate', 'NN'), ('programming', 'NN'), ('based', 'VBN'), ('behavior', 'JJ'), ('aspects', 'NNS'), ('smart', 'JJ'), ('tv', 'NN'), ('implemented', 'VBD'), ('detect', 'NN'), (\"'s\", 'POS'), ('presence', 'NN'), ('based', 'VBN'), ('detection', 'NN'), ('automatically', 'RB'), ('change', 'JJ'), ('tv', 'NN'), ('channel', 'NN'), ('media', 'NNS'), ('programming', 'VBG'), ('analyzed', 'VBN'), ('desirable', 'JJ'), ('aspects', 'NNS'), ('smart', 'JJ'), ('tv', 'NN'), ('implemented', 'VBD'), ('transmit', 'NN'), ('notification', 'NN'), ('instructions', 'NNS'), ('electronic', 'JJ'), ('within', 'IN'), ('network', 'NN'), ('attempt', 'NN'), ('alert', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('additionally', 'RB'), ('smart', 'JJ'), ('tv', 'NN'), ('implemented', 'VBD'), ('transmit', 'NN'), ('detection', 'NN'), ('instructions', 'NNS'), ('electronic', 'JJ'), ('within', 'IN'), ('network', 'NN'), ('whereby', 'WRB'), ('electronic', 'JJ'), ('attempt', 'NN'), ('detect', 'NN'), (\"'s\", 'POS'), ('presence', 'NN'), ('voice', 'NN'), ('configured', 'VBD'), ('output', 'NN'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('including', 'VBG'), ('pixels', 'NNS'), ('pixel', 'JJ'), ('corresponds', 'VBZ'), ('one', 'CD'), ('sensors', 'NNS'), ('sensor', 'VBP'), ('array', 'JJ'), ('camera', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('depth', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('light', 'JJ'), ('sub-band', 'JJ'), ('spectral', 'JJ'), ('illuminators', 'NNS'), ('camera', 'VBP'), ('face', 'NN'), ('machine', 'NN'), ('previously', 'RB'), ('trained', 'VBN'), ('set', 'VBN'), ('labeled', 'JJ'), ('training', 'NN'), ('depth+multi-spectral', 'JJ'), ('structure', 'NN'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('confidence', 'NN'), ('value', 'NN'), ('indicating', 'VBG'), ('likelihood', 'JJ'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('includes', 'VBZ'), ('faceembodiments', 'NNS'), ('present', 'JJ'), ('disclosure', 'NN'), ('relate', 'NN'), ('processing', 'NN'), ('method', 'NN'), ('apparatus', 'NN'), ('electronic', 'JJ'), ('method', 'NN'), ('includes', 'VBZ'), ('acquiring', 'VBG'), ('photo', 'NN'), ('album', 'NN'), ('obtained', 'VBD'), ('face', 'NN'), ('clustering', 'VBG'), ('collecting', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('respective', 'JJ'), ('photo', 'NN'), ('album', 'NN'), ('acquiring', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('selecting', 'VBG'), ('cover', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('taking', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('setting', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('photo', 'NN'), ('albumtechniques', 'NNS'), ('described', 'VBD'), ('herein', 'JJ'), ('provide', 'IN'), ('location-based', 'JJ'), ('access', 'NN'), ('control', 'NN'), ('secured', 'VBD'), ('resources', 'NNS'), ('generally', 'RB'), ('described', 'VBN'), ('configurations', 'NNS'), ('disclosed', 'VBD'), ('herein', 'RBR'), ('enable', 'JJ'), ('dynamically', 'RB'), ('modify', 'JJ'), ('access', 'NN'), ('secured', 'VBD'), ('resources', 'NNS'), ('based', 'VBN'), ('one', 'CD'), ('location-related', 'JJ'), ('actions', 'NNS'), ('example', 'NN'), ('techniques', 'NNS'), ('disclosed', 'VBN'), ('herein', 'RB'), ('enable', 'JJ'), ('computing', 'VBG'), ('control', 'NN'), ('access', 'NN'), ('resources', 'NNS'), ('computing', 'VBG'), ('display', 'NN'), ('secured', 'VBN'), ('locations', 'NNS'), ('secured', 'VBN'), ('data', 'NNS'), ('configurations', 'NNS'), ('techniques', 'NNS'), ('disclosed', 'VBD'), ('herein', 'RBR'), ('enable', 'JJ'), ('controlled', 'JJ'), ('access', 'NN'), ('secured', 'VBD'), ('resources', 'NNS'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('invitation', 'NN'), ('associated', 'VBN'), ('location', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicating', 'VBG'), ('location', 'NN'), ('one', 'CD'), ('embodiment', 'NN'), ('provides', 'VBZ'), ('method', 'JJ'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('piece', 'NN'), ('content', 'NN'), ('salient', 'NN'), ('moments', 'NNS'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('method', 'NN'), ('comprises', 'NNS'), ('based', 'VBN'), ('salient', 'JJ'), ('moments', 'NNS'), ('data', 'NNS'), ('determining', 'VBG'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('method', 'NN'), ('comprises', 'VBZ'), ('displaying', 'VBG'), ('viewport', 'NN'), ('display', 'NN'), ('movement', 'NN'), ('viewport', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('path', 'NN'), ('playback', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('method', 'NN'), ('comprises', 'VBZ'), ('generating', 'VBG'), ('augmentation', 'NN'), ('salient', 'JJ'), ('moment', 'NN'), ('occurring', 'VBG'), ('piece', 'NN'), ('content', 'NN'), ('presenting', 'VBG'), ('augmentation', 'NN'), ('viewport', 'NN'), ('portion', 'NN'), ('playback', 'NN'), ('augmentation', 'NN'), ('comprises', 'VBZ'), ('interactive', 'JJ'), ('hint', 'NN'), ('guiding', 'VBG'), ('viewport', 'NN'), ('salient', 'NN'), ('momenta', 'VBD'), ('computer-implemented', 'JJ'), ('method', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('provided', 'VBD'), ('facial', 'JJ'), ('method', 'NN'), ('includes', 'VBZ'), ('receiving', 'VBG'), ('method', 'NN'), ('also', 'RB'), ('includes', 'VBZ'), ('extracting', 'JJ'), ('feature', 'NN'), ('extractor', 'NN'), ('utilizing', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('cnn', 'NNS'), ('enlarged', 'VBD'), ('intra-class', 'JJ'), ('variance', 'NN'), ('long-tail', 'JJ'), ('classes', 'NNS'), ('feature', 'VBP'), ('vectors', 'NNS'), ('method', 'VBP'), ('additionally', 'RB'), ('includes', 'VBZ'), ('generating', 'JJ'), ('feature', 'NN'), ('generator', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vectors', 'NNS'), ('feature', 'VBP'), ('vectors', 'NNS'), ('method', 'VBP'), ('includes', 'VBZ'), ('classifying', 'VBG'), ('utilizing', 'VBG'), ('fully', 'RB'), ('connected', 'VBN'), ('classifier', 'JJR'), ('identity', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vector', 'NN'), ('method', 'NN'), ('also', 'RB'), ('includes', 'VBZ'), ('control', 'NN'), ('operation', 'NN'), ('-based', 'VBD'), ('machine', 'NN'), ('react', 'NN'), ('accordance', 'NN'), ('identitysome', 'JJ'), ('embodiments', 'NNS'), ('invention', 'NN'), ('provide', 'VBP'), ('efficient', 'JJ'), ('expressive', 'JJ'), ('machine-trained', 'JJ'), ('networks', 'NNS'), ('performing', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('machine-trained', 'JJ'), ('mt', 'NN'), ('networks', 'NNS'), ('embodiments', 'NNS'), ('use', 'VBP'), ('novel', 'JJ'), ('processing', 'NN'), ('nodes', 'NNS'), ('novel', 'JJ'), ('activation', 'NN'), ('functions', 'NNS'), ('allow', 'VBP'), ('mt', 'NN'), ('network', 'NN'), ('efficiently', 'RB'), ('define', 'VBZ'), ('fewer', 'JJR'), ('processing', 'NN'), ('node', 'NN'), ('layers', 'NNS'), ('complex', 'JJ'), ('mathematical', 'JJ'), ('expression', 'NN'), ('solves', 'VBZ'), ('particular', 'JJ'), ('problem', 'NN'), ('eg', 'NN'), ('face', 'NN'), ('speech', 'NN'), ('etc', 'FW'), ('embodiments', 'NNS'), ('activation', 'NN'), ('function', 'NN'), ('eg', 'FW'), ('cup', 'NN'), ('function', 'NN'), ('used', 'VBN'), ('numerous', 'JJ'), ('processing', 'VBG'), ('nodes', 'NNS'), ('mt', 'JJ'), ('network', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('activation', 'NN'), ('function', 'NN'), ('configured', 'VBD'), ('differently', 'RB'), ('different', 'JJ'), ('processing', 'VBG'), ('nodes', 'NNS'), ('different', 'JJ'), ('nodes', 'NNS'), ('emulate', 'VBP'), ('implement', 'JJ'), ('two', 'CD'), ('different', 'JJ'), ('functions', 'NNS'), ('eg', 'VBP'), ('two', 'CD'), ('boolean', 'JJ'), ('logical', 'JJ'), ('operators', 'NNS'), ('xor', 'VBP'), ('activation', 'NN'), ('function', 'NN'), ('embodiments', 'NNS'), ('periodic', 'JJ'), ('function', 'NN'), ('configured', 'VBD'), ('implement', 'JJ'), ('different', 'JJ'), ('functions', 'NNS'), ('eg', 'VBP'), ('different', 'JJ'), ('sinusoidal', 'JJ'), ('functionsmethods', 'NNS'), ('may', 'MD'), ('provide', 'VB'), ('facial', 'JJ'), ('least', 'JJS'), ('one', 'CD'), ('input', 'NN'), ('utilizing', 'VBG'), ('hierarchical', 'JJ'), ('feature', 'NN'), ('learning', 'VBG'), ('pair-wise', 'JJ'), ('receptive', 'JJ'), ('field', 'NN'), ('theory', 'NN'), ('may', 'MD'), ('used', 'VBN'), ('input', 'VB'), ('generate', 'JJ'), ('pre-processed', 'JJ'), ('multi-channel', 'JJ'), ('channels', 'NNS'), ('pre-processed', 'JJ'), ('may', 'MD'), ('activated', 'VB'), ('based', 'VBN'), ('amount', 'NN'), ('feature', 'NN'), ('rich', 'JJ'), ('details', 'NNS'), ('within', 'IN'), ('channels', 'NNS'), ('similarly', 'RB'), ('local', 'JJ'), ('patches', 'NNS'), ('may', 'MD'), ('activated', 'VB'), ('based', 'VBN'), ('discriminant', 'NN'), ('within', 'IN'), ('local', 'JJ'), ('patches', 'NNS'), ('may', 'MD'), ('extracted', 'VB'), ('local', 'JJ'), ('patches', 'NNS'), ('discriminant', 'NN'), ('may', 'MD'), ('selected', 'VBN'), ('order', 'NN'), ('perform', 'NN'), ('feature', 'NN'), ('matching', 'VBG'), ('pair', 'JJ'), ('sets', 'NNS'), ('may', 'MD'), ('utilize', 'VB'), ('patch', 'NN'), ('feature', 'NN'), ('pooling', 'VBG'), ('pair-wise', 'JJ'), ('matching', 'JJ'), ('large-scale', 'JJ'), ('training', 'NN'), ('order', 'NN'), ('quickly', 'RB'), ('accurately', 'RB'), ('perform', 'JJ'), ('facial', 'JJ'), ('low', 'JJ'), ('cost', 'NN'), ('memory', 'NN'), ('computationa', 'NN'), ('method', 'NN'), ('controlling', 'VBG'), ('terminal', 'NN'), ('provided', 'VBD'), ('terminal', 'JJ'), ('includes', 'VBZ'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('acquired', 'VBD'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('motion', 'NN'), ('parameter', 'NN'), ('terminal', 'NN'), ('obtained', 'VBD'), ('processing', 'NN'), ('acquired', 'VBD'), ('controlled', 'VBN'), ('performed', 'NNS'), ('based', 'VBN'), ('motion', 'NN'), ('parameter', 'NN'), ('equal', 'JJ'), ('less', 'RBR'), ('preset', 'JJ'), ('parameter', 'NN'), ('threshold', 'NN'), ('skipped', 'VBD'), ('based', 'VBN'), ('motion', 'NN'), ('parameter', 'NN'), ('greater', 'JJR'), ('preset', 'NN'), ('parameter', 'NN'), ('thresholda', 'VBD'), ('drive-through', 'JJ'), ('order', 'NN'), ('processing', 'NN'), ('method', 'NN'), ('apparatus', 'NN'), ('disclosed', 'VBD'), ('drive-through', 'JJ'), ('order', 'NN'), ('processing', 'NN'), ('method', 'NN'), ('includes', 'VBZ'), ('receiving', 'VBG'), ('customer', 'NN'), ('information', 'NN'), ('detected', 'VBN'), ('vision', 'NN'), ('providing', 'VBG'), ('product', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('processing', 'VBG'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('according', 'VBG'), ('present', 'JJ'), ('disclosure', 'NN'), ('possible', 'JJ'), ('rapidly', 'RB'), ('process', 'JJ'), ('order', 'NN'), ('using', 'VBG'), ('customer', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('using', 'VBG'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('ai', 'JJ'), ('model', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('g', 'JJ'), ('networkan', 'JJ'), ('processing', 'NN'), ('method', 'NN'), ('performed', 'VBD'), ('computing', 'VBG'), ('includes', 'VBZ'), ('identifying', 'VBG'), ('using', 'VBG'), ('face', 'NN'), ('one', 'CD'), ('faces', 'VBZ'), ('face', 'NN'), ('corresponding', 'VBG'), ('respective', 'JJ'), ('person', 'NN'), ('captured', 'VBD'), ('first', 'RB'), ('identified', 'VBN'), ('face', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('selecting', 'VBG'), ('tiles', 'NNS'), ('first', 'RB'), ('tile', 'JJ'), ('matches', 'NNS'), ('face', 'VBP'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'JJ'), ('accordance', 'NN'), ('predefined', 'VBD'), ('correspondence', 'NN'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('set', 'VBN'), ('pre-stored', 'JJ'), ('description', 'NN'), ('parameters', 'NNS'), ('first', 'RB'), ('tile', 'IN'), ('generating', 'VBG'), ('second', 'JJ'), ('covering', 'VBG'), ('faces', 'VBZ'), ('respective', 'JJ'), ('persons', 'NNS'), ('first', 'RB'), ('corresponding', 'VBG'), ('first', 'JJ'), ('tiles', 'NNS'), ('sharing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('predefined', 'VBN'), ('order', 'NN'), ('via', 'IN'), ('group', 'NN'), ('chat', 'WP'), ('sessionin', 'VBZ'), ('one', 'CD'), ('embodiment', 'NN'), ('artificial', 'JJ'), ('reality', 'NN'), ('determines', 'NNS'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('tracking', 'VBG'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'NN'), ('eye', 'NN'), ('tracking', 'VBG'), ('associated', 'VBN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('worn', 'VBN'), ('artificial', 'JJ'), ('reality', 'NN'), ('receives', 'NNS'), ('first', 'RB'), ('inputs', 'RB'), ('associated', 'VBN'), ('body', 'NN'), ('determines', 'VBZ'), ('region', 'NN'), ('looking', 'VBG'), ('within', 'IN'), ('field', 'NN'), ('view', 'NN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('received', 'VBD'), ('first', 'JJ'), ('inputs', 'NNS'), ('determines', 'NNS'), ('vergence', 'NN'), ('distance', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('region', 'NN'), ('looking', 'VBG'), ('locations', 'NNS'), ('one', 'CD'), ('objects', 'VBZ'), ('scene', 'NN'), ('displayed', 'VBD'), ('head-mounted', 'JJ'), ('display', 'NN'), ('adjusts', 'VBZ'), ('one', 'CD'), ('configurations', 'NNS'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('vergence', 'NN'), ('distance', 'NN'), ('computer-implemented', 'JJ'), ('method', 'NN'), ('provided', 'VBD'), ('-based', 'JJ'), ('self-guided', 'JJ'), ('object', 'JJ'), ('detection', 'NN'), ('method', 'NN'), ('includes', 'VBZ'), ('receiving', 'VBG'), ('set', 'VBN'), ('respective', 'JJ'), ('grid', 'JJ'), ('thereon', 'NN'), ('labeled', 'VBD'), ('regarding', 'VBG'), ('respective', 'JJ'), ('object', 'NN'), ('detected', 'VBD'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('method', 'NN'), ('includes', 'VBZ'), ('training', 'VBG'), ('grid-based', 'JJ'), ('object', 'JJ'), ('detector', 'NN'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('method', 'NN'), ('also', 'RB'), ('includes', 'VBZ'), ('determining', 'VBG'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('applying', 'VBG'), ('local', 'JJ'), ('segmentation', 'NN'), ('method', 'NN'), ('additionally', 'RB'), ('includes', 'VBZ'), ('training', 'VBG'), ('region-based', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('rcnn', 'NN'), ('joint', 'NN'), ('object', 'JJ'), ('localization', 'NN'), ('object', 'NN'), ('using', 'VBG'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('input', 'NN'), ('rcnna', 'NN'), ('method', 'NN'), ('face', 'NN'), ('comprising', 'VBG'), ('multiple', 'JJ'), ('phases', 'NNS'), ('implemented', 'VBN'), ('parallel', 'JJ'), ('architecture', 'NN'), ('first', 'JJ'), ('phase', 'NN'), ('normalization', 'NN'), ('phase', 'NN'), ('whereby', 'WRB'), ('captured', 'VBD'), ('normalized', 'JJ'), ('size', 'NN'), ('orientation', 'NN'), ('illumination', 'NN'), ('stored', 'VBD'), ('preexisting', 'JJ'), ('database', 'NN'), ('second', 'JJ'), ('phase', 'NN'), ('feature', 'NN'), ('extractiondistance', 'NN'), ('matrix', 'NN'), ('phase', 'NN'), ('distance', 'NN'), ('matrix', 'NN'), ('generated', 'VBD'), ('captured', 'JJ'), ('coarse', 'JJ'), ('phase', 'NN'), ('generated', 'VBD'), ('distance', 'NN'), ('matrix', 'NNS'), ('compared', 'VBN'), ('distance', 'NN'), ('matrices', 'NNS'), ('database', 'VBP'), ('using', 'VBG'), ('euclidean', 'JJ'), ('distance', 'NN'), ('matches', 'NNS'), ('create', 'VBP'), ('candidate', 'NN'), ('lists', 'NNS'), ('detailed', 'VBD'), ('phase', 'NN'), ('multiple', 'JJ'), ('face', 'NN'), ('algorithms', 'NN'), ('applied', 'JJ'), ('candidate', 'NN'), ('lists', 'NNS'), ('produce', 'VBP'), ('final', 'JJ'), ('result', 'NN'), ('distance', 'NN'), ('matrices', 'NNS'), ('normalized', 'JJ'), ('database', 'NN'), ('may', 'MD'), ('broken', 'VB'), ('parallel', 'JJ'), ('lists', 'NNS'), ('parallelization', 'VBP'), ('feature', 'NN'), ('extractiondistance', 'NN'), ('matrix', 'NN'), ('phase', 'NN'), ('candidate', 'NN'), ('lists', 'NNS'), ('may', 'MD'), ('also', 'RB'), ('grouped', 'VB'), ('according', 'VBG'), ('dissimilarity', 'NN'), ('algorithm', 'NN'), ('parallel', 'NN'), ('processing', 'NN'), ('detailed', 'JJ'), ('phasean', 'JJ'), ('imaging', 'VBG'), ('including', 'VBG'), ('pixel', 'JJ'), ('matrix', 'NN'), ('provided', 'VBD'), ('pixel', 'JJ'), ('matrix', 'NN'), ('includes', 'VBZ'), ('phase', 'JJ'), ('detection', 'NN'), ('pixels', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('performs', 'NNS'), ('autofocusing', 'VBG'), ('according', 'VBG'), ('pixel', 'NN'), ('data', 'NNS'), ('phase', 'NN'), ('detection', 'NN'), ('pixels', 'NNS'), ('determines', 'VBZ'), ('operating', 'VBG'), ('resolution', 'NN'), ('regular', 'JJ'), ('pixels', 'NNS'), ('according', 'VBG'), ('autofocused', 'JJ'), ('pixel', 'NN'), ('data', 'NNS'), ('phase', 'NN'), ('detection', 'NN'), ('pixels', 'NNS'), ('wherein', 'VBP'), ('phase', 'JJ'), ('detection', 'NN'), ('pixels', 'NNS'), ('always-on', 'JJ'), ('pixels', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('selectively', 'RB'), ('turned', 'VBD'), ('autofocusing', 'VBG'), ('accomplishedan', 'NN'), ('apparatus', 'NN'), ('includes', 'VBZ'), ('first', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('providing', 'VBG'), ('first', 'JJ'), ('object', 'JJ'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('second', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('providing', 'VBG'), ('second', 'JJ'), ('object', 'JJ'), ('second', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('different', 'JJ'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('generator', 'NN'), ('generates', 'NNS'), ('first', 'RB'), ('depth', 'VB'), ('map', 'NN'), ('first', 'RB'), ('based', 'VBN'), ('first', 'JJ'), ('second', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('generator', 'NN'), ('generates', 'VBZ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('second', 'NN'), ('based', 'VBN'), ('first', 'RB'), ('second', 'JJ'), ('first', 'JJ'), ('depth', 'NN'), ('mapmethods', 'NNS'), ('apparatus', 'VBP'), ('including', 'VBG'), ('computer', 'NN'), ('programs', 'NNS'), ('encoded', 'VBD'), ('computer', 'NN'), ('storage', 'NN'), ('media', 'NNS'), ('payment', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('provided', 'VBD'), ('one', 'CD'), ('methods', 'NNS'), ('includes', 'VBZ'), ('acquiring', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('target', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('wherein', 'IN'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('includes', 'VBZ'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('target', 'NN'), ('willingness', 'NN'), ('pay', 'NN'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('including', 'VBG'), ('determining', 'VBG'), ('whether', 'IN'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('less', 'RBR'), ('angle', 'JJ'), ('threshold', 'NN'), ('whether', 'IN'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('greater', 'JJR'), ('probability', 'NN'), ('threshold', 'JJ'), ('response', 'NN'), ('determining', 'VBG'), ('target', 'NN'), ('willingness', 'JJ'), ('pay', 'NN'), ('completing', 'VBG'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('novel', 'JJ'), ('method', 'NN'), ('apparatus', 'NN'), ('face', 'NN'), ('authentication', 'NN'), ('disclosed', 'VBD'), ('disclosed', 'VBN'), ('method', 'NN'), ('comprises', 'VBZ'), ('detecting', 'VBG'), ('motion', 'NN'), ('subject', 'NN'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('assigning', 'VBG'), ('unique', 'JJ'), ('session', 'NN'), ('identification', 'NN'), ('number', 'NN'), ('subject', 'JJ'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('generating', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('assessing', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('conducing', 'VBG'), ('incremental', 'JJ'), ('training', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('determining', 'VBG'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('identifying', 'VBG'), ('intent', 'NN'), ('subject', 'JJ'), ('authorizing', 'VBG'), ('access', 'NN'), ('point', 'NN'), ('entry', 'NN'), ('based', 'VBN'), ('determined', 'VBN'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('intent', 'NN'), ('subjectdisclosed', 'VBN'), ('herein', 'NN'), ('robot', 'VBZ'), ('electronic', 'JJ'), ('acquiring', 'VBG'), ('video', 'NN'), ('method', 'NN'), ('acquiring', 'VBG'), ('video', 'NN'), ('using', 'VBG'), ('robot', 'JJ'), ('robot', 'NN'), ('includes', 'VBZ'), ('camera', 'NN'), ('configured', 'VBD'), ('rotate', 'JJ'), ('lateral', 'JJ'), ('direction', 'NN'), ('tilt', 'VBD'), ('vertical', 'JJ'), ('direction', 'NN'), ('controls', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('direction', 'NN'), ('rotation', 'NN'), ('camera', 'NN'), ('angle', 'NN'), ('tilt', 'NN'), ('camera', 'NN'), ('focal', 'JJ'), ('distance', 'NN'), ('camera', 'NN'), ('tracking', 'VBG'), ('video', 'NN'), ('acquired', 'VBD'), ('cameras', 'NNS'), ('methods', 'NNS'), ('disclosed', 'VBD'), ('inferring', 'VBG'), ('topics', 'NNS'), ('file', 'NN'), ('containing', 'VBG'), ('audio', 'JJ'), ('video', 'NN'), ('example', 'NN'), ('multimodal', 'JJ'), ('multimedia', 'NN'), ('file', 'NN'), ('order', 'NN'), ('facilitate', 'NN'), ('video', 'NN'), ('indexing', 'VBG'), ('set', 'VBN'), ('entities', 'NNS'), ('extracted', 'VBD'), ('file', 'NN'), ('linked', 'VBN'), ('produce', 'VBP'), ('graph', 'JJ'), ('reference', 'NN'), ('information', 'NN'), ('also', 'RB'), ('obtained', 'VBD'), ('set', 'JJ'), ('entities', 'NNS'), ('entities', 'NNS'), ('may', 'MD'), ('drawn', 'VB'), ('example', 'NN'), ('wikipedia', 'NN'), ('categories', 'NNS'), ('large', 'JJ'), ('ontological', 'JJ'), ('data', 'NNS'), ('sources', 'NNS'), ('analysis', 'NN'), ('graph', 'NN'), ('using', 'VBG'), ('unsupervised', 'JJ'), ('learning', 'VBG'), ('permits', 'NNS'), ('determining', 'VBG'), ('clusters', 'NNS'), ('graph', 'VBP'), ('extracting', 'VBG'), ('clusters', 'NNS'), ('possibly', 'RB'), ('using', 'VBG'), ('supervised', 'VBN'), ('learning', 'VBG'), ('provides', 'VBZ'), ('selection', 'NN'), ('topic', 'NN'), ('identifiers', 'NNS'), ('topic', 'VBP'), ('identifiers', 'NNS'), ('used', 'VBD'), ('indexing', 'VBG'), ('filea', 'JJ'), ('face', 'NN'), ('method', 'NN'), ('neural', 'JJ'), ('network', 'NN'), ('training', 'VBG'), ('method', 'NN'), ('apparatus', 'NN'), ('electronic', 'JJ'), ('method', 'NN'), ('comprises', 'NNS'), ('obtaining', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('means', 'VBZ'), ('first', 'JJ'), ('camera', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('feature', 'NN'), ('first', 'JJ'), ('face', 'NN'), ('comparing', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('feature', 'NN'), ('pre-stored', 'JJ'), ('second', 'JJ'), ('face', 'NN'), ('feature', 'NN'), ('obtain', 'VB'), ('reference', 'NN'), ('similarity', 'NN'), ('second', 'JJ'), ('face', 'NN'), ('feature', 'NN'), ('obtained', 'VBN'), ('extracting', 'JJ'), ('feature', 'JJ'), ('second', 'JJ'), ('face', 'NN'), ('obtained', 'VBN'), ('second', 'JJ'), ('camera', 'NN'), ('second', 'JJ'), ('camera', 'NN'), ('first', 'RB'), ('camera', 'VB'), ('different', 'JJ'), ('types', 'NNS'), ('cameras', 'NNS'), ('determining', 'VBG'), ('according', 'VBG'), ('reference', 'NN'), ('similarity', 'NN'), ('whether', 'IN'), ('first', 'JJ'), ('face', 'NN'), ('feature', 'NN'), ('second', 'JJ'), ('face', 'NN'), ('feature', 'NN'), ('correspond', 'NN'), ('person', 'NN'), ('present', 'JJ'), ('invention', 'NN'), ('discloses', 'VBZ'), ('technique', 'JJ'), ('alerting', 'VBG'), ('vision', 'NN'), ('impairment', 'NN'), ('comprises', 'VBZ'), ('processing', 'VBG'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('receiving', 'VBG'), ('scene', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('scene', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('environment', 'NN'), ('identifying', 'VBG'), ('scene', 'NN'), ('data', 'NNS'), ('certain', 'JJ'), ('consumer', 'NN'), ('identifying', 'VBG'), ('event', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('upon', 'IN'), ('identification', 'NN'), ('event', 'NN'), ('sending', 'VBG'), ('notification', 'NN'), ('relating', 'VBG'), ('vision', 'NN'), ('impairment', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagging_a = nltk.pos_tag(tokenized_vector_a)\n",
    "print(pos_tagging_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04fe07af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('configured', 'VBN'), ('make', 'VBP'), ('screen', 'JJ'), ('display', 'NN'), ('frames', 'NNS'), ('comprising', 'VBG'), ('capturing', 'VBG'), ('device', 'NN'), ('storage', 'NN'), ('device', 'NN'), ('storing', 'VBG'), ('modules', 'NNS'), ('coupled', 'VBN'), ('capturing', 'VBG'), ('device', 'NN'), ('storage', 'NN'), ('device', 'NN'), ('configured', 'VBD'), ('execute', 'JJ'), ('modules', 'NNS'), ('storage', 'NN'), ('device', 'NN'), ('configure', 'NN'), ('screen', 'NN'), ('display', 'NN'), ('marker', 'NN'), ('objects', 'NNS'), ('predetermined', 'VBD'), ('positions', 'NNS'), ('configure', 'NN'), ('capturing', 'VBG'), ('device', 'NN'), ('capture', 'NN'), ('first', 'JJ'), ('head', 'NN'), ('looking', 'VBG'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('perform', 'VBP'), ('first', 'JJ'), ('face', 'NN'), ('operations', 'NNS'), ('first', 'RB'), ('head', 'VBP'), ('obtain', 'VB'), ('first', 'JJ'), ('face', 'NN'), ('regions', 'NNS'), ('corresponding', 'VBG'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('detect', 'VBP'), ('first', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('corresponding', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('regions', 'NNS'), ('calculate', 'VBP'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('looking', 'VBG'), ('predetermined', 'VBD'), ('positions', 'NNS'), ('according', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('configure', 'VBP'), ('capturing', 'VBG'), ('device', 'NN'), ('capture', 'NN'), ('second', 'JJ'), ('head', 'NN'), ('perform', 'VB'), ('second', 'JJ'), ('head', 'NN'), ('obtain', 'VB'), ('second', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('detect', 'JJ'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('within', 'IN'), ('second', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('estimate', 'NN'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('calculate', 'VBP'), ('gaze', 'JJ'), ('position', 'NN'), ('screen', 'NN'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'JJ'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('predetermined', 'VBD'), ('positions', 'NNS'), ('configure', 'NN'), ('screen', 'NN'), ('display', 'NN'), ('corresponding', 'VBG'), ('visual', 'JJ'), ('effect', 'NN'), ('according', 'VBG'), ('gaze', 'NN'), ('position', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('gaze', 'JJ'), ('position', 'NN'), ('comprises', 'NNS'), ('first', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('first', 'RB'), ('axial', 'JJ'), ('direction', 'NN'), ('second', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('second', 'JJ'), ('axial', 'JJ'), ('direction', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('head', 'NN'), ('posture', 'NN'), ('angles', 'VBZ'), ('comprise', 'RB'), ('head', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('head', 'NN'), ('yaw', 'NN'), ('angle', 'JJ'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('comprise', 'VBP'), ('first', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('second', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('first', 'RB'), ('yaw', 'RB'), ('angle', 'JJ'), ('second', 'JJ'), ('yaw', 'NN'), ('angle', 'NN'), ('corresponding', 'VBG'), ('predetermined', 'VBN'), ('positions', 'NNS'), ('according', 'VBG'), ('wherein', 'NN'), ('performs', 'NNS'), ('interpolation', 'NN'), ('operation', 'NN'), ('extrapolation', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('first', 'JJ'), ('yaw', 'NN'), ('angle', 'JJ'), ('second', 'JJ'), ('yaw', 'NN'), ('angle', 'NN'), ('first', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('yaw', 'JJ'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('second', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('second', 'JJ'), ('yaw', 'JJ'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('head', 'VBP'), ('yaw', 'RB'), ('angle', 'JJ'), ('thereby', 'RB'), ('obtaining', 'VBG'), ('first', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('gaze', 'JJ'), ('position', 'NN'), ('performs', 'NNS'), ('interpolation', 'NN'), ('operation', 'NN'), ('extrapolation', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('first', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('second', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('third', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('fourth', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('second', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('head', 'VBP'), ('pitch', 'NN'), ('angle', 'NN'), ('thereby', 'RB'), ('obtaining', 'VBG'), ('second', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('gaze', 'NN'), ('position', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('calculates', 'NNS'), ('first', 'RB'), ('viewing', 'VBG'), ('distances', 'NNS'), ('screen', 'NN'), ('according', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NN'), ('estimates', 'NNS'), ('second', 'JJ'), ('viewing', 'VBG'), ('distance', 'NN'), ('screen', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('adjusts', 'VBZ'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('gaze', 'VBP'), ('position', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('viewing', 'NN'), ('distance', 'NN'), ('first', 'RB'), ('viewing', 'VBG'), ('distances', 'NNS'), ('according', 'VBG'), ('wherein', 'JJ'), ('maps', 'NNS'), ('two-dimensional', 'JJ'), ('position', 'NN'), ('coordinates', 'NNS'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('plane', 'NN'), ('coordinate', 'NN'), ('system', 'NN'), ('three-dimensional', 'JJ'), ('position', 'NN'), ('coordinates', 'VBZ'), ('three-dimensional', 'JJ'), ('coordinate', 'NN'), ('system', 'NN'), ('estimates', 'VBZ'), ('head', 'JJ'), ('posture', 'NN'), ('angle', 'NN'), ('according', 'VBG'), ('three-dimensional', 'JJ'), ('position', 'NN'), ('coordinates', 'NNS'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('according', 'VBG'), ('wherein', 'JJ'), ('second', 'JJ'), ('head', 'NN'), ('comprises', 'VBZ'), ('wearable', 'JJ'), ('device', 'NN'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('comprise', 'VBP'), ('third', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('covered', 'VBD'), ('wearable', 'JJ'), ('device', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('second', 'JJ'), ('head', 'NN'), ('comprises', 'VBZ'), ('wearable', 'JJ'), ('device', 'NN'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('comprise', 'VBP'), ('one', 'CD'), ('simulated', 'VBN'), ('landmarks', 'NN'), ('marked', 'VBD'), ('wearable', 'JJ'), ('device', 'NN'), ('operating', 'VBG'), ('adapted', 'VBD'), ('comprising', 'VBG'), ('capturing', 'VBG'), ('device', 'NN'), ('making', 'VBG'), ('screen', 'JJ'), ('display', 'NN'), ('frames', 'NNS'), ('comprising', 'VBG'), ('configuring', 'VBG'), ('screen', 'JJ'), ('display', 'NN'), ('marker', 'NN'), ('objects', 'NNS'), ('predetermined', 'VBD'), ('positions', 'NNS'), ('configuring', 'VBG'), ('capturing', 'VBG'), ('device', 'NN'), ('capture', 'NN'), ('first', 'JJ'), ('head', 'NN'), ('looking', 'VBG'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('performing', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('operations', 'NNS'), ('first', 'RB'), ('head', 'VBP'), ('obtain', 'VB'), ('first', 'JJ'), ('face', 'NN'), ('regions', 'NNS'), ('corresponding', 'VBG'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('detecting', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('corresponding', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('regions', 'NNS'), ('calculating', 'VBG'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('looking', 'VBG'), ('predetermined', 'VBD'), ('positions', 'NNS'), ('according', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('configuring', 'VBG'), ('capturing', 'VBG'), ('device', 'JJ'), ('capture', 'NN'), ('second', 'JJ'), ('head', 'NN'), ('performing', 'VBG'), ('second', 'JJ'), ('head', 'NN'), ('obtain', 'VB'), ('second', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('detecting', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('within', 'IN'), ('second', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('estimating', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('calculating', 'VBG'), ('gaze', 'JJ'), ('position', 'NN'), ('screen', 'NN'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'JJ'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('predetermined', 'VBD'), ('positions', 'NNS'), ('configuring', 'VBG'), ('screen', 'NN'), ('display', 'NN'), ('corresponding', 'VBG'), ('visual', 'JJ'), ('effect', 'NN'), ('according', 'VBG'), ('gaze', 'JJ'), ('position', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('gaze', 'JJ'), ('position', 'NN'), ('comprises', 'NNS'), ('first', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('first', 'RB'), ('axial', 'JJ'), ('direction', 'NN'), ('second', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('second', 'JJ'), ('axial', 'JJ'), ('direction', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('head', 'NN'), ('posture', 'NN'), ('angles', 'VBZ'), ('comprise', 'RB'), ('head', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('head', 'NN'), ('yaw', 'NN'), ('angle', 'JJ'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('comprise', 'VBP'), ('first', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('second', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('first', 'RB'), ('yaw', 'RB'), ('angle', 'JJ'), ('second', 'JJ'), ('yaw', 'NN'), ('angle', 'NN'), ('corresponding', 'VBG'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('operation', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('step', 'NN'), ('calculating', 'VBG'), ('gaze', 'JJ'), ('position', 'NN'), ('screen', 'NN'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'JJ'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('predetermined', 'VBD'), ('positions', 'NNS'), ('comprises', 'NNS'), ('performing', 'VBG'), ('interpolation', 'NN'), ('operation', 'NN'), ('extrapolation', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('first', 'JJ'), ('yaw', 'NN'), ('angle', 'JJ'), ('second', 'JJ'), ('yaw', 'NN'), ('angle', 'NN'), ('first', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('yaw', 'JJ'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('second', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('second', 'JJ'), ('yaw', 'JJ'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('head', 'VBP'), ('yaw', 'RB'), ('angle', 'JJ'), ('thereby', 'RB'), ('obtaining', 'VBG'), ('first', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('gaze', 'JJ'), ('position', 'NN'), ('performing', 'VBG'), ('interpolation', 'NN'), ('operation', 'NN'), ('extrapolation', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('first', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('second', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('third', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('fourth', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('second', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('head', 'VBP'), ('pitch', 'NN'), ('angle', 'NN'), ('thereby', 'RB'), ('obtaining', 'VBG'), ('second', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('gaze', 'JJ'), ('position', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('comprises', 'NNS'), ('calculating', 'VBG'), ('first', 'JJ'), ('viewing', 'VBG'), ('distances', 'NNS'), ('screen', 'NN'), ('according', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('estimating', 'VBG'), ('second', 'JJ'), ('viewing', 'VBG'), ('distance', 'NN'), ('screen', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('adjusting', 'VBG'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('gaze', 'VBP'), ('position', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('viewing', 'NN'), ('distance', 'NN'), ('first', 'RB'), ('viewing', 'VBG'), ('distances', 'NNS'), ('operation', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('comprises', 'NNS'), ('mapping', 'VBG'), ('two-dimensional', 'JJ'), ('position', 'NN'), ('coordinates', 'NNS'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('plane', 'NN'), ('coordinate', 'NN'), ('system', 'NN'), ('three-dimensional', 'JJ'), ('position', 'NN'), ('coordinates', 'VBZ'), ('three-dimensional', 'JJ'), ('coordinate', 'NN'), ('system', 'NN'), ('estimating', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'IN'), ('according', 'VBG'), ('three-dimensional', 'JJ'), ('position', 'NN'), ('coordinates', 'NNS'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('operation', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('second', 'JJ'), ('head', 'NN'), ('comprises', 'VBZ'), ('wearable', 'JJ'), ('device', 'NN'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('comprise', 'VBP'), ('third', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('covered', 'VBD'), ('wearable', 'JJ'), ('device', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('second', 'JJ'), ('head', 'NN'), ('comprises', 'VBZ'), ('wearable', 'JJ'), ('device', 'NN'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('comprise', 'VBP'), ('one', 'CD'), ('simulated', 'VBN'), ('landmarks', 'NN'), ('marked', 'VBD'), ('wearable', 'JJ'), ('device', 'NN'), ('computation', 'NN'), ('applied', 'VBD'), ('computing', 'VBG'), ('system', 'NN'), ('wherein', 'VBD'), ('computing', 'VBG'), ('system', 'NN'), ('comprises', 'VBZ'), ('control', 'VB'), ('unit', 'NN'), ('computation', 'NN'), ('group', 'NN'), ('general', 'JJ'), ('storage', 'NN'), ('unit', 'NN'), ('wherein', 'VBZ'), ('control', 'JJ'), ('unit', 'NN'), ('comprises', 'VBZ'), ('first', 'JJ'), ('memory', 'NN'), ('decoding', 'VBG'), ('logic', 'JJ'), ('controller', 'NN'), ('wherein', 'NN'), ('computation', 'NN'), ('group', 'NN'), ('comprises', 'VBZ'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('general', 'JJ'), ('storage', 'NN'), ('unit', 'NN'), ('configured', 'VBD'), ('store', 'NN'), ('data', 'NNS'), ('computation', 'NN'), ('comprises', 'VBZ'), ('receiving', 'VBG'), ('controller', 'NN'), ('first', 'RB'), ('level', 'JJ'), ('instruction', 'NN'), ('sequence', 'NN'), ('partitioning', 'VBG'), ('decoding', 'VBG'), ('logic', 'JJ'), ('first', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequence', 'NN'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('creating', 'VBG'), ('controller', 'NN'), ('threads', 'NNS'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('allocating', 'VBG'), ('controller', 'NN'), ('independent', 'JJ'), ('register', 'NN'), ('well', 'RB'), ('configuring', 'VBG'), ('independent', 'JJ'), ('addressing', 'VBG'), ('function', 'NN'), ('thread', 'NN'), ('threads', 'NNS'), ('wherein', 'VBP'), ('integer', 'JJ'), ('greater', 'JJR'), ('equal', 'JJ'), ('obtaining', 'VBG'), ('group', 'NN'), ('controller', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('obtaining', 'VBG'), ('corresponding', 'VBG'), ('fusion', 'NN'), ('computation', 'NN'), ('manner', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('according', 'VBG'), ('computation', 'NN'), ('types', 'NNS'), ('adopting', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('fusion', 'NN'), ('computation', 'NN'), ('manner', 'NN'), ('call', 'NN'), ('threads', 'NNS'), ('performing', 'VBG'), ('computations', 'NNS'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('obtain', 'VB'), ('final', 'JJ'), ('result', 'NN'), ('wherein', 'NN'), ('obtaining', 'VBG'), ('group', 'NN'), ('controller', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('obtaining', 'VBG'), ('corresponding', 'VBG'), ('fusion', 'NN'), ('computation', 'NN'), ('manner', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('according', 'VBG'), ('computation', 'NN'), ('types', 'NNS'), ('adopting', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('fusion', 'NN'), ('computation', 'NN'), ('manner', 'NN'), ('call', 'NN'), ('threads', 'NNS'), ('performing', 'VBG'), ('computations', 'NNS'), ('second', 'JJ'), ('instruction', 'NN'), ('sequences', 'NNS'), ('obtain', 'VB'), ('final', 'JJ'), ('result', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('represent', 'JJ'), ('computation', 'NN'), ('operations', 'NNS'), ('type', 'VBP'), ('group', 'NN'), ('controller', 'NN'), ('calls', 'VBZ'), ('combined', 'VBN'), ('computation', 'NN'), ('manner', 'NN'), ('single', 'JJ'), ('instruction', 'NN'), ('multiple', 'NN'), ('data', 'NNS'), ('type', 'NN'), ('combination', 'NN'), ('single', 'JJ'), ('instruction', 'NN'), ('multiple', 'JJ'), ('threads', 'NNS'), ('uses', 'VBZ'), ('threads', 'NNS'), ('perform', 'NN'), ('combined', 'VBN'), ('computation', 'NN'), ('manner', 'NN'), ('obtain', 'VB'), ('final', 'JJ'), ('result', 'NN'), ('includes', 'VBZ'), ('partitioning', 'VBG'), ('decoding', 'VBG'), ('logic', 'JJ'), ('threads', 'NNS'), ('n', 'RB'), ('wraps', 'VBP'), ('allocating', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('converting', 'VBG'), ('group', 'NN'), ('controller', 'NN'), ('second', 'JJ'), ('instruction', 'NN'), ('sequences', 'NNS'), ('second', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('sending', 'VBG'), ('second', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('computing', 'VBG'), ('units', 'NNS'), ('calling', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('wraps', 'NNS'), ('allocated', 'VBD'), ('computing', 'VBG'), ('units', 'NNS'), ('second', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('fetch', 'VBP'), ('corresponding', 'VBG'), ('data', 'NNS'), ('according', 'VBG'), ('independent', 'JJ'), ('addressing', 'VBG'), ('function', 'NN'), ('performing', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('computations', 'NNS'), ('data', 'NNS'), ('obtain', 'VB'), ('intermediate', 'JJ'), ('results', 'NNS'), ('splicing', 'VBG'), ('intermediate', 'JJ'), ('results', 'NNS'), ('obtain', 'VB'), ('final', 'JJ'), ('result', 'NN'), ('wherein', 'NN'), ('obtaining', 'VBG'), ('group', 'NN'), ('controller', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('obtaining', 'VBG'), ('corresponding', 'VBG'), ('fusion', 'NN'), ('computation', 'NN'), ('manner', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('according', 'VBG'), ('computation', 'NN'), ('types', 'NNS'), ('adopting', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('fusion', 'NN'), ('computation', 'NN'), ('manner', 'NN'), ('call', 'NN'), ('threads', 'NNS'), ('performing', 'VBG'), ('computations', 'NNS'), ('second', 'JJ'), ('instruction', 'NN'), ('sequences', 'NNS'), ('obtain', 'VB'), ('final', 'JJ'), ('result', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('represent', 'JJ'), ('computation', 'NN'), ('operations', 'NNS'), ('different', 'JJ'), ('types', 'NNS'), ('group', 'NN'), ('controller', 'NN'), ('calls', 'VBZ'), ('simultaneous', 'JJ'), ('multi-threading', 'JJ'), ('threads', 'NNS'), ('perform', 'VB'), ('computations', 'NNS'), ('obtain', 'VB'), ('final', 'JJ'), ('result', 'NN'), ('includes', 'VBZ'), ('partitioning', 'VBG'), ('decoding', 'VBG'), ('logic', 'JJ'), ('threads', 'NNS'), ('n', 'RB'), ('wraps', 'VBP'), ('converting', 'VBG'), ('second', 'JJ'), ('instruction', 'NN'), ('sequences', 'NNS'), ('second', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('obtaining', 'VBG'), ('group', 'NN'), ('controller', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('supported', 'VBD'), ('computing', 'VBG'), ('units', 'NNS'), ('allocating', 'VBG'), ('controller', 'NN'), ('n', 'JJ'), ('wraps', 'NNS'), ('second', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('corresponding', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('support', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('wraps', 'VBP'), ('second', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('calling', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('wraps', 'NNS'), ('allocated', 'VBD'), ('computing', 'VBG'), ('units', 'NNS'), ('second', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('fetching', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('corresponding', 'VBG'), ('data', 'NNS'), ('performing', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('computations', 'NNS'), ('data', 'NNS'), ('obtain', 'VB'), ('intermediate', 'JJ'), ('results', 'NNS'), ('splicing', 'VBG'), ('intermediate', 'JJ'), ('results', 'NNS'), ('obtain', 'VB'), ('final', 'JJ'), ('result', 'NN'), ('comprising', 'VBG'), ('wrap', 'NN'), ('wraps', 'NNS'), ('blocked', 'VBD'), ('adding', 'VBG'), ('wrap', 'NN'), ('waiting', 'VBG'), ('queue', 'NN'), ('data', 'NNS'), ('wrap', 'NN'), ('already', 'RB'), ('fetched', 'VBD'), ('adding', 'VBG'), ('wrap', 'NN'), ('preparation', 'NN'), ('queue', 'NN'), ('wherein', 'WRB'), ('preparation', 'NN'), ('queue', 'NN'), ('queue', 'NN'), ('wrap', 'NN'), ('scheduled', 'VBN'), ('executing', 'VBG'), ('located', 'VBN'), ('computing', 'VBG'), ('resource', 'NN'), ('idle', 'JJ'), ('wherein', 'NN'), ('first', 'RB'), ('level', 'JJ'), ('instruction', 'NN'), ('sequence', 'NN'), ('includes', 'VBZ'), ('long', 'JJ'), ('instruction', 'NN'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequence', 'NN'), ('includes', 'VBZ'), ('instruction', 'NN'), ('sequence', 'NN'), ('wherein', 'NN'), ('computing', 'VBG'), ('system', 'NN'), ('includes', 'VBZ'), ('tree', 'JJ'), ('module', 'NN'), ('wherein', 'VBD'), ('tree', 'JJ'), ('module', 'NN'), ('includes', 'VBZ'), ('root', 'NN'), ('port', 'NN'), ('branch', 'NN'), ('ports', 'NNS'), ('wherein', 'VBP'), ('root', 'JJ'), ('port', 'NN'), ('tree', 'NN'), ('module', 'NN'), ('connected', 'VBN'), ('group', 'NN'), ('controller', 'NN'), ('branch', 'NN'), ('ports', 'NNS'), ('tree', 'VBP'), ('module', 'NN'), ('connected', 'VBN'), ('computing', 'VBG'), ('unit', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('respectively', 'RB'), ('tree', 'VBP'), ('module', 'NN'), ('configured', 'VBN'), ('forward', 'RB'), ('data', 'NN'), ('blocks', 'NNS'), ('wraps', 'VBP'), ('instruction', 'NN'), ('sequences', 'NNS'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('wherein', 'VBP'), ('tree', 'JJ'), ('module', 'NN'), ('n-ary', 'JJ'), ('tree', 'NN'), ('wherein', 'NN'), ('n', 'RB'), ('integer', 'RB'), ('greater', 'JJR'), ('equal', 'JJ'), ('wherein', 'NN'), ('computing', 'VBG'), ('system', 'NN'), ('includes', 'VBZ'), ('branch', 'NN'), ('processing', 'VBG'), ('circuit', 'NN'), ('wherein', 'NN'), ('branch', 'NN'), ('processing', 'VBG'), ('circuit', 'NN'), ('connected', 'VBN'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('branch', 'NN'), ('processing', 'VBG'), ('circuit', 'NN'), ('configured', 'VBD'), ('forward', 'RB'), ('data', 'NNS'), ('wraps', 'NNS'), ('instruction', 'NN'), ('sequences', 'NNS'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('computing', 'VBG'), ('system', 'NN'), ('comprising', 'VBG'), ('control', 'NN'), ('unit', 'NN'), ('computation', 'NN'), ('group', 'NN'), ('general', 'JJ'), ('storage', 'NN'), ('unit', 'NN'), ('wherein', 'VBZ'), ('control', 'JJ'), ('unit', 'NN'), ('includes', 'VBZ'), ('first', 'JJ'), ('memory', 'NN'), ('decoding', 'VBG'), ('logic', 'JJ'), ('controller', 'NN'), ('computation', 'NN'), ('group', 'NN'), ('includes', 'VBZ'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('general', 'JJ'), ('storage', 'NN'), ('unit', 'NN'), ('configured', 'VBD'), ('store', 'NN'), ('data', 'NNS'), ('controller', 'NN'), ('configured', 'VBD'), ('receive', 'JJ'), ('first', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequence', 'NN'), ('control', 'NN'), ('first', 'JJ'), ('memory', 'NN'), ('decoding', 'VBG'), ('logic', 'JJ'), ('decoding', 'VBG'), ('logic', 'NN'), ('configured', 'VBD'), ('partition', 'NN'), ('first', 'RB'), ('level', 'JJ'), ('instruction', 'NN'), ('sequence', 'NN'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('controller', 'VBP'), ('configured', 'JJ'), ('create', 'NN'), ('threads', 'NNS'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('allocate', 'VBP'), ('independent', 'JJ'), ('register', 'NN'), ('configure', 'NN'), ('independent', 'JJ'), ('addressing', 'VBG'), ('function', 'NN'), ('thread', 'NN'), ('threads', 'NNS'), ('integer', 'VBP'), ('greater', 'JJR'), ('equal', 'JJ'), ('controller', 'NN'), ('configured', 'VBD'), ('convert', 'JJ'), ('second', 'JJ'), ('instruction', 'NN'), ('sequences', 'NNS'), ('control', 'NN'), ('signals', 'NNS'), ('sending', 'VBG'), ('group', 'NN'), ('controller', 'NN'), ('group', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('receive', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('obtain', 'VB'), ('computational', 'JJ'), ('types', 'NNS'), ('control', 'NN'), ('signals', 'NNS'), ('divide', 'VBP'), ('threads', 'NNS'), ('n', 'RB'), ('wraps', 'VBP'), ('allocate', 'JJ'), ('n', 'JJ'), ('wraps', 'NNS'), ('control', 'NN'), ('signals', 'NNS'), ('computing', 'VBG'), ('units', 'NNS'), ('according', 'VBG'), ('computational', 'JJ'), ('types', 'NNS'), ('computing', 'VBG'), ('units', 'NNS'), ('configured', 'VBN'), ('fetch', 'RB'), ('data', 'NNS'), ('general', 'JJ'), ('storage', 'NN'), ('unit', 'NN'), ('allocated', 'VBD'), ('wraps', 'NNS'), ('control', 'NN'), ('signals', 'NNS'), ('perform', 'VBP'), ('computations', 'NNS'), ('obtain', 'VB'), ('intermediate', 'JJ'), ('result', 'NN'), ('group', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('splice', 'JJ'), ('intermediate', 'JJ'), ('results', 'NNS'), ('obtain', 'VB'), ('final', 'JJ'), ('computation', 'NN'), ('result', 'NN'), ('computing', 'VBG'), ('system', 'NN'), ('wherein', 'JJ'), ('computing', 'VBG'), ('units', 'NNS'), ('includes', 'VBZ'), ('addition', 'NN'), ('computing', 'VBG'), ('unit', 'NN'), ('multiplication', 'NN'), ('computing', 'VBG'), ('unit', 'NN'), ('activation', 'NN'), ('computing', 'VBG'), ('unit', 'NN'), ('dedicated', 'VBN'), ('computing', 'VBG'), ('unit', 'NN'), ('computing', 'VBG'), ('system', 'NN'), ('wherein', 'NN'), ('dedicated', 'VBN'), ('computing', 'VBG'), ('unit', 'NN'), ('includes', 'VBZ'), ('face', 'VBP'), ('computing', 'VBG'), ('unit', 'NN'), ('graphics', 'NNS'), ('computing', 'VBG'), ('unit', 'NN'), ('fingerprint', 'NN'), ('computing', 'VBG'), ('unit', 'NN'), ('neural', 'JJ'), ('network', 'NN'), ('computing', 'VBG'), ('unit', 'NN'), ('computing', 'VBG'), ('system', 'NN'), ('wherein', 'JJ'), ('group', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('computation', 'NN'), ('types', 'NNS'), ('control', 'NN'), ('signals', 'NNS'), ('graphics', 'NNS'), ('computations', 'NNS'), ('fingerprint', 'VBP'), ('identification', 'NN'), ('face', 'NN'), ('neural', 'JJ'), ('network', 'NN'), ('operations', 'NNS'), ('allocate', 'VBP'), ('control', 'NN'), ('signals', 'NNS'), ('face', 'VBP'), ('computing', 'VBG'), ('unit', 'NN'), ('graphics', 'NNS'), ('computing', 'VBG'), ('unit', 'NN'), ('fingerprint', 'NN'), ('computing', 'VBG'), ('unit', 'NN'), ('neural', 'JJ'), ('network', 'NN'), ('computing', 'VBG'), ('unit', 'NN'), ('respectively', 'RB'), ('computing', 'VBG'), ('system', 'NN'), ('wherein', 'VBD'), ('first', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequence', 'NN'), ('includes', 'VBZ'), ('long', 'JJ'), ('instruction', 'NN'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequence', 'NN'), ('includes', 'VBZ'), ('instruction', 'NN'), ('sequence', 'NN'), ('computing', 'VBG'), ('system', 'NN'), ('comprising', 'VBG'), ('tree', 'JJ'), ('module', 'NN'), ('wherein', 'VBD'), ('tree', 'JJ'), ('module', 'NN'), ('includes', 'VBZ'), ('root', 'NN'), ('port', 'NN'), ('branch', 'NN'), ('ports', 'NNS'), ('wherein', 'VBP'), ('root', 'JJ'), ('port', 'NN'), ('tree', 'NN'), ('module', 'NN'), ('connected', 'VBN'), ('group', 'NN'), ('controller', 'NN'), ('branch', 'NN'), ('ports', 'NNS'), ('tree', 'VBP'), ('module', 'NN'), ('connected', 'VBN'), ('computing', 'VBG'), ('unit', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('respectively', 'RB'), ('tree', 'VBP'), ('module', 'NN'), ('configured', 'VBN'), ('forward', 'RB'), ('data', 'NN'), ('blocks', 'NNS'), ('wraps', 'VBP'), ('instruction', 'NN'), ('sequences', 'NNS'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('computing', 'VBG'), ('system', 'NN'), ('wherein', 'VBD'), ('tree', 'JJ'), ('module', 'NN'), ('n-ary', 'JJ'), ('tree', 'NN'), ('wherein', 'NN'), ('n', 'RB'), ('integer', 'RB'), ('greater', 'JJR'), ('equal', 'JJ'), ('computing', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('computing', 'VBG'), ('system', 'NN'), ('includes', 'VBZ'), ('branch', 'NN'), ('processing', 'NN'), ('circuit', 'NN'), ('branch', 'NN'), ('processing', 'VBG'), ('circuit', 'NN'), ('connected', 'VBN'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('branch', 'NN'), ('processing', 'VBG'), ('circuit', 'NN'), ('configured', 'VBD'), ('forward', 'RB'), ('data', 'NNS'), ('wraps', 'NNS'), ('instruction', 'NN'), ('sequences', 'NNS'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('comprising', 'VBG'), ('non-instant', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'VBP'), ('computer', 'NN'), ('program', 'NN'), ('stored', 'VBD'), ('non-instant', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('computer', 'NN'), ('program', 'NN'), ('capable', 'JJ'), ('causing', 'VBG'), ('computer', 'NN'), ('perform', 'NN'), ('-', ':'), ('operations', 'NNS'), ('detecting', 'VBG'), ('body', 'NN'), ('information', 'NN'), ('one', 'CD'), ('passengers', 'NN'), ('vehicle', 'NN'), ('based', 'VBN'), ('humans', 'NNS'), (\"'\", 'POS'), ('status', 'NN'), ('comprising', 'VBG'), ('steps', 'NNS'), ('least', 'JJS'), ('one', 'CD'), ('interior', 'JJ'), ('interior', 'JJ'), ('vehicle', 'NN'), ('acquired', 'VBD'), ('passenger', 'JJR'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('performing', 'VBG'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('face', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('face', 'NN'), ('network', 'NN'), ('detect', 'JJ'), ('passengers', 'NNS'), ('interior', 'VBP'), ('thus', 'RB'), ('output', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('detected', 'VBD'), ('ii', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('body', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('body', 'NN'), ('network', 'NN'), ('detect', 'JJ'), ('bodies', 'NNS'), ('passengers', 'NNS'), ('interior', 'JJ'), ('thus', 'RB'), ('output', 'NN'), ('body-part', 'JJ'), ('length', 'NN'), ('information', 'NN'), ('detected', 'VBD'), ('bodies', 'NNS'), ('b', 'IN'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('performing', 'VBG'), ('process', 'NN'), ('retrieving', 'VBG'), ('specific', 'JJ'), ('height', 'JJ'), ('mapping', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('height', 'VBD'), ('mapping', 'VBG'), ('table', 'NN'), ('stores', 'NNS'), ('height', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('representing', 'VBG'), ('respective', 'JJ'), ('one', 'CD'), ('predetermined', 'VBD'), ('ratios', 'NNS'), ('one', 'CD'), ('segment', 'NN'), ('body', 'NN'), ('portions', 'NNS'), ('human', 'JJ'), ('groups', 'NNS'), ('heights', 'NNS'), ('per', 'IN'), ('human', 'JJ'), ('groups', 'NNS'), ('process', 'NN'), ('acquiring', 'VBG'), ('specific', 'JJ'), ('height', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('specific', 'JJ'), ('height', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('referring', 'VBG'), ('specific', 'JJ'), ('body-part', 'JJ'), ('length', 'NN'), ('information', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('process', 'NN'), ('retrieving', 'VBG'), ('specific', 'JJ'), ('weight', 'NN'), ('mapping', 'VBG'), ('information', 'NN'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('weight', 'VBD'), ('mapping', 'VBG'), ('table', 'NN'), ('stores', 'NNS'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('weight', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('representing', 'VBG'), ('predetermined', 'VBN'), ('correlations', 'NNS'), ('heights', 'NNS'), ('weights', 'NNS'), ('per', 'IN'), ('human', 'JJ'), ('groups', 'NNS'), ('process', 'NN'), ('acquiring', 'VBG'), ('weight', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('specific', 'JJ'), ('weight', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('referring', 'VBG'), ('specific', 'JJ'), ('height', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('wherein', 'JJ'), ('step', 'NN'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('body', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('body', 'NN'), ('network', 'NN'), ('output', 'NN'), ('one', 'CD'), ('one', 'CD'), ('channels', 'NNS'), ('corresponding', 'VBG'), ('interior', 'JJ'), ('via', 'IN'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('ii', 'JJ'), ('generate', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('one', 'CD'), ('channels', 'NNS'), ('corresponding', 'VBG'), ('via', 'IN'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('iii', 'JJ'), ('extract', 'JJ'), ('keypoints', 'NNS'), ('keypoint', 'VB'), ('heatmap', 'NN'), ('via', 'IN'), ('keypoint', 'NN'), ('detector', 'NN'), ('group', 'NN'), ('extracted', 'VBD'), ('keypoints', 'NNS'), ('referring', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('thus', 'RB'), ('generate', 'VB'), ('body', 'NN'), ('parts', 'NNS'), ('per', 'IN'), ('passengers', 'NNS'), ('result', 'VBP'), ('allow', 'IN'), ('body', 'NN'), ('network', 'NN'), ('output', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('body-part', 'JJ'), ('length', 'NN'), ('information', 'NN'), ('passengers', 'NNS'), ('referring', 'VBG'), ('body', 'NN'), ('parts', 'NNS'), ('per', 'IN'), ('passengers', 'NNS'), ('wherein', 'VBP'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('convolutional', 'JJ'), ('layer', 'NN'), ('applies', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('convolution', 'NN'), ('operation', 'NN'), ('interior', 'JJ'), ('thereby', 'RB'), ('output', 'NN'), ('wherein', 'NNS'), ('keypoint', 'VBP'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('includes', 'VBZ'), ('one', 'CD'), ('fully', 'RB'), ('convolutional', 'JJ'), ('network', 'NN'), ('×', 'NNP'), ('convolutional', 'NN'), ('layer', 'NN'), ('applies', 'VBZ'), ('fully-convolution', 'NN'), ('operation', 'NN'), ('×', 'NNP'), ('convolution', 'NN'), ('operation', 'NN'), ('thereby', 'RB'), ('generate', 'JJ'), ('keypoint', 'NN'), ('heatmap', 'NNS'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('wherein', 'VBD'), ('keypoint', 'JJ'), ('detector', 'NN'), ('connects', 'NNS'), ('referring', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('pairs', 'VBZ'), ('respectively', 'RB'), ('highest', 'JJS'), ('mutual', 'JJ'), ('connection', 'NN'), ('probabilities', 'NNS'), ('connected', 'VBN'), ('among', 'IN'), ('extracted', 'JJ'), ('keypoints', 'NNS'), ('thereby', 'RB'), ('group', 'NN'), ('extracted', 'VBD'), ('keypoints', 'NNS'), ('wherein', 'JJ'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('learned', 'VBD'), ('learning', 'JJ'), ('device', 'NN'), ('performing', 'VBG'), ('process', 'NN'), ('inputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('training', 'NN'), ('including', 'VBG'), ('one', 'CD'), ('objects', 'VBZ'), ('training', 'VBG'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('generate', 'VBP'), ('one', 'CD'), ('training', 'VBG'), ('one', 'CD'), ('channels', 'NNS'), ('applying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('convolutional', 'JJ'), ('operation', 'NN'), ('training', 'VBG'), ('ii', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('training', 'VBG'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('generate', 'VBP'), ('one', 'CD'), ('keypoint', 'NN'), ('heatmaps', 'VBZ'), ('training', 'VBG'), ('one', 'CD'), ('part', 'NN'), ('affinity', 'NN'), ('fields', 'NNS'), ('training', 'VBG'), ('one', 'CD'), ('channels', 'NNS'), ('training', 'VBG'), ('iii', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('keypoint', 'NN'), ('heatmaps', 'NNS'), ('training', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('fields', 'NNS'), ('training', 'VBG'), ('keypoint', 'NN'), ('detector', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('keypoint', 'NN'), ('detector', 'NN'), ('extract', 'JJ'), ('keypoints', 'NNS'), ('training', 'VBG'), ('keypoint', 'NN'), ('heatmaps', 'NNS'), ('training', 'VBG'), ('process', 'NN'), ('grouping', 'NN'), ('extracted', 'VBD'), ('keypoints', 'NNS'), ('training', 'VBG'), ('referring', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('fields', 'NNS'), ('training', 'VBG'), ('thereby', 'RB'), ('detect', 'JJ'), ('keypoints', 'NNS'), ('per', 'IN'), ('objects', 'NNS'), ('training', 'VBG'), ('iv', 'JJ'), ('process', 'NN'), ('allowing', 'VBG'), ('loss', 'NN'), ('layer', 'NN'), ('calculate', 'VBP'), ('one', 'CD'), ('losses', 'NNS'), ('referring', 'VBG'), ('keypoints', 'NNS'), ('per', 'IN'), ('objects', 'NNS'), ('training', 'VBG'), ('corresponding', 'VBG'), ('ground', 'NN'), ('truths', 'NNS'), ('thereby', 'RB'), ('adjust', 'VBP'), ('one', 'CD'), ('parameters', 'NNS'), ('feature', 'VBP'), ('extraction', 'NN'), ('network', 'NN'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('losses', 'NNS'), ('minimized', 'VBN'), ('backpropagation', 'NN'), ('using', 'VBG'), ('losses', 'NNS'), ('wherein', 'JJ'), ('step', 'JJ'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('face', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('face', 'NN'), ('network', 'NN'), ('detect', 'JJ'), ('passengers', 'NNS'), ('located', 'VBN'), ('interior', 'JJ'), ('via', 'IN'), ('face', 'NN'), ('detector', 'NN'), ('output', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('facial', 'JJ'), ('via', 'IN'), ('facial', 'JJ'), ('feature', 'NN'), ('classifier', 'NN'), ('wherein', 'JJ'), ('step', 'NN'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('face', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('face', 'NN'), ('network', 'NN'), ('apply', 'RB'), ('least', 'JJS'), ('one', 'CD'), ('convolution', 'NN'), ('operation', 'NN'), ('interior', 'JJ'), ('thus', 'RB'), ('output', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('map', 'NN'), ('corresponding', 'VBG'), ('interior', 'JJ'), ('via', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('convolutional', 'JJ'), ('layer', 'NN'), ('ii', 'NN'), ('output', 'NN'), ('one', 'CD'), ('proposal', 'NN'), ('boxes', 'VBZ'), ('passengers', 'NNS'), ('estimated', 'VBN'), ('located', 'JJ'), ('feature', 'NN'), ('map', 'NN'), ('via', 'IN'), ('region', 'NN'), ('proposal', 'NN'), ('network', 'NN'), ('iii', 'VBP'), ('apply', 'VB'), ('pooling', 'VBG'), ('operation', 'NN'), ('one', 'CD'), ('regions', 'NNS'), ('corresponding', 'VBG'), ('proposal', 'NN'), ('boxes', 'NNS'), ('feature', 'VBP'), ('map', 'JJ'), ('thus', 'RB'), ('output', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('vector', 'NN'), ('via', 'IN'), ('pooling', 'VBG'), ('layer', 'NN'), ('iv', 'JJ'), ('apply', 'RB'), ('fully-connected', 'JJ'), ('operation', 'NN'), ('feature', 'NN'), ('vector', 'NN'), ('thus', 'RB'), ('output', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('passengers', 'NNS'), ('corresponding', 'VBG'), ('proposal', 'NN'), ('boxes', 'NNS'), ('via', 'IN'), ('fully', 'RB'), ('connected', 'VBN'), ('layer', 'NN'), ('wherein', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('include', 'VBP'), ('ages', 'VBZ'), ('genders', 'NNS'), ('races', 'NNS'), ('corresponding', 'VBG'), ('passengers', 'NNS'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('detecting', 'VBG'), ('body', 'NN'), ('information', 'NN'), ('one', 'CD'), ('passengers', 'NN'), ('vehicle', 'NN'), ('based', 'VBN'), ('humans', 'NNS'), (\"'\", 'POS'), ('status', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('memory', 'NN'), ('stores', 'NNS'), ('instructions', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('configured', 'VBN'), ('execute', 'NN'), ('instructions', 'NNS'), ('perform', 'VB'), ('support', 'NN'), ('another', 'DT'), ('device', 'NN'), ('perform', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('interior', 'JJ'), ('interior', 'JJ'), ('vehicle', 'NN'), ('acquired', 'VBD'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('face', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('face', 'NN'), ('network', 'NN'), ('detect', 'JJ'), ('passengers', 'NNS'), ('interior', 'VBP'), ('thus', 'RB'), ('output', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('detected', 'VBD'), ('ii', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('body', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('body', 'NN'), ('network', 'NN'), ('detect', 'JJ'), ('bodies', 'NNS'), ('passengers', 'NNS'), ('interior', 'JJ'), ('thus', 'RB'), ('output', 'NN'), ('body-part', 'JJ'), ('length', 'NN'), ('information', 'NN'), ('detected', 'VBD'), ('bodies', 'NNS'), ('ii', 'JJ'), ('process', 'NN'), ('retrieving', 'VBG'), ('specific', 'JJ'), ('height', 'JJ'), ('mapping', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('height', 'VBD'), ('mapping', 'VBG'), ('table', 'NN'), ('stores', 'NNS'), ('height', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('representing', 'VBG'), ('respective', 'JJ'), ('one', 'CD'), ('predetermined', 'VBD'), ('ratios', 'NNS'), ('one', 'CD'), ('segment', 'NN'), ('body', 'NN'), ('portions', 'NNS'), ('human', 'JJ'), ('groups', 'NNS'), ('heights', 'NNS'), ('per', 'IN'), ('human', 'JJ'), ('groups', 'NNS'), ('process', 'NN'), ('acquiring', 'VBG'), ('specific', 'JJ'), ('height', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('specific', 'JJ'), ('height', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('referring', 'VBG'), ('specific', 'JJ'), ('body-part', 'JJ'), ('length', 'NN'), ('information', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('process', 'NN'), ('retrieving', 'VBG'), ('specific', 'JJ'), ('weight', 'NN'), ('mapping', 'VBG'), ('information', 'NN'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('weight', 'VBD'), ('mapping', 'VBG'), ('table', 'NN'), ('stores', 'NNS'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('weight', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('representing', 'VBG'), ('predetermined', 'VBN'), ('correlations', 'NNS'), ('heights', 'NNS'), ('weights', 'NNS'), ('per', 'IN'), ('human', 'JJ'), ('groups', 'NNS'), ('process', 'NN'), ('acquiring', 'VBG'), ('weight', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('specific', 'JJ'), ('weight', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('referring', 'VBG'), ('specific', 'JJ'), ('height', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('wherein', 'NN'), ('process', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('body', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('body', 'NN'), ('network', 'NN'), ('output', 'NN'), ('one', 'CD'), ('one', 'CD'), ('channels', 'NNS'), ('corresponding', 'VBG'), ('interior', 'JJ'), ('via', 'IN'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('ii', 'JJ'), ('generate', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('one', 'CD'), ('channels', 'NNS'), ('corresponding', 'VBG'), ('via', 'IN'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('iii', 'JJ'), ('extract', 'JJ'), ('keypoints', 'NNS'), ('keypoint', 'VB'), ('heatmap', 'NN'), ('via', 'IN'), ('keypoint', 'NN'), ('detector', 'NN'), ('group', 'NN'), ('extracted', 'VBD'), ('keypoints', 'NNS'), ('referring', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('thus', 'RB'), ('generate', 'VB'), ('body', 'NN'), ('parts', 'NNS'), ('per', 'IN'), ('passengers', 'NNS'), ('result', 'VBP'), ('allow', 'IN'), ('body', 'NN'), ('network', 'NN'), ('output', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('body-part', 'JJ'), ('length', 'NN'), ('information', 'NN'), ('passengers', 'NNS'), ('referring', 'VBG'), ('body', 'NN'), ('parts', 'NNS'), ('per', 'IN'), ('passengers', 'NNS'), ('passenger', 'VBP'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('wherein', 'NN'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('includes', 'VBZ'), ('one', 'CD'), ('fully', 'RB'), ('convolutional', 'JJ'), ('network', 'NN'), ('×', 'NNP'), ('convolutional', 'NN'), ('layer', 'NN'), ('applies', 'VBZ'), ('fully-convolution', 'NN'), ('operation', 'NN'), ('×', 'NNP'), ('convolution', 'NN'), ('operation', 'NN'), ('thereby', 'RB'), ('generate', 'JJ'), ('keypoint', 'NN'), ('heatmap', 'NNS'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('wherein', 'NN'), ('keypoint', 'NN'), ('detector', 'NN'), ('connects', 'NNS'), ('referring', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('pairs', 'VBZ'), ('respectively', 'RB'), ('highest', 'JJS'), ('mutual', 'JJ'), ('connection', 'NN'), ('probabilities', 'NNS'), ('connected', 'VBN'), ('among', 'IN'), ('extracted', 'JJ'), ('keypoints', 'NNS'), ('thereby', 'RB'), ('group', 'NN'), ('extracted', 'VBD'), ('keypoints', 'NNS'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('wherein', 'NN'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('learned', 'VBD'), ('learning', 'JJ'), ('device', 'NN'), ('performing', 'VBG'), ('process', 'NN'), ('inputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('training', 'NN'), ('including', 'VBG'), ('one', 'CD'), ('objects', 'VBZ'), ('training', 'VBG'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('generate', 'VBP'), ('one', 'CD'), ('training', 'VBG'), ('one', 'CD'), ('channels', 'NNS'), ('applying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('convolutional', 'JJ'), ('operation', 'NN'), ('training', 'VBG'), ('ii', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('training', 'VBG'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('generate', 'VBP'), ('one', 'CD'), ('keypoint', 'NN'), ('heatmaps', 'VBZ'), ('training', 'VBG'), ('one', 'CD'), ('part', 'NN'), ('affinity', 'NN'), ('fields', 'NNS'), ('training', 'VBG'), ('one', 'CD'), ('channels', 'NNS'), ('training', 'VBG'), ('iii', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('keypoint', 'NN'), ('heatmaps', 'NNS'), ('training', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('fields', 'NNS'), ('training', 'VBG'), ('keypoint', 'NN'), ('detector', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('keypoint', 'NN'), ('detector', 'NN'), ('extract', 'JJ'), ('keypoints', 'NNS'), ('training', 'VBG'), ('keypoint', 'NN'), ('heatmaps', 'NNS'), ('training', 'VBG'), ('process', 'NN'), ('grouping', 'NN'), ('extracted', 'VBD'), ('keypoints', 'NNS'), ('training', 'VBG'), ('referring', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('fields', 'NNS'), ('training', 'VBG'), ('thereby', 'RB'), ('detect', 'JJ'), ('keypoints', 'NNS'), ('per', 'IN'), ('objects', 'NNS'), ('training', 'VBG'), ('iv', 'JJ'), ('process', 'NN'), ('allowing', 'VBG'), ('loss', 'NN'), ('layer', 'NN'), ('calculate', 'VBP'), ('one', 'CD'), ('losses', 'NNS'), ('referring', 'VBG'), ('keypoints', 'NNS'), ('per', 'IN'), ('objects', 'NNS'), ('training', 'VBG'), ('corresponding', 'VBG'), ('ground', 'NN'), ('truths', 'NNS'), ('thereby', 'RB'), ('adjust', 'VBP'), ('one', 'CD'), ('parameters', 'NNS'), ('feature', 'VBP'), ('extraction', 'NN'), ('network', 'NN'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('losses', 'NNS'), ('minimized', 'VBN'), ('backpropagation', 'NN'), ('using', 'VBG'), ('losses', 'NNS'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('wherein', 'NN'), ('process', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('face', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('face', 'NN'), ('network', 'NN'), ('apply', 'RB'), ('least', 'JJS'), ('one', 'CD'), ('convolution', 'NN'), ('operation', 'NN'), ('interior', 'JJ'), ('thus', 'RB'), ('output', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('map', 'NN'), ('corresponding', 'VBG'), ('interior', 'JJ'), ('via', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('convolutional', 'JJ'), ('layer', 'NN'), ('ii', 'NN'), ('output', 'NN'), ('one', 'CD'), ('proposal', 'NN'), ('boxes', 'VBZ'), ('passengers', 'NNS'), ('estimated', 'VBN'), ('located', 'JJ'), ('feature', 'NN'), ('map', 'NN'), ('via', 'IN'), ('region', 'NN'), ('proposal', 'NN'), ('network', 'NN'), ('iii', 'VBP'), ('apply', 'VB'), ('pooling', 'VBG'), ('operation', 'NN'), ('one', 'CD'), ('regions', 'NNS'), ('corresponding', 'VBG'), ('proposal', 'NN'), ('boxes', 'NNS'), ('feature', 'VBP'), ('map', 'JJ'), ('thus', 'RB'), ('output', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('vector', 'NN'), ('via', 'IN'), ('pooling', 'VBG'), ('layer', 'NN'), ('iv', 'JJ'), ('apply', 'RB'), ('fully-connected', 'JJ'), ('operation', 'NN'), ('feature', 'NN'), ('vector', 'NN'), ('thus', 'RB'), ('output', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('passengers', 'NNS'), ('corresponding', 'VBG'), ('proposal', 'NN'), ('boxes', 'NNS'), ('via', 'IN'), ('fully', 'RB'), ('connected', 'VBN'), ('layer', 'NN'), ('computer', 'NN'), ('implemented', 'VBD'), ('performing', 'VBG'), ('video', 'NN'), ('coding', 'VBG'), ('based', 'VBN'), ('face', 'NN'), ('detection', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('video', 'NN'), ('frames', 'VBZ'), ('video', 'JJ'), ('sequence', 'NN'), ('determining', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('key', 'JJ'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('performing', 'VBG'), ('response', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('key', 'JJ'), ('frame', 'NN'), ('video', 'JJ'), ('sequence', 'NN'), ('multi-stage', 'NN'), ('facial', 'JJ'), ('search', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('predetermined', 'JJ'), ('feature', 'NN'), ('templates', 'NNS'), ('predetermined', 'VBD'), ('number', 'NN'), ('stages', 'NNS'), ('determine', 'VBP'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('testing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('regions', 'NNS'), ('based', 'VBN'), ('skin', 'JJ'), ('tone', 'NN'), ('information', 'NN'), ('determine', 'NN'), ('first', 'RB'), ('candidate', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('invalid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('rejecting', 'VBG'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('outputting', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('generate', 'NN'), ('coded', 'VBD'), ('bitstream', 'NN'), ('wherein', 'NN'), ('skin', 'VBD'), ('tone', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('skin', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('wherein', 'NN'), ('said', 'VBD'), ('testing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('regions', 'NNS'), ('based', 'VBN'), ('skin', 'JJ'), ('tone', 'NN'), ('information', 'NN'), ('performed', 'VBN'), ('response', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('key', 'JJ'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('rectangular', 'JJ'), ('region', 'NN'), ('comprising', 'VBG'), ('determining', 'VBG'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'VBD'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('pixel', 'NN'), ('accuracy', 'NN'), ('small', 'JJ'), ('block', 'NN'), ('pixels', 'NNS'), ('accuracy', 'VBP'), ('wherein', 'IN'), ('determining', 'VBG'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('generating', 'VBG'), ('enhanced', 'VBN'), ('skip', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('binarizing', 'VBG'), ('enhanced', 'VBD'), ('skip', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('overlaying', 'VBG'), ('binarized', 'VBN'), ('enhanced', 'JJ'), ('skip', 'NN'), ('probability', 'NN'), ('map', 'NN'), ('least', 'JJS'), ('portion', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('provide', 'VBP'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'IN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('comprises', 'VBZ'), ('non-key', 'JJ'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('comprising', 'VBG'), ('performing', 'VBG'), ('face', 'NN'), ('detection', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('based', 'VBN'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprising', 'VBG'), ('second', 'JJ'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('wherein', 'JJ'), ('second', 'JJ'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('location', 'NN'), ('second', 'JJ'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('displacement', 'JJ'), ('offset', 'NN'), ('respect', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprising', 'VBG'), ('determining', 'VBG'), ('displacement', 'NN'), ('offset', 'NN'), ('based', 'VBN'), ('offset', 'VBN'), ('centroid', 'JJ'), ('bounding', 'VBG'), ('box', 'NN'), ('around', 'IN'), ('skin', 'NN'), ('enhanced', 'VBN'), ('region', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('centroid', 'JJ'), ('second', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('around', 'IN'), ('second', 'JJ'), ('skin', 'NN'), ('enhanced', 'VBD'), ('region', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('wherein', 'NN'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('reducing', 'VBG'), ('quantization', 'NN'), ('parameter', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('adjusting', 'VBG'), ('lambda', 'NN'), ('value', 'NN'), ('first', 'RB'), ('candidate', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('disabling', 'VBG'), ('skip', 'NN'), ('coding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'WRB'), ('bitstream', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('hadvanced', 'VBD'), ('video', 'NN'), ('coding', 'VBG'), ('avc', 'JJ'), ('compliant', 'JJ'), ('bitstream', 'NN'), ('hhigh', 'NN'), ('efficiency', 'NN'), ('video', 'NN'), ('coding', 'VBG'), ('hevc', 'NN'), ('compliant', 'JJ'), ('bitstream', 'NN'), ('vp', 'NN'), ('compliant', 'JJ'), ('bitstream', 'NN'), ('vp', 'NN'), ('compliant', 'JJ'), ('bitstream', 'NN'), ('alliance', 'NN'), ('open', 'JJ'), ('media', 'NNS'), ('aom', 'VBP'), ('av', 'JJ'), ('compliant', 'NN'), ('bitstream', 'NN'), ('computer', 'NN'), ('implemented', 'VBD'), ('performing', 'VBG'), ('face', 'NN'), ('detection', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('sequence', 'NN'), ('video', 'NN'), ('frames', 'NNS'), ('performing', 'VBG'), ('multi-stage', 'JJ'), ('facial', 'JJ'), ('search', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('predetermined', 'JJ'), ('feature', 'NN'), ('templates', 'NNS'), ('predetermined', 'VBD'), ('number', 'NN'), ('stages', 'NNS'), ('determine', 'VBP'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('testing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('regions', 'NNS'), ('based', 'VBN'), ('skin', 'JJ'), ('tone', 'NN'), ('information', 'NN'), ('determine', 'NN'), ('first', 'RB'), ('candidate', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('invalid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('rejecting', 'VBG'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('outputting', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('processing', 'VBG'), ('providing', 'VBG'), ('index', 'NN'), ('indicative', 'JJ'), ('person', 'NN'), ('present', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'VBD'), ('sequence', 'NN'), ('video', 'NN'), ('frames', 'VBZ'), ('comprises', 'NNS'), ('sequence', 'NN'), ('surveillance', 'NN'), ('video', 'NN'), ('frames', 'NNS'), ('comprising', 'VBG'), ('performing', 'VBG'), ('face', 'NN'), ('surveillance', 'NN'), ('video', 'NN'), ('frames', 'NNS'), ('based', 'VBN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'VBD'), ('sequence', 'NN'), ('video', 'NN'), ('frames', 'VBZ'), ('comprises', 'NNS'), ('sequence', 'NN'), ('decoded', 'VBD'), ('video', 'NN'), ('frames', 'NNS'), ('comprising', 'VBG'), ('adding', 'VBG'), ('marker', 'NN'), ('corresponding', 'VBG'), ('received', 'VBD'), ('video', 'JJ'), ('frame', 'NN'), ('perform', 'NN'), ('face', 'NN'), ('received', 'VBD'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'VBD'), ('sequence', 'NN'), ('video', 'NN'), ('frames', 'NNS'), ('received', 'VBD'), ('device', 'NN'), ('login', 'NN'), ('attempt', 'NN'), ('comprising', 'VBG'), ('performing', 'VBG'), ('face', 'NN'), ('based', 'VBN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('allowing', 'VBG'), ('access', 'NN'), ('device', 'NN'), ('secured', 'VBD'), ('face', 'NN'), ('recognized', 'VBN'), ('wherein', 'JJ'), ('sequence', 'NN'), ('video', 'NN'), ('frames', 'VBZ'), ('comprises', 'NNS'), ('sequence', 'NN'), ('videoconferencing', 'VBG'), ('frames', 'NNS'), ('comprising', 'VBG'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('generate', 'NN'), ('coded', 'VBD'), ('bitstream', 'NN'), ('wherein', 'NN'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('comprises', 'VBZ'), ('encoding', 'VBG'), ('background', 'RP'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('bitstream', 'NN'), ('comprising', 'VBG'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('generate', 'NN'), ('coded', 'VBD'), ('bitstream', 'NN'), ('wherein', 'NN'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('comprises', 'VBZ'), ('including', 'VBG'), ('metadata', 'NNS'), ('corresponding', 'VBG'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('bitstream', 'NN'), ('comprising', 'VBG'), ('decoding', 'VBG'), ('coded', 'VBN'), ('bitstream', 'NN'), ('generate', 'NN'), ('decoded', 'VBD'), ('video', 'NN'), ('frame', 'NN'), ('determine', 'NN'), ('metadata', 'NN'), ('corresponding', 'VBG'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('bitstream', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('replacing', 'VBG'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('based', 'VBN'), ('decoded', 'VBN'), ('metadata', 'NNS'), ('cropping', 'VBG'), ('displaying', 'VBG'), ('data', 'NNS'), ('corresponding', 'VBG'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('based', 'VBN'), ('decoded', 'VBN'), ('metadata', 'NNS'), ('indexing', 'VBG'), ('decoded', 'VBD'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('decoded', 'VBN'), ('metadata', 'NN'), ('system', 'NN'), ('performing', 'VBG'), ('video', 'NN'), ('coding', 'VBG'), ('based', 'VBN'), ('face', 'NN'), ('detection', 'NN'), ('comprising', 'VBG'), ('memory', 'NN'), ('configured', 'VBD'), ('store', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('video', 'NN'), ('frames', 'VBZ'), ('video', 'JJ'), ('sequence', 'NN'), ('coupled', 'VBD'), ('memory', 'NN'), ('receive', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('determine', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('key', 'JJ'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('perform', 'NN'), ('response', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('key', 'JJ'), ('frame', 'NN'), ('video', 'JJ'), ('sequence', 'NN'), ('multi-stage', 'NN'), ('facial', 'JJ'), ('search', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('predetermined', 'JJ'), ('feature', 'NN'), ('templates', 'NNS'), ('predetermined', 'VBD'), ('number', 'NN'), ('stages', 'NNS'), ('determine', 'VBP'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('test', 'NN'), ('first', 'RB'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('regions', 'NNS'), ('based', 'VBN'), ('skin', 'JJ'), ('tone', 'NN'), ('information', 'NN'), ('determine', 'NN'), ('first', 'RB'), ('candidate', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('invalid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('reject', 'JJ'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('outputting', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('encode', 'FW'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('generate', 'NN'), ('coded', 'VBD'), ('bitstream', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('skin', 'VBZ'), ('tone', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('skin', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('rectangular', 'JJ'), ('region', 'NN'), ('determine', 'NN'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'VBD'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('pixel', 'NN'), ('accuracy', 'NN'), ('small', 'JJ'), ('block', 'NN'), ('pixels', 'NNS'), ('accuracy', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('determine', 'JJ'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('generate', 'NN'), ('enhanced', 'VBD'), ('skip', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('binarize', 'NN'), ('enhanced', 'VBD'), ('skip', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('overlay', 'NN'), ('binarized', 'VBD'), ('enhanced', 'JJ'), ('skip', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('least', 'JJS'), ('portion', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('provide', 'VBP'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('comprises', 'VBZ'), ('non-key', 'JJ'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('perform', 'NN'), ('face', 'NN'), ('detection', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('based', 'VBN'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('track', 'JJ'), ('second', 'JJ'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('encode', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('reduce', 'VB'), ('quantization', 'NN'), ('parameter', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('adjust', 'VBP'), ('lambda', 'NN'), ('value', 'NN'), ('first', 'RB'), ('candidate', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('disable', 'JJ'), ('skip', 'NN'), ('coding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('instructions', 'NNS'), ('response', 'NN'), ('executed', 'VBD'), ('device', 'NN'), ('cause', 'NN'), ('device', 'NN'), ('perform', 'NN'), ('video', 'NN'), ('coding', 'VBG'), ('based', 'VBN'), ('face', 'NN'), ('detection', 'NN'), ('receiving', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('video', 'NN'), ('frames', 'VBZ'), ('video', 'JJ'), ('sequence', 'NN'), ('determining', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('key', 'JJ'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('performing', 'VBG'), ('response', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('key', 'JJ'), ('frame', 'NN'), ('video', 'JJ'), ('sequence', 'NN'), ('multi-stage', 'NN'), ('facial', 'JJ'), ('search', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('predetermined', 'JJ'), ('feature', 'NN'), ('templates', 'NNS'), ('predetermined', 'VBD'), ('number', 'NN'), ('stages', 'NNS'), ('determine', 'VBP'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('testing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('regions', 'NNS'), ('based', 'VBN'), ('skin', 'JJ'), ('tone', 'NN'), ('information', 'NN'), ('determine', 'NN'), ('first', 'RB'), ('candidate', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('invalid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('rejecting', 'VBG'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('outputting', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('generate', 'NN'), ('coded', 'VBD'), ('bitstream', 'JJ'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('skin', 'VBD'), ('tone', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('skin', 'JJ'), ('probability', 'NN'), ('map', 'VBP'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('rectangular', 'JJ'), ('region', 'NN'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('instructions', 'NNS'), ('response', 'NN'), ('executed', 'VBD'), ('device', 'NN'), ('cause', 'NN'), ('device', 'NN'), ('perform', 'NN'), ('video', 'NN'), ('coding', 'VBG'), ('based', 'VBN'), ('face', 'NN'), ('detection', 'NN'), ('determining', 'VBG'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'VBD'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('pixel', 'NN'), ('accuracy', 'NN'), ('small', 'JJ'), ('block', 'NN'), ('pixels', 'NNS'), ('accuracy', 'IN'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('determining', 'VBG'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('generating', 'VBG'), ('enhanced', 'VBN'), ('skip', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('binarizing', 'VBG'), ('enhanced', 'VBD'), ('skip', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('overlaying', 'VBG'), ('binarized', 'VBN'), ('enhanced', 'JJ'), ('skip', 'NN'), ('probability', 'NN'), ('map', 'NN'), ('least', 'JJS'), ('portion', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('provide', 'VBP'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('comprises', 'VBZ'), ('non-key', 'JJ'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('instructions', 'NNS'), ('response', 'NN'), ('executed', 'VBD'), ('device', 'NN'), ('cause', 'NN'), ('device', 'NN'), ('perform', 'NN'), ('video', 'NN'), ('coding', 'VBG'), ('based', 'VBN'), ('face', 'NN'), ('detection', 'NN'), ('performing', 'VBG'), ('face', 'NN'), ('detection', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('based', 'VBN'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('instructions', 'NNS'), ('response', 'NN'), ('executed', 'VBD'), ('device', 'NN'), ('cause', 'NN'), ('device', 'NN'), ('perform', 'NN'), ('video', 'NN'), ('coding', 'VBG'), ('based', 'VBN'), ('face', 'NN'), ('detection', 'NN'), ('second', 'JJ'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('reducing', 'VBG'), ('quantization', 'NN'), ('parameter', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('adjusting', 'VBG'), ('lambda', 'NN'), ('value', 'NN'), ('first', 'RB'), ('candidate', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('disabling', 'VBG'), ('skip', 'NN'), ('coding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('managing', 'VBG'), ('smart', 'JJ'), ('database', 'NN'), ('stores', 'NNS'), ('facial', 'JJ'), ('face', 'NN'), ('comprising', 'VBG'), ('steps', 'NNS'), ('managing', 'VBG'), ('device', 'NN'), ('performing', 'VBG'), ('process', 'NN'), ('counting', 'VBG'), ('one', 'CD'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('specific', 'JJ'), ('person', 'NN'), ('stored', 'VBD'), ('smart', 'JJ'), ('database', 'NN'), ('new', 'JJ'), ('facial', 'JJ'), ('face', 'NN'), ('continuously', 'RB'), ('stored', 'VBD'), ('process', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('first', 'JJ'), ('counted', 'VBN'), ('value', 'NN'), ('representing', 'VBG'), ('count', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('satisfies', 'NNS'), ('preset', 'VBP'), ('first', 'RB'), ('set', 'VBN'), ('value', 'NN'), ('b', 'NN'), ('first', 'RB'), ('counted', 'VBD'), ('value', 'NN'), ('determined', 'VBD'), ('satisfying', 'VBG'), ('first', 'RB'), ('set', 'VBN'), ('value', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('performing', 'VBG'), ('process', 'NN'), ('inputting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('generate', 'VBP'), ('quality', 'NN'), ('scores', 'NNS'), ('specific', 'JJ'), ('facial', 'JJ'), ('aggregation', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('process', 'NN'), ('sorting', 'VBG'), ('quality', 'NN'), ('scores', 'NNS'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('descending', 'NN'), ('order', 'NN'), ('quality', 'NN'), ('scores', 'VBZ'), ('process', 'NN'), ('counting', 'NN'), ('sorted', 'VBN'), ('specific', 'JJ'), ('facial', 'JJ'), ('descending', 'NN'), ('order', 'NN'), ('second', 'JJ'), ('counted', 'VBN'), ('value', 'NN'), ('represents', 'VBZ'), ('number', 'NN'), ('counted', 'VBN'), ('part', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('becomes', 'NNS'), ('equal', 'JJ'), ('preset', 'JJ'), ('second', 'NN'), ('set', 'VBN'), ('value', 'NN'), ('process', 'NN'), ('deleting', 'VBG'), ('uncounted', 'JJ'), ('part', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('smart', 'JJ'), ('database', 'NN'), ('comprising', 'VBG'), ('step', 'NN'), ('c', 'RB'), ('managing', 'VBG'), ('device', 'NN'), ('performing', 'VBG'), ('process', 'NN'), ('generating', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('optimal', 'JJ'), ('feature', 'NN'), ('weighted', 'VBD'), ('summation', 'NN'), ('one', 'CD'), ('features', 'VBZ'), ('specific', 'JJ'), ('facial', 'JJ'), ('using', 'VBG'), ('counted', 'JJ'), ('part', 'NN'), ('quality', 'NN'), ('scores', 'VBZ'), ('process', 'JJ'), ('setting', 'VBG'), ('optimal', 'JJ'), ('feature', 'NN'), ('representative', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('person', 'NN'), ('wherein', 'JJ'), ('step', 'NN'), ('b', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('inputting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('cnn', 'NN'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('cnn', 'NN'), ('generate', 'NN'), ('one', 'CD'), ('features', 'VBZ'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('vector', 'NN'), ('features', 'NNS'), ('embedded', 'VBD'), ('aggregation', 'NN'), ('module', 'NN'), ('including', 'VBG'), ('least', 'JJS'), ('two', 'CD'), ('attention', 'NN'), ('blocks', 'NNS'), ('thereby', 'RB'), ('allow', 'VB'), ('aggregation', 'NN'), ('module', 'NN'), ('generate', 'VBP'), ('quality', 'NN'), ('scores', 'NNS'), ('features', 'VBZ'), ('wherein', 'JJ'), ('step', 'NN'), ('b', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('matching', 'VBG'), ('i-', 'JJ'), ('one', 'CD'), ('features', 'VBZ'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('stored', 'VBN'), ('smart', 'JJ'), ('database', 'NN'), ('i-', 'JJ'), ('quality', 'NN'), ('scores', 'NNS'), ('ii', 'VBP'), ('specific', 'JJ'), ('person', 'NN'), ('process', 'NN'), ('storing', 'VBG'), ('matched', 'VBN'), ('features', 'NNS'), ('matched', 'VBN'), ('quality', 'NN'), ('scores', 'NNS'), ('smart', 'VBP'), ('database', 'NN'), ('comprising', 'VBG'), ('step', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('performing', 'VBG'), ('one', 'CD'), ('process', 'NN'), ('learning', 'VBG'), ('face', 'NN'), ('system', 'NN'), ('using', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('specific', 'JJ'), ('person', 'NN'), ('stored', 'VBD'), ('smart', 'JJ'), ('database', 'NN'), ('ii', 'NN'), ('process', 'NN'), ('transmitting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('specific', 'JJ'), ('person', 'NN'), ('learning', 'JJ'), ('device', 'NN'), ('corresponding', 'VBG'), ('face', 'NN'), ('system', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('learning', 'VBG'), ('device', 'NN'), ('learn', 'FW'), ('face', 'NN'), ('system', 'NN'), ('using', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('wherein', 'NN'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('learned', 'VBD'), ('learning', 'JJ'), ('device', 'NN'), ('repeating', 'VBG'), ('process', 'NN'), ('inputting', 'VBG'), ('multiple', 'JJ'), ('facial', 'JJ'), ('training', 'NN'), ('corresponding', 'VBG'), ('set', 'VBN'), ('single', 'JJ'), ('face', 'NN'), ('video', 'NN'), ('single', 'JJ'), ('face', 'NN'), ('cnn', 'JJ'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('cnn', 'NN'), ('generate', 'NN'), ('one', 'CD'), ('features', 'VBZ'), ('training', 'VBG'), ('applying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('convolution', 'NN'), ('operation', 'NN'), ('facial', 'JJ'), ('training', 'NN'), ('ii', 'NN'), ('process', 'NN'), ('inputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('vector', 'NN'), ('training', 'NN'), ('features', 'NNS'), ('training', 'VBG'), ('embedded', 'JJ'), ('aggregation', 'NN'), ('module', 'NN'), ('including', 'VBG'), ('least', 'JJS'), ('two', 'CD'), ('attention', 'NN'), ('blocks', 'NNS'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('aggregation', 'NN'), ('module', 'NN'), ('generate', 'VBP'), ('quality', 'NN'), ('scores', 'NNS'), ('training', 'VBG'), ('features', 'NNS'), ('training', 'VBG'), ('aggregation', 'NN'), ('features', 'NNS'), ('training', 'VBG'), ('using', 'VBG'), ('one', 'CD'), ('attention', 'NN'), ('parameters', 'NNS'), ('learned', 'VBD'), ('previous', 'JJ'), ('iteration', 'NN'), ('iii', 'NN'), ('process', 'NN'), ('outputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('optimal', 'JJ'), ('feature', 'NN'), ('training', 'NN'), ('weighted', 'JJ'), ('summation', 'NN'), ('features', 'NNS'), ('training', 'VBG'), ('using', 'VBG'), ('quality', 'NN'), ('scores', 'NNS'), ('training', 'VBG'), ('iv', 'JJ'), ('process', 'NN'), ('updating', 'VBG'), ('attention', 'NN'), ('parameters', 'NNS'), ('learned', 'VBD'), ('previous', 'JJ'), ('iteration', 'NN'), ('least', 'JJS'), ('two', 'CD'), ('attention', 'NN'), ('blocks', 'NNS'), ('one', 'CD'), ('losses', 'NNS'), ('minimized', 'VBN'), ('outputted', 'JJ'), ('loss', 'NN'), ('layer', 'NN'), ('referring', 'VBG'), ('optimal', 'JJ'), ('feature', 'NN'), ('training', 'VBG'), ('corresponding', 'VBG'), ('ground', 'NN'), ('truth', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('managing', 'VBG'), ('smart', 'JJ'), ('database', 'NN'), ('stores', 'NNS'), ('facial', 'JJ'), ('face', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('memory', 'NN'), ('stores', 'NNS'), ('instructions', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('configured', 'VBN'), ('execute', 'NN'), ('instructions', 'NNS'), ('perform', 'VB'), ('support', 'NN'), ('another', 'DT'), ('device', 'NN'), ('perform', 'NN'), ('process', 'NN'), ('counting', 'VBG'), ('one', 'CD'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('specific', 'JJ'), ('person', 'NN'), ('stored', 'VBD'), ('smart', 'JJ'), ('database', 'NN'), ('new', 'JJ'), ('facial', 'JJ'), ('face', 'NN'), ('continuously', 'RB'), ('stored', 'VBD'), ('process', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('first', 'JJ'), ('counted', 'VBN'), ('value', 'NN'), ('representing', 'VBG'), ('count', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('satisfies', 'NNS'), ('preset', 'VBP'), ('first', 'RB'), ('set', 'VBN'), ('value', 'NN'), ('ii', 'NN'), ('first', 'RB'), ('counted', 'VBD'), ('value', 'NN'), ('determined', 'VBD'), ('satisfying', 'VBG'), ('first', 'RB'), ('set', 'VBN'), ('value', 'NN'), ('process', 'NN'), ('inputting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('generate', 'VBP'), ('quality', 'NN'), ('scores', 'NNS'), ('specific', 'JJ'), ('facial', 'JJ'), ('aggregation', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('process', 'NN'), ('sorting', 'VBG'), ('quality', 'NN'), ('scores', 'NNS'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('descending', 'NN'), ('order', 'NN'), ('quality', 'NN'), ('scores', 'VBZ'), ('process', 'NN'), ('counting', 'NN'), ('sorted', 'VBN'), ('specific', 'JJ'), ('facial', 'JJ'), ('descending', 'NN'), ('order', 'NN'), ('second', 'JJ'), ('counted', 'VBN'), ('value', 'NN'), ('represents', 'VBZ'), ('number', 'NN'), ('counted', 'VBN'), ('part', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('becomes', 'NNS'), ('equal', 'JJ'), ('preset', 'JJ'), ('second', 'NN'), ('set', 'VBN'), ('value', 'NN'), ('process', 'NN'), ('deleting', 'VBG'), ('uncounted', 'JJ'), ('part', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('smart', 'JJ'), ('database', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('performs', 'VBZ'), ('iii', 'JJ'), ('process', 'NN'), ('generating', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('optimal', 'JJ'), ('feature', 'NN'), ('weighted', 'VBD'), ('summation', 'NN'), ('one', 'CD'), ('features', 'VBZ'), ('specific', 'JJ'), ('facial', 'JJ'), ('using', 'VBG'), ('counted', 'JJ'), ('part', 'NN'), ('quality', 'NN'), ('scores', 'VBZ'), ('process', 'JJ'), ('setting', 'VBG'), ('optimal', 'JJ'), ('feature', 'NN'), ('representative', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('person', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('process', 'NN'), ('ii', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('inputting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('cnn', 'NN'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('cnn', 'NN'), ('generate', 'NN'), ('one', 'CD'), ('features', 'VBZ'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('vector', 'NN'), ('features', 'NNS'), ('embedded', 'VBD'), ('aggregation', 'NN'), ('module', 'NN'), ('including', 'VBG'), ('least', 'JJS'), ('two', 'CD'), ('attention', 'NN'), ('blocks', 'NNS'), ('thereby', 'RB'), ('allow', 'VB'), ('aggregation', 'NN'), ('module', 'NN'), ('generate', 'VBP'), ('quality', 'NN'), ('scores', 'NNS'), ('features', 'VBZ'), ('managing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('process', 'NN'), ('ii', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('matching', 'VBG'), ('i-', 'JJ'), ('one', 'CD'), ('features', 'VBZ'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('stored', 'VBN'), ('smart', 'JJ'), ('database', 'NN'), ('i-', 'JJ'), ('quality', 'NN'), ('scores', 'NNS'), ('ii', 'VBP'), ('specific', 'JJ'), ('person', 'NN'), ('process', 'NN'), ('storing', 'VBG'), ('matched', 'VBN'), ('features', 'NNS'), ('matched', 'VBN'), ('quality', 'NN'), ('scores', 'NNS'), ('smart', 'VBP'), ('database', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('performs', 'VBZ'), ('iv', 'VBP'), ('one', 'CD'), ('process', 'NN'), ('learning', 'VBG'), ('face', 'NN'), ('system', 'NN'), ('using', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('specific', 'JJ'), ('person', 'NN'), ('stored', 'VBD'), ('smart', 'JJ'), ('database', 'NN'), ('ii', 'NN'), ('process', 'NN'), ('transmitting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('specific', 'JJ'), ('person', 'NN'), ('learning', 'JJ'), ('device', 'NN'), ('corresponding', 'VBG'), ('face', 'NN'), ('system', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('learning', 'VBG'), ('device', 'NN'), ('learn', 'FW'), ('face', 'NN'), ('system', 'NN'), ('using', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('managing', 'NN'), ('device', 'NN'), ('wherein', 'IN'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('learned', 'VBD'), ('learning', 'JJ'), ('device', 'NN'), ('repeating', 'VBG'), ('process', 'NN'), ('inputting', 'VBG'), ('multiple', 'JJ'), ('facial', 'JJ'), ('training', 'NN'), ('corresponding', 'VBG'), ('set', 'VBN'), ('single', 'JJ'), ('face', 'NN'), ('video', 'NN'), ('single', 'JJ'), ('face', 'NN'), ('cnn', 'JJ'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('cnn', 'NN'), ('generate', 'NN'), ('one', 'CD'), ('features', 'VBZ'), ('training', 'VBG'), ('applying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('convolution', 'NN'), ('operation', 'NN'), ('facial', 'JJ'), ('training', 'NN'), ('ii', 'NN'), ('process', 'NN'), ('inputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('vector', 'NN'), ('training', 'NN'), ('features', 'NNS'), ('training', 'VBG'), ('embedded', 'JJ'), ('aggregation', 'NN'), ('module', 'NN'), ('including', 'VBG'), ('least', 'JJS'), ('two', 'CD'), ('attention', 'NN'), ('blocks', 'NNS'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('aggregation', 'NN'), ('module', 'NN'), ('generate', 'VBP'), ('quality', 'NN'), ('scores', 'NNS'), ('training', 'VBG'), ('features', 'NNS'), ('training', 'VBG'), ('aggregation', 'NN'), ('features', 'NNS'), ('training', 'VBG'), ('using', 'VBG'), ('one', 'CD'), ('attention', 'NN'), ('parameters', 'NNS'), ('learned', 'VBD'), ('previous', 'JJ'), ('iteration', 'NN'), ('iii', 'NN'), ('process', 'NN'), ('outputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('optimal', 'JJ'), ('feature', 'NN'), ('training', 'NN'), ('weighted', 'JJ'), ('summation', 'NN'), ('features', 'NNS'), ('training', 'VBG'), ('using', 'VBG'), ('quality', 'NN'), ('scores', 'NNS'), ('training', 'VBG'), ('iv', 'JJ'), ('process', 'NN'), ('updating', 'VBG'), ('attention', 'NN'), ('parameters', 'NNS'), ('learned', 'VBD'), ('previous', 'JJ'), ('iteration', 'NN'), ('least', 'JJS'), ('two', 'CD'), ('attention', 'NN'), ('blocks', 'NNS'), ('one', 'CD'), ('losses', 'NNS'), ('minimized', 'VBN'), ('outputted', 'JJ'), ('loss', 'NN'), ('layer', 'NN'), ('referring', 'VBG'), ('optimal', 'JJ'), ('feature', 'NN'), ('training', 'VBG'), ('corresponding', 'VBG'), ('ground', 'NN'), ('truth', 'NN'), ('object', 'IN'), ('data', 'NNS'), ('processing', 'VBG'), ('system', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('configured', 'VBN'), ('execute', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('implementation', 'NN'), ('algorithms', 'NN'), ('stored', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('algorithm', 'JJ'), ('feature', 'NN'), ('density', 'NN'), ('selection', 'NN'), ('criteria', 'NNS'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('code', 'NN'), ('executed', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('data', 'NN'), ('preprocessing', 'NN'), ('code', 'NN'), ('comprising', 'VBG'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'RB'), ('configured', 'VBD'), ('obtain', 'VB'), ('digital', 'JJ'), ('representation', 'NN'), ('scene', 'NN'), ('scene', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('textual', 'JJ'), ('media', 'NNS'), ('generate', 'NN'), ('set', 'VBN'), ('invariant', 'JJ'), ('features', 'NNS'), ('applying', 'VBG'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'IN'), ('digital', 'JJ'), ('representation', 'NN'), ('cluster', 'NN'), ('set', 'VBN'), ('invariant', 'JJ'), ('features', 'NNS'), ('regions', 'NNS'), ('interest', 'NN'), ('digital', 'JJ'), ('representation', 'NN'), ('scene', 'NN'), ('region', 'NN'), ('interest', 'NN'), ('region', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('classify', 'VB'), ('region', 'NN'), ('classifier', 'JJR'), ('code', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('regions', 'NNS'), ('interest', 'NN'), ('according', 'VBG'), ('object', 'JJ'), ('type', 'NN'), ('function', 'NN'), ('attributes', 'VBZ'), ('derived', 'JJ'), ('region', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('digital', 'JJ'), ('representation', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('classified', 'JJ'), ('regions', 'NNS'), ('interest', 'NN'), ('corresponds', 'NNS'), ('text', 'JJ'), ('use', 'NN'), ('classification', 'NN'), ('result', 'NN'), ('corresponding', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('regions', 'NNS'), ('interest', 'NN'), ('classify', 'NN'), ('another', 'DT'), ('regions', 'NNS'), ('interest', 'NN'), ('according', 'VBG'), ('object', 'JJ'), ('type', 'NN'), ('wherein', 'NN'), ('another', 'DT'), ('regions', 'NNS'), ('interest', 'NN'), ('corresponds', 'VBZ'), ('region', 'NN'), ('interest', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('preprocessing', 'VBG'), ('code', 'NN'), ('based', 'VBN'), ('feature', 'NN'), ('density', 'NN'), ('selection', 'NN'), ('criteria', 'NNS'), ('determines', 'VBZ'), ('ocr', 'JJ'), ('algorithm', 'NN'), ('applicable', 'JJ'), ('text', 'NN'), ('algorithms', 'NN'), ('applicable', 'JJ'), ('aspects', 'NNS'), ('photographs', 'VBP'), ('logos', 'JJ'), ('system', 'NN'), ('wherein', 'NN'), ('creates', 'VBZ'), ('profile', 'IN'), ('camera-equipped', 'JJ'), ('smartphone', 'NN'), ('includes', 'VBZ'), ('information', 'NN'), ('visually', 'RB'), ('impaired', 'JJ'), ('causes', 'NNS'), ('prioritized', 'JJ'), ('execution', 'NN'), ('ocr', 'IN'), ('algorithm', 'JJ'), ('text', 'JJ'), ('reader', 'NN'), ('program', 'NN'), ('begins', 'VBZ'), ('reading', 'VBG'), ('text', 'JJ'), ('quickly', 'RB'), ('possible', 'JJ'), ('system', 'NN'), ('comprising', 'VBG'), ('audio', 'JJ'), ('tactile', 'NN'), ('feedback', 'NN'), ('mechanism', 'NN'), ('helps', 'VBZ'), ('position', 'NN'), ('smart', 'JJ'), ('phone', 'NN'), ('relative', 'JJ'), ('text', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('``', '``'), ('hold', 'VB'), ('still', 'RB'), (\"''\", \"''\"), ('audio', 'JJ'), ('feedback', 'NN'), ('signal', 'JJ'), ('sent', 'VBD'), ('text', 'NN'), ('center', 'NN'), ('captured', 'VBN'), ('scene', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('digital', 'JJ'), ('representation', 'NN'), ('comprises', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('following', 'VBG'), ('types', 'NNS'), ('digital', 'JJ'), ('data', 'NNS'), ('data', 'NNS'), ('video', 'NN'), ('data', 'NNS'), ('audio', 'RB'), ('data', 'NNS'), ('system', 'NN'), ('wherein', 'VBP'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('feature', 'NN'), ('identification', 'NN'), ('algorithms', 'IN'), ('fast', 'JJ'), ('sift', 'NN'), ('freak', 'NN'), ('brisk', 'JJ'), ('harris', 'NN'), ('daisy', 'NN'), ('mser', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('edge', 'NN'), ('detection', 'NN'), ('algorithm', 'NN'), ('corner', 'NN'), ('detection', 'NN'), ('algorithm', 'JJ'), ('saliency', 'NN'), ('map', 'NN'), ('algorithm', 'NN'), ('curve', 'NN'), ('detection', 'NN'), ('algorithm', 'IN'), ('texton', 'NN'), ('identification', 'NN'), ('algorithm', 'IN'), ('wavelets', 'NNS'), ('algorithm', 'JJ'), ('system', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('region', 'NN'), ('interest', 'NN'), ('represents', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('physical', 'JJ'), ('object', 'NN'), ('scene', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('region', 'NN'), ('interest', 'NN'), ('represents', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('textual', 'JJ'), ('media', 'NNS'), ('scene', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('region', 'NN'), ('interest', 'NN'), ('represents', 'VBZ'), ('document', 'JJ'), ('textual', 'JJ'), ('media', 'NNS'), ('system', 'NN'), ('wherein', 'JJ'), ('region', 'NN'), ('interest', 'NN'), ('represents', 'VBZ'), ('financial', 'JJ'), ('document', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('region', 'NN'), ('interest', 'NN'), ('represents', 'VBZ'), ('structured', 'VBN'), ('document', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('implementation', 'NN'), ('algorithms', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('template', 'NN'), ('driven', 'VBN'), ('algorithm', 'JJ'), ('face', 'NN'), ('algorithm', 'JJ'), ('optical', 'JJ'), ('character', 'NN'), ('algorithm', 'NN'), ('speech', 'NN'), ('algorithm', 'NN'), ('object', 'VBP'), ('algorithm', 'NN'), ('system', 'NN'), ('wherein', 'WRB'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('code', 'NN'), ('configured', 'VBD'), ('assign', 'JJ'), ('region', 'NN'), ('interest', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('algorithm', 'NN'), ('function', 'NN'), ('scene', 'NN'), ('context', 'NN'), ('derived', 'VBD'), ('digital', 'JJ'), ('representation', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('scene', 'NN'), ('context', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('types', 'NNS'), ('data', 'NNS'), ('location', 'NN'), ('position', 'NN'), ('time', 'NN'), ('identity', 'NN'), ('news', 'NN'), ('event', 'NN'), ('medical', 'JJ'), ('event', 'NN'), ('promotion', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('mobile', 'JJ'), ('device', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('implementation', 'NN'), ('algorithms', 'NN'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('code', 'NN'), ('system', 'NN'), ('wherein', 'VBP'), ('mobile', 'JJ'), ('device', 'NN'), ('comprises', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('following', 'VBG'), ('smart', 'JJ'), ('phone', 'NN'), ('tablet', 'NN'), ('wearable', 'JJ'), ('glass', 'NN'), ('toy', 'NN'), ('vehicle', 'NN'), ('computer', 'NN'), ('phablet', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('network-accessible', 'JJ'), ('server', 'NN'), ('device', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('implementation', 'NN'), ('algorithms', 'NN'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('code', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('object', 'JJ'), ('type', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('face', 'NN'), ('animal', 'JJ'), ('vehicle', 'NN'), ('document', 'NN'), ('plant', 'NN'), ('building', 'NN'), ('appliance', 'NN'), ('clothing', 'NN'), ('body', 'NN'), ('part', 'NN'), ('toy', 'NN'), ('object', 'VBP'), ('data', 'NNS'), ('processing', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('configured', 'VBN'), ('execute', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('implementation', 'NN'), ('algorithms', 'NN'), ('stored', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('algorithm', 'JJ'), ('feature', 'NN'), ('density', 'NN'), ('selection', 'NN'), ('criteria', 'NNS'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('code', 'NN'), ('executed', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('data', 'NN'), ('preprocessing', 'NN'), ('code', 'NN'), ('comprising', 'VBG'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'RB'), ('configured', 'VBD'), ('obtain', 'VB'), ('digital', 'JJ'), ('representation', 'NN'), ('scene', 'NN'), ('scene', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('textual', 'JJ'), ('media', 'NNS'), ('generate', 'NN'), ('set', 'VBN'), ('invariant', 'JJ'), ('features', 'NNS'), ('applying', 'VBG'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'IN'), ('digital', 'JJ'), ('representation', 'NN'), ('cluster', 'NN'), ('set', 'VBN'), ('invariant', 'JJ'), ('features', 'NNS'), ('regions', 'NNS'), ('interest', 'NN'), ('digital', 'JJ'), ('representation', 'NN'), ('scene', 'NN'), ('region', 'NN'), ('interest', 'NN'), ('region', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('classify', 'VB'), ('region', 'NN'), ('classifier', 'JJR'), ('code', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('regions', 'NNS'), ('interest', 'NN'), ('according', 'VBG'), ('object', 'JJ'), ('type', 'NN'), ('function', 'NN'), ('attributes', 'VBZ'), ('derived', 'JJ'), ('region', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('digital', 'JJ'), ('representation', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('classified', 'JJ'), ('regions', 'NNS'), ('interest', 'NN'), ('corresponds', 'NNS'), ('text', 'JJ'), ('use', 'NN'), ('classification', 'NN'), ('result', 'NN'), ('corresponding', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('regions', 'NNS'), ('interest', 'NN'), ('classify', 'NN'), ('another', 'DT'), ('regions', 'NNS'), ('interest', 'NN'), ('according', 'VBG'), ('object', 'JJ'), ('type', 'NN'), ('wherein', 'NN'), ('another', 'DT'), ('regions', 'NNS'), ('interest', 'NN'), ('corresponds', 'VBZ'), ('region', 'NN'), ('interest', 'NN'), ('assign', 'NN'), ('region', 'NN'), ('interest', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('algorithm', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('implementation', 'NN'), ('diverse', 'NN'), ('algorithms', 'NN'), ('function', 'NN'), ('region', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('region', 'NN'), ('interest', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('selection', 'NN'), ('criteria', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('implementation', 'NN'), ('diverse', 'NN'), ('algorithms', 'JJ'), ('configure', 'NN'), ('assigned', 'VBD'), ('algorithms', 'JJ'), ('process', 'NN'), ('respective', 'JJ'), ('regions', 'NNS'), ('interest', 'NN'), ('wherein', 'NN'), ('preprocessing', 'VBG'), ('code', 'NN'), ('based', 'VBN'), ('feature', 'NN'), ('density', 'NN'), ('selection', 'NN'), ('criteria', 'NNS'), ('determines', 'VBZ'), ('ocr', 'JJ'), ('algorithm', 'NN'), ('applicable', 'JJ'), ('text', 'NN'), ('algorithms', 'NN'), ('applicable', 'JJ'), ('aspects', 'NNS'), ('photographs', 'VBP'), ('logos', 'JJ'), ('device', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('configured', 'VBN'), ('execute', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('implementation', 'NN'), ('algorithms', 'NN'), ('stored', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('algorithm', 'JJ'), ('feature', 'NN'), ('density', 'NN'), ('selection', 'NN'), ('criteria', 'NNS'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('code', 'NN'), ('executed', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('data', 'NN'), ('preprocessing', 'NN'), ('code', 'NN'), ('comprising', 'VBG'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'RB'), ('configured', 'VBD'), ('obtain', 'VB'), ('digital', 'JJ'), ('representation', 'NN'), ('scene', 'NN'), ('scene', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('textual', 'JJ'), ('media', 'NNS'), ('generate', 'NN'), ('set', 'VBN'), ('invariant', 'JJ'), ('features', 'NNS'), ('applying', 'VBG'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'IN'), ('digital', 'JJ'), ('representation', 'NN'), ('cluster', 'NN'), ('set', 'VBN'), ('invariant', 'JJ'), ('features', 'NNS'), ('regions', 'NNS'), ('interest', 'NN'), ('digital', 'JJ'), ('representation', 'NN'), ('scene', 'NN'), ('region', 'NN'), ('interest', 'NN'), ('region', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('classify', 'VB'), ('region', 'NN'), ('classifier', 'JJR'), ('code', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('regions', 'NNS'), ('interest', 'NN'), ('according', 'VBG'), ('object', 'JJ'), ('type', 'NN'), ('function', 'NN'), ('attributes', 'VBZ'), ('derived', 'JJ'), ('region', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('digital', 'JJ'), ('representation', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('classified', 'JJ'), ('regions', 'NNS'), ('interest', 'NN'), ('corresponds', 'NNS'), ('text', 'JJ'), ('use', 'NN'), ('classification', 'NN'), ('result', 'NN'), ('corresponding', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('regions', 'NNS'), ('interest', 'NN'), ('classify', 'NN'), ('another', 'DT'), ('regions', 'NNS'), ('interest', 'NN'), ('according', 'VBG'), ('object', 'JJ'), ('type', 'NN'), ('wherein', 'NN'), ('another', 'DT'), ('regions', 'NNS'), ('interest', 'NN'), ('corresponds', 'VBZ'), ('region', 'NN'), ('interest', 'NN'), ('mobile', 'IN'), ('terminal', 'JJ'), ('comprising', 'VBG'), ('front', 'JJ'), ('camera', 'NN'), ('configured', 'VBD'), ('obtain', 'VB'), ('two-dimensional', 'JJ'), ('face', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('tilted', 'VBD'), ('certain', 'JJ'), ('angle', 'NN'), ('disposed', 'VBD'), ('adjacent', 'JJ'), ('front', 'JJ'), ('camera', 'NN'), ('obtain', 'VB'), ('metadata', 'JJ'), ('face', 'NN'), ('controller', 'NN'), ('obtaining', 'VBG'), ('distance', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('front', 'NN'), ('camera', 'NN'), ('distance', 'NN'), ('enabling', 'VBG'), ('area', 'NN'), ('overlap', 'IN'), ('region', 'NN'), ('first', 'JJ'), ('region', 'NN'), ('representing', 'VBG'), ('range', 'NN'), ('photographable', 'JJ'), ('front', 'NN'), ('camera', 'NN'), ('overlaps', 'VBZ'), ('second', 'JJ'), ('region', 'NN'), ('representing', 'VBG'), ('range', 'NN'), ('photographable', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('maximum', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('obtain', 'VB'), ('distance', 'NN'), ('enabling', 'VBG'), ('area', 'NN'), ('overlap', 'JJ'), ('region', 'NN'), ('maximum', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('front', 'NN'), ('camera', 'NN'), ('varying', 'VBG'), ('tilting', 'VBG'), ('angle', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('set', 'VBN'), ('distance', 'NN'), ('enabling', 'VBG'), ('area', 'NN'), ('overlap', 'JJ'), ('region', 'NN'), ('maximum', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('front', 'NN'), ('camera', 'NN'), ('tilting', 'VBG'), ('angle', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('optimal', 'JJ'), ('disposition', 'NN'), ('location', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('set', 'VBN'), ('disposition', 'NN'), ('location', 'NN'), ('front', 'NN'), ('camera', 'NN'), ('original', 'JJ'), ('point', 'NN'), ('calculates', 'NNS'), ('coordinates', 'NNS'), ('first', 'RB'), ('triangle', 'VBP'), ('representing', 'VBG'), ('first', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('field', 'NN'), ('view', 'NN'), ('front', 'JJ'), ('camera', 'NN'), ('maximum', 'JJ'), ('photographing', 'VBG'), ('distance', 'NN'), ('front', 'NN'), ('camera', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('calculate', 'JJ'), ('coordinates', 'NNS'), ('second', 'JJ'), ('triangle', 'JJ'), ('representing', 'VBG'), ('second', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('field', 'NN'), ('view', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('maximum', 'NN'), ('photographing', 'VBG'), ('distance', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('distance', 'NN'), ('front', 'JJ'), ('camera', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('tilting', 'VBG'), ('angle', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('tilted', 'VBD'), ('controller', 'NN'), ('configured', 'JJ'), ('calculate', 'NN'), ('coordinates', 'NNS'), ('third', 'JJ'), ('triangle', 'JJ'), ('representing', 'VBG'), ('third', 'JJ'), ('region', 'NN'), ('photographable', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('rotation-convert', 'JJ'), ('coordinates', 'NNS'), ('third', 'JJ'), ('triangle', 'NNS'), ('based', 'VBN'), ('tilting', 'VBG'), ('angle', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('calculate', 'NN'), ('coordinates', 'NNS'), ('second', 'JJ'), ('triangle', 'VBP'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('calculate', 'NN'), ('coordinates', 'NNS'), ('overlap', 'VBP'), ('region', 'NN'), ('based', 'VBN'), ('coordinates', 'NNS'), ('first', 'JJ'), ('triangle', 'JJ'), ('coordinates', 'NNS'), ('second', 'JJ'), ('triangle', 'NN'), ('calculates', 'NNS'), ('area', 'NN'), ('overlap', 'VBP'), ('region', 'NN'), ('based', 'VBN'), ('coordinates', 'NNS'), ('overlap', 'JJ'), ('region', 'NN'), ('mobile', 'IN'), ('terminal', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('generate', 'JJ'), ('three-dimensional', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('obtained', 'VBN'), ('front', 'JJ'), ('camera', 'NN'), ('metadata', 'NN'), ('obtained', 'VBD'), ('glance', 'NN'), ('sensor', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('metadata', 'NN'), ('comprises', 'VBZ'), ('one', 'CD'), ('angle', 'NN'), ('face', 'NN'), ('size', 'NN'), ('face', 'NN'), ('location', 'NN'), ('face', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('angle', 'JJ'), ('face', 'NN'), ('comprises', 'NNS'), ('angle', 'VBP'), ('face', 'NN'), ('rotated', 'VBD'), ('one', 'CD'), ('pitch', 'NN'), ('axis', 'NN'), ('roll', 'NN'), ('axis', 'NN'), ('yaw', 'NN'), ('axis', 'VBP'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('comprising', 'VBG'), ('memory', 'NN'), ('storing', 'VBG'), ('generated', 'VBD'), ('face', 'NN'), ('information', 'NN'), ('wherein', 'WRB'), ('controller', 'NN'), ('configured', 'VBD'), ('performs', 'NNS'), ('authentication', 'NN'), ('process', 'NN'), ('comparing', 'VBG'), ('stored', 'VBD'), ('face', 'NN'), ('information', 'NN'), ('face', 'NN'), ('information', 'NN'), ('obtained', 'VBN'), ('authentication', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('controlled', 'VBD'), ('permanently', 'RB'), ('activated', 'VBN'), ('low', 'JJ'), ('power', 'NN'), ('obtain', 'VB'), ('front', 'JJ'), ('metadata', 'NNS'), ('front', 'VBP'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('front', 'JJ'), ('camera', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('disposed', 'VBD'), ('line', 'NN'), ('upper', 'JJ'), ('end', 'NN'), ('mobile', 'JJ'), ('terminal', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('tilted', 'VBD'), ('one', 'CD'), ('direction', 'NN'), ('direction', 'NN'), ('direction', 'NN'), ('left', 'VBD'), ('direction', 'NN'), ('right', 'JJ'), ('direction', 'NN'), ('mobile', 'IN'), ('terminal', 'JJ'), ('wherein', 'NN'), ('metadata', 'NNS'), ('data', 'NNS'), ('changed', 'VBD'), ('mobile', 'JJ'), ('terminal', 'NN'), ('tilted', 'VBD'), ('external', 'JJ'), ('physical', 'JJ'), ('force', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('smart', 'JJ'), ('television', 'NN'), ('tv', 'NN'), ('indication', 'NN'), ('upcoming', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('wherein', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('based', 'VBN'), ('profile', 'IN'), ('identifying', 'VBG'), ('one', 'CD'), ('devices', 'NNS'), ('communication', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('including', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('microphone', 'NN'), ('camera', 'NN'), ('instructing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('identified', 'JJ'), ('device', 'NN'), ('detect', 'NN'), ('audio', 'NN'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('microphone', 'NN'), ('detect', 'JJ'), ('visual', 'JJ'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('camera', 'NN'), ('selecting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('device', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('based', 'VBN'), ('detected', 'JJ'), ('audio', 'JJ'), ('signal', 'NN'), ('detected', 'VBD'), ('visual', 'JJ'), ('signal', 'NN'), ('providing', 'VBG'), ('instructions', 'NNS'), ('selected', 'VBN'), ('device', 'NN'), ('output', 'NN'), ('notification', 'NN'), ('related', 'VBN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('wherein', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('one', 'CD'), ('live', 'JJ'), ('television', 'NN'), ('program', 'NN'), ('recorded', 'VBN'), ('television', 'NN'), ('program', 'NN'), ('broadcast', 'NN'), ('television', 'NN'), ('program', 'NN'), ('application-provided', 'JJ'), ('program', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('detected', 'JJ'), ('audio', 'JJ'), ('signal', 'NN'), ('includes', 'VBZ'), ('voice', 'NN'), ('comprising', 'VBG'), ('determining', 'VBG'), ('distance', 'NN'), ('recognized', 'VBN'), ('voice', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('distance', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('detected', 'VBN'), ('visual', 'JJ'), ('signals', 'NNS'), ('includes', 'VBZ'), ('face', 'NN'), ('wherein', 'NN'), ('face', 'NN'), ('includes', 'VBZ'), ('face', 'NN'), ('technique', 'NN'), ('comprising', 'VBG'), ('presenting', 'VBG'), ('smart', 'JJ'), ('tv', 'NN'), ('upcoming', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('favorite', 'JJ'), ('channel', 'NNS'), ('list', 'NN'), ('comprising', 'VBG'), ('obtaining', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('wherein', 'RB'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('historical', 'JJ'), ('time', 'NN'), ('historical', 'JJ'), ('date', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('obtaining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('current', 'JJ'), ('time', 'NN'), ('current', 'JJ'), ('date', 'NN'), ('processing', 'NN'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('determine', 'NN'), ('probability', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('current', 'JJ'), ('time', 'NN'), ('current', 'JJ'), ('date', 'NN'), ('presenting', 'NN'), ('favorite', 'JJ'), ('channel', 'NNS'), ('list', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('probability', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('wherein', 'JJ'), ('processing', 'NN'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('includes', 'VBZ'), ('employing', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('model', 'NN'), ('wherein', 'NN'), ('employing', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('model', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('duration', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('historical', 'JJ'), ('time', 'NN'), ('historical', 'JJ'), ('date', 'NN'), ('setting', 'VBG'), ('threshold', 'JJ'), ('time', 'NN'), ('duration', 'NN'), ('comparing', 'VBG'), ('determined', 'VBN'), ('duration', 'NN'), ('threshold', 'JJ'), ('time', 'NN'), ('duration', 'NN'), ('filtering', 'VBG'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('threshold', 'JJ'), ('time', 'NN'), ('duration', 'NN'), ('smart', 'JJ'), ('television', 'NN'), ('tv', 'NN'), ('comprising', 'VBG'), ('network', 'NN'), ('interface', 'JJ'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('communication', 'NN'), ('network', 'NN'), ('interface', 'JJ'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('capable', 'JJ'), ('executing', 'VBG'), ('-executable', 'JJ'), ('program', 'NN'), ('code', 'NN'), ('stored', 'VBD'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('cause', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('receive', 'NN'), ('indication', 'NN'), ('upcoming', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('wherein', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('based', 'VBN'), ('profile', 'JJ'), ('identify', 'VB'), ('one', 'CD'), ('devices', 'NNS'), ('communication', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('including', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('microphone', 'NN'), ('camera', 'NN'), ('instruct', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('identified', 'JJ'), ('device', 'NN'), ('detect', 'NN'), ('audio', 'NN'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('microphone', 'NN'), ('detect', 'JJ'), ('visual', 'JJ'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('camera', 'NN'), ('select', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('device', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('based', 'VBN'), ('detected', 'JJ'), ('audio', 'JJ'), ('signal', 'NN'), ('detected', 'VBD'), ('visual', 'JJ'), ('signal', 'NN'), ('provide', 'NN'), ('instructions', 'NNS'), ('selected', 'VBN'), ('device', 'NN'), ('output', 'NN'), ('notification', 'NN'), ('related', 'VBN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('detected', 'JJ'), ('audio', 'JJ'), ('signal', 'NN'), ('includes', 'VBZ'), ('voice', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('capable', 'JJ'), ('executing', 'VBG'), ('-executable', 'JJ'), ('program', 'NN'), ('code', 'NN'), ('determine', 'NN'), ('distance', 'NN'), ('recognized', 'VBN'), ('voice', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('distance', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('detected', 'VBN'), ('visual', 'JJ'), ('signals', 'NNS'), ('includes', 'VBZ'), ('detecting', 'VBG'), ('presence', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('detecting', 'VBG'), ('presence', 'NN'), ('includes', 'VBZ'), ('employing', 'VBG'), ('one', 'CD'), ('camera', 'NN'), ('microphone', 'NN'), ('fingerprint', 'NN'), ('sensor', 'NN'), ('associated', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('smart', 'JJ'), ('tv', 'NN'), ('mobile', 'NN'), ('device', 'NN'), ('smartphone', 'NN'), ('laptop', 'JJ'), ('computer', 'NN'), ('tablet', 'NN'), ('device', 'NN'), ('wearable', 'JJ'), ('device', 'NN'), ('internet', 'NN'), ('things', 'NNS'), ('iot', 'JJ'), ('device', 'JJ'), ('internet', 'NN'), ('everything', 'NN'), ('ioe', 'NN'), ('device', 'NN'), ('iot', 'NN'), ('hub', 'NN'), ('ioe', 'NN'), ('hub', 'NN'), ('smart', 'JJ'), ('television', 'NN'), ('tv', 'NN'), ('comprising', 'NN'), ('means', 'VBZ'), ('receiving', 'VBG'), ('indication', 'NN'), ('upcoming', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('wherein', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('based', 'VBN'), ('profile', 'NN'), ('means', 'VBZ'), ('identifying', 'VBG'), ('one', 'CD'), ('devices', 'NNS'), ('communication', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('including', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('microphone', 'NN'), ('camera', 'NN'), ('means', 'VBZ'), ('instructing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('identified', 'JJ'), ('device', 'NN'), ('detect', 'NN'), ('audio', 'NN'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('microphone', 'NN'), ('detect', 'JJ'), ('visual', 'JJ'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('camera', 'NN'), ('means', 'VBZ'), ('selecting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('device', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('based', 'VBN'), ('detected', 'JJ'), ('audio', 'JJ'), ('signal', 'NN'), ('detected', 'VBD'), ('visual', 'JJ'), ('signal', 'NN'), ('means', 'NNS'), ('providing', 'VBG'), ('instructions', 'NNS'), ('selected', 'VBN'), ('device', 'NN'), ('output', 'NN'), ('notification', 'NN'), ('related', 'VBN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'VBD'), ('one', 'CD'), ('devices', 'NNS'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('mobile', 'JJ'), ('device', 'NN'), ('smartphone', 'NN'), ('laptop', 'JJ'), ('computer', 'NN'), ('tablet', 'NN'), ('device', 'NN'), ('wearable', 'JJ'), ('device', 'NN'), ('internet', 'NN'), ('things', 'NNS'), ('iot', 'JJ'), ('device', 'JJ'), ('internet', 'NN'), ('everything', 'NN'), ('ioe', 'NN'), ('device', 'NN'), ('iot', 'NN'), ('hub', 'NN'), ('ioe', 'NN'), ('hub', 'NN'), ('another', 'DT'), ('smart', 'JJ'), ('tv', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('one', 'CD'), ('live', 'JJ'), ('television', 'NN'), ('program', 'NN'), ('recorded', 'VBN'), ('television', 'NN'), ('program', 'NN'), ('broadcast', 'NN'), ('television', 'NN'), ('program', 'NN'), ('application-provided', 'JJ'), ('program', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('notification', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('push', 'NN'), ('message', 'NN'), ('sms', 'JJ'), ('message', 'NN'), ('waysms', 'JJ'), ('message', 'NN'), ('audio', 'NN'), ('alert', 'NN'), ('audio', 'JJ'), ('message', 'NN'), ('email', 'JJ'), ('message', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('comprising', 'VBG'), ('presenting', 'VBG'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('favorite', 'JJ'), ('channel', 'NNS'), ('list', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('comprising', 'NN'), ('means', 'VBZ'), ('obtaining', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('wherein', 'RB'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('historical', 'JJ'), ('time', 'NN'), ('historical', 'JJ'), ('date', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('smart', 'JJ'), ('tv', 'NN'), ('means', 'NNS'), ('obtaining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('current', 'JJ'), ('time', 'NN'), ('current', 'JJ'), ('date', 'NN'), ('means', 'VBZ'), ('processing', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('determine', 'NN'), ('probability', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('smart', 'JJ'), ('tv', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('current', 'JJ'), ('time', 'NN'), ('current', 'JJ'), ('date', 'NN'), ('means', 'NNS'), ('presenting', 'VBG'), ('favorite', 'JJ'), ('channel', 'NNS'), ('list', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('probability', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('means', 'VBZ'), ('processing', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('includes', 'VBZ'), ('employing', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('model', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('employing', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('model', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('duration', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('smart', 'JJ'), ('tv', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('historical', 'JJ'), ('time', 'NN'), ('historical', 'JJ'), ('date', 'NN'), ('setting', 'VBG'), ('threshold', 'JJ'), ('time', 'NN'), ('duration', 'NN'), ('comparing', 'VBG'), ('determined', 'VBN'), ('duration', 'NN'), ('threshold', 'JJ'), ('time', 'NN'), ('duration', 'NN'), ('filtering', 'VBG'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('threshold', 'JJ'), ('time', 'NN'), ('duration', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('comprising', 'NN'), ('means', 'VBZ'), ('adjusting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('volume', 'NN'), ('brightness', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('adjusting', 'VBG'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('historical', 'JJ'), ('time', 'NN'), ('historical', 'JJ'), ('date', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('comprising', 'NN'), ('means', 'VBZ'), ('restricting', 'VBG'), ('access', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('-executable', 'JJ'), ('program', 'NN'), ('code', 'NN'), ('configured', 'VBD'), ('cause', 'NN'), ('smart', 'JJ'), ('television', 'NN'), ('tv', 'NN'), ('receive', 'VBP'), ('indication', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('wherein', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('based', 'VBN'), ('profile', 'JJ'), ('identify', 'VB'), ('one', 'CD'), ('devices', 'NNS'), ('communication', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('including', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('microphone', 'NN'), ('camera', 'NN'), ('instruct', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('identified', 'JJ'), ('device', 'NN'), ('detect', 'NN'), ('audio', 'NN'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('microphone', 'NN'), ('detect', 'JJ'), ('visual', 'JJ'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('camera', 'NN'), ('select', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('device', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('based', 'VBN'), ('detected', 'JJ'), ('audio', 'JJ'), ('signal', 'NN'), ('detected', 'VBD'), ('visual', 'JJ'), ('signal', 'NN'), ('provide', 'NN'), ('instructions', 'NNS'), ('selected', 'VBN'), ('device', 'NN'), ('output', 'NN'), ('notification', 'NN'), ('related', 'VBN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('detected', 'JJ'), ('audio', 'JJ'), ('signal', 'NN'), ('includes', 'VBZ'), ('voice', 'JJ'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('capable', 'JJ'), ('executing', 'VBG'), ('-executable', 'JJ'), ('program', 'NN'), ('code', 'NN'), ('determine', 'NN'), ('distance', 'NN'), ('recognized', 'VBN'), ('voice', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('distance', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('detected', 'VBN'), ('visual', 'JJ'), ('signals', 'NNS'), ('includes', 'VBZ'), ('face', 'VBP'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('face', 'NN'), ('includes', 'VBZ'), ('face', 'NN'), ('technique', 'NN'), ('camera', 'NN'), ('comprising', 'VBG'), ('sensor', 'JJ'), ('array', 'NN'), ('including', 'VBG'), ('sensors', 'NNS'), ('infrared', 'VBD'), ('ir', 'JJ'), ('illuminator', 'NN'), ('configured', 'VBD'), ('emit', 'RB'), ('active', 'JJ'), ('ir', 'NN'), ('light', 'JJ'), ('ir', 'JJ'), ('light', 'JJ'), ('sub-band', 'JJ'), ('spectral', 'JJ'), ('illuminators', 'NNS'), ('spectral', 'JJ'), ('illuminator', 'NN'), ('configured', 'VBD'), ('emit', 'RB'), ('active', 'JJ'), ('spectral', 'JJ'), ('light', 'NN'), ('different', 'JJ'), ('spectral', 'JJ'), ('light', 'NN'), ('sub-band', 'NN'), ('depth', 'NN'), ('controller', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('determine', 'JJ'), ('depth', 'NN'), ('value', 'NN'), ('sensors', 'NNS'), ('based', 'VBN'), ('active', 'JJ'), ('ir', 'NN'), ('light', 'JJ'), ('spectral', 'JJ'), ('controller', 'NN'), ('machine', 'NN'), ('configured', 'VBD'), ('sensors', 'NNS'), ('determine', 'VBP'), ('spectral', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('light', 'JJ'), ('sub-band', 'JJ'), ('spectral', 'JJ'), ('illuminators', 'NNS'), ('output', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('test', 'RBS'), ('depth+multi-spectral', 'JJ'), ('including', 'VBG'), ('pixels', 'NNS'), ('pixel', 'RB'), ('corresponding', 'VBG'), ('one', 'CD'), ('sensors', 'NNS'), ('sensor', 'VBP'), ('array', 'JJ'), ('including', 'VBG'), ('least', 'JJS'), ('depth', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('light', 'JJ'), ('sub-band', 'JJ'), ('spectral', 'JJ'), ('illuminators', 'NNS'), ('face', 'VBP'), ('machine', 'NN'), ('previously', 'RB'), ('trained', 'VBN'), ('set', 'VBN'), ('labeled', 'JJ'), ('training', 'NN'), ('depth+multi-spectral', 'JJ'), ('structure', 'NN'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('confidence', 'NN'), ('value', 'NN'), ('indicating', 'VBG'), ('likelihood', 'JJ'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('includes', 'VBZ'), ('face', 'NN'), ('camera', 'NN'), ('wherein', 'VBZ'), ('spectral', 'JJ'), ('value', 'NN'), ('calculated', 'VBD'), ('based', 'VBN'), ('depth', 'NN'), ('value', 'NN'), ('determined', 'VBD'), ('sensor', 'JJ'), ('corresponds', 'NNS'), ('pixel', 'VBP'), ('camera', 'NN'), ('wherein', 'NN'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('use', 'IN'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('determine', 'NN'), ('confidence', 'NN'), ('value', 'NN'), ('camera', 'NN'), ('wherein', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('includes', 'VBZ'), ('input', 'NN'), ('nodes', 'NNS'), ('wherein', 'VBP'), ('input', 'JJ'), ('node', 'RB'), ('configured', 'VBD'), ('receive', 'JJ'), ('pixel', 'NN'), ('value', 'NN'), ('array', 'NN'), ('corresponding', 'VBG'), ('different', 'JJ'), ('pixel', 'NN'), ('pixels', 'NNS'), ('test', 'VBP'), ('depth+multi-spectral', 'JJ'), ('wherein', 'NN'), ('pixel', 'VBZ'), ('value', 'NN'), ('array', 'NN'), ('includes', 'VBZ'), ('depth', 'JJ'), ('value', 'NN'), ('multi-spectral', 'JJ'), ('values', 'NNS'), ('pixel', 'VBP'), ('camera', 'NN'), ('wherein', 'NN'), ('multi-spectral', 'JJ'), ('values', 'NNS'), ('pixel', 'VBP'), ('include', 'VBP'), ('three', 'CD'), ('spectral', 'JJ'), ('values', 'NNS'), ('camera', 'VBP'), ('wherein', 'NN'), ('output', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('surface', 'NN'), ('normal', 'JJ'), ('pixel', 'JJ'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('wherein', 'NN'), ('pixel', 'VBZ'), ('value', 'NN'), ('array', 'NN'), ('includes', 'VBZ'), ('surface', 'NN'), ('normal', 'JJ'), ('camera', 'NN'), ('wherein', 'NN'), ('output', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('curvature', 'NN'), ('pixel', 'JJ'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('wherein', 'NN'), ('pixel', 'VBZ'), ('value', 'NN'), ('array', 'NN'), ('includes', 'VBZ'), ('curvature', 'NN'), ('camera', 'NN'), ('wherein', 'NN'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('use', 'NN'), ('models', 'NNS'), ('determine', 'VBP'), ('confidence', 'NN'), ('value', 'NN'), ('wherein', 'NN'), ('models', 'NNS'), ('includes', 'VBZ'), ('channel-specific', 'JJ'), ('models', 'NNS'), ('wherein', 'IN'), ('channel-specific', 'JJ'), ('model', 'NN'), ('configured', 'VBD'), ('process', 'JJ'), ('different', 'JJ'), ('pixel', 'NN'), ('parameter', 'NN'), ('pixels', 'NNS'), ('test', 'VBP'), ('depth+multi-spectral', 'JJ'), ('wherein', 'NN'), ('channel-specific', 'JJ'), ('model', 'NN'), ('includes', 'VBZ'), ('input', 'JJ'), ('nodes', 'NNS'), ('wherein', 'VBP'), ('channel-specific', 'JJ'), ('model', 'NN'), ('input', 'NN'), ('node', 'RB'), ('configured', 'VBD'), ('receive', 'JJ'), ('pixel', 'NN'), ('parameter', 'NN'), ('value', 'NN'), ('different', 'JJ'), ('pixel', 'NN'), ('pixels', 'NNS'), ('test', 'VBP'), ('depth+multi-spectral', 'JJ'), ('camera', 'NN'), ('wherein', 'NN'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('use', 'RB'), ('statistical', 'JJ'), ('model', 'NN'), ('determine', 'NN'), ('confidence', 'NN'), ('value', 'NN'), ('camera', 'NN'), ('wherein', 'VBZ'), ('statistical', 'JJ'), ('model', 'NN'), ('includes', 'VBZ'), ('nearest', 'JJS'), ('neighbor', 'NN'), ('algorithm', 'NN'), ('camera', 'NN'), ('wherein', 'VBZ'), ('statistical', 'JJ'), ('model', 'NN'), ('includes', 'VBZ'), ('support', 'NN'), ('vector', 'NN'), ('machine', 'NN'), ('camera', 'NN'), ('wherein', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('location', 'NN'), ('test', 'IN'), ('depth+multi-spectral', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('around', 'IN'), ('recognized', 'VBN'), ('face', 'NN'), ('camera', 'NN'), ('wherein', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('location', 'NN'), ('test', 'IN'), ('depth+multi-spectral', 'JJ'), ('identified', 'VBN'), ('two-dimensional', 'JJ'), ('facial', 'JJ'), ('feature', 'NN'), ('recognized', 'VBN'), ('face', 'NN'), ('camera', 'NN'), ('wherein', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('location', 'NN'), ('test', 'IN'), ('depth+multi-spectral', 'JJ'), ('identified', 'VBN'), ('three-dimensional', 'JJ'), ('facial', 'JJ'), ('feature', 'NN'), ('recognized', 'VBN'), ('face', 'NN'), ('camera', 'NN'), ('wherein', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('location', 'NN'), ('test', 'IN'), ('depth+multi-spectral', 'JJ'), ('identified', 'JJ'), ('spectral', 'JJ'), ('feature', 'NN'), ('recognized', 'VBN'), ('face', 'NN'), ('camera', 'NN'), ('wherein', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('pixel', 'JJ'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('confidence', 'NN'), ('value', 'NN'), ('indicating', 'VBG'), ('likelihood', 'NN'), ('pixel', 'NN'), ('included', 'VBD'), ('face', 'NN'), ('camera', 'NN'), ('wherein', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('identity', 'NN'), ('face', 'NN'), ('recognized', 'VBN'), ('test', 'IN'), ('depth+multi-spectral', 'JJ'), ('camera', 'NN'), ('wherein', 'NN'), ('sensors', 'NNS'), ('sensor', 'VBP'), ('array', 'JJ'), ('differential', 'JJ'), ('sensors', 'NNS'), ('wherein', 'VBP'), ('spectral', 'JJ'), ('value', 'NN'), ('determined', 'VBD'), ('based', 'VBN'), ('depth', 'NN'), ('value', 'NN'), ('differential', 'JJ'), ('measurement', 'JJ'), ('differential', 'NN'), ('sensor', 'NN'), ('camera', 'NN'), ('comprising', 'VBG'), ('sensor', 'JJ'), ('array', 'NN'), ('including', 'VBG'), ('sensors', 'NNS'), ('infrared', 'VBD'), ('ir', 'JJ'), ('illuminator', 'NN'), ('configured', 'VBD'), ('emit', 'RB'), ('active', 'JJ'), ('ir', 'NN'), ('light', 'JJ'), ('ir', 'JJ'), ('light', 'JJ'), ('sub-band', 'JJ'), ('spectral', 'JJ'), ('illuminators', 'NNS'), ('spectral', 'JJ'), ('illuminator', 'NN'), ('configured', 'VBD'), ('emit', 'RB'), ('active', 'JJ'), ('spectral', 'JJ'), ('light', 'NN'), ('different', 'JJ'), ('spectral', 'JJ'), ('light', 'NN'), ('sub-band', 'NN'), ('depth', 'NN'), ('controller', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('determine', 'JJ'), ('depth', 'NN'), ('value', 'NN'), ('sensors', 'NNS'), ('based', 'VBN'), ('active', 'JJ'), ('ir', 'NN'), ('light', 'JJ'), ('spectral', 'JJ'), ('controller', 'NN'), ('machine', 'NN'), ('configured', 'VBD'), ('sensors', 'NNS'), ('determine', 'VBP'), ('spectral', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('light', 'JJ'), ('sub-band', 'JJ'), ('spectral', 'JJ'), ('illuminators', 'NNS'), ('wherein', 'VBP'), ('spectral', 'JJ'), ('value', 'NN'), ('calculated', 'VBD'), ('based', 'VBN'), ('depth', 'NN'), ('value', 'NN'), ('determined', 'VBD'), ('sensor', 'JJ'), ('corresponds', 'NNS'), ('pixel', 'JJ'), ('output', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('test', 'RBS'), ('depth+multi-spectral', 'JJ'), ('including', 'VBG'), ('pixels', 'NNS'), ('pixel', 'RB'), ('corresponding', 'VBG'), ('one', 'CD'), ('sensors', 'NNS'), ('sensor', 'VBP'), ('array', 'JJ'), ('including', 'VBG'), ('least', 'JJS'), ('depth', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('light', 'JJ'), ('sub-band', 'JJ'), ('spectral', 'JJ'), ('illuminators', 'NNS'), ('face', 'VBP'), ('machine', 'NN'), ('including', 'VBG'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('previously', 'RB'), ('trained', 'VBN'), ('set', 'VBN'), ('labeled', 'JJ'), ('training', 'NN'), ('depth+multi-spectral', 'JJ'), ('structure', 'NN'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('confidence', 'NN'), ('value', 'NN'), ('indicating', 'VBG'), ('likelihood', 'JJ'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('includes', 'VBZ'), ('face', 'NN'), ('processing', 'VBG'), ('comprising', 'VBG'), ('acquiring', 'VBG'), ('photo', 'NN'), ('album', 'NN'), ('obtained', 'VBD'), ('face', 'NN'), ('clustering', 'VBG'), ('collecting', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('respective', 'JJ'), ('photo', 'NN'), ('album', 'NN'), ('acquiring', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('selecting', 'VBG'), ('cover', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('taking', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('setting', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('photo', 'NN'), ('album', 'JJ'), ('wherein', 'NN'), ('selecting', 'VBG'), ('cover', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('performing', 'VBG'), ('calculation', 'NN'), ('face', 'NN'), ('parameter', 'NN'), ('preset', 'VB'), ('way', 'NN'), ('obtain', 'VB'), ('cover', 'JJ'), ('score', 'NN'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'JJ'), ('comprises', 'VBZ'), ('acquiring', 'VBG'), ('source', 'NN'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('coming', 'VBG'), ('preset', 'VBN'), ('source', 'NN'), ('cover', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'JJ'), ('comprises', 'VBZ'), ('acquiring', 'VBG'), ('number', 'NN'), ('contained', 'VBN'), ('determining', 'VBG'), ('single-person', 'JJ'), ('according', 'VBG'), ('number', 'NN'), ('selecting', 'VBG'), ('single-person', 'JJ'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'JJ'), ('comprises', 'VBZ'), ('single-person', 'JJ'), ('photo', 'NN'), ('album', 'NN'), ('determining', 'VBG'), ('including', 'VBG'), ('two', 'CD'), ('photo', 'NN'), ('album', 'IN'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('including', 'VBG'), ('two', 'CD'), ('cover', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('face', 'NN'), ('feature', 'NN'), ('points', 'NNS'), ('face', 'VBP'), ('parameter', 'NN'), ('comprises', 'NNS'), ('face', 'VBP'), ('turning', 'VBG'), ('angle', 'RP'), ('acquiring', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('acquiring', 'VBG'), ('coordinate', 'NN'), ('values', 'NNS'), ('face', 'VBP'), ('feature', 'NN'), ('points', 'NNS'), ('determining', 'VBG'), ('distances', 'NNS'), ('angles', 'NNS'), ('face', 'VBP'), ('feature', 'NN'), ('points', 'NNS'), ('determining', 'VBG'), ('face', 'NN'), ('turning', 'VBG'), ('angle', 'RP'), ('according', 'VBG'), ('distances', 'NNS'), ('angles', 'NNS'), ('according', 'VBG'), ('wherein', 'JJ'), ('face', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('face', 'VBP'), ('ratio', 'NN'), ('acquiring', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('face', 'NN'), ('region', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('calculating', 'VBG'), ('ratio', 'NN'), ('area', 'NN'), ('face', 'NN'), ('region', 'NN'), ('area', 'NN'), ('obtain', 'VB'), ('face', 'NN'), ('ratio', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('calculating', 'VBG'), ('face', 'NN'), ('ratio', 'NN'), ('comprises', 'VBZ'), ('one', 'CD'), ('face', 'NN'), ('subtracting', 'VBG'), ('area', 'NN'), ('occupied', 'VBD'), ('face', 'NN'), ('corresponding', 'VBG'), ('photo', 'NN'), ('album', 'NN'), ('face', 'NN'), ('region', 'NN'), ('obtain', 'VB'), ('remaining', 'VBG'), ('area', 'NN'), ('calculating', 'VBG'), ('ratio', 'NN'), ('remaining', 'VBG'), ('area', 'NN'), ('area', 'NN'), ('obtain', 'VB'), ('face', 'NN'), ('ratio', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('collecting', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('respective', 'JJ'), ('photo', 'NN'), ('album', 'NN'), ('comprises', 'VBZ'), ('acquiring', 'VBG'), ('identifications', 'NNS'), ('photo', 'VBP'), ('album', 'IN'), ('extracting', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('identifications', 'NNS'), ('face', 'VBP'), ('database', 'JJ'), ('face', 'NN'), ('database', 'NN'), ('stored', 'VBD'), ('face', 'NN'), ('results', 'NNS'), ('face', 'VBP'), ('results', 'NNS'), ('including', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('processing', 'NN'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('memory', 'NN'), ('configured', 'VBD'), ('store', 'NN'), ('instructions', 'NNS'), ('executable', 'JJ'), ('wherein', 'NN'), ('configured', 'VBD'), ('run', 'VBN'), ('program', 'NN'), ('corresponding', 'NN'), ('instructions', 'NNS'), ('reading', 'VBG'), ('instructions', 'NNS'), ('stored', 'VBD'), ('memory', 'NN'), ('perform', 'NN'), ('acquiring', 'VBG'), ('photo', 'NN'), ('album', 'NN'), ('obtained', 'VBD'), ('face', 'NN'), ('clustering', 'VBG'), ('collecting', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('photo', 'NN'), ('album', 'NN'), ('acquiring', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('selecting', 'VBG'), ('cover', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('taking', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('setting', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('photo', 'NN'), ('album', 'JJ'), ('wherein', 'NN'), ('configured', 'VBD'), ('perform', 'JJ'), ('calculation', 'NN'), ('face', 'NN'), ('parameter', 'NN'), ('preset', 'VB'), ('way', 'NN'), ('obtain', 'VB'), ('cover', 'JJ'), ('score', 'NN'), ('select', 'JJ'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'NN'), ('wherein', 'NN'), ('configured', 'VBD'), ('acquire', 'VB'), ('source', 'NN'), ('select', 'NN'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('coming', 'VBG'), ('preset', 'VBN'), ('source', 'NN'), ('cover', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('configured', 'VBD'), ('acquire', 'VB'), ('number', 'NN'), ('contained', 'VBN'), ('determine', 'JJ'), ('single-person', 'JJ'), ('according', 'VBG'), ('number', 'NN'), ('select', 'JJ'), ('single-person', 'JJ'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('configured', 'VBD'), ('single-person', 'JJ'), ('photo', 'NN'), ('album', 'NN'), ('determine', 'NN'), ('including', 'VBG'), ('two', 'CD'), ('photo', 'NN'), ('album', 'NN'), ('select', 'JJ'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('including', 'VBG'), ('two', 'CD'), ('cover', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('face', 'NN'), ('feature', 'NN'), ('points', 'NNS'), ('face', 'VBP'), ('parameter', 'NN'), ('comprises', 'NNS'), ('face', 'VBP'), ('turning', 'VBG'), ('angle', 'NN'), ('configured', 'VBD'), ('acquire', 'VB'), ('coordinate', 'NN'), ('values', 'NNS'), ('face', 'VBP'), ('feature', 'NN'), ('points', 'NNS'), ('determine', 'JJ'), ('distances', 'NNS'), ('angles', 'NNS'), ('face', 'VBP'), ('feature', 'NN'), ('points', 'NNS'), ('determine', 'JJ'), ('face', 'NN'), ('turning', 'VBG'), ('angle', 'RP'), ('according', 'VBG'), ('distances', 'NNS'), ('angles', 'NNS'), ('apparatus', 'VBP'), ('according', 'VBG'), ('wherein', 'NN'), ('face', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('face', 'VBP'), ('ratio', 'NN'), ('configured', 'VBN'), ('determine', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('calculate', 'NN'), ('ratio', 'NN'), ('area', 'NN'), ('face', 'NN'), ('region', 'NN'), ('area', 'NN'), ('obtain', 'VB'), ('face', 'NN'), ('ratio', 'NN'), ('apparatus', 'IN'), ('according', 'VBG'), ('wherein', 'NN'), ('configured', 'VBD'), ('one', 'CD'), ('face', 'NN'), ('subtract', 'JJ'), ('area', 'NN'), ('occupied', 'VBD'), ('face', 'NN'), ('corresponding', 'VBG'), ('photo', 'NN'), ('album', 'NN'), ('face', 'NN'), ('region', 'NN'), ('obtain', 'VB'), ('remaining', 'VBG'), ('area', 'NN'), ('calculate', 'NN'), ('ratio', 'NN'), ('remaining', 'VBG'), ('area', 'NN'), ('area', 'NN'), ('obtain', 'VB'), ('face', 'NN'), ('ratio', 'NN'), ('apparatus', 'IN'), ('according', 'VBG'), ('wherein', 'NN'), ('configured', 'VBD'), ('acquire', 'VB'), ('identifications', 'NNS'), ('photo', 'VB'), ('album', 'JJ'), ('extract', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('identifications', 'NNS'), ('face', 'VBP'), ('database', 'JJ'), ('face', 'NN'), ('database', 'NN'), ('stored', 'VBD'), ('face', 'NN'), ('results', 'NNS'), ('face', 'VBP'), ('results', 'NNS'), ('including', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('comprising', 'VBG'), ('memory', 'NN'), ('display', 'NN'), ('screen', 'NN'), ('input', 'NN'), ('device', 'NN'), ('connected', 'VBN'), ('via', 'IN'), ('system', 'NN'), ('bus', 'JJ'), ('wherein', 'JJ'), ('memory', 'NN'), ('stored', 'VBN'), ('computer', 'NN'), ('programs', 'NNS'), ('executed', 'VBD'), ('cause', 'NN'), ('implement', 'NN'), ('processing', 'VBG'), ('processing', 'VBG'), ('comprising', 'VBG'), ('acquiring', 'VBG'), ('photo', 'NN'), ('album', 'NN'), ('obtained', 'VBD'), ('face', 'NN'), ('clustering', 'VBG'), ('collecting', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('respective', 'JJ'), ('photo', 'NN'), ('album', 'NN'), ('acquiring', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('selecting', 'VBG'), ('cover', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('taking', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('setting', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('photo', 'NN'), ('album', 'JJ'), ('wherein', 'NN'), ('selecting', 'VBG'), ('cover', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('performing', 'VBG'), ('calculation', 'NN'), ('face', 'NN'), ('parameter', 'NN'), ('preset', 'VB'), ('way', 'NN'), ('obtain', 'VB'), ('cover', 'JJ'), ('score', 'NN'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'JJ'), ('comprises', 'VBZ'), ('acquiring', 'VBG'), ('source', 'NN'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('coming', 'VBG'), ('preset', 'VBN'), ('source', 'NN'), ('cover', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('comprises', 'NNS'), ('least', 'JJS'), ('one', 'CD'), ('mobile', 'JJ'), ('phone', 'NN'), ('tablet', 'NN'), ('computer', 'NN'), ('personal', 'JJ'), ('digital', 'NN'), ('assistant', 'NN'), ('wearable', 'JJ'), ('device', 'NN'), ('computer-implemented', 'JJ'), ('comprising', 'NN'), ('receiving', 'VBG'), ('computing', 'VBG'), ('device', 'NN'), ('meeting', 'NN'), ('invitation', 'NN'), ('identifying', 'VBG'), ('location', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('meeting', 'NN'), ('invitation', 'NN'), ('configured', 'VBD'), ('provide', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('wherein', 'NN'), ('meeting', 'NN'), ('invitation', 'NN'), ('causes', 'VBZ'), ('system', 'NN'), ('control', 'NN'), ('pathway', 'RB'), ('allowing', 'VBG'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('providing', 'VBG'), ('based', 'VBN'), ('meeting', 'VBG'), ('invitation', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('controlling', 'VBG'), ('pathway', 'RB'), ('allowing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('physically', 'RB'), ('access', 'NN'), ('location', 'NN'), ('pathway', 'NN'), ('response', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicating', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('predetermined', 'VBN'), ('location', 'NN'), ('near', 'IN'), ('location', 'NN'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('based', 'VBN'), ('part', 'NN'), ('face', 'NN'), ('camera', 'NN'), ('system', 'NN'), ('identifying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('receiving', 'VBG'), ('positioning', 'VBG'), ('data', 'NNS'), ('face', 'NN'), ('camera', 'NN'), ('system', 'NN'), ('identifying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('pattern', 'JJ'), ('movement', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('determining', 'VBG'), ('pattern', 'JJ'), ('movement', 'NN'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('revoking', 'VBG'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('identified', 'VBD'), ('meeting', 'VBG'), ('invitation', 'NN'), ('controlling', 'VBG'), ('pathway', 'RB'), ('restrict', 'VB'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('identified', 'VBD'), ('meeting', 'VBG'), ('invitation', 'NN'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('pathway', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('pattern', 'JJ'), ('movement', 'NN'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('computer-implemented', 'JJ'), ('wherein', 'NN'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('passed', 'VBD'), ('egress', 'RB'), ('associated', 'VBN'), ('location', 'NN'), ('predetermined', 'VBN'), ('direction', 'NN'), ('computer-implemented', 'JJ'), ('wherein', 'NN'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('moved', 'VBD'), ('area', 'NN'), ('predetermined', 'VBN'), ('direction', 'NN'), ('computer-implemented', 'JJ'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('second', 'JJ'), ('pattern', 'JJ'), ('movement', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('wherein', 'NN'), ('access', 'NN'), ('secured', 'VBN'), ('data', 'NNS'), ('associated', 'VBN'), ('location', 'NN'), ('provided', 'VBN'), ('response', 'NN'), ('detecting', 'VBG'), ('second', 'JJ'), ('pattern', 'JJ'), ('movement', 'NN'), ('computer-implemented', 'JJ'), ('comprising', 'VBG'), ('collating', 'NN'), ('secured', 'VBN'), ('data', 'NNS'), ('public', 'JJ'), ('data', 'NNS'), ('generate', 'VBP'), ('resource', 'NN'), ('data', 'NNS'), ('communicating', 'VBG'), ('resource', 'NN'), ('data', 'NNS'), ('client', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('associated', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('access', 'NN'), ('location', 'NN'), ('provided', 'VBD'), ('computer-implemented', 'JJ'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('predetermined', 'VBN'), ('location', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('passes', 'NNS'), ('predetermined', 'VBD'), ('location', 'NN'), ('computer-implemented', 'JJ'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('predetermined', 'VBN'), ('location', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('passes', 'NNS'), ('predetermined', 'VBD'), ('location', 'NN'), ('near', 'IN'), ('location', 'NN'), ('predetermined', 'VBN'), ('direction', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('memory', 'NN'), ('communication', 'NN'), ('memory', 'NN'), ('computer-readable', 'JJ'), ('instructions', 'NNS'), ('stored', 'VBD'), ('thereupon', 'RB'), ('executed', 'VBN'), ('cause', 'NN'), ('receive', 'JJ'), ('meeting', 'NN'), ('invitation', 'NN'), ('indicating', 'VBG'), ('location', 'NN'), ('identity', 'NN'), ('meeting', 'NN'), ('invitation', 'NN'), ('configured', 'VBD'), ('provide', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('wherein', 'NN'), ('meeting', 'NN'), ('invitation', 'NN'), ('causes', 'VBZ'), ('system', 'NN'), ('control', 'NN'), ('pathway', 'RB'), ('allowing', 'VBG'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('provide', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('associated', 'VBN'), ('identity', 'NN'), ('access', 'NN'), ('location', 'NN'), ('controlling', 'VBG'), ('pathway', 'RB'), ('allowing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('physically', 'RB'), ('access', 'NN'), ('location', 'NN'), ('pathway', 'NN'), ('response', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicating', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('predetermined', 'VBN'), ('location', 'NN'), ('near', 'IN'), ('location', 'NN'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('based', 'VBN'), ('part', 'NN'), ('face', 'NN'), ('camera', 'NN'), ('system', 'NN'), ('identifying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('receive', 'VBP'), ('positioning', 'VBG'), ('data', 'NNS'), ('face', 'NN'), ('camera', 'NN'), ('system', 'NN'), ('identifying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('pattern', 'JJ'), ('movement', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('determine', 'NN'), ('pattern', 'JJ'), ('movement', 'NN'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('revoke', 'VBD'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('identified', 'VBD'), ('meeting', 'VBG'), ('invitation', 'NN'), ('controlling', 'VBG'), ('pathway', 'RB'), ('restrict', 'VB'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('identified', 'VBD'), ('meeting', 'VBG'), ('invitation', 'NN'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('pathway', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('pattern', 'JJ'), ('movement', 'NN'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('passed', 'VBD'), ('egress', 'RB'), ('associated', 'VBN'), ('location', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('moved', 'VBD'), ('area', 'NN'), ('predetermined', 'VBN'), ('direction', 'NN'), ('system', 'NN'), ('wherein', 'VBP'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('second', 'JJ'), ('pattern', 'JJ'), ('movement', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('wherein', 'NN'), ('access', 'NN'), ('secured', 'VBN'), ('data', 'NNS'), ('associated', 'VBN'), ('location', 'NN'), ('provided', 'VBN'), ('response', 'NN'), ('detecting', 'VBG'), ('second', 'JJ'), ('pattern', 'JJ'), ('movement', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('instructions', 'NNS'), ('cause', 'VBP'), ('collate', 'NN'), ('secured', 'VBN'), ('data', 'NNS'), ('public', 'JJ'), ('data', 'NNS'), ('generate', 'VBP'), ('resource', 'NN'), ('data', 'NNS'), ('communicate', 'VBP'), ('resource', 'NN'), ('data', 'NNS'), ('client', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('associated', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('access', 'NN'), ('location', 'NN'), ('provided', 'VBD'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('computer-executable', 'JJ'), ('instructions', 'NNS'), ('stored', 'VBD'), ('thereupon', 'RB'), ('executed', 'VBN'), ('one', 'CD'), ('computing', 'VBG'), ('device', 'NN'), ('cause', 'NN'), ('one', 'CD'), ('computing', 'NN'), ('device', 'NN'), ('receive', 'VBP'), ('meeting', 'NN'), ('invitation', 'NN'), ('indicating', 'VBG'), ('location', 'NN'), ('identity', 'NN'), ('meeting', 'NN'), ('invitation', 'NN'), ('configured', 'VBD'), ('provide', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('wherein', 'NN'), ('meeting', 'NN'), ('invitation', 'NN'), ('causes', 'VBZ'), ('system', 'NN'), ('control', 'NN'), ('pathway', 'RB'), ('allowing', 'VBG'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('provide', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('associated', 'VBN'), ('identity', 'NN'), ('access', 'NN'), ('location', 'NN'), ('controlling', 'VBG'), ('pathway', 'RB'), ('allowing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('physically', 'RB'), ('access', 'NN'), ('location', 'NN'), ('pathway', 'NN'), ('response', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicating', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('predetermined', 'VBN'), ('location', 'NN'), ('near', 'IN'), ('location', 'NN'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('based', 'VBN'), ('part', 'NN'), ('face', 'NN'), ('camera', 'NN'), ('system', 'NN'), ('identifying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('receive', 'VBP'), ('positioning', 'VBG'), ('data', 'NNS'), ('face', 'NN'), ('camera', 'NN'), ('system', 'NN'), ('identifying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('pattern', 'JJ'), ('movement', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('determine', 'NN'), ('pattern', 'JJ'), ('movement', 'NN'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('revoke', 'VBD'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('identified', 'VBD'), ('meeting', 'VBG'), ('invitation', 'NN'), ('controlling', 'VBG'), ('pathway', 'RB'), ('restrict', 'VB'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('identified', 'VBD'), ('meeting', 'VBG'), ('invitation', 'NN'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('pathway', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('pattern', 'JJ'), ('movement', 'NN'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('passed', 'VBD'), ('egress', 'RB'), ('associated', 'VBN'), ('location', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('second', 'JJ'), ('pattern', 'JJ'), ('movement', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('wherein', 'NN'), ('access', 'NN'), ('secured', 'VBN'), ('data', 'NNS'), ('associated', 'VBN'), ('location', 'NN'), ('provided', 'VBN'), ('response', 'NN'), ('detecting', 'VBG'), ('second', 'JJ'), ('pattern', 'JJ'), ('movement', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('instructions', 'NNS'), ('cause', 'VBP'), ('one', 'CD'), ('collate', 'NN'), ('secured', 'VBN'), ('data', 'NNS'), ('public', 'JJ'), ('data', 'NNS'), ('generate', 'VBP'), ('resource', 'NN'), ('data', 'NNS'), ('communicate', 'VBP'), ('resource', 'NN'), ('data', 'NNS'), ('client', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('associated', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('access', 'NN'), ('location', 'NN'), ('provided', 'VBD'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('piece', 'NN'), ('content', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('based', 'VBN'), ('salient', 'JJ'), ('data', 'NNS'), ('determining', 'VBG'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('different', 'JJ'), ('salient', 'JJ'), ('events', 'NNS'), ('occurring', 'VBG'), ('piece', 'NN'), ('content', 'NN'), ('different', 'JJ'), ('times', 'NNS'), ('playback', 'VBP'), ('piece', 'NN'), ('content', 'NN'), ('providing', 'VBG'), ('viewport', 'NN'), ('display', 'NN'), ('device', 'NN'), ('wherein', 'VBD'), ('movement', 'NN'), ('viewport', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('salient', 'NN'), ('data', 'NNS'), ('playback', 'NN'), ('detecting', 'VBG'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('included', 'VBD'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('providing', 'VBG'), ('indication', 'JJ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('viewport', 'NN'), ('playback', 'NN'), ('wherein', 'NN'), ('salient', 'NN'), ('data', 'NNS'), ('identifies', 'NNS'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('indicates', 'VBZ'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('corresponding', 'VBG'), ('point', 'NN'), ('location', 'NN'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('corresponding', 'VBG'), ('time', 'NN'), ('salient', 'JJ'), ('event', 'NN'), ('occurs', 'VBZ'), ('playback', 'RB'), ('wherein', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('indicates', 'VBZ'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('corresponding', 'VBG'), ('type', 'JJ'), ('salient', 'JJ'), ('event', 'NN'), ('corresponding', 'VBG'), ('strength', 'NN'), ('value', 'NN'), ('salient', 'JJ'), ('event', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('controls', 'NNS'), ('movement', 'NN'), ('viewport', 'NN'), ('put', 'VBD'), ('different', 'JJ'), ('salient', 'JJ'), ('events', 'NNS'), ('view', 'VBP'), ('viewport', 'RB'), ('different', 'JJ'), ('times', 'NNS'), ('playback', 'VBP'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('visual', 'JJ'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('audio', 'NN'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('content', 'JJ'), ('consumption', 'NN'), ('experience', 'NN'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('wherein', 'WRB'), ('salient', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('salient', 'JJ'), ('event', 'NN'), ('detected', 'VBD'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('face', 'NN'), ('facial', 'JJ'), ('emotion', 'NN'), ('object', 'JJ'), ('motion', 'NN'), ('metadata', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('wherein', 'WRB'), ('salient', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('salient', 'JJ'), ('event', 'NN'), ('detected', 'VBD'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('interaction', 'NN'), ('indication', 'NN'), ('wherein', 'WRB'), ('indication', 'NN'), ('comprises', 'VBZ'), ('interactive', 'JJ'), ('hint', 'NN'), ('response', 'NN'), ('detecting', 'VBG'), ('interaction', 'NN'), ('adapting', 'VBG'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('based', 'VBN'), ('interaction', 'NN'), ('wherein', 'WRB'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('providing', 'VBG'), ('updated', 'JJ'), ('viewport', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('display', 'NN'), ('device', 'NN'), ('wherein', 'VBD'), ('movement', 'NN'), ('updated', 'VBN'), ('viewport', 'NN'), ('based', 'VBN'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('salient', 'NN'), ('data', 'NNS'), ('playback', 'VBP'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('controls', 'NNS'), ('movement', 'NN'), ('updated', 'VBN'), ('viewport', 'NN'), ('put', 'VBD'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('view', 'NN'), ('updated', 'VBD'), ('viewport', 'NN'), ('comprising', 'VBG'), ('changing', 'VBG'), ('weight', 'NN'), ('assigned', 'VBD'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'JJ'), ('type', 'JJ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('wherein', 'JJ'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'JJ'), ('type', 'JJ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('-readable', 'JJ'), ('memory', 'NN'), ('device', 'NN'), ('storing', 'VBG'), ('instructions', 'NNS'), ('executed', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('causes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('perform', 'NN'), ('operations', 'NNS'), ('including', 'VBG'), ('receiving', 'VBG'), ('piece', 'NN'), ('content', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('based', 'VBN'), ('salient', 'JJ'), ('data', 'NNS'), ('determining', 'VBG'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('different', 'JJ'), ('salient', 'JJ'), ('events', 'NNS'), ('occurring', 'VBG'), ('piece', 'NN'), ('content', 'NN'), ('different', 'JJ'), ('times', 'NNS'), ('playback', 'VBP'), ('piece', 'NN'), ('content', 'NN'), ('providing', 'VBG'), ('viewport', 'NN'), ('display', 'NN'), ('device', 'NN'), ('wherein', 'VBD'), ('movement', 'NN'), ('viewport', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('salient', 'NN'), ('data', 'NNS'), ('playback', 'NN'), ('detecting', 'VBG'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('included', 'VBD'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('providing', 'VBG'), ('indication', 'JJ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('viewport', 'NN'), ('playback', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('identifies', 'NNS'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('indicates', 'VBZ'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('corresponding', 'VBG'), ('point', 'NN'), ('location', 'NN'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('corresponding', 'VBG'), ('time', 'NN'), ('salient', 'JJ'), ('event', 'NN'), ('occurs', 'VBZ'), ('playback', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('indicates', 'VBZ'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('corresponding', 'VBG'), ('type', 'JJ'), ('salient', 'JJ'), ('event', 'NN'), ('corresponding', 'VBG'), ('strength', 'NN'), ('value', 'NN'), ('salient', 'NN'), ('event', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('generated', 'VBD'), ('offline', 'JJ'), ('server', 'NN'), ('system', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('visual', 'JJ'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('audio', 'NN'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('content', 'JJ'), ('consumption', 'NN'), ('experience', 'NN'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('wherein', 'WRB'), ('salient', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('salient', 'JJ'), ('event', 'NN'), ('detected', 'VBD'), ('system', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('face', 'NN'), ('facial', 'JJ'), ('emotion', 'NN'), ('object', 'JJ'), ('motion', 'NN'), ('metadata', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('wherein', 'WRB'), ('salient', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('salient', 'JJ'), ('event', 'NN'), ('detected', 'VBD'), ('system', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('interaction', 'NN'), ('indication', 'NN'), ('wherein', 'WRB'), ('indication', 'NN'), ('comprises', 'VBZ'), ('interactive', 'JJ'), ('hint', 'NN'), ('response', 'NN'), ('detecting', 'VBG'), ('interaction', 'NN'), ('adapting', 'VBG'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('based', 'VBN'), ('interaction', 'NN'), ('wherein', 'WRB'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('providing', 'VBG'), ('updated', 'JJ'), ('viewport', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('display', 'NN'), ('device', 'NN'), ('wherein', 'VBD'), ('movement', 'NN'), ('updated', 'VBN'), ('viewport', 'NN'), ('based', 'VBN'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('salient', 'NN'), ('data', 'NNS'), ('playback', 'VBP'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('controls', 'NNS'), ('movement', 'NN'), ('updated', 'VBN'), ('viewport', 'NN'), ('put', 'VBD'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('view', 'NN'), ('updated', 'VBN'), ('viewport', 'NN'), ('system', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('changing', 'VBG'), ('weight', 'NN'), ('assigned', 'VBD'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'JJ'), ('type', 'JJ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'JJ'), ('type', 'JJ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('including', 'VBG'), ('instructions', 'NNS'), ('perform', 'VBP'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('piece', 'NN'), ('content', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('based', 'VBN'), ('salient', 'JJ'), ('data', 'NNS'), ('determining', 'VBG'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('different', 'JJ'), ('salient', 'JJ'), ('events', 'NNS'), ('occurring', 'VBG'), ('piece', 'NN'), ('content', 'NN'), ('different', 'JJ'), ('times', 'NNS'), ('playback', 'VBP'), ('piece', 'NN'), ('content', 'NN'), ('providing', 'VBG'), ('viewport', 'NN'), ('display', 'NN'), ('device', 'NN'), ('wherein', 'VBD'), ('movement', 'NN'), ('viewport', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('salient', 'NN'), ('data', 'NNS'), ('playback', 'NN'), ('detecting', 'VBG'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('included', 'VBD'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('providing', 'VBG'), ('indication', 'JJ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('viewport', 'NN'), ('playback', 'NN'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('interaction', 'NN'), ('indication', 'NN'), ('wherein', 'WRB'), ('indication', 'NN'), ('comprises', 'VBZ'), ('interactive', 'JJ'), ('hint', 'NN'), ('response', 'NN'), ('detecting', 'VBG'), ('interaction', 'NN'), ('adapting', 'VBG'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('based', 'VBN'), ('interaction', 'NN'), ('wherein', 'WRB'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('providing', 'VBG'), ('updated', 'JJ'), ('viewport', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('display', 'NN'), ('device', 'NN'), ('wherein', 'VBD'), ('movement', 'NN'), ('updated', 'VBN'), ('viewport', 'NN'), ('based', 'VBN'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('salient', 'NN'), ('data', 'NNS'), ('playback', 'VBP'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('controls', 'NNS'), ('movement', 'NN'), ('updated', 'VBN'), ('viewport', 'NN'), ('put', 'VBD'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('view', 'NN'), ('updated', 'VBD'), ('viewport', 'NN'), ('mobile', 'NN'), ('device', 'NN'), ('facial', 'JJ'), ('mobile', 'JJ'), ('device', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('cameras', 'JJ'), ('device', 'NN'), ('memory', 'NN'), ('coupled', 'VBN'), ('device', 'NN'), ('processing', 'NN'), ('system', 'NN'), ('programmed', 'VBD'), ('receive', 'JJ'), ('one', 'CD'), ('cameras', 'NN'), ('extract', 'JJ'), ('feature', 'NN'), ('extractor', 'NN'), ('utilizing', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('cnn', 'NNS'), ('enlarged', 'VBD'), ('intra-class', 'JJ'), ('variance', 'NN'), ('long-tail', 'JJ'), ('classes', 'NNS'), ('feature', 'NN'), ('vectors', 'NNS'), ('generate', 'VBP'), ('feature', 'NN'), ('generator', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vectors', 'NNS'), ('feature', 'VBP'), ('vectors', 'NNS'), ('classify', 'VBP'), ('fully', 'RB'), ('connected', 'VBN'), ('classifier', 'JJR'), ('identity', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vectors', 'NNS'), ('control', 'VBP'), ('operation', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('react', 'NN'), ('accordance', 'NN'), ('identity', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('includes', 'VBZ'), ('communication', 'NN'), ('system', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'JJ'), ('operation', 'NN'), ('tags', 'NNS'), ('video', 'VBP'), ('identity', 'NN'), ('uploads', 'NNS'), ('video', 'VBP'), ('social', 'JJ'), ('media', 'NNS'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'JJ'), ('operation', 'NN'), ('tags', 'NNS'), ('video', 'VBP'), ('identity', 'NN'), ('sends', 'NNS'), ('video', 'VBP'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'RB'), ('mobile', 'JJ'), ('device', 'NN'), ('smart', 'VBD'), ('phone', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'RB'), ('mobile', 'JJ'), ('device', 'NN'), ('body', 'NN'), ('cam', 'VBP'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('programmed', 'JJ'), ('train', 'NN'), ('feature', 'NN'), ('extractor', 'NN'), ('feature', 'NN'), ('generator', 'NN'), ('fully', 'RB'), ('connected', 'VBN'), ('classifier', 'JJ'), ('alternative', 'JJ'), ('bi-stage', 'NN'), ('strategy', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'JJ'), ('feature', 'NN'), ('extractor', 'NN'), ('shares', 'NNS'), ('covariance', 'NN'), ('matrices', 'NNS'), ('across', 'IN'), ('classes', 'NNS'), ('transfer', 'VBP'), ('intra-class', 'JJ'), ('variance', 'NN'), ('regular', 'JJ'), ('classes', 'NNS'), ('long-tail', 'JJ'), ('classes', 'NNS'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'JJ'), ('feature', 'NN'), ('generator', 'NN'), ('optimizes', 'VBZ'), ('softmax', 'JJ'), ('loss', 'NN'), ('joint', 'NN'), ('regularization', 'NN'), ('weights', 'NNS'), ('features', 'NNS'), ('magnitude', 'VBP'), ('inner', 'JJ'), ('product', 'NN'), ('weights', 'NNS'), ('features', 'NNS'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'JJ'), ('feature', 'NN'), ('extractor', 'NN'), ('averages', 'NNS'), ('feature', 'VBP'), ('vector', 'NN'), ('flipped', 'VBD'), ('feature', 'NN'), ('vector', 'NN'), ('flipped', 'VBD'), ('feature', 'NN'), ('vector', 'NN'), ('generated', 'VBD'), ('horizontally', 'RB'), ('flipped', 'VBN'), ('frame', 'VB'), ('one', 'CD'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'RB'), ('selected', 'VBN'), ('group', 'NN'), ('consisting', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('video', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'JJ'), ('communication', 'NN'), ('system', 'NN'), ('connects', 'VBZ'), ('remote', 'JJ'), ('server', 'NN'), ('includes', 'VBZ'), ('facial', 'JJ'), ('network', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'WP'), ('one', 'CD'), ('stage', 'NN'), ('alternative', 'JJ'), ('bi-stage', 'NN'), ('strategy', 'NN'), ('fixes', 'NNS'), ('feature', 'VBP'), ('extractor', 'NN'), ('applies', 'NNS'), ('feature', 'VBP'), ('generator', 'NN'), ('generate', 'VBP'), ('new', 'JJ'), ('transferred', 'VBN'), ('features', 'NNS'), ('diverse', 'JJ'), ('violate', 'JJ'), ('decision', 'NN'), ('boundary', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'WP'), ('one', 'CD'), ('stage', 'NN'), ('alternative', 'JJ'), ('bi-stage', 'NN'), ('strategy', 'NN'), ('fixes', 'NNS'), ('fully', 'RB'), ('connected', 'VBN'), ('classifier', 'JJR'), ('updates', 'JJ'), ('feature', 'NN'), ('extractor', 'NN'), ('feature', 'NN'), ('generator', 'NN'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('facial', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('comprising', 'VBG'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('program', 'NN'), ('instructions', 'NNS'), ('embodied', 'VBD'), ('therewith', 'JJ'), ('program', 'NN'), ('instructions', 'NNS'), ('executable', 'JJ'), ('computer', 'NN'), ('cause', 'NN'), ('computer', 'NN'), ('perform', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('device', 'NN'), ('extracting', 'VBG'), ('device', 'NN'), ('feature', 'NN'), ('extractor', 'NN'), ('utilizing', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('cnn', 'NNS'), ('enlarged', 'VBD'), ('intra-class', 'JJ'), ('variance', 'NN'), ('long-tail', 'JJ'), ('classes', 'NNS'), ('feature', 'VBP'), ('vectors', 'NNS'), ('generating', 'VBG'), ('device', 'NN'), ('feature', 'NN'), ('generator', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vectors', 'NNS'), ('feature', 'VBP'), ('vectors', 'NNS'), ('classifying', 'VBG'), ('device', 'NN'), ('utilizing', 'VBG'), ('fully', 'RB'), ('connected', 'VBN'), ('classifier', 'JJR'), ('identity', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vector', 'NN'), ('controlling', 'VBG'), ('operation', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('react', 'NN'), ('accordance', 'NN'), ('identity', 'NN'), ('computer-implemented', 'JJ'), ('facial', 'JJ'), ('mobile', 'JJ'), ('device', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('device', 'NN'), ('extracting', 'VBG'), ('device', 'NN'), ('feature', 'NN'), ('extractor', 'NN'), ('utilizing', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('cnn', 'NNS'), ('enlarged', 'VBD'), ('intra-class', 'JJ'), ('variance', 'NN'), ('long-tail', 'JJ'), ('classes', 'NNS'), ('feature', 'VBP'), ('vectors', 'NNS'), ('generating', 'VBG'), ('device', 'NN'), ('feature', 'NN'), ('generator', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vectors', 'NNS'), ('feature', 'VBP'), ('vectors', 'NNS'), ('classifying', 'VBG'), ('device', 'NN'), ('utilizing', 'VBG'), ('fully', 'RB'), ('connected', 'VBN'), ('classifier', 'JJR'), ('identity', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vector', 'NN'), ('controlling', 'VBG'), ('operation', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('react', 'NN'), ('accordance', 'NN'), ('identity', 'NN'), ('computer-implemented', 'JJ'), ('recited', 'VBD'), ('wherein', 'JJ'), ('controlling', 'VBG'), ('includes', 'VBZ'), ('tagging', 'VBG'), ('video', 'NN'), ('identity', 'NN'), ('uploading', 'VBG'), ('video', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), ('computer-implemented', 'JJ'), ('recited', 'VBD'), ('wherein', 'JJ'), ('controlling', 'VBG'), ('includes', 'VBZ'), ('tagging', 'VBG'), ('video', 'NN'), ('identity', 'NN'), ('sending', 'VBG'), ('video', 'JJ'), ('computer-implemented', 'JJ'), ('recited', 'VBN'), ('wherein', 'NN'), ('extracting', 'VBG'), ('includes', 'VBZ'), ('sharing', 'VBG'), ('covariance', 'NN'), ('matrices', 'NNS'), ('across', 'IN'), ('classes', 'NNS'), ('transfer', 'VBP'), ('intra-class', 'JJ'), ('variance', 'NN'), ('regular', 'JJ'), ('classes', 'NNS'), ('long-tail', 'JJ'), ('classes', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('comprising', 'VBG'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('storing', 'VBG'), ('machine', 'NN'), ('trained', 'VBN'), ('mt', 'JJ'), ('network', 'NN'), ('comprising', 'VBG'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('processing', 'VBG'), ('node', 'RB'), ('configured', 'VBN'), ('compute', 'NN'), ('first', 'RB'), ('output', 'NN'), ('value', 'NN'), ('combining', 'VBG'), ('set', 'VBN'), ('output', 'NN'), ('values', 'NNS'), ('set', 'VBD'), ('processing', 'VBG'), ('nodes', 'NNS'), ('use', 'VBP'), ('piecewise', 'JJ'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('compute', 'JJ'), ('second', 'JJ'), ('output', 'NN'), ('value', 'NN'), ('first', 'RB'), ('output', 'NN'), ('value', 'NN'), ('processing', 'VBG'), ('node', 'JJ'), ('wherein', 'NN'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('prior', 'RB'), ('training', 'VBG'), ('mt', 'NN'), ('network', 'NN'), ('comprises', 'NNS'), ('least', 'VBP'), ('first', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('first', 'RB'), ('slope', 'NN'), ('followed', 'VBD'), ('ii', 'JJ'), ('second', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('negative', 'JJ'), ('second', 'JJ'), ('slope', 'NN'), ('followed', 'VBD'), ('iii', 'JJ'), ('third', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('negative', 'JJ'), ('third', 'JJ'), ('slope', 'NN'), ('different', 'JJ'), ('second', 'JJ'), ('slope', 'NN'), ('followed', 'VBD'), ('iv', 'JJ'), ('fourth', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('positive', 'JJ'), ('fourth', 'JJ'), ('slope', 'NN'), ('followed', 'VBD'), ('v', 'JJ'), ('fifth', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('positive', 'JJ'), ('fifth', 'JJ'), ('slope', 'NN'), ('different', 'JJ'), ('fourth', 'JJ'), ('slope', 'NN'), ('followed', 'VBD'), ('vi', 'JJ'), ('sixth', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('sixth', 'VBD'), ('slope', 'NN'), ('wherein', 'NN'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('symmetric', 'JJ'), ('vertical', 'JJ'), ('axis', 'NN'), ('third', 'JJ'), ('fourth', 'JJ'), ('linear', 'JJ'), ('sections', 'NNS'), ('prior', 'RB'), ('training', 'VBG'), ('mt', 'NN'), ('network', 'NN'), ('content', 'JJ'), ('capturing', 'NN'), ('circuit', 'NN'), ('capturing', 'VBG'), ('content', 'JJ'), ('processing', 'NN'), ('mt', 'NN'), ('network', 'NN'), ('set', 'VBN'), ('processing', 'VBG'), ('units', 'NNS'), ('executing', 'VBG'), ('processing', 'VBG'), ('nodes', 'NNS'), ('process', 'NN'), ('content', 'NN'), ('captured', 'VBD'), ('content', 'JJ'), ('capturing', 'VBG'), ('circuit', 'NN'), ('wherein', 'NN'), ('training', 'NN'), ('set', 'VBN'), ('parameters', 'NNS'), ('define', 'VBP'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('node', 'IN'), ('first', 'JJ'), ('second', 'JJ'), ('pluralities', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('processing', 'VBG'), ('node', 'NN'), ('first', 'RB'), ('processing', 'VBG'), ('nodes', 'NNS'), ('configured', 'JJ'), ('emulate', 'JJ'), ('boolean', 'NN'), ('operator', 'NN'), ('output', 'NN'), ('value', 'NN'), ('processing', 'NN'), ('node', 'JJ'), ('range', 'NN'), ('associated', 'VBN'), ('``', '``'), (\"''\", \"''\"), ('value', 'NN'), ('set', 'VBN'), ('inputs', 'NNS'), ('processing', 'VBG'), ('node', 'NN'), ('set', 'VBN'), ('values', 'NNS'), ('range', 'VBP'), ('associated', 'VBN'), ('``', '``'), (\"''\", \"''\"), ('ii', 'NN'), ('processing', 'NN'), ('node', 'JJ'), ('second', 'JJ'), ('processing', 'NN'), ('nodes', 'NNS'), ('configured', 'VBD'), ('emulate', 'JJ'), ('boolean', 'JJ'), ('xnor', 'NN'), ('operator', 'NN'), ('output', 'NN'), ('value', 'NN'), ('processing', 'NN'), ('node', 'JJ'), ('range', 'NN'), ('associated', 'VBN'), ('``', '``'), (\"''\", \"''\"), ('set', 'NN'), ('inputs', 'NNS'), ('node', 'JJ'), ('set', 'NN'), ('values', 'NNS'), ('range', 'VBP'), ('associated', 'VBN'), ('``', '``'), (\"''\", \"''\"), ('b', 'NN'), ('set', 'VBN'), ('inputs', 'NNS'), ('node', 'JJ'), ('set', 'NN'), ('values', 'NNS'), ('range', 'VBP'), ('associated', 'VBN'), ('``', '``'), (\"''\", \"''\"), ('value', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NNS'), ('third', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('first', 'RB'), ('processing', 'VBG'), ('node', 'NN'), ('mt', 'NN'), ('network', 'NN'), ('different', 'JJ'), ('slope', 'NN'), ('third', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('second', 'JJ'), ('processing', 'NN'), ('node', 'NN'), ('mt', 'NN'), ('network', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('length', 'JJ'), ('third', 'JJ'), ('section', 'NN'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('first', 'RB'), ('processing', 'VBG'), ('node', 'NN'), ('mt', 'NN'), ('network', 'NN'), ('different', 'JJ'), ('length', 'NN'), ('third', 'JJ'), ('section', 'NN'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('second', 'JJ'), ('processing', 'NN'), ('node', 'NN'), ('mt', 'NN'), ('network', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('sets', 'VBZ'), ('parameters', 'NNS'), ('trained', 'VBD'), ('part', 'NN'), ('back', 'RB'), ('propagating', 'VBG'), ('module', 'NN'), ('back', 'RB'), ('propagating', 'JJ'), ('errors', 'NNS'), ('output', 'NN'), ('values', 'NNS'), ('later', 'RBR'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('earlier', 'RBR'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('adjusting', 'VBG'), ('set', 'NN'), ('parameters', 'NNS'), ('define', 'VBP'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('functions', 'NNS'), ('earlier', 'RBR'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('processing', 'NN'), ('node', 'JJ'), ('uses', 'NNS'), ('linear', 'JJ'), ('function', 'NN'), ('defined', 'VBD'), ('set', 'VBN'), ('parameters', 'NNS'), ('compute', 'VBP'), ('first', 'JJ'), ('output', 'NN'), ('value', 'NN'), ('processing', 'VBG'), ('node', 'CC'), ('wherein', 'JJ'), ('back', 'RB'), ('propagating', 'VBG'), ('module', 'NN'), ('back', 'RB'), ('propagates', 'VBZ'), ('errors', 'NNS'), ('output', 'NN'), ('values', 'NNS'), ('later', 'RBR'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('earlier', 'RBR'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('adjusting', 'VBG'), ('set', 'NN'), ('parameters', 'NNS'), ('define', 'VBP'), ('linear', 'JJ'), ('functions', 'NNS'), ('earlier', 'RBR'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'IN'), ('first', 'JJ'), ('processing', 'NN'), ('nodes', 'NNS'), ('emulate', 'VBP'), ('boolean', 'JJ'), ('operator', 'NN'), ('second', 'JJ'), ('processing', 'NN'), ('nodes', 'NNS'), ('emulate', 'VBP'), ('boolean', 'JJ'), ('xnor', 'NNP'), ('operator', 'NN'), ('enable', 'JJ'), ('mt', 'NN'), ('network', 'NN'), ('implement', 'JJ'), ('mathematical', 'JJ'), ('problems', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('processing', 'VBG'), ('node', 'JJ'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('receive', 'JJ'), ('input', 'NN'), ('values', 'NNS'), ('output', 'NN'), ('values', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('set', 'VBN'), ('prior', 'JJ'), ('layers', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('processing', 'NN'), ('node', 'JJ'), ('uses', 'NNS'), ('linear', 'JJ'), ('function', 'NN'), ('compute', 'NN'), ('first', 'RB'), ('output', 'NN'), ('value', 'NN'), ('processing', 'VBG'), ('node', 'JJ'), ('wherein', 'NN'), ('processing', 'VBG'), ('node', 'NN'), (\"'s\", 'POS'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('defined', 'VBD'), ('along', 'IN'), ('first', 'JJ'), ('second', 'JJ'), ('axes', 'NNS'), ('first', 'RB'), ('axis', 'VBP'), ('defining', 'VBG'), ('range', 'NN'), ('output', 'NN'), ('values', 'NNS'), ('processing', 'VBG'), ('node', 'NN'), (\"'s\", 'POS'), ('linear', 'JJ'), ('function', 'NN'), ('second', 'JJ'), ('axis', 'NN'), ('defining', 'VBG'), ('range', 'NN'), ('output', 'NN'), ('values', 'NNS'), ('produced', 'VBD'), ('piecewise', 'JJ'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('range', 'NN'), ('output', 'NN'), ('values', 'NNS'), ('processing', 'VBG'), ('node', 'NN'), (\"'s\", 'POS'), ('linear', 'JJ'), ('function', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('comprising', 'VBG'), ('content', 'JJ'), ('output', 'NN'), ('circuit', 'NN'), ('presenting', 'VBG'), ('output', 'NN'), ('based', 'VBN'), ('processing', 'NN'), ('content', 'NN'), ('mt', 'NN'), ('network', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('captured', 'VBD'), ('content', 'JJ'), ('one', 'CD'), ('audio', 'NN'), ('segment', 'NN'), ('wherein', 'NN'), ('presented', 'VBD'), ('output', 'NN'), ('output', 'NN'), ('display', 'VBP'), ('display', 'NN'), ('screen', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('audio', 'NN'), ('presentation', 'NN'), ('output', 'NN'), ('speaker', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('mt', 'NN'), ('network', 'NN'), ('mt', 'NN'), ('neural', 'JJ'), ('network', 'NN'), ('processing', 'VBG'), ('nodes', 'NNS'), ('mt', 'JJ'), ('neurons', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('set', 'VBN'), ('parameters', 'NNS'), ('configured', 'VBD'), ('training', 'VBG'), ('processing', 'VBG'), ('nodes', 'NNS'), ('comprise', 'NN'), ('least', 'VBP'), ('one', 'CD'), ('negative', 'JJ'), ('second', 'JJ'), ('third', 'JJ'), ('slopes', 'NNS'), ('second', 'JJ'), ('third', 'JJ'), ('linear', 'JJ'), ('sections', 'NNS'), ('positive', 'JJ'), ('fourth', 'JJ'), ('fifth', 'JJ'), ('slopes', 'NNS'), ('fourth', 'JJ'), ('fifth', 'JJ'), ('linear', 'NN'), ('sections', 'NNS'), ('first', 'RB'), ('intercept', 'JJ'), ('second', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('second', 'JJ'), ('intercept', 'NN'), ('fifth', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('set', 'VBN'), ('lengths', 'NNS'), ('least', 'JJS'), ('second', 'JJ'), ('third', 'JJ'), ('fourth', 'JJ'), ('fifth', 'JJ'), ('sections', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('trained', 'VBD'), ('set', 'VBN'), ('parameters', 'NNS'), ('define', 'VBP'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('node', 'NN'), ('comprise', 'NN'), ('output', 'NN'), ('values', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('first', 'RB'), ('sixth', 'JJ'), ('slopes', 'NNS'), ('zerowe', 'VBP'), ('system', 'NN'), ('comprising', 'VBG'), ('memory', 'NN'), ('device', 'NN'), ('store', 'NN'), ('input', 'NN'), ('including', 'VBG'), ('input', 'JJ'), ('interface', 'NN'), ('receive', 'JJ'), ('input', 'NN'), ('pre-', 'JJ'), ('model', 'NN'), ('input', 'JJ'), ('yield', 'NN'), ('multi-channel', 'JJ'), ('feature', 'NN'), ('extractor', 'NN'), ('extract', 'NN'), ('set', 'VBN'), ('features', 'NNS'), ('based', 'VBN'), ('multi-channel', 'JJ'), ('feature', 'NN'), ('selector', 'NN'), ('select', 'VBP'), ('one', 'CD'), ('features', 'VBZ'), ('set', 'VBN'), ('features', 'NNS'), ('multi-channel', 'JJ'), ('wherein', 'VBP'), ('one', 'CD'), ('features', 'NNS'), ('selected', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('feature', 'VBP'), ('matcher', 'JJR'), ('match', 'NN'), ('one', 'CD'), ('features', 'NNS'), ('learned', 'VBD'), ('feature', 'NN'), ('set', 'VBN'), ('similarity', 'NN'), ('detector', 'NN'), ('determine', 'NN'), ('whether', 'IN'), ('one', 'CD'), ('features', 'VBZ'), ('meet', 'RBS'), ('pre-defined', 'JJ'), ('similarity', 'NN'), ('threshold', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('pre-', 'JJ'), ('activate', 'NN'), ('one', 'CD'), ('channels', 'VBZ'), ('multi-channel', 'JJ'), ('yield', 'NN'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('system', 'NN'), ('wherein', 'VBP'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('determined', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('system', 'NN'), ('wherein', 'VBD'), ('pre-', 'JJ'), ('activate', 'NN'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'VBZ'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('system', 'NN'), ('wherein', 'VBP'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'NNS'), ('determined', 'VBD'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('system', 'NN'), ('wherein', 'JJ'), ('feature', 'NN'), ('matcher', 'NN'), ('utilize', 'JJ'), ('large-scale', 'JJ'), ('data', 'NNS'), ('learning', 'VBG'), ('process', 'NN'), ('perform', 'NN'), ('feature', 'NN'), ('matching', 'VBG'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('input', 'JJ'), ('interface', 'NN'), ('receive', 'JJ'), ('input', 'NN'), ('pre-', 'JJ'), ('model', 'NN'), ('input', 'JJ'), ('yield', 'NN'), ('multi-channel', 'JJ'), ('feature', 'NN'), ('extractor', 'NN'), ('extract', 'NN'), ('set', 'VBN'), ('features', 'NNS'), ('based', 'VBN'), ('multi-channel', 'JJ'), ('feature', 'NN'), ('selector', 'NN'), ('select', 'VBP'), ('one', 'CD'), ('features', 'VBZ'), ('set', 'VBN'), ('features', 'NNS'), ('multi-channel', 'JJ'), ('wherein', 'VBP'), ('one', 'CD'), ('features', 'NNS'), ('selected', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('feature', 'VBP'), ('matcher', 'JJR'), ('match', 'NN'), ('one', 'CD'), ('features', 'NNS'), ('learned', 'VBD'), ('feature', 'NN'), ('set', 'VBN'), ('similarity', 'NN'), ('detector', 'NN'), ('determine', 'NN'), ('whether', 'IN'), ('one', 'CD'), ('features', 'VBZ'), ('meet', 'RBS'), ('pre-defined', 'JJ'), ('similarity', 'NN'), ('threshold', 'NN'), ('apparatus', 'NN'), ('wherein', 'VBD'), ('pre-', 'JJ'), ('activate', 'NN'), ('one', 'CD'), ('channels', 'VBZ'), ('multi-channel', 'JJ'), ('yield', 'NN'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('apparatus', 'VBP'), ('wherein', 'IN'), ('one', 'CD'), ('activated', 'JJ'), ('channels', 'NNS'), ('determined', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('apparatus', 'VBP'), ('wherein', 'JJ'), ('pre-', 'JJ'), ('activate', 'NN'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'VBZ'), ('one', 'CD'), ('activated', 'JJ'), ('channels', 'NNS'), ('apparatus', 'VBP'), ('wherein', 'IN'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'NNS'), ('determined', 'VBD'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('apparatus', 'VBP'), ('wherein', 'JJ'), ('feature', 'NN'), ('matcher', 'NN'), ('utilize', 'JJ'), ('large-scale', 'JJ'), ('data', 'NNS'), ('learning', 'VBG'), ('process', 'NN'), ('perform', 'NN'), ('feature', 'NN'), ('matching', 'VBG'), ('comprising', 'VBG'), ('modeling', 'VBG'), ('input', 'JJ'), ('yield', 'NN'), ('multi-channel', 'RB'), ('extracting', 'VBG'), ('set', 'VBN'), ('features', 'NNS'), ('based', 'VBN'), ('multi-channel', 'NNS'), ('selecting', 'VBG'), ('one', 'CD'), ('features', 'VBZ'), ('set', 'VBN'), ('features', 'NNS'), ('multi-channel', 'JJ'), ('wherein', 'VBP'), ('one', 'CD'), ('features', 'NNS'), ('selected', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('matching', 'VBG'), ('one', 'CD'), ('features', 'NNS'), ('learned', 'VBD'), ('feature', 'NN'), ('set', 'VBN'), ('determining', 'VBG'), ('whether', 'IN'), ('one', 'CD'), ('features', 'VBZ'), ('meet', 'RBS'), ('pre-defined', 'JJ'), ('similarity', 'NN'), ('threshold', 'NN'), ('wherein', 'NN'), ('modeling', 'VBG'), ('input', 'NN'), ('include', 'VBP'), ('activating', 'VBG'), ('one', 'CD'), ('channels', 'NNS'), ('multi-channel', 'JJ'), ('yield', 'NN'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('wherein', 'VBP'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('determined', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('wherein', 'VBP'), ('extracting', 'VBG'), ('features', 'NNS'), ('input', 'VBP'), ('include', 'VBP'), ('activating', 'VBG'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'VBZ'), ('one', 'CD'), ('activated', 'JJ'), ('channels', 'NNS'), ('wherein', 'VBP'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'NNS'), ('determined', 'VBD'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('wherein', 'VBP'), ('feature', 'NN'), ('matcher', 'NN'), ('utilizes', 'JJ'), ('large-scale', 'JJ'), ('data', 'NNS'), ('learning', 'VBG'), ('process', 'NN'), ('perform', 'NN'), ('feature', 'NN'), ('matching', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('comprising', 'VBG'), ('set', 'VBN'), ('instructions', 'NNS'), ('executed', 'VBD'), ('computing', 'VBG'), ('device', 'NN'), ('cause', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('model', 'NN'), ('input', 'JJ'), ('yield', 'NN'), ('multi-channel', 'JJ'), ('extract', 'NN'), ('set', 'VBN'), ('features', 'NNS'), ('based', 'VBN'), ('multi-channel', 'NNS'), ('select', 'VBP'), ('one', 'CD'), ('features', 'VBZ'), ('set', 'VBN'), ('features', 'NNS'), ('multi-channel', 'JJ'), ('wherein', 'NN'), ('features', 'NNS'), ('selected', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('match', 'VBP'), ('one', 'CD'), ('features', 'NNS'), ('learned', 'VBD'), ('feature', 'NN'), ('set', 'VBN'), ('determine', 'NN'), ('whether', 'IN'), ('one', 'CD'), ('features', 'VBZ'), ('meet', 'RBS'), ('pre-defined', 'JJ'), ('similarity', 'NN'), ('threshold', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('instructions', 'NNS'), ('executed', 'VBD'), ('cause', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('activate', 'VBP'), ('one', 'CD'), ('channels', 'NNS'), ('multi-channel', 'JJ'), ('yield', 'NN'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('instructions', 'NNS'), ('executed', 'VBD'), ('cause', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('determine', 'NN'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('extracting', 'VBG'), ('features', 'NNS'), ('input', 'VBP'), ('include', 'VBP'), ('activating', 'VBG'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'VBZ'), ('one', 'CD'), ('activated', 'JJ'), ('channels', 'NNS'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'VBP'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'NNS'), ('determined', 'VBD'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'JJ'), ('feature', 'NN'), ('matcher', 'NN'), ('utilize', 'JJ'), ('large-scale', 'JJ'), ('data', 'NNS'), ('learning', 'VBG'), ('process', 'NN'), ('perform', 'NN'), ('feature', 'NN'), ('matching', 'VBG'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('means', 'NNS'), ('modeling', 'VBG'), ('input', 'JJ'), ('yield', 'NN'), ('multi-channel', 'NN'), ('means', 'NNS'), ('extracting', 'VBG'), ('set', 'NN'), ('features', 'NNS'), ('based', 'VBN'), ('multi-channel', 'NNS'), ('means', 'VBZ'), ('selecting', 'VBG'), ('one', 'CD'), ('features', 'VBZ'), ('set', 'VBN'), ('features', 'NNS'), ('multi-channel', 'JJ'), ('wherein', 'VBP'), ('one', 'CD'), ('features', 'NNS'), ('selected', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('means', 'VBZ'), ('matching', 'VBG'), ('one', 'CD'), ('features', 'NNS'), ('learned', 'VBD'), ('feature', 'NN'), ('set', 'VBN'), ('means', 'VBZ'), ('determining', 'VBG'), ('whether', 'IN'), ('one', 'CD'), ('features', 'VBZ'), ('meet', 'RBS'), ('pre-defined', 'JJ'), ('similarity', 'NN'), ('threshold', 'NN'), ('controlling', 'VBG'), ('terminal', 'JJ'), ('terminal', 'JJ'), ('comprising', 'VBG'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('comprising', 'VBG'), ('acquiring', 'VBG'), ('capturing', 'VBG'), ('apparatus', 'JJ'), ('obtaining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('motion', 'NN'), ('parameter', 'NN'), ('terminal', 'JJ'), ('motion', 'NN'), ('parameter', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('two', 'CD'), ('parameters', 'NNS'), ('among', 'IN'), ('acceleration', 'NN'), ('angular', 'JJ'), ('velocity', 'NN'), ('motion', 'NN'), ('amplitude', 'NN'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('transmitting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('parameter', 'NN'), ('threshold', 'NN'), ('obtaining', 'VBG'), ('request', 'NN'), ('data', 'NNS'), ('management', 'NN'), ('server', 'RB'), ('parameter', 'RB'), ('threshold', 'JJ'), ('obtaining', 'VBG'), ('request', 'NN'), ('comprising', 'VBG'), ('configuration', 'NN'), ('information', 'NN'), ('terminal', 'NN'), ('receiving', 'NN'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('correspond', 'NN'), ('configuration', 'NN'), ('information', 'NN'), ('response', 'NN'), ('parameter', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('request', 'NN'), ('comparing', 'VBG'), ('two', 'CD'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('controlling', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('perform', 'NN'), ('processing', 'NN'), ('acquired', 'VBD'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'NN'), ('based', 'VBN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('respectively', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('wherein', 'VBP'), ('acquiring', 'VBG'), ('comprises', 'NNS'), ('acquiring', 'VBG'), ('real', 'JJ'), ('time', 'NN'), ('obtaining', 'VBG'), ('comprises', 'NNS'), ('obtaining', 'VBG'), ('motion', 'NN'), ('parameter', 'NN'), ('terminal', 'JJ'), ('real', 'JJ'), ('time', 'NN'), ('comprising', 'VBG'), ('response', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('motion', 'NN'), ('parameter', 'FW'), ('terminal', 'JJ'), ('response', 'NN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('obtained', 'VBN'), ('latest', 'JJS'), ('time', 'NN'), ('less', 'CC'), ('equal', 'JJ'), ('corresponding', 'NN'), ('preset', 'NN'), ('thresholds', 'NNS'), ('performing', 'VBG'), ('processing', 'NN'), ('acquired', 'VBD'), ('latest', 'JJS'), ('time', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('acquiring', 'VBG'), ('comprises', 'NNS'), ('controlling', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('turn', 'NN'), ('capturing', 'VBG'), ('apparatus', 'NNS'), ('based', 'VBN'), ('face', 'NN'), ('instruction', 'NN'), ('acquiring', 'VBG'), ('capturing', 'VBG'), ('apparatus', 'JJ'), ('face', 'NN'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('turned', 'VBD'), ('according', 'VBG'), ('wherein', 'NN'), ('controlling', 'VBG'), ('perform', 'NN'), ('processing', 'NN'), ('comprises', 'VBZ'), ('skipping', 'VBG'), ('performing', 'VBG'), ('face', 'NN'), ('acquired', 'VBD'), ('face', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'NN'), ('based', 'VBN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('respectively', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('according', 'VBG'), ('wherein', 'NN'), ('obtaining', 'VBG'), ('comprises', 'NNS'), ('least', 'JJS'), ('one', 'CD'), ('obtaining', 'VBG'), ('acceleration', 'NN'), ('terminal', 'NN'), ('using', 'VBG'), ('acceleration', 'NN'), ('sensor', 'NN'), ('obtaining', 'VBG'), ('angular', 'JJ'), ('velocity', 'NN'), ('terminal', 'NN'), ('using', 'VBG'), ('gyro', 'NN'), ('sensor', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('transmitting', 'NN'), ('comprises', 'NNS'), ('transmitting', 'VBG'), ('parameter', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('request', 'NN'), ('data', 'NNS'), ('management', 'NN'), ('server', 'NN'), ('according', 'VBG'), ('preset', 'JJ'), ('time', 'NN'), ('period', 'NN'), ('according', 'VBG'), ('comprising', 'VBG'), ('generating', 'VBG'), ('prompt', 'JJ'), ('information', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'VBD'), ('prompt', 'JJ'), ('information', 'NN'), ('used', 'VBN'), ('prompting', 'VBG'), ('terminal', 'JJ'), ('stop', 'NN'), ('moving', 'VBG'), ('according', 'VBG'), ('wherein', 'JJ'), ('motion', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('terminal', 'JJ'), ('comprising', 'VBG'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('memory', 'NN'), ('configured', 'VBD'), ('store', 'NN'), ('program', 'NN'), ('code', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('configured', 'JJ'), ('access', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('memory', 'NN'), ('operate', 'NN'), ('according', 'VBG'), ('program', 'NN'), ('code', 'NN'), ('program', 'NN'), ('code', 'NN'), ('comprising', 'VBG'), ('motion', 'NN'), ('parameter', 'NN'), ('obtaining', 'VBG'), ('code', 'NN'), ('configured', 'VBN'), ('cause', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('acquire', 'VB'), ('using', 'VBG'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('obtain', 'VB'), ('motion', 'NN'), ('parameter', 'NN'), ('terminal', 'JJ'), ('motion', 'NN'), ('parameter', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('two', 'CD'), ('parameters', 'NNS'), ('among', 'IN'), ('acceleration', 'NN'), ('angular', 'JJ'), ('velocity', 'NN'), ('motion', 'NN'), ('amplitude', 'NN'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('request', 'NN'), ('transmitting', 'VBG'), ('code', 'NN'), ('configured', 'VBN'), ('cause', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('transmit', 'NN'), ('parameter', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('request', 'NN'), ('data', 'NNS'), ('management', 'NN'), ('server', 'RB'), ('parameter', 'RB'), ('threshold', 'JJ'), ('obtaining', 'VBG'), ('request', 'NN'), ('comprising', 'VBG'), ('configuration', 'NN'), ('information', 'NN'), ('terminal', 'NN'), ('parameter', 'NN'), ('threshold', 'VBD'), ('receiving', 'VBG'), ('code', 'NN'), ('configured', 'VBN'), ('cause', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('receive', 'NN'), ('corresponding', 'NN'), ('preset', 'NN'), ('thresholds', 'NNS'), ('correspond', 'NN'), ('configuration', 'NN'), ('information', 'NN'), ('response', 'NN'), ('parameter', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('request', 'NN'), ('comparing', 'VBG'), ('code', 'NN'), ('configured', 'VBN'), ('cause', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('compare', 'NN'), ('two', 'CD'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('control', 'NN'), ('code', 'NN'), ('configured', 'VBN'), ('cause', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('perform', 'NN'), ('processing', 'NN'), ('acquired', 'VBD'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'NN'), ('based', 'VBN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('respectively', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('wherein', 'VBP'), ('motion', 'NN'), ('parameter', 'NN'), ('obtaining', 'VBG'), ('code', 'NN'), ('causes', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('acquire', 'VB'), ('real', 'JJ'), ('time', 'NN'), ('obtain', 'VB'), ('motion', 'NN'), ('parameter', 'IN'), ('terminal', 'JJ'), ('real', 'JJ'), ('time', 'NN'), ('response', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'VBN'), ('threshold', 'JJ'), ('obtain', 'VB'), ('motion', 'NN'), ('parameter', 'NN'), ('terminal', 'JJ'), ('wherein', 'NN'), ('control', 'NN'), ('code', 'NN'), ('causes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('response', 'NN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('obtained', 'VBN'), ('latest', 'JJS'), ('time', 'NN'), ('less', 'CC'), ('equal', 'JJ'), ('corresponding', 'NN'), ('preset', 'NN'), ('thresholds', 'NNS'), ('perform', 'VBP'), ('processing', 'VBG'), ('acquired', 'VBN'), ('latest', 'JJS'), ('time', 'NN'), ('terminal', 'JJ'), ('according', 'VBG'), ('wherein', 'JJ'), ('program', 'NN'), ('code', 'NN'), ('comprises', 'VBZ'), ('face', 'VBP'), ('instruction', 'NN'), ('receiving', 'VBG'), ('code', 'NN'), ('configured', 'VBN'), ('cause', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('receive', 'JJ'), ('face', 'NN'), ('instruction', 'NN'), ('wherein', 'WRB'), ('motion', 'NN'), ('parameter', 'NN'), ('obtaining', 'VBG'), ('code', 'NN'), ('causes', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('control', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('instruction', 'NN'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('turn', 'NN'), ('acquire', 'VB'), ('face', 'NN'), ('using', 'VBG'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('turned', 'VBD'), ('wherein', 'JJ'), ('control', 'NN'), ('code', 'NN'), ('causes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('skip', 'NN'), ('performing', 'VBG'), ('face', 'NN'), ('acquired', 'VBD'), ('face', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'NN'), ('based', 'VBN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('respectively', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('terminal', 'JJ'), ('according', 'VBG'), ('wherein', 'JJ'), ('request', 'NN'), ('transmitting', 'VBG'), ('code', 'NN'), ('causes', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('transmit', 'NN'), ('parameter', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('request', 'NN'), ('data', 'NNS'), ('management', 'NN'), ('server', 'NN'), ('according', 'VBG'), ('preset', 'JJ'), ('time', 'NN'), ('period', 'NN'), ('terminal', 'JJ'), ('according', 'VBG'), ('wherein', 'JJ'), ('program', 'NN'), ('code', 'NN'), ('comprises', 'VBZ'), ('prompt', 'JJ'), ('information', 'NN'), ('generation', 'NN'), ('code', 'NN'), ('configured', 'VBN'), ('cause', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('generate', 'NN'), ('prompt', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'VBD'), ('prompt', 'JJ'), ('information', 'NN'), ('used', 'VBN'), ('prompting', 'VBG'), ('terminal', 'JJ'), ('stop', 'NN'), ('moving', 'VBG'), ('terminal', 'JJ'), ('according', 'VBG'), ('wherein', 'JJ'), ('motion', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('storing', 'VBG'), ('machine', 'NN'), ('instruction', 'NN'), ('executed', 'VBD'), ('one', 'CD'), ('causes', 'VBZ'), ('one', 'CD'), ('perform', 'NN'), ('obtaining', 'VBG'), ('acquired', 'VBD'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('obtaining', 'VBG'), ('motion', 'NN'), ('parameter', 'NN'), ('terminal', 'JJ'), ('terminal', 'NN'), ('comprising', 'VBG'), ('capturing', 'VBG'), ('apparatus', 'JJ'), ('motion', 'NN'), ('parameter', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('two', 'CD'), ('parameters', 'NNS'), ('among', 'IN'), ('acceleration', 'NN'), ('angular', 'JJ'), ('velocity', 'NN'), ('motion', 'NN'), ('amplitude', 'NN'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('transmitting', 'VBG'), ('parameter', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('request', 'NN'), ('data', 'NNS'), ('management', 'NN'), ('server', 'RB'), ('parameter', 'RB'), ('threshold', 'JJ'), ('obtaining', 'VBG'), ('request', 'NN'), ('comprising', 'VBG'), ('configuration', 'NN'), ('information', 'NN'), ('terminal', 'NN'), ('receiving', 'NN'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('correspond', 'NN'), ('configuration', 'NN'), ('information', 'NN'), ('response', 'NN'), ('parameter', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('request', 'NN'), ('comparing', 'VBG'), ('two', 'CD'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('controlling', 'VBG'), ('perform', 'NN'), ('processing', 'NN'), ('acquired', 'VBD'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'NN'), ('based', 'VBN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('respectively', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('wherein', 'VBP'), ('acquiring', 'VBG'), ('comprises', 'NNS'), ('acquiring', 'VBG'), ('real', 'JJ'), ('time', 'NN'), ('obtaining', 'VBG'), ('comprises', 'NNS'), ('obtaining', 'VBG'), ('motion', 'NN'), ('parameter', 'NN'), ('terminal', 'JJ'), ('real', 'JJ'), ('time', 'NN'), ('comprising', 'VBG'), ('response', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('motion', 'NN'), ('parameter', 'FW'), ('terminal', 'JJ'), ('response', 'NN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('obtained', 'VBN'), ('latest', 'JJS'), ('time', 'NN'), ('less', 'CC'), ('equal', 'JJ'), ('corresponding', 'NN'), ('preset', 'NN'), ('thresholds', 'NNS'), ('performing', 'VBG'), ('processing', 'NN'), ('acquired', 'VBD'), ('latest', 'JJS'), ('time', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('acquired', 'VBD'), ('face', 'NN'), ('processing', 'NN'), ('comprises', 'VBZ'), ('performing', 'VBG'), ('face', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('obtaining', 'VBG'), ('motion', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('obtaining', 'VBG'), ('acceleration', 'NN'), ('terminal', 'NN'), ('using', 'VBG'), ('acceleration', 'NN'), ('sensor', 'NN'), ('obtaining', 'VBG'), ('angular', 'JJ'), ('velocity', 'NN'), ('terminal', 'NN'), ('using', 'VBG'), ('gyro', 'JJ'), ('sensor', 'JJ'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('motion', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('processing', 'VBG'), ('drive-through', 'JJ'), ('order', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('customer', 'NN'), ('information', 'NN'), ('detected', 'VBN'), ('vision', 'NN'), ('providing', 'VBG'), ('product', 'NN'), ('information', 'NN'), ('customer', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('processing', 'VBG'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('receiving', 'VBG'), ('customer', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('receiving', 'VBG'), ('customer', 'NN'), ('information', 'NN'), ('associated', 'VBN'), ('vehicle', 'NN'), ('information', 'NN'), ('detected', 'VBD'), ('vehicle', 'NN'), ('receiving', 'VBG'), ('customer', 'NN'), ('information', 'NN'), ('associated', 'VBN'), ('identification', 'NN'), ('information', 'NN'), ('detected', 'VBD'), ('face', 'NN'), ('according', 'VBG'), ('comprising', 'VBG'), ('determining', 'VBG'), ('whether', 'IN'), ('customer', 'NN'), ('pre-order', 'NN'), ('customer', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('wherein', 'NN'), ('customer', 'NN'), ('determined', 'VBD'), ('pre-order', 'JJ'), ('customer', 'NN'), ('providing', 'VBG'), ('product', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('providing', 'VBG'), ('pre-order', 'JJ'), ('information', 'NN'), ('using', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('audio', 'JJ'), ('video', 'NN'), ('processing', 'VBG'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('comprises', 'VBZ'), ('providing', 'VBG'), ('information', 'NN'), ('promptly', 'RB'), ('guiding', 'VBG'), ('vehicle', 'NN'), ('pickup', 'NN'), ('stand', 'VBP'), ('using', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('audio', 'NN'), ('video', 'NN'), ('providing', 'VBG'), ('information', 'NN'), ('additional', 'JJ'), ('order', 'NN'), ('available', 'JJ'), ('according', 'VBG'), ('wherein', 'JJ'), ('product', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('recently', 'RB'), ('ordered', 'VBN'), ('product', 'NN'), ('component', 'NN'), ('frequently', 'RB'), ('ordered', 'VBD'), ('product', 'NN'), ('component', 'NN'), ('order', 'NN'), ('history', 'NN'), ('customer', 'NN'), ('information', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('receiving', 'VBG'), ('customer', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('receiving', 'VBG'), ('information', 'NN'), ('age', 'NN'), ('gender', 'NN'), ('passenger', 'NN'), ('detected', 'VBD'), ('face', 'NN'), ('providing', 'VBG'), ('product', 'NN'), ('information', 'NN'), ('customer', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('providing', 'VBG'), ('recommended', 'VBD'), ('menu', 'JJ'), ('information', 'NN'), ('differentiated', 'VBD'), ('according', 'VBG'), ('age', 'NN'), ('gender', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('processing', 'NN'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('product', 'NN'), ('component', 'NN'), ('past', 'JJ'), ('order', 'NN'), ('history', 'NN'), ('component', 'NN'), ('modified', 'VBD'), ('product', 'NN'), ('component', 'NN'), ('product', 'NN'), ('order', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('processing', 'NN'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('comprises', 'VBZ'), ('paying', 'VBG'), ('product', 'NN'), ('price', 'NN'), ('according', 'VBG'), ('biometrics-based', 'JJ'), ('authentication', 'NN'), ('communication', 'NN'), ('system', 'NN'), ('vehicle', 'NN'), ('mobile', 'IN'), ('terminal', 'JJ'), ('according', 'VBG'), ('wherein', 'JJ'), ('processing', 'NN'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('comprises', 'VBZ'), ('issuing', 'VBG'), ('payment', 'NN'), ('number', 'NN'), ('divided', 'VBN'), ('payment', 'NN'), ('performing', 'VBG'), ('divided', 'VBD'), ('payments', 'NNS'), ('according', 'VBG'), ('payment', 'NN'), ('requests', 'NNS'), ('mobile', 'VBP'), ('terminals', 'NNS'), ('payment', 'NN'), ('numbers', 'NNS'), ('inputted', 'VBD'), ('according', 'VBG'), ('wherein', 'NN'), ('processing', 'NN'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('comprises', 'VBZ'), ('accumulating', 'VBG'), ('mileage', 'NN'), ('account', 'NN'), ('corresponding', 'VBG'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('undergoing', 'JJ'), ('payment', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('processing', 'NN'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('comprises', 'VBZ'), ('suggesting', 'VBG'), ('takeout', 'RP'), ('packaging', 'VBG'), ('according', 'VBG'), ('temperature', 'NN'), ('product', 'NN'), ('atmospheric', 'JJ'), ('temperature', 'NN'), ('weather', 'NN'), ('vehicle', 'NN'), ('type', 'NN'), ('apparatus', 'NN'), ('configured', 'VBD'), ('process', 'JJ'), ('drive-through', 'JJ'), ('order', 'NN'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('transceiver', 'RB'), ('configured', 'VBN'), ('receive', 'JJ'), ('customer', 'NN'), ('information', 'NN'), ('detected', 'VBN'), ('vision', 'NN'), ('digital', 'JJ'), ('signage', 'NN'), ('configured', 'VBD'), ('provide', 'JJ'), ('product', 'NN'), ('information', 'NN'), ('customer', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('configured', 'VBD'), ('process', 'NN'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('transceiver', 'NN'), ('receives', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('customer', 'NN'), ('information', 'NN'), ('associated', 'VBN'), ('vehicle', 'NN'), ('information', 'NN'), ('detected', 'VBD'), ('vehicle', 'NN'), ('customer', 'NN'), ('information', 'NN'), ('associated', 'VBN'), ('identification', 'NN'), ('information', 'NN'), ('detected', 'VBD'), ('face', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('configured', 'VBD'), ('determine', 'NN'), ('whether', 'IN'), ('customer', 'NN'), ('pre-order', 'NN'), ('customer', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('customer', 'NN'), ('determined', 'VBD'), ('pre-order', 'JJ'), ('customer', 'NN'), ('perform', 'NN'), ('control', 'NN'), ('operation', 'NN'), ('provide', 'IN'), ('pre-order', 'JJ'), ('information', 'NN'), ('control', 'NN'), ('digital', 'JJ'), ('signage', 'NN'), ('output', 'NN'), ('information', 'NN'), ('promptly', 'RB'), ('guiding', 'VBG'), ('vehicle', 'NN'), ('pickup', 'NN'), ('stand', 'VB'), ('provide', 'JJ'), ('information', 'NN'), ('additional', 'JJ'), ('order', 'NN'), ('available', 'JJ'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('product', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('recently', 'RB'), ('ordered', 'VBN'), ('product', 'NN'), ('component', 'NN'), ('frequently', 'RB'), ('ordered', 'VBD'), ('product', 'NN'), ('component', 'NN'), ('order', 'NN'), ('history', 'NN'), ('customer', 'NN'), ('information', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('transceiver', 'NN'), ('configured', 'VBD'), ('receive', 'JJ'), ('information', 'NN'), ('age', 'NN'), ('gender', 'NN'), ('passenger', 'NN'), ('detected', 'VBD'), ('face', 'NN'), ('configured', 'VBN'), ('control', 'NN'), ('digital', 'JJ'), ('signage', 'NN'), ('provide', 'NN'), ('recommended', 'VBD'), ('menu', 'JJ'), ('information', 'NN'), ('differentiated', 'VBD'), ('according', 'VBG'), ('age', 'NN'), ('gender', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('configured', 'VBD'), ('determine', 'JJ'), ('product', 'NN'), ('component', 'NN'), ('past', 'JJ'), ('order', 'NN'), ('history', 'NN'), ('component', 'NN'), ('modified', 'VBD'), ('product', 'NN'), ('component', 'NN'), ('product', 'NN'), ('order', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('configured', 'JJ'), ('pay', 'NN'), ('product', 'NN'), ('price', 'NN'), ('according', 'VBG'), ('biometrics-based', 'JJ'), ('authentication', 'NN'), ('communication', 'NN'), ('system', 'NN'), ('vehicle', 'NN'), ('mobile', 'JJ'), ('terminal', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('configured', 'VBD'), ('issue', 'NN'), ('payment', 'NN'), ('number', 'NN'), ('divided', 'VBN'), ('payment', 'NN'), ('perform', 'NN'), ('divided', 'VBD'), ('payments', 'NNS'), ('according', 'VBG'), ('requests', 'NNS'), ('mobile', 'JJ'), ('terminals', 'NNS'), ('payment', 'NN'), ('numbers', 'NNS'), ('inputted', 'VBD'), ('apparatus', 'RP'), ('according', 'VBG'), ('wherein', 'NN'), ('configured', 'VBD'), ('accumulate', 'JJ'), ('mileage', 'NN'), ('account', 'NN'), ('corresponding', 'VBG'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('undergoing', 'JJ'), ('payment', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('configured', 'VBD'), ('control', 'NN'), ('digital', 'JJ'), ('signage', 'NN'), ('suggest', 'VBP'), ('takeout', 'IN'), ('packaging', 'VBG'), ('according', 'VBG'), ('temperature', 'NN'), ('product', 'NN'), ('atmospheric', 'JJ'), ('temperature', 'NN'), ('weather', 'NN'), ('vehicle', 'NN'), ('type', 'NN'), ('information', 'NN'), ('processing', 'NN'), ('performed', 'VBD'), ('computing', 'VBG'), ('device', 'NN'), ('one', 'CD'), ('memory', 'NN'), ('storing', 'VBG'), ('programs', 'NNS'), ('executed', 'VBD'), ('one', 'CD'), ('comprising', 'VBG'), ('identifying', 'VBG'), ('using', 'VBG'), ('face', 'NN'), ('one', 'CD'), ('face', 'NN'), ('corresponding', 'VBG'), ('respective', 'JJ'), ('person', 'NN'), ('captured', 'VBD'), ('first', 'RB'), ('identified', 'VBN'), ('face', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('selecting', 'VBG'), ('tiles', 'NNS'), ('first', 'RB'), ('tile', 'JJ'), ('matches', 'NNS'), ('face', 'VBP'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'JJ'), ('accordance', 'NN'), ('predefined', 'VBD'), ('correspondence', 'NN'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('set', 'VBN'), ('pre-stored', 'JJ'), ('description', 'NN'), ('parameters', 'NNS'), ('first', 'RB'), ('tile', 'IN'), ('generating', 'VBG'), ('second', 'JJ'), ('covering', 'VBG'), ('respective', 'JJ'), ('persons', 'NNS'), ('first', 'RB'), ('corresponding', 'VBG'), ('first', 'JJ'), ('tiles', 'NNS'), ('sharing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('predefined', 'VBN'), ('order', 'NN'), ('via', 'IN'), ('group', 'NN'), ('chat', 'WP'), ('session', 'NN'), ('wherein', 'VBD'), ('first', 'JJ'), ('second', 'JJ'), ('displayed', 'VBN'), ('group', 'NN'), ('chat', 'DT'), ('session', 'NN'), ('one', 'CD'), ('time', 'NN'), ('one', 'CD'), ('two', 'CD'), ('replaced', 'VBD'), ('two', 'CD'), ('periodically', 'RB'), ('wherein', 'VBP'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('includes', 'VBZ'), ('determining', 'VBG'), ('one', 'CD'), ('descriptive', 'JJ'), ('labels', 'NNS'), ('corresponding', 'VBG'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('using', 'VBG'), ('first', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('trained', 'VBD'), ('facial', 'JJ'), ('corresponding', 'VBG'), ('descriptive', 'JJ'), ('labels', 'NNS'), ('wherein', 'VBP'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('includes', 'VBZ'), ('determining', 'VBG'), ('identity', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('based', 'VBN'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('locating', 'VBG'), ('respective', 'JJ'), ('profile', 'NN'), ('information', 'NN'), ('first', 'RB'), ('person', 'NN'), ('based', 'VBN'), ('determined', 'VBN'), ('identity', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('using', 'VBG'), ('one', 'CD'), ('characteristics', 'NNS'), ('respective', 'JJ'), ('profile', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('person', 'NN'), ('set', 'VBN'), ('profile', 'NN'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('first', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('tiles', 'VBZ'), ('dynamic', 'JJ'), ('tile', 'JJ'), ('least', 'JJS'), ('second', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('tiles', 'VBZ'), ('static', 'JJ'), ('tile', 'NN'), ('including', 'VBG'), ('receiving', 'VBG'), ('comments', 'NNS'), ('different', 'JJ'), ('group', 'NN'), ('chat', 'WP'), ('session', 'NN'), ('comment', 'NN'), ('including', 'VBG'), ('descriptive', 'JJ'), ('term', 'NN'), ('respective', 'JJ'), ('person', 'NN'), ('identified', 'VBD'), ('first', 'JJ'), ('choosing', 'VBG'), ('descriptive', 'JJ'), ('label', 'NN'), ('respective', 'JJ'), ('person', 'NN'), ('according', 'VBG'), ('comments', 'NNS'), ('updating', 'VBG'), ('second', 'JJ'), ('adding', 'VBG'), ('descriptive', 'JJ'), ('label', 'NN'), ('adjacent', 'NN'), ('first', 'RB'), ('tile', 'RB'), ('respective', 'JJ'), ('person', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('information', 'NN'), ('processing', 'VBG'), ('comprising', 'VBG'), ('one', 'CD'), ('memory', 'NN'), ('storing', 'VBG'), ('instructions', 'NNS'), ('executed', 'VBD'), ('one', 'CD'), ('cause', 'NN'), ('perform', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('identifying', 'VBG'), ('using', 'VBG'), ('face', 'NN'), ('one', 'CD'), ('face', 'NN'), ('corresponding', 'VBG'), ('respective', 'JJ'), ('person', 'NN'), ('captured', 'VBD'), ('first', 'RB'), ('identified', 'VBN'), ('face', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('selecting', 'VBG'), ('tiles', 'NNS'), ('first', 'RB'), ('tile', 'JJ'), ('matches', 'NNS'), ('face', 'VBP'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'JJ'), ('accordance', 'NN'), ('predefined', 'VBD'), ('correspondence', 'NN'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('set', 'VBN'), ('pre-stored', 'JJ'), ('description', 'NN'), ('parameters', 'NNS'), ('first', 'RB'), ('tile', 'IN'), ('generating', 'VBG'), ('second', 'JJ'), ('covering', 'VBG'), ('respective', 'JJ'), ('persons', 'NNS'), ('first', 'RB'), ('corresponding', 'VBG'), ('first', 'JJ'), ('tiles', 'NNS'), ('sharing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('predefined', 'VBN'), ('order', 'NN'), ('via', 'IN'), ('group', 'NN'), ('chat', 'WP'), ('session', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'VBD'), ('first', 'JJ'), ('second', 'JJ'), ('displayed', 'VBN'), ('group', 'NN'), ('chat', 'DT'), ('session', 'NN'), ('one', 'CD'), ('time', 'NN'), ('one', 'CD'), ('two', 'CD'), ('replaced', 'VBD'), ('two', 'CD'), ('periodically', 'RB'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'RB'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('includes', 'VBZ'), ('determining', 'VBG'), ('one', 'CD'), ('descriptive', 'JJ'), ('labels', 'NNS'), ('corresponding', 'VBG'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('using', 'VBG'), ('first', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('trained', 'VBD'), ('facial', 'JJ'), ('corresponding', 'VBG'), ('descriptive', 'JJ'), ('labels', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'RB'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('includes', 'VBZ'), ('determining', 'VBG'), ('identity', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('based', 'VBN'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('locating', 'VBG'), ('respective', 'JJ'), ('profile', 'NN'), ('information', 'NN'), ('first', 'RB'), ('person', 'NN'), ('based', 'VBN'), ('determined', 'VBN'), ('identity', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('using', 'VBG'), ('one', 'CD'), ('characteristics', 'NNS'), ('respective', 'JJ'), ('profile', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('person', 'NN'), ('set', 'VBN'), ('profile', 'NN'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('least', 'VBD'), ('first', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('tiles', 'VBZ'), ('dynamic', 'JJ'), ('tile', 'JJ'), ('least', 'JJS'), ('second', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('tiles', 'VBZ'), ('static', 'JJ'), ('tile', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('operations', 'NNS'), ('include', 'VBP'), ('receiving', 'VBG'), ('comments', 'NNS'), ('different', 'JJ'), ('group', 'NN'), ('chat', 'WP'), ('session', 'NN'), ('comment', 'NN'), ('including', 'VBG'), ('descriptive', 'JJ'), ('term', 'NN'), ('respective', 'JJ'), ('person', 'NN'), ('identified', 'VBD'), ('first', 'JJ'), ('choosing', 'VBG'), ('descriptive', 'JJ'), ('label', 'NN'), ('respective', 'JJ'), ('person', 'NN'), ('according', 'VBG'), ('comments', 'NNS'), ('updating', 'VBG'), ('second', 'JJ'), ('adding', 'VBG'), ('descriptive', 'JJ'), ('label', 'NN'), ('adjacent', 'NN'), ('first', 'RB'), ('tile', 'RB'), ('respective', 'JJ'), ('person', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('storing', 'VBG'), ('instructions', 'NNS'), ('executed', 'VBD'), ('computing', 'VBG'), ('device', 'NN'), ('one', 'CD'), ('cause', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('perform', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('identifying', 'VBG'), ('using', 'VBG'), ('face', 'NN'), ('one', 'CD'), ('face', 'NN'), ('corresponding', 'VBG'), ('respective', 'JJ'), ('person', 'NN'), ('captured', 'VBD'), ('first', 'RB'), ('identified', 'VBN'), ('face', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('selecting', 'VBG'), ('tiles', 'NNS'), ('first', 'RB'), ('tile', 'JJ'), ('matches', 'NNS'), ('face', 'VBP'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'JJ'), ('accordance', 'NN'), ('predefined', 'VBD'), ('correspondence', 'NN'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('set', 'VBN'), ('pre-stored', 'JJ'), ('description', 'NN'), ('parameters', 'NNS'), ('first', 'RB'), ('tile', 'IN'), ('generating', 'VBG'), ('second', 'JJ'), ('covering', 'VBG'), ('respective', 'JJ'), ('persons', 'NNS'), ('first', 'RB'), ('corresponding', 'VBG'), ('first', 'JJ'), ('tiles', 'NNS'), ('sharing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('predefined', 'VBN'), ('order', 'NN'), ('via', 'IN'), ('group', 'NN'), ('chat', 'DT'), ('session', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('second', 'JJ'), ('displayed', 'VBN'), ('group', 'NN'), ('chat', 'DT'), ('session', 'NN'), ('one', 'CD'), ('time', 'NN'), ('one', 'CD'), ('two', 'CD'), ('replaced', 'VBD'), ('two', 'CD'), ('periodically', 'RB'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('includes', 'VBZ'), ('determining', 'VBG'), ('one', 'CD'), ('descriptive', 'JJ'), ('labels', 'NNS'), ('corresponding', 'VBG'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('using', 'VBG'), ('first', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('trained', 'VBD'), ('facial', 'JJ'), ('corresponding', 'VBG'), ('descriptive', 'JJ'), ('labels', 'NNS'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('includes', 'VBZ'), ('determining', 'VBG'), ('identity', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('based', 'VBN'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('locating', 'VBG'), ('respective', 'JJ'), ('profile', 'NN'), ('information', 'NN'), ('first', 'RB'), ('person', 'NN'), ('based', 'VBN'), ('determined', 'VBN'), ('identity', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('using', 'VBG'), ('one', 'CD'), ('characteristics', 'NNS'), ('respective', 'JJ'), ('profile', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('person', 'NN'), ('set', 'VBN'), ('profile', 'NN'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('least', 'VBD'), ('first', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('tiles', 'VBZ'), ('dynamic', 'JJ'), ('tile', 'JJ'), ('least', 'JJS'), ('second', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('tiles', 'VBZ'), ('static', 'JJ'), ('tile', 'IN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'JJ'), ('operations', 'NNS'), ('include', 'VBP'), ('receiving', 'VBG'), ('comments', 'NNS'), ('different', 'JJ'), ('group', 'NN'), ('chat', 'WP'), ('session', 'NN'), ('comment', 'NN'), ('including', 'VBG'), ('descriptive', 'JJ'), ('term', 'NN'), ('respective', 'JJ'), ('person', 'NN'), ('identified', 'VBD'), ('first', 'JJ'), ('choosing', 'VBG'), ('descriptive', 'JJ'), ('label', 'NN'), ('respective', 'JJ'), ('person', 'NN'), ('according', 'VBG'), ('comments', 'NNS'), ('updating', 'VBG'), ('second', 'JJ'), ('adding', 'VBG'), ('descriptive', 'JJ'), ('label', 'NN'), ('adjacent', 'NN'), ('first', 'RB'), ('tile', 'RB'), ('respective', 'JJ'), ('person', 'NN'), ('comprising', 'VBG'), ('computing', 'VBG'), ('system', 'NN'), ('determining', 'VBG'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'NN'), ('wherein', 'NN'), ('eye', 'NN'), ('system', 'NN'), ('associated', 'VBN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('worn', 'NNS'), ('based', 'VBN'), ('determination', 'NN'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'VBN'), ('computer', 'NN'), ('system', 'NN'), ('performing', 'VBG'), ('receiving', 'VBG'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('estimating', 'VBG'), ('region', 'NN'), ('looking', 'VBG'), ('within', 'IN'), ('field', 'NN'), ('view', 'NN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('received', 'VBD'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('determining', 'VBG'), ('vergence', 'NN'), ('distance', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('estimated', 'VBN'), ('region', 'NN'), ('looking', 'VBG'), ('locations', 'NNS'), ('one', 'CD'), ('objects', 'VBZ'), ('scene', 'NN'), ('displayed', 'VBD'), ('head-mounted', 'JJ'), ('display', 'NN'), ('adjusting', 'VBG'), ('one', 'CD'), ('configurations', 'NNS'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('vergence', 'NN'), ('distance', 'NN'), ('wherein', 'VBD'), ('one', 'CD'), ('configurations', 'NNS'), ('head-mounted', 'JJ'), ('display', 'NN'), ('comprise', 'NN'), ('one', 'CD'), ('rendering', 'NN'), ('position', 'NN'), ('display', 'NN'), ('screen', 'JJ'), ('position', 'NN'), ('optics', 'NNS'), ('block', 'VBP'), ('comprising', 'VBG'), ('determining', 'VBG'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('second', 'JJ'), ('performance', 'NN'), ('threshold', 'VBD'), ('receiving', 'VBG'), ('eye', 'NN'), ('data', 'NNS'), ('eye', 'NN'), ('system', 'NN'), ('determining', 'VBG'), ('vergence', 'NN'), ('distance', 'NN'), ('based', 'VBN'), ('eye', 'NN'), ('data', 'NNS'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('one', 'CD'), ('second', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBD'), ('one', 'CD'), ('displaying', 'NN'), ('elements', 'NNS'), ('scene', 'VBP'), ('displayed', 'VBN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('determining', 'VBG'), ('vergence', 'NN'), ('distance', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('eye', 'NN'), ('data', 'NNS'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('one', 'CD'), ('second', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBD'), ('one', 'CD'), ('displaying', 'NN'), ('elements', 'NNS'), ('scene', 'VBP'), ('comprising', 'VBG'), ('feeding', 'VBG'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('fusion', 'NN'), ('algorithm', 'IN'), ('wherein', 'JJ'), ('fusion', 'NN'), ('algorithm', 'NN'), ('assigns', 'NNS'), ('weight', 'VBD'), ('score', 'RB'), ('input', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'VBZ'), ('determining', 'VBG'), ('vergence', 'NN'), ('distance', 'NN'), ('using', 'VBG'), ('fusion', 'NN'), ('algorithm', 'NNS'), ('based', 'VBN'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('determining', 'VBG'), ('z-depth', 'JJ'), ('display', 'NN'), ('screen', 'NN'), ('confidence', 'NN'), ('score', 'NN'), ('based', 'VBN'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('comprising', 'VBG'), ('comparing', 'VBG'), ('confidence', 'NN'), ('score', 'NN'), ('confidence', 'NN'), ('level', 'NN'), ('threshold', 'JJ'), ('response', 'NN'), ('determination', 'NN'), ('confidence', 'NN'), ('score', 'NN'), ('confidence', 'NN'), ('level', 'NN'), ('threshold', 'VBD'), ('feeding', 'VBG'), ('one', 'CD'), ('second', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBD'), ('one', 'CD'), ('displaying', 'NN'), ('elements', 'NNS'), ('scene', 'JJ'), ('fusion', 'NN'), ('algorithm', 'IN'), ('determining', 'VBG'), ('z-depth', 'JJ'), ('display', 'NN'), ('screen', 'NN'), ('using', 'VBG'), ('fusion', 'NN'), ('algorithm', 'NNS'), ('based', 'VBN'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('one', 'CD'), ('second', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBD'), ('one', 'CD'), ('displaying', 'NN'), ('elements', 'NNS'), ('scene', 'VBP'), ('comparing', 'VBG'), ('comparing', 'VBG'), ('fusion', 'NN'), ('algorithm', 'NN'), ('confidence', 'NN'), ('scores', 'NNS'), ('associated', 'VBN'), ('combinations', 'NNS'), ('inputs', 'NNS'), ('determining', 'VBG'), ('fusion', 'NN'), ('algorithm', 'IN'), ('z-depth', 'JJ'), ('display', 'NN'), ('screen', 'NN'), ('based', 'VBN'), ('combination', 'NN'), ('inputs', 'NNS'), ('associated', 'VBN'), ('highest', 'JJS'), ('confidence', 'NN'), ('score', 'NN'), ('wherein', 'VBD'), ('z-depth', 'JJ'), ('confidence', 'NN'), ('score', 'NN'), ('determined', 'VBD'), ('fusion', 'NN'), ('algorithm', 'IN'), ('using', 'VBG'), ('piecewise', 'NN'), ('comparison', 'NN'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'VBZ'), ('one', 'CD'), ('second', 'NN'), ('inputs', 'VBZ'), ('wherein', 'JJ'), ('z-depth', 'JJ'), ('confidence', 'NN'), ('score', 'NN'), ('determined', 'VBD'), ('based', 'VBN'), ('correlation', 'NN'), ('two', 'CD'), ('inputs', 'NNS'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'VBZ'), ('one', 'CD'), ('second', 'NN'), ('inputs', 'VBZ'), ('wherein', 'JJ'), ('fusion', 'NN'), ('algorithm', 'NN'), ('comprises', 'VBZ'), ('machine', 'NN'), ('learning', 'VBG'), ('ml', 'JJ'), ('algorithm', 'JJ'), ('wherein', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('ml', 'JJ'), ('algorithm', 'JJ'), ('determines', 'NNS'), ('combination', 'NN'), ('first', 'RB'), ('inputs', 'VBZ'), ('fed', 'JJ'), ('fusion', 'NN'), ('algorithm', 'VBD'), ('wherein', 'WP'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('comprise', 'NN'), ('one', 'CD'), ('hand', 'NN'), ('position', 'NN'), ('hand', 'NN'), ('direction', 'NN'), ('hand', 'NN'), ('movement', 'NN'), ('hand', 'NN'), ('gesture', 'NN'), ('head', 'NN'), ('position', 'NN'), ('head', 'NN'), ('direction', 'NN'), ('head', 'NN'), ('movement', 'NN'), ('head', 'NN'), ('gesture', 'NN'), ('gaze', 'NN'), ('angle', 'VBP'), ('rea', 'NN'), ('body', 'NN'), ('gesture', 'NN'), ('body', 'NN'), ('posture', 'NN'), ('body', 'NN'), ('movement', 'NN'), ('behavior', 'NN'), ('weighted', 'VBD'), ('combination', 'NN'), ('one', 'CD'), ('related', 'JJ'), ('parameters', 'NNS'), ('wherein', 'VBP'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('received', 'VBD'), ('one', 'CD'), ('controller', 'NN'), ('sensor', 'NN'), ('camera', 'NN'), ('microphone', 'NN'), ('accelerometer', 'NN'), ('headset', 'VBN'), ('worn', 'VBP'), ('mobile', 'JJ'), ('device', 'NN'), ('wherein', 'VBD'), ('one', 'CD'), ('second', 'NN'), ('inputs', 'NNS'), ('associated', 'VBD'), ('one', 'CD'), ('displaying', 'NN'), ('elements', 'NNS'), ('comprise', 'VBP'), ('one', 'CD'), ('z-buffer', 'NN'), ('value', 'NN'), ('associated', 'VBN'), ('displaying', 'VBG'), ('element', 'NN'), ('displaying', 'VBG'), ('element', 'NN'), ('marked', 'VBD'), ('developer', 'NN'), ('analysis', 'NN'), ('result', 'NN'), ('shape', 'NN'), ('displaying', 'VBG'), ('element', 'JJ'), ('face', 'NN'), ('result', 'NN'), ('object', 'VBP'), ('result', 'NN'), ('person', 'NN'), ('identified', 'VBD'), ('displaying', 'VBG'), ('content', 'NN'), ('object', 'NN'), ('identified', 'VBD'), ('displaying', 'VBG'), ('content', 'JJ'), ('correlation', 'NN'), ('two', 'CD'), ('displaying', 'VBG'), ('elements', 'NNS'), ('weighted', 'VBN'), ('combination', 'NN'), ('one', 'CD'), ('second', 'NN'), ('inputs', 'VBZ'), ('comprising', 'VBG'), ('determining', 'VBG'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('second', 'JJ'), ('performance', 'NN'), ('threshold', 'VBD'), ('receiving', 'VBG'), ('one', 'CD'), ('second', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBD'), ('one', 'CD'), ('displaying', 'NN'), ('elements', 'NNS'), ('scene', 'VBP'), ('displayed', 'VBN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('determining', 'VBG'), ('vergence', 'NN'), ('distance', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('one', 'CD'), ('second', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBD'), ('one', 'CD'), ('displaying', 'NN'), ('elements', 'NNS'), ('wherein', 'VBP'), ('determining', 'VBG'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('second', 'JJ'), ('performance', 'NN'), ('threshold', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('eye', 'NN'), ('system', 'NN'), ('exist', 'VBP'), ('fails', 'NNS'), ('provide', 'VBP'), ('eye', 'NN'), ('data', 'NNS'), ('wherein', 'VBD'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('comprises', 'VBZ'), ('one', 'CD'), ('accuracy', 'NN'), ('parameter', 'NN'), ('eye', 'NN'), ('system', 'NN'), ('precision', 'NN'), ('parameter', 'NN'), ('eye', 'NN'), ('system', 'NN'), ('value', 'NN'), ('parameter', 'NN'), ('eye', 'NN'), ('system', 'NN'), ('detectability', 'NN'), ('pupil', 'VBP'), ('metric', 'JJ'), ('based', 'VBN'), ('one', 'CD'), ('parameters', 'NNS'), ('associated', 'JJ'), ('parameter', 'NN'), ('change', 'NN'), ('parameter', 'NN'), ('changing', 'VBG'), ('trend', 'NN'), ('data', 'NNS'), ('availability', 'NN'), ('weighted', 'VBD'), ('combination', 'NN'), ('one', 'CD'), ('performance', 'NN'), ('related', 'JJ'), ('parameters', 'NNS'), ('wherein', 'VBP'), ('one', 'CD'), ('parameters', 'NNS'), ('associated', 'VBD'), ('comprise', 'NN'), ('one', 'CD'), ('eye', 'NN'), ('distance', 'NN'), ('pupil', 'JJ'), ('position', 'NN'), ('pupil', 'NN'), ('status', 'NN'), ('correlation', 'NN'), ('two', 'CD'), ('pupils', 'NNS'), ('head', 'VBP'), ('size', 'NN'), ('position', 'NN'), ('headset', 'VBN'), ('worn', 'JJ'), ('angle', 'NN'), ('headset', 'NN'), ('worn', 'JJ'), ('direction', 'NN'), ('headset', 'NN'), ('worn', 'JJ'), ('alignment', 'JJ'), ('eyes', 'NNS'), ('weighted', 'VBD'), ('combination', 'NN'), ('one', 'CD'), ('related', 'JJ'), ('parameters', 'NNS'), ('associated', 'VBD'), ('wherein', 'JJ'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'NN'), ('comprises', 'VBZ'), ('one', 'CD'), ('pre-determined', 'JJ'), ('value', 'NN'), ('pre-determined', 'JJ'), ('range', 'NN'), ('state', 'NN'), ('data', 'NNS'), ('changing', 'VBG'), ('speed', 'NN'), ('data', 'NNS'), ('trend', 'NN'), ('data', 'NNS'), ('change', 'VBP'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('media', 'NNS'), ('embodying', 'VBG'), ('software', 'NN'), ('operable', 'JJ'), ('executed', 'VBN'), ('computing', 'NN'), ('system', 'NN'), ('determine', 'JJ'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'NN'), ('wherein', 'NN'), ('eye', 'NN'), ('system', 'NN'), ('associated', 'VBN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('worn', 'NNS'), ('based', 'VBN'), ('determination', 'NN'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'JJ'), ('media', 'NNS'), ('embodying', 'VBG'), ('software', 'NN'), ('operable', 'JJ'), ('executed', 'VBN'), ('computing', 'VBG'), ('system', 'NN'), ('receive', 'VBP'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('estimate', 'NN'), ('region', 'NN'), ('looking', 'VBG'), ('within', 'IN'), ('field', 'NN'), ('view', 'NN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('received', 'VBD'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('determine', 'JJ'), ('vergence', 'NN'), ('distance', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('estimated', 'VBN'), ('region', 'NN'), ('looking', 'VBG'), ('locations', 'NNS'), ('one', 'CD'), ('objects', 'VBZ'), ('scene', 'NN'), ('displayed', 'VBD'), ('head-mounted', 'JJ'), ('display', 'NN'), ('adjust', 'VBP'), ('one', 'CD'), ('configurations', 'NNS'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('vergence', 'NN'), ('distance', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('media', 'NNS'), ('embodying', 'VBG'), ('instructions', 'NNS'), ('one', 'CD'), ('coupled', 'VBN'), ('storage', 'NN'), ('media', 'NNS'), ('operable', 'JJ'), ('execute', 'JJ'), ('instructions', 'NNS'), ('determine', 'VBP'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'NN'), ('wherein', 'NN'), ('eye', 'NN'), ('system', 'NN'), ('associated', 'VBN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('worn', 'NNS'), ('based', 'VBN'), ('determination', 'NN'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'NN'), ('system', 'NN'), ('configured', 'VBD'), ('receive', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('estimate', 'NN'), ('region', 'NN'), ('looking', 'VBG'), ('within', 'IN'), ('field', 'NN'), ('view', 'NN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('received', 'VBD'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('determine', 'JJ'), ('vergence', 'NN'), ('distance', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('estimated', 'VBN'), ('region', 'NN'), ('looking', 'VBG'), ('locations', 'NNS'), ('one', 'CD'), ('objects', 'VBZ'), ('scene', 'NN'), ('displayed', 'VBD'), ('head-mounted', 'JJ'), ('display', 'NN'), ('adjust', 'VBP'), ('one', 'CD'), ('configurations', 'NNS'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('vergence', 'NN'), ('distance', 'NN'), ('computer-implemented', 'JJ'), ('-based', 'VBD'), ('self-guided', 'JJ'), ('object', 'JJ'), ('detection', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('device', 'NN'), ('set', 'VBN'), ('respective', 'JJ'), ('grid', 'JJ'), ('thereon', 'NN'), ('labeled', 'VBD'), ('regarding', 'VBG'), ('respective', 'JJ'), ('object', 'NN'), ('detected', 'VBD'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('training', 'NN'), ('device', 'NN'), ('grid-based', 'JJ'), ('object', 'NN'), ('detector', 'NN'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('determining', 'VBG'), ('device', 'NN'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('applying', 'VBG'), ('local', 'JJ'), ('segmentation', 'NN'), ('training', 'NN'), ('device', 'NN'), ('region-based', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('rcnn', 'NN'), ('joint', 'NN'), ('object', 'JJ'), ('localization', 'NN'), ('object', 'NN'), ('classification', 'NN'), ('using', 'VBG'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('input', 'NN'), ('rcnn', 'VBD'), ('computer-implemented', 'JJ'), ('comprising', 'NN'), ('performing', 'VBG'), ('action', 'NN'), ('responsive', 'JJ'), ('object', 'NN'), ('localization', 'NN'), ('object', 'JJ'), ('classification', 'NN'), ('respective', 'JJ'), ('new', 'JJ'), ('object', 'JJ'), ('new', 'JJ'), ('rcnn', 'NN'), ('applied', 'VBN'), ('computer-implemented', 'JJ'), ('wherein', 'JJ'), ('action', 'NN'), ('comprises', 'VBZ'), ('autonomously', 'RB'), ('controlling', 'VBG'), ('motor', 'NN'), ('vehicle', 'NN'), ('avoid', 'VBP'), ('collision', 'NN'), ('new', 'JJ'), ('object', 'JJ'), ('responsive', 'JJ'), ('object', 'NN'), ('localization', 'NN'), ('object', 'JJ'), ('classification', 'NN'), ('respective', 'JJ'), ('new', 'JJ'), ('object', 'JJ'), ('computer-implemented', 'JJ'), ('wherein', 'NN'), ('local', 'JJ'), ('segmentation', 'NN'), ('performed', 'VBD'), ('using', 'VBG'), ('self-similarity', 'JJ'), ('search', 'NN'), ('template', 'NN'), ('matching', 'VBG'), ('provide', 'RB'), ('respective', 'JJ'), ('bounding', 'VBG'), ('box', 'NN'), ('around', 'IN'), ('respective', 'JJ'), ('object', 'JJ'), ('set', 'VBN'), ('computer-implemented', 'JJ'), ('wherein', 'JJ'), ('local', 'JJ'), ('segmentation', 'NN'), ('applied', 'VBD'), ('segment', 'NN'), ('respective', 'JJ'), ('target', 'NN'), ('region', 'NN'), ('therein', 'IN'), ('computer-implemented', 'JJ'), ('wherein', 'JJ'), ('region-based', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('rcnn', 'NN'), ('forms', 'NNS'), ('model', 'VBP'), ('object', 'JJ'), ('training', 'NN'), ('stage', 'NN'), ('detect', 'JJ'), ('objects', 'VBZ'), ('new', 'JJ'), ('inference', 'NN'), ('stage', 'NN'), ('computer-implemented', 'JJ'), ('wherein', 'NN'), ('performed', 'VBD'), ('system', 'NN'), ('selected', 'VBN'), ('group', 'NN'), ('consisting', 'VBG'), ('surveillance', 'NN'), ('system', 'NN'), ('face', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('face', 'NN'), ('system', 'NN'), ('cancer', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('object', 'JJ'), ('system', 'NN'), ('advanced', 'VBD'), ('driver-assistance', 'NN'), ('system', 'NN'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('-based', 'VBD'), ('self-guided', 'JJ'), ('object', 'JJ'), ('detection', 'NN'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('comprising', 'VBG'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('program', 'NN'), ('instructions', 'NNS'), ('embodied', 'VBD'), ('therewith', 'JJ'), ('program', 'NN'), ('instructions', 'NNS'), ('executable', 'JJ'), ('computer', 'NN'), ('cause', 'NN'), ('computer', 'NN'), ('perform', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('device', 'NN'), ('set', 'VBN'), ('respective', 'JJ'), ('grid', 'JJ'), ('thereon', 'NN'), ('labeled', 'VBD'), ('regarding', 'VBG'), ('respective', 'JJ'), ('object', 'NN'), ('detected', 'VBD'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('training', 'NN'), ('device', 'NN'), ('grid-based', 'JJ'), ('object', 'NN'), ('detector', 'NN'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('determining', 'VBG'), ('device', 'NN'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('applying', 'VBG'), ('local', 'JJ'), ('segmentation', 'NN'), ('training', 'NN'), ('device', 'NN'), ('region-based', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('rcnn', 'NN'), ('joint', 'NN'), ('object', 'JJ'), ('localization', 'NN'), ('object', 'NN'), ('classification', 'NN'), ('using', 'VBG'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('input', 'NN'), ('rcnn', 'VBP'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('wherein', 'NN'), ('comprises', 'VBZ'), ('performing', 'VBG'), ('action', 'NN'), ('responsive', 'JJ'), ('object', 'NN'), ('localization', 'NN'), ('object', 'JJ'), ('classification', 'NN'), ('respective', 'JJ'), ('new', 'JJ'), ('object', 'JJ'), ('new', 'JJ'), ('rcnn', 'NN'), ('applied', 'VBN'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('wherein', 'VBZ'), ('action', 'NN'), ('comprises', 'VBZ'), ('autonomously', 'RB'), ('controlling', 'VBG'), ('motor', 'NN'), ('vehicle', 'NN'), ('avoid', 'VBP'), ('collision', 'NN'), ('new', 'JJ'), ('object', 'JJ'), ('responsive', 'JJ'), ('object', 'NN'), ('localization', 'NN'), ('object', 'JJ'), ('classification', 'NN'), ('respective', 'JJ'), ('new', 'JJ'), ('object', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('wherein', 'VBZ'), ('local', 'JJ'), ('segmentation', 'NN'), ('performed', 'VBD'), ('using', 'VBG'), ('self-similarity', 'JJ'), ('search', 'NN'), ('template', 'NN'), ('matching', 'VBG'), ('provide', 'RB'), ('respective', 'JJ'), ('bounding', 'VBG'), ('box', 'NN'), ('around', 'IN'), ('respective', 'JJ'), ('object', 'NN'), ('set', 'VBN'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('wherein', 'VBZ'), ('local', 'JJ'), ('segmentation', 'NN'), ('applied', 'VBD'), ('segment', 'NN'), ('respective', 'JJ'), ('target', 'NN'), ('region', 'NN'), ('therein', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('wherein', 'VBZ'), ('region-based', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('rcnn', 'NN'), ('forms', 'NNS'), ('model', 'VBP'), ('object', 'JJ'), ('training', 'NN'), ('stage', 'NN'), ('detect', 'JJ'), ('objects', 'VBZ'), ('new', 'JJ'), ('inference', 'NN'), ('stage', 'NN'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('wherein', 'NN'), ('performed', 'VBD'), ('system', 'NN'), ('selected', 'VBN'), ('group', 'NN'), ('consisting', 'VBG'), ('surveillance', 'NN'), ('system', 'NN'), ('face', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('face', 'NN'), ('system', 'NN'), ('cancer', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('object', 'JJ'), ('system', 'NN'), ('advanced', 'VBD'), ('driver-assistance', 'NN'), ('system', 'NN'), ('computer', 'NN'), ('processing', 'VBG'), ('system', 'NN'), ('-based', 'VBD'), ('self-guided', 'JJ'), ('object', 'JJ'), ('detection', 'NN'), ('comprising', 'VBG'), ('memory', 'NN'), ('device', 'NN'), ('storing', 'VBG'), ('program', 'NN'), ('code', 'NN'), ('device', 'NN'), ('running', 'VBG'), ('program', 'NN'), ('code', 'NN'), ('receive', 'VBP'), ('set', 'VBN'), ('respective', 'JJ'), ('grid', 'JJ'), ('thereon', 'NN'), ('labeled', 'VBD'), ('regarding', 'VBG'), ('respective', 'JJ'), ('object', 'NN'), ('detected', 'VBD'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('train', 'VBP'), ('grid-based', 'JJ'), ('object', 'NN'), ('detector', 'NN'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('determine', 'VBP'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('applying', 'VBG'), ('local', 'JJ'), ('segmentation', 'NN'), ('train', 'NN'), ('region-based', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('rcnn', 'NN'), ('joint', 'NN'), ('object', 'JJ'), ('localization', 'NN'), ('object', 'NN'), ('classification', 'NN'), ('using', 'VBG'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('input', 'NN'), ('rcnn', 'VBP'), ('computer', 'NN'), ('processing', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('device', 'NN'), ('runs', 'VBZ'), ('program', 'NN'), ('code', 'NN'), ('perform', 'VB'), ('action', 'NN'), ('responsive', 'JJ'), ('object', 'NN'), ('localization', 'NN'), ('object', 'JJ'), ('classification', 'NN'), ('respective', 'JJ'), ('new', 'JJ'), ('object', 'JJ'), ('new', 'JJ'), ('rcnn', 'NN'), ('applied', 'VBN'), ('computer', 'NN'), ('processing', 'VBG'), ('system', 'NN'), ('wherein', 'JJ'), ('action', 'NN'), ('comprises', 'VBZ'), ('autonomously', 'RB'), ('controlling', 'VBG'), ('motor', 'NN'), ('vehicle', 'NN'), ('avoid', 'VBP'), ('collision', 'NN'), ('new', 'JJ'), ('object', 'JJ'), ('responsive', 'JJ'), ('object', 'NN'), ('localization', 'NN'), ('object', 'JJ'), ('classification', 'NN'), ('respective', 'JJ'), ('new', 'JJ'), ('object', 'JJ'), ('computer', 'NN'), ('processing', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('local', 'JJ'), ('segmentation', 'NN'), ('performed', 'VBD'), ('using', 'VBG'), ('self-similarity', 'JJ'), ('search', 'NN'), ('template', 'NN'), ('matching', 'VBG'), ('provide', 'RB'), ('respective', 'JJ'), ('bounding', 'VBG'), ('box', 'NN'), ('around', 'IN'), ('respective', 'JJ'), ('object', 'NN'), ('set', 'VBN'), ('computer', 'NN'), ('processing', 'VBG'), ('system', 'NN'), ('wherein', 'JJ'), ('region-based', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('rcnn', 'NN'), ('forms', 'NNS'), ('model', 'VBP'), ('object', 'JJ'), ('training', 'NN'), ('stage', 'NN'), ('detect', 'JJ'), ('objects', 'VBZ'), ('new', 'JJ'), ('inference', 'NN'), ('stage', 'NN'), ('computer', 'NN'), ('processing', 'NN'), ('system', 'NN'), ('wherein', 'VBP'), ('computer', 'NN'), ('processing', 'NN'), ('system', 'NN'), ('comprised', 'VBD'), ('system', 'NN'), ('selected', 'VBN'), ('group', 'NN'), ('consisting', 'VBG'), ('surveillance', 'NN'), ('system', 'NN'), ('face', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('face', 'NN'), ('system', 'NN'), ('cancer', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('object', 'JJ'), ('system', 'NN'), ('advanced', 'VBD'), ('driver-assistance', 'NN'), ('system', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('utilizing', 'VBG'), ('database', 'NN'), ('normalized', 'VBN'), ('stored', 'VBD'), ('comprising', 'VBG'), ('capturing', 'VBG'), ('using', 'VBG'), ('camera', 'NN'), ('detecting', 'VBG'), ('face', 'NN'), ('captured', 'VBD'), ('normalizing', 'JJ'), ('detected', 'JJ'), ('facial', 'JJ'), ('match', 'NN'), ('normalized', 'VBN'), ('stored', 'VBD'), ('identifying', 'VBG'), ('facial', 'JJ'), ('features', 'NNS'), ('normalized', 'VBN'), ('detected', 'JJ'), ('facial', 'JJ'), ('generating', 'VBG'), ('facial', 'JJ'), ('metrics', 'NNS'), ('facial', 'JJ'), ('features', 'NNS'), ('calculating', 'VBG'), ('euclidean', 'JJ'), ('distances', 'NNS'), ('facial', 'JJ'), ('metrics', 'NNS'), ('normalized', 'VBN'), ('detected', 'JJ'), ('facial', 'JJ'), ('corresponding', 'VBG'), ('facial', 'JJ'), ('metrics', 'NNS'), ('stored', 'VBD'), ('comparing', 'VBG'), ('euclidean', 'JJ'), ('distance', 'NN'), ('predetermined', 'VBD'), ('threshold', 'JJ'), ('responsive', 'JJ'), ('euclidean', 'JJ'), ('distance', 'NN'), ('comparison', 'NN'), ('producing', 'VBG'), ('reduced', 'JJ'), ('candidate', 'JJ'), ('list', 'NN'), ('best', 'JJS'), ('possible', 'JJ'), ('matches', 'NNS'), ('normalized', 'VBN'), ('stored', 'VBD'), ('comparing', 'VBG'), ('parallel', 'RB'), ('normalized', 'VBN'), ('detected', 'VBN'), ('facial', 'JJ'), ('normalized', 'VBN'), ('stored', 'VBD'), ('reduced', 'JJ'), ('candidate', 'NN'), ('list', 'NN'), ('utilizing', 'VBG'), ('face', 'NN'), ('algorithms', 'NN'), ('parallel', 'RB'), ('processing', 'VBG'), ('system', 'NN'), ('uses', 'VBZ'), ('different', 'JJ'), ('face', 'NN'), ('algorithm', 'VBP'), ('responsive', 'JJ'), ('comparison', 'NN'), ('producing', 'VBG'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('parallel', 'JJ'), ('subset', 'NN'), ('reduced', 'VBN'), ('candidate', 'JJ'), ('list', 'NN'), ('selecting', 'VBG'), ('final', 'JJ'), ('match', 'NN'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('using', 'VBG'), ('deep', 'JJ'), ('learning', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('face', 'NN'), ('algorithm', 'NN'), ('trained', 'VBD'), ('outputs', 'NNS'), ('individual', 'JJ'), ('face', 'NN'), ('algorithms', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('detecting', 'VBG'), ('face', 'NN'), ('captured', 'VBD'), ('comprises', 'NNS'), ('utilizing', 'VBG'), ('opencv', 'NN'), ('detect', 'JJ'), ('face', 'NN'), ('captured', 'VBD'), ('extracting', 'VBG'), ('location', 'NN'), ('eyes', 'NNS'), ('tip', 'VBP'), ('nose', 'JJ'), ('face', 'NN'), ('determining', 'VBG'), ('distance', 'NN'), ('eyes', 'NNS'), ('cropping', 'VBG'), ('face', 'NN'), ('captured', 'VBD'), ('width', 'JJ'), ('height', 'NN'), ('cropped', 'VBD'), ('face', 'NN'), ('function', 'NN'), ('distance', 'NN'), ('eyes', 'NNS'), ('rotating', 'VBG'), ('face', 'NN'), ('angle', 'NN'), ('rotation', 'NN'), ('function', 'NN'), ('distance', 'NN'), ('eyes', 'NNS'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('width', 'NN'), ('cropped', 'VBD'), ('face', 'NN'), ('times', 'NNS'), ('distance', 'JJ'), ('eyes', 'NNS'), ('height', 'VBD'), ('cropped', 'VBD'), ('face', 'NN'), ('times', 'NNS'), ('distance', 'JJ'), ('eyes', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('angle', 'NN'), ('formed', 'VBD'), ('straight', 'JJ'), ('line', 'NN'), ('joining', 'VBG'), ('eyes', 'NNS'), ('x-axis', 'JJ'), ('face', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('rotating', 'VBG'), ('face', 'NN'), ('comprises', 'NNS'), ('rotating', 'VBG'), ('face', 'NN'), ('provide', 'VBP'), ('frontal', 'JJ'), ('face', 'NN'), ('pattern', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('comprising', 'VBG'), ('step', 'NN'), ('proportionally', 'RB'), ('rescaling', 'VBG'), ('cropped', 'VBN'), ('rotated', 'VBN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('proportional', 'JJ'), ('rescaling', 'NN'), ('yields', 'NNS'), ('cropped', 'VBD'), ('rotated', 'JJ'), ('size', 'NN'), ('=', 'NN'), ('pixels', 'NNS'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'VBP'), ('facial', 'JJ'), ('features', 'NNS'), ('identified', 'VBN'), ('normalized', 'JJ'), ('detected', 'VBN'), ('facial', 'JJ'), ('comprise', 'NN'), ('pair', 'NN'), ('eyes', 'NNS'), ('tip', 'VBP'), ('nose', 'JJ'), ('mouth', 'NN'), ('center', 'NN'), ('mouth', 'NN'), ('chin', 'JJ'), ('area', 'NN'), ('comprising', 'VBG'), ('bottom', 'JJ'), ('top', 'JJ'), ('left', 'VBN'), ('landmark', 'NN'), ('top', 'JJ'), ('right', 'NN'), ('landmark', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('generating', 'VBG'), ('facial', 'JJ'), ('metrics', 'NNS'), ('comprises', 'NNS'), ('calculating', 'VBG'), ('distance', 'NN'), ('pair', 'NN'), ('eyes', 'NNS'), ('distance', 'VB'), ('eyes', 'NNS'), ('tip', 'VBP'), ('nose', 'JJ'), ('distance', 'NN'), ('equal', 'JJ'), ('width', 'NN'), ('mouth', 'NN'), ('distance', 'NN'), ('tip', 'NN'), ('nose', 'RB'), ('center', 'JJ'), ('mouth', 'NN'), ('distance', 'NN'), ('bottom', 'NN'), ('chin', 'NN'), ('center', 'NN'), ('mouth', 'NN'), ('distance', 'NN'), ('top', 'NN'), ('left', 'VBD'), ('landmark', 'NN'), ('chin', 'NN'), ('tip', 'NN'), ('nose', 'JJ'), ('distance', 'NN'), ('top', 'JJ'), ('right', 'NN'), ('landmark', 'NN'), ('chin', 'JJ'), ('tip', 'NN'), ('nose', 'RB'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('performing', 'VBG'), ('euclidean', 'JJ'), ('distance', 'NN'), ('match', 'NN'), ('comprises', 'VBZ'), ('partitioning', 'VBG'), ('normalized', 'VBN'), ('stored', 'VBN'), ('substantially', 'RB'), ('equal', 'JJ'), ('subsets', 'NNS'), ('performing', 'VBG'), ('euclidean', 'JJ'), ('distance', 'NN'), ('match', 'NN'), ('facial', 'JJ'), ('metrics', 'NNS'), ('normalized', 'VBN'), ('detected', 'JJ'), ('facial', 'JJ'), ('corresponding', 'VBG'), ('facial', 'JJ'), ('metrics', 'NNS'), ('stored', 'VBD'), ('subsets', 'NNS'), ('normalized', 'VBN'), ('stored', 'JJ'), ('separate', 'JJ'), ('parallel', 'NN'), ('processing', 'NN'), ('system', 'NN'), ('generate', 'JJ'), ('euclidean', 'JJ'), ('distance', 'NN'), ('stored', 'VBD'), ('subset', 'JJ'), ('comparing', 'VBG'), ('euclidean', 'JJ'), ('distance', 'NN'), ('predetermined', 'VBD'), ('threshold', 'JJ'), ('separate', 'JJ'), ('responsive', 'JJ'), ('euclidean', 'JJ'), ('distance', 'NN'), ('comparison', 'NN'), ('producing', 'VBG'), ('reduced', 'JJ'), ('candidate', 'JJ'), ('list', 'NN'), ('best', 'JJS'), ('possible', 'JJ'), ('matches', 'NNS'), ('normalized', 'VBN'), ('stored', 'JJ'), ('subset', 'NN'), ('combining', 'NN'), ('reduced', 'JJ'), ('candidate', 'NN'), ('lists', 'NNS'), ('subset', 'VBP'), ('produce', 'VBP'), ('single', 'JJ'), ('reduced', 'VBN'), ('candidate', 'JJ'), ('list', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('face', 'NN'), ('algorithms', 'VBP'), ('utilized', 'JJ'), ('comparing', 'NN'), ('parallel', 'RB'), ('normalized', 'VBN'), ('detected', 'VBN'), ('facial', 'JJ'), ('normalized', 'VBN'), ('stored', 'VBD'), ('reduced', 'JJ'), ('candidate', 'NN'), ('list', 'NN'), ('consists', 'VBZ'), ('face', 'VBP'), ('algorithms', 'RB'), ('selected', 'VBN'), ('group', 'NN'), ('consisting', 'VBG'), ('principle', 'JJ'), ('component', 'JJ'), ('analysis', 'NN'), ('pca-based', 'JJ'), ('algorithms', 'NN'), ('linear', 'JJ'), ('discriminant', 'JJ'), ('analysis', 'NN'), ('lda', 'NN'), ('algorithms', 'JJ'), ('independent', 'JJ'), ('component', 'NN'), ('analysis', 'NN'), ('ica', 'NN'), ('algorithms', 'IN'), ('kernel-based', 'JJ'), ('algorithms', 'JJ'), ('feature-based', 'JJ'), ('techniques', 'NNS'), ('algorithms', 'VBP'), ('based', 'VBN'), ('neural', 'JJ'), ('networks', 'NNS'), ('algorithms', 'VBP'), ('based', 'VBN'), ('transforms', 'NNS'), ('model-based', 'JJ'), ('face', 'NN'), ('algorithms', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'IN'), ('pca-based', 'JJ'), ('algorithms', 'NNS'), ('include', 'VBP'), ('eigen', 'JJ'), ('face', 'NN'), ('detection', 'NN'), ('lda', 'VBZ'), ('algorithms', 'JJ'), ('include', 'VBP'), ('fisher', 'JJ'), ('face', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('comparing', 'VBG'), ('parallel', 'RB'), ('captured', 'VBN'), ('normalized', 'JJ'), ('stored', 'VBD'), ('reduced', 'JJ'), ('candidate', 'NN'), ('list', 'NN'), ('comprises', 'VBZ'), ('partitioning', 'VBG'), ('reduced', 'VBN'), ('candidate', 'JJ'), ('list', 'NN'), ('substantially', 'RB'), ('equal', 'JJ'), ('subsets', 'NNS'), ('processing', 'VBG'), ('subset', 'NN'), ('different', 'JJ'), ('parallel', 'RB'), ('processing', 'VBG'), ('system', 'NN'), ('uses', 'VBZ'), ('unique', 'JJ'), ('face', 'NN'), ('algorithm', 'NN'), ('produce', 'VBP'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('using', 'VBG'), ('reduce', 'VB'), ('function', 'NN'), ('mapreduce', 'NN'), ('program', 'NN'), ('combine', 'NN'), ('best', 'RBS'), ('match', 'NN'), ('results', 'NNS'), ('subsets', 'NNS'), ('produce', 'VBP'), ('single', 'JJ'), ('set', 'NN'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('partitioning', 'VBG'), ('reduced', 'VBN'), ('candidate', 'JJ'), ('list', 'NN'), ('comprises', 'NNS'), ('selecting', 'VBG'), ('comprising', 'VBG'), ('subset', 'NN'), ('optimizing', 'VBG'), ('variance', 'NN'), ('according', 'VBG'), ('following', 'VBG'), ('equation', 'NN'), ('n', 'IN'), ('number', 'NN'), ('rows', 'NNS'), ('columns', 'VBP'), ('face', 'NN'), ('vector', 'NN'), ('n', 'IN'), ('number', 'NN'), ('groups', 'NNS'), ('σij', 'VBP'), ('standard', 'JJ'), ('deviation', 'NN'), ('dimension', 'NN'), ('group', 'NN'), ('j', 'VBD'), ('face', 'NN'), ('vector', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('comprising', 'VBG'), ('subset', 'NN'), ('optimizing', 'VBG'), ('variance', 'NN'), ('according', 'VBG'), ('following', 'VBG'), ('equation', 'NN'), ('dμi', 'FW'), ('μj', 'JJ'), ('euclidean', 'JJ'), ('distance', 'NN'), ('mean', 'NN'), ('group', 'NN'), ('mean', 'VBD'), ('group', 'NN'), ('j', 'NN'), ('face', 'NN'), ('vector', 'NN'), ('l', 'JJ'), ('number', 'NN'), ('group', 'NN'), ('levels', 'NNS'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('selecting', 'VBG'), ('final', 'JJ'), ('match', 'NN'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('utilizing', 'JJ'), ('deep', 'JJ'), ('learning', 'NN'), ('neural', 'JJ'), ('network', 'NN'), ('face', 'NN'), ('algorithm', 'NN'), ('comprises', 'VBZ'), ('utilizing', 'VBG'), ('either', 'DT'), ('adaboost', 'JJ'), ('machine-learning', 'JJ'), ('algorithm', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('machine-learning', 'JJ'), ('model', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('normalizing', 'VBG'), ('detected', 'VBN'), ('facial', 'JJ'), ('match', 'NN'), ('normalized', 'VBN'), ('stored', 'VBD'), ('includes', 'VBZ'), ('normalizing', 'NN'), ('detected', 'VBN'), ('facial', 'JJ'), ('size', 'NN'), ('illumination', 'NN'), ('normalized', 'VBN'), ('stored', 'JJ'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('containing', 'VBG'), ('executable', 'JJ'), ('program', 'NN'), ('instructions', 'NNS'), ('causing', 'VBG'), ('computer', 'NN'), ('perform', 'NN'), ('face', 'NN'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('face', 'NN'), ('captured', 'VBD'), ('camera', 'NN'), ('normalizing', 'NN'), ('detected', 'VBN'), ('facial', 'JJ'), ('match', 'NN'), ('normalized', 'VBN'), ('stored', 'VBD'), ('identifying', 'VBG'), ('facial', 'JJ'), ('features', 'NNS'), ('normalized', 'VBN'), ('detected', 'JJ'), ('facial', 'JJ'), ('generating', 'VBG'), ('facial', 'JJ'), ('metrics', 'NNS'), ('facial', 'JJ'), ('features', 'NNS'), ('calculating', 'VBG'), ('euclidean', 'JJ'), ('distances', 'NNS'), ('facial', 'JJ'), ('metrics', 'NNS'), ('normalized', 'VBN'), ('detected', 'JJ'), ('facial', 'JJ'), ('corresponding', 'VBG'), ('facial', 'JJ'), ('metrics', 'NNS'), ('stored', 'VBD'), ('comparing', 'VBG'), ('euclidean', 'JJ'), ('distance', 'NN'), ('predetermined', 'VBD'), ('threshold', 'JJ'), ('responsive', 'JJ'), ('euclidean', 'JJ'), ('distance', 'NN'), ('comparison', 'NN'), ('producing', 'VBG'), ('reduced', 'JJ'), ('candidate', 'JJ'), ('list', 'NN'), ('best', 'JJS'), ('possible', 'JJ'), ('matches', 'NNS'), ('normalized', 'VBN'), ('stored', 'VBD'), ('comparing', 'VBG'), ('parallel', 'RB'), ('captured', 'VBN'), ('normalized', 'JJ'), ('stored', 'VBD'), ('reduced', 'JJ'), ('candidate', 'NN'), ('list', 'NN'), ('utilizing', 'VBG'), ('face', 'NN'), ('algorithms', 'NN'), ('parallel', 'RB'), ('processing', 'VBG'), ('system', 'NN'), ('uses', 'VBZ'), ('different', 'JJ'), ('face', 'NN'), ('algorithm', 'VBP'), ('responsive', 'JJ'), ('comparison', 'NN'), ('producing', 'VBG'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('parallel', 'JJ'), ('subset', 'NN'), ('reduced', 'VBN'), ('candidate', 'JJ'), ('list', 'NN'), ('selecting', 'VBG'), ('final', 'JJ'), ('match', 'NN'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('using', 'VBG'), ('deep', 'JJ'), ('learning', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('face', 'NN'), ('algorithm', 'NN'), ('trained', 'VBD'), ('outputs', 'NNS'), ('individual', 'JJ'), ('face', 'NN'), ('algorithms', 'IN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('containing', 'VBG'), ('executable', 'JJ'), ('program', 'NN'), ('instructions', 'NNS'), ('wherein', 'VBP'), ('face', 'NN'), ('algorithms', 'NN'), ('utilized', 'JJ'), ('comparing', 'NN'), ('parallel', 'RB'), ('normalized', 'VBN'), ('detected', 'VBN'), ('facial', 'JJ'), ('normalized', 'VBN'), ('stored', 'VBD'), ('reduced', 'JJ'), ('candidate', 'NN'), ('list', 'NN'), ('consists', 'VBZ'), ('face', 'VBP'), ('algorithms', 'RB'), ('selected', 'VBN'), ('group', 'NN'), ('consisting', 'VBG'), ('principle', 'JJ'), ('component', 'JJ'), ('analysis', 'NN'), ('pca-based', 'JJ'), ('algorithms', 'NN'), ('linear', 'JJ'), ('discriminant', 'JJ'), ('analysis', 'NN'), ('lda', 'NN'), ('algorithms', 'JJ'), ('independent', 'JJ'), ('component', 'NN'), ('analysis', 'NN'), ('ica', 'NN'), ('algorithms', 'IN'), ('kernel-based', 'JJ'), ('algorithms', 'JJ'), ('feature-based', 'JJ'), ('techniques', 'NNS'), ('algorithms', 'VBP'), ('based', 'VBN'), ('neural', 'JJ'), ('networks', 'NNS'), ('algorithms', 'VBP'), ('based', 'VBN'), ('transforms', 'NNS'), ('model-based', 'JJ'), ('face', 'NN'), ('algorithms', 'IN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('containing', 'VBG'), ('executable', 'JJ'), ('program', 'NN'), ('instructions', 'NNS'), ('wherein', 'VBP'), ('pca-based', 'JJ'), ('algorithms', 'NNS'), ('include', 'VBP'), ('eigen', 'JJ'), ('face', 'NN'), ('detection', 'NN'), ('lda', 'VBZ'), ('algorithms', 'JJ'), ('include', 'VBP'), ('fisher', 'JJ'), ('face', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('containing', 'VBG'), ('executable', 'JJ'), ('program', 'NN'), ('instructions', 'NNS'), ('selecting', 'VBG'), ('final', 'JJ'), ('match', 'NN'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('utilizing', 'JJ'), ('deep', 'JJ'), ('learning', 'NN'), ('neural', 'JJ'), ('network', 'NN'), ('face', 'NN'), ('algorithm', 'NN'), ('comprises', 'VBZ'), ('utilizing', 'VBG'), ('either', 'DT'), ('adaboost', 'JJ'), ('machine-learning', 'JJ'), ('algorithm', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('machine-learning', 'JJ'), ('model', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('comprising', 'VBG'), ('condensing', 'VBG'), ('lens', 'NNS'), ('sensor', 'NN'), ('configured', 'VBD'), ('detect', 'JJ'), ('light', 'JJ'), ('passing', 'VBG'), ('condensing', 'VBG'), ('lens', 'NNS'), ('comprising', 'VBG'), ('pixel', 'NN'), ('matrix', 'NN'), ('wherein', 'NN'), ('pixel', 'NN'), ('matrix', 'NN'), ('comprises', 'VBZ'), ('phase', 'JJ'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('configured', 'VBN'), ('turn', 'VBP'), ('phase', 'JJ'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('autofocusing', 'VBG'), ('output', 'NN'), ('autofocused', 'VBD'), ('pixel', 'JJ'), ('data', 'NNS'), ('completing', 'VBG'), ('autofocusing', 'VBG'), ('divide', 'NN'), ('autofocused', 'VBD'), ('pixel', 'NN'), ('data', 'NNS'), ('first', 'RB'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('calculate', 'NN'), ('features', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('first', 'JJ'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('wherein', 'NN'), ('features', 'VBZ'), ('comprise', 'VBP'), ('module', 'JJ'), ('widths', 'NNS'), ('finder', 'VBP'), ('pattern', 'JJ'), ('finder', 'NN'), ('pattern', 'NN'), ('predetermined', 'VBN'), ('ratio', 'JJ'), ('harr-like', 'JJ'), ('feature', 'NN'), ('gabor', 'NN'), ('feature', 'NN'), ('determine', 'NN'), ('operating', 'VBG'), ('resolution', 'NN'), ('regular', 'JJ'), ('pixels', 'NNS'), ('according', 'VBG'), ('features', 'NNS'), ('calculated', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('divided', 'VBD'), ('autofocused', 'JJ'), ('pixel', 'NN'), ('data', 'NNS'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'JJ'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NN'), ('comprises', 'VBZ'), ('first', 'JJ'), ('pixel', 'JJ'), ('second', 'JJ'), ('pixel', 'NN'), ('cover', 'NN'), ('layer', 'NN'), ('covering', 'VBG'), ('upon', 'IN'), ('first', 'JJ'), ('region', 'NN'), ('first', 'RB'), ('pixel', 'VBZ'), ('upon', 'IN'), ('second', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('pixel', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('region', 'NN'), ('mirror', 'NN'), ('symmetrical', 'JJ'), ('microlens', 'NNS'), ('aligned', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('pixel', 'JJ'), ('second', 'JJ'), ('pixel', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('region', 'NN'), ('%', 'NN'), ('%', 'NN'), ('area', 'NN'), ('single', 'JJ'), ('pixel', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('configured', 'VBD'), ('perform', 'NN'), ('autofocusing', 'VBG'), ('using', 'VBG'), ('dual', 'JJ'), ('pixel', 'NN'), ('autofocus', 'NN'), ('technique', 'NN'), ('according', 'VBG'), ('pixel', 'NN'), ('data', 'NNS'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('completing', 'VBG'), ('autofocusing', 'VBG'), ('imaging', 'JJ'), ('device', 'NN'), ('ed', 'NN'), ('wherein', 'NN'), ('configured', 'VBD'), ('divide', 'JJ'), ('pixel', 'NN'), ('data', 'NNS'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('third', 'JJ'), ('subframe', 'JJ'), ('fourth', 'JJ'), ('subframe', 'NN'), ('completing', 'VBG'), ('autofocusing', 'VBG'), ('perform', 'NN'), ('autofocusing', 'VBG'), ('according', 'VBG'), ('third', 'JJ'), ('subframe', 'JJ'), ('fourth', 'JJ'), ('subframe', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('configured', 'VBD'), ('calibrate', 'JJ'), ('brightness', 'JJ'), ('third', 'JJ'), ('subframe', 'NN'), ('fourth', 'JJ'), ('subframe', 'JJ'), ('identical', 'JJ'), ('using', 'VBG'), ('shading', 'VBG'), ('algorithm', 'JJ'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('operating', 'VBG'), ('resolution', 'NN'), ('selected', 'VBN'), ('first', 'JJ'), ('resolution', 'NN'), ('smaller', 'JJR'), ('number', 'NN'), ('regular', 'JJ'), ('pixels', 'NNS'), ('second', 'JJ'), ('resolution', 'NN'), ('larger', 'JJR'), ('first', 'JJ'), ('resolution', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'NN'), ('wherein', 'IN'), ('regular', 'JJ'), ('pixels', 'NNS'), ('turned', 'VBD'), ('autofocusing', 'VBG'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'JJ'), ('number', 'NN'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'VBZ'), ('smaller', 'JJR'), ('regular', 'JJ'), ('pixels', 'NNS'), ('imaging', 'VBG'), ('device', 'NN'), ('comprising', 'VBG'), ('condensing', 'VBG'), ('lens', 'NNS'), ('sensor', 'NN'), ('configured', 'VBD'), ('detect', 'JJ'), ('light', 'JJ'), ('passing', 'VBG'), ('condensing', 'VBG'), ('lens', 'NNS'), ('comprising', 'VBG'), ('pixel', 'NN'), ('matrix', 'NN'), ('wherein', 'NN'), ('pixel', 'NN'), ('matrix', 'NN'), ('comprises', 'VBZ'), ('phase', 'JJ'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('configured', 'VBN'), ('turn', 'VBP'), ('phase', 'JJ'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('autofocusing', 'VBG'), ('output', 'NN'), ('autofocused', 'VBD'), ('pixel', 'JJ'), ('data', 'NNS'), ('completing', 'VBG'), ('autofocusing', 'VBG'), ('divide', 'NN'), ('autofocused', 'VBD'), ('pixel', 'NN'), ('data', 'NNS'), ('first', 'RB'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('calculate', 'NN'), ('features', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('first', 'JJ'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('wherein', 'NN'), ('features', 'VBZ'), ('comprise', 'VBP'), ('module', 'JJ'), ('widths', 'NNS'), ('finder', 'VBP'), ('pattern', 'JJ'), ('finder', 'NN'), ('pattern', 'NN'), ('predetermined', 'VBN'), ('ratio', 'JJ'), ('harr-like', 'JJ'), ('feature', 'NN'), ('gabor', 'NN'), ('feature', 'NN'), ('select', 'VBP'), ('decoding', 'VBG'), ('using', 'VBG'), ('pixel', 'JJ'), ('data', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('according', 'VBG'), ('features', 'NNS'), ('calculated', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('divided', 'VBD'), ('autofocused', 'JJ'), ('pixel', 'NN'), ('data', 'NNS'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'JJ'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NN'), ('comprises', 'VBZ'), ('first', 'JJ'), ('pixel', 'JJ'), ('second', 'JJ'), ('pixel', 'NN'), ('cover', 'NN'), ('layer', 'NN'), ('covering', 'VBG'), ('upon', 'IN'), ('first', 'JJ'), ('region', 'NN'), ('first', 'RB'), ('pixel', 'VBZ'), ('upon', 'IN'), ('second', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('pixel', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('region', 'NN'), ('mirror', 'NN'), ('symmetrical', 'JJ'), ('microlens', 'NNS'), ('aligned', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('pixel', 'JJ'), ('second', 'JJ'), ('pixel', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('configured', 'VBD'), ('perform', 'NN'), ('autofocusing', 'VBG'), ('using', 'VBG'), ('dual', 'JJ'), ('pixel', 'NN'), ('autofocus', 'NN'), ('technique', 'NN'), ('according', 'VBG'), ('pixel', 'NN'), ('data', 'NNS'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('completing', 'VBG'), ('autofocusing', 'VBG'), ('imaging', 'JJ'), ('device', 'NN'), ('ed', 'NN'), ('wherein', 'NN'), ('configured', 'VBD'), ('divide', 'JJ'), ('pixel', 'NN'), ('data', 'NNS'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('third', 'JJ'), ('subframe', 'JJ'), ('fourth', 'JJ'), ('subframe', 'NN'), ('completing', 'VBG'), ('autofocusing', 'VBG'), ('calibrate', 'JJ'), ('brightness', 'JJ'), ('third', 'JJ'), ('subframe', 'NN'), ('fourth', 'JJ'), ('subframe', 'JJ'), ('identical', 'JJ'), ('using', 'VBG'), ('shading', 'VBG'), ('algorithm', 'JJ'), ('perform', 'NN'), ('autofocusing', 'VBG'), ('according', 'VBG'), ('third', 'JJ'), ('subframe', 'JJ'), ('fourth', 'JJ'), ('subframe', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('configured', 'VBD'), ('calculate', 'NN'), ('features', 'NNS'), ('using', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('rule', 'NN'), ('based', 'VBN'), ('algorithm', 'RB'), ('machine', 'NN'), ('learning', 'VBG'), ('algorithm', 'JJ'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('decoding', 'VBG'), ('decoding', 'VBG'), ('qr', 'NN'), ('codes', 'NNS'), ('face', 'VBP'), ('operating', 'VBG'), ('imaging', 'VBG'), ('device', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('comprising', 'VBG'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('operating', 'VBG'), ('comprising', 'VBG'), ('turning', 'VBG'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('autofocusing', 'VBG'), ('outputting', 'VBG'), ('autofocused', 'JJ'), ('frame', 'NN'), ('completing', 'VBG'), ('autofocusing', 'VBG'), ('dividing', 'VBG'), ('autofocused', 'JJ'), ('frame', 'NN'), ('acquired', 'VBD'), ('phase', 'JJ'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('first', 'RB'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('calculating', 'VBG'), ('features', 'NNS'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('wherein', 'JJ'), ('feature', 'NN'), ('comprise', 'NN'), ('module', 'NN'), ('widths', 'NNS'), ('finder', 'VBP'), ('pattern', 'JJ'), ('finder', 'NN'), ('pattern', 'NN'), ('predetermined', 'VBN'), ('ratio', 'JJ'), ('harr-like', 'JJ'), ('feature', 'NN'), ('gabor', 'NN'), ('feature', 'NN'), ('selectively', 'RB'), ('activating', 'VBG'), ('least', 'JJS'), ('part', 'NN'), ('regular', 'JJ'), ('pixels', 'NNS'), ('according', 'VBG'), ('features', 'NNS'), ('calculated', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('divided', 'VBD'), ('autofocused', 'JJ'), ('frame', 'NN'), ('operating', 'VBG'), ('ed', 'NN'), ('wherein', 'NN'), ('selectively', 'RB'), ('activating', 'VBG'), ('comprises', 'NNS'), ('activating', 'VBG'), ('first', 'JJ'), ('part', 'NN'), ('regular', 'JJ'), ('pixels', 'NNS'), ('perform', 'VBP'), ('decoding', 'VBG'), ('according', 'VBG'), ('pixel', 'NN'), ('data', 'NNS'), ('first', 'JJ'), ('part', 'NN'), ('regular', 'JJ'), ('pixels', 'NNS'), ('activating', 'VBG'), ('regular', 'JJ'), ('pixels', 'NNS'), ('perform', 'VBP'), ('according', 'VBG'), ('pixel', 'NN'), ('data', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('operating', 'VBG'), ('ed', 'NN'), ('wherein', 'NN'), ('pixel', 'NN'), ('data', 'NNS'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('captured', 'VBD'), ('frame', 'JJ'), ('pixel', 'NN'), ('data', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('also', 'RB'), ('used', 'VBD'), ('performing', 'VBG'), ('decoding', 'VBG'), ('operating', 'VBG'), ('ed', 'NN'), ('wherein', 'NN'), ('decoding', 'VBG'), ('decoding', 'VBG'), ('qr', 'NN'), ('codes', 'NNS'), ('face', 'VBP'), ('operating', 'VBG'), ('ed', 'NN'), ('wherein', 'NN'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('partially', 'RB'), ('covered', 'VBD'), ('pixels', 'NNS'), ('structure', 'NN'), ('dual', 'JJ'), ('pixel', 'NN'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('first', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('configured', 'VBD'), ('obtain', 'VB'), ('first', 'JJ'), ('object', 'JJ'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('second', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('configured', 'VBD'), ('obtain', 'VB'), ('second', 'JJ'), ('object', 'JJ'), ('second', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('different', 'JJ'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('generator', 'NN'), ('configured', 'VBD'), ('generate', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('first', 'RB'), ('based', 'VBN'), ('first', 'JJ'), ('second', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('generator', 'NN'), ('configured', 'VBD'), ('generate', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('second', 'NN'), ('based', 'VBN'), ('first', 'RB'), ('second', 'JJ'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('apparatus', 'NN'), ('wherein', 'VBP'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('narrow', 'JJ'), ('angle', 'JJ'), ('second', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('wider', 'VBP'), ('angle', 'NN'), ('apparatus', 'NN'), ('wherein', 'JJ'), ('second', 'NN'), ('divided', 'VBD'), ('primary', 'JJ'), ('region', 'NN'), ('residual', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('generator', 'NN'), ('comprises', 'VBZ'), ('relationship', 'NN'), ('estimating', 'VBG'), ('module', 'NN'), ('configured', 'VBD'), ('estimate', 'NN'), ('relationship', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('residual', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('estimating', 'VBG'), ('module', 'NN'), ('configured', 'VBD'), ('estimate', 'NN'), ('depth', 'NN'), ('map', 'VBP'), ('residual', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('estimated', 'VBN'), ('relationship', 'NN'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('apparatus', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('relationship', 'NN'), ('estimating', 'VBG'), ('module', 'NN'), ('depth', 'NN'), ('map', 'NN'), ('estimating', 'VBG'), ('module', 'NN'), ('performs', 'NNS'), ('estimating', 'VBG'), ('operation', 'NN'), ('based', 'VBN'), ('neural', 'JJ'), ('network', 'NN'), ('module', 'NN'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('depth', 'JJ'), ('map', 'JJ'), ('fusion', 'NN'), ('unit', 'NN'), ('configured', 'VBD'), ('generate', 'JJ'), ('third', 'JJ'), ('depth', 'NN'), ('map', 'JJ'), ('second', 'JJ'), ('performing', 'VBG'), ('fusion', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('first', 'RB'), ('depth', 'JJ'), ('map', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('apparatus', 'NN'), ('wherein', 'NN'), ('depth', 'NN'), ('map', 'JJ'), ('fusion', 'NN'), ('unit', 'NN'), ('comprises', 'VBZ'), ('tone', 'CD'), ('mapping', 'NN'), ('module', 'NN'), ('configured', 'VBD'), ('generate', 'JJ'), ('tone-mapped', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('correspond', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('performing', 'VBG'), ('bias', 'JJ'), ('removing', 'VBG'), ('operation', 'NN'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('fusion', 'NN'), ('module', 'NN'), ('configured', 'VBD'), ('generate', 'JJ'), ('third', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('fusing', 'VBG'), ('tone-mapped', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('apparatus', 'NN'), ('wherein', 'NN'), ('depth', 'NN'), ('map', 'JJ'), ('fusion', 'NN'), ('unit', 'NN'), ('comprises', 'VBZ'), ('propagating', 'VBG'), ('module', 'NN'), ('configured', 'VBD'), ('generate', 'NN'), ('propagated', 'VBN'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('second', 'JJ'), ('iterated', 'VBD'), ('propagating', 'VBG'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('based', 'VBN'), ('first', 'RB'), ('depth', 'JJ'), ('map', 'JJ'), ('second', 'JJ'), ('fusion', 'NN'), ('module', 'NN'), ('generates', 'VBZ'), ('third', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('fusing', 'VBG'), ('tone-mapped', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('propagated', 'VBD'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('apparatus', 'NN'), ('wherein', 'NN'), ('depth', 'NN'), ('map', 'JJ'), ('fusion', 'NN'), ('unit', 'NN'), ('comprises', 'VBZ'), ('post-processing', 'JJ'), ('module', 'NN'), ('configured', 'VBD'), ('perform', 'JJ'), ('post-processing', 'JJ'), ('operation', 'NN'), ('third', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('generated', 'VBD'), ('fusion', 'NN'), ('module', 'NN'), ('provide', 'IN'), ('post-processed', 'JJ'), ('third', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('apparatus', 'NN'), ('wherein', 'VBD'), ('post-processing', 'JJ'), ('module', 'NN'), ('performs', 'NNS'), ('post-processing', 'JJ'), ('operation', 'NN'), ('filtering', 'VBG'), ('interface', 'NN'), ('generated', 'VBD'), ('third', 'JJ'), ('depth', 'JJ'), ('map', 'NN'), ('accordance', 'NN'), ('fusion', 'NN'), ('fusion', 'NN'), ('module', 'NN'), ('apparatus', 'IN'), ('wherein', 'JJ'), ('post-processing', 'JJ'), ('module', 'NN'), ('removes', 'VBZ'), ('artifacts', 'NNS'), ('generated', 'VBD'), ('third', 'JJ'), ('depth', 'JJ'), ('map', 'NN'), ('accordance', 'NN'), ('fusion', 'NN'), ('fusion', 'NN'), ('module', 'NN'), ('apparatus', 'NN'), ('wherein', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('generator', 'NN'), ('analyses', 'VBZ'), ('distance', 'NN'), ('relationship', 'NN'), ('first', 'JJ'), ('second', 'NN'), ('generates', 'NNS'), ('first', 'RB'), ('depth', 'VB'), ('map', 'NN'), ('first', 'RB'), ('based', 'VBN'), ('distance', 'NN'), ('relationship', 'NN'), ('processing', 'VBG'), ('electronic', 'JJ'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('obtaining', 'VBG'), ('first', 'JJ'), ('object', 'NN'), ('using', 'VBG'), ('first', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('obtaining', 'VBG'), ('second', 'JJ'), ('object', 'JJ'), ('using', 'VBG'), ('second', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('generating', 'VBG'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('first', 'RB'), ('based', 'VBN'), ('first', 'JJ'), ('second', 'JJ'), ('estimating', 'VBG'), ('relationship', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('residual', 'JJ'), ('region', 'NN'), ('second', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('second', 'JJ'), ('generating', 'VBG'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('second', 'JJ'), ('based', 'VBN'), ('estimated', 'VBN'), ('relationship', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('residual', 'JJ'), ('region', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('wherein', 'NN'), ('electronic', 'JJ'), ('apparatus', 'NN'), ('comprises', 'NNS'), ('first', 'RB'), ('camera', 'VBP'), ('module', 'NN'), ('including', 'VBG'), ('first', 'JJ'), ('lens', 'NNS'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('second', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('including', 'VBG'), ('second', 'JJ'), ('lens', 'JJ'), ('second', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('wider', 'VBP'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('wherein', 'VBP'), ('generating', 'VBG'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('comprises', 'VBZ'), ('estimating', 'VBG'), ('depth', 'NN'), ('map', 'FW'), ('residual', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('estimated', 'VBN'), ('relationship', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('residual', 'JJ'), ('region', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'JJ'), ('generating', 'VBG'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NNS'), ('based', 'VBN'), ('depth', 'JJ'), ('map', 'JJ'), ('residual', 'JJ'), ('region', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('wherein', 'NN'), ('estimating', 'VBG'), ('relationship', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('second', 'NN'), ('performed', 'VBD'), ('using', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('model', 'NN'), ('comprising', 'VBG'), ('performing', 'VBG'), ('pre-processing', 'JJ'), ('operation', 'NN'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('generating', 'VBG'), ('third', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('residual', 'JJ'), ('fusing', 'VBG'), ('second', 'JJ'), ('depth', 'JJ'), ('map', 'NN'), ('pre-processing', 'JJ'), ('operation', 'NN'), ('performed', 'VBD'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('wherein', 'IN'), ('performing', 'VBG'), ('pre-processing', 'JJ'), ('operation', 'NN'), ('comprises', 'NNS'), ('performing', 'VBG'), ('tone', 'NN'), ('mapping', 'NN'), ('operation', 'NN'), ('depth', 'NN'), ('map', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('depth', 'NN'), ('map', 'VBP'), ('residual', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('operating', 'VBG'), ('electronic', 'JJ'), ('apparatus', 'NN'), ('electronic', 'JJ'), ('apparatus', 'NN'), ('including', 'VBG'), ('first', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('providing', 'VBG'), ('first', 'JJ'), ('object', 'NN'), ('using', 'VBG'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('second', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('providing', 'VBG'), ('second', 'JJ'), ('object', 'JJ'), ('using', 'VBG'), ('second', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('wider', 'VBP'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('generating', 'VBG'), ('depth', 'NN'), ('map', 'FW'), ('second', 'JJ'), ('based', 'VBN'), ('primary', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('residual', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('operating', 'NN'), ('comprising', 'VBG'), ('generating', 'VBG'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('estimating', 'VBG'), ('relationship', 'NN'), ('first', 'JJ'), ('second', 'JJ'), ('estimating', 'VBG'), ('relationship', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('residual', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('second', 'JJ'), ('generating', 'VBG'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'JJ'), ('second', 'JJ'), ('estimating', 'VBG'), ('depth', 'NN'), ('map', 'FW'), ('second', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('estimated', 'VBN'), ('relationship', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('residual', 'JJ'), ('region', 'NN'), ('generating', 'VBG'), ('depth', 'JJ'), ('map', 'JJ'), ('second', 'NN'), ('fusing', 'VBG'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('operation', 'NN'), ('comprising', 'VBG'), ('executing', 'VBG'), ('application', 'NN'), ('applies', 'NNS'), ('effect', 'NN'), ('second', 'NN'), ('based', 'VBN'), ('depth', 'NN'), ('map', 'JJ'), ('residual', 'JJ'), ('operation', 'NN'), ('wherein', 'WRB'), ('application', 'NN'), ('applies', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('effect', 'NN'), ('auto-focusing', 'JJ'), ('out-focusing', 'JJ'), ('forebackground', 'NN'), ('separation', 'NN'), ('face', 'NN'), ('object', 'JJ'), ('detection', 'NN'), ('within', 'IN'), ('frame', 'NN'), ('augmented', 'JJ'), ('reality', 'NN'), ('second', 'NN'), ('based', 'VBN'), ('depth', 'JJ'), ('map', 'JJ'), ('second', 'JJ'), ('payment', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('comprising', 'VBG'), ('acquiring', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('target', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('wherein', 'IN'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('includes', 'VBZ'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('target', 'NN'), ('willingness', 'NN'), ('pay', 'NN'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('including', 'VBG'), ('determining', 'VBG'), ('whether', 'IN'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('less', 'RBR'), ('angle', 'JJ'), ('threshold', 'NN'), ('wherein', 'NN'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('includes', 'VBZ'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('greater', 'JJR'), ('probability', 'NN'), ('threshold', 'NN'), ('wherein', 'NN'), ('gaze', 'JJ'), ('information', 'NN'), ('includes', 'VBZ'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('less', 'RBR'), ('angle', 'JJ'), ('threshold', 'NN'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('greater', 'JJR'), ('probability', 'NN'), ('threshold', 'VBD'), ('determining', 'VBG'), ('target', 'NN'), ('willingness', 'JJ'), ('pay', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('target', 'NN'), ('willingness', 'JJ'), ('pay', 'NN'), ('completing', 'VBG'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('ed', 'JJ'), ('wherein', 'NN'), ('completing', 'VBG'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('comprises', 'VBZ'), ('triggering', 'VBG'), ('performing', 'VBG'), ('payment', 'NN'), ('initiating', 'NN'), ('operation', 'NN'), ('acquire', 'VB'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('triggering', 'VBG'), ('performing', 'VBG'), ('payment', 'NN'), ('confirmation', 'NN'), ('operation', 'NN'), ('complete', 'JJ'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('payment', 'NN'), ('account', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('target', 'NN'), ('ed', 'JJ'), ('wherein', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('comprises', 'NNS'), ('determining', 'VBG'), ('whether', 'IN'), ('current', 'JJ'), ('corresponding', 'VBG'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('consistent', 'JJ'), ('target', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('current', 'JJ'), ('consistent', 'JJ'), ('target', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('target', 'NN'), ('willingness', 'NN'), ('pay', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('using', 'VBG'), ('head', 'JJ'), ('posture', 'NN'), ('model', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('using', 'VBG'), ('gaze', 'JJ'), ('information', 'NN'), ('model', 'NN'), ('based', 'VBN'), ('characteristics', 'NNS'), ('eye', 'NN'), ('region', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('head', 'NN'), ('posture', 'NN'), ('model', 'NN'), ('obtained', 'VBD'), ('training', 'VBG'), ('acquiring', 'VBG'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('wherein', 'NN'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('includes', 'VBZ'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('includes', 'VBZ'), ('correspondence', 'NN'), ('sample', 'NN'), ('face', 'VBP'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('mean', 'NN'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('sample', 'NN'), ('face', 'NN'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('sample', 'JJ'), ('face', 'NN'), ('contained', 'VBD'), ('pieces', 'NNS'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('based', 'VBN'), ('mean', 'JJ'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('obtain', 'VB'), ('preprocessed', 'JJ'), ('sample', 'JJ'), ('face', 'NN'), ('setting', 'VBG'), ('preprocessed', 'JJ'), ('sample', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('head', 'JJ'), ('posture', 'NN'), ('information', 'NN'), ('first', 'RB'), ('model', 'VBZ'), ('training', 'VBG'), ('sample', 'JJ'), ('performing', 'VBG'), ('training', 'NN'), ('using', 'VBG'), ('machine', 'NN'), ('learning', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('model', 'NN'), ('training', 'NN'), ('samples', 'NNS'), ('obtain', 'VB'), ('head', 'JJ'), ('posture', 'NN'), ('model', 'NN'), ('ed', 'JJ'), ('wherein', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('model', 'NN'), ('obtained', 'VBD'), ('training', 'VBG'), ('acquiring', 'VBG'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('wherein', 'JJ'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('includes', 'VBZ'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'JJ'), ('data', 'NNS'), ('includes', 'VBZ'), ('correspondence', 'NN'), ('sample', 'NN'), ('eye', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('mean', 'NN'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('sample', 'NN'), ('eye', 'NN'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('sample', 'NN'), ('eye', 'NN'), ('contained', 'VBD'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('based', 'VBN'), ('mean', 'JJ'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('obtain', 'VB'), ('preprocessed', 'JJ'), ('sample', 'NN'), ('eye', 'NN'), ('setting', 'VBG'), ('preprocessed', 'JJ'), ('sample', 'NN'), ('eye', 'NN'), ('corresponding', 'VBG'), ('gaze', 'JJ'), ('information', 'NN'), ('second', 'JJ'), ('model', 'NN'), ('training', 'VBG'), ('sample', 'JJ'), ('performing', 'VBG'), ('training', 'NN'), ('using', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('based', 'VBN'), ('second', 'JJ'), ('model', 'NN'), ('training', 'NN'), ('samples', 'NNS'), ('obtain', 'VB'), ('gaze', 'JJ'), ('information', 'NN'), ('model', 'NN'), ('ed', 'NN'), ('wherein', 'VBP'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('comprises', 'VBZ'), ('pitch', 'VBP'), ('angle', 'NN'), ('yaw', 'NN'), ('angle', 'NN'), ('roll', 'NN'), ('angle', 'NN'), ('wherein', 'NN'), ('pitch', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('x-axis', 'JJ'), ('yaw', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('y-axis', 'JJ'), ('roll', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('z-axis', 'JJ'), ('payment', 'NN'), ('device', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('comprising', 'VBG'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('storing', 'VBG'), ('instructions', 'NNS'), ('executable', 'JJ'), ('cause', 'NN'), ('device', 'NN'), ('perform', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('acquiring', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('target', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('wherein', 'IN'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('includes', 'VBZ'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('target', 'NN'), ('willingness', 'NN'), ('pay', 'NN'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('including', 'VBG'), ('determining', 'VBG'), ('whether', 'IN'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('less', 'RBR'), ('angle', 'JJ'), ('threshold', 'NN'), ('wherein', 'NN'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('includes', 'VBZ'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('greater', 'JJR'), ('probability', 'NN'), ('threshold', 'NN'), ('wherein', 'NN'), ('gaze', 'JJ'), ('information', 'NN'), ('includes', 'VBZ'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('less', 'RBR'), ('angle', 'JJ'), ('threshold', 'NN'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('greater', 'JJR'), ('probability', 'NN'), ('threshold', 'VBD'), ('determining', 'VBG'), ('target', 'NN'), ('willingness', 'JJ'), ('pay', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('target', 'NN'), ('willingness', 'JJ'), ('pay', 'NN'), ('completing', 'VBG'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('completing', 'VBG'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('comprises', 'VBZ'), ('triggering', 'VBG'), ('performing', 'VBG'), ('payment', 'NN'), ('initiating', 'NN'), ('operation', 'NN'), ('acquire', 'VB'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('triggering', 'VBG'), ('performing', 'VBG'), ('payment', 'NN'), ('confirmation', 'NN'), ('operation', 'NN'), ('complete', 'JJ'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('payment', 'NN'), ('account', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('target', 'NN'), ('device', 'NN'), ('ed', 'NN'), ('wherein', 'IN'), ('determining', 'VBG'), ('whether', 'IN'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('comprises', 'NNS'), ('determining', 'VBG'), ('whether', 'IN'), ('current', 'JJ'), ('corresponding', 'VBG'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('consistent', 'JJ'), ('target', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('current', 'JJ'), ('consistent', 'JJ'), ('target', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('target', 'NN'), ('willingness', 'NN'), ('pay', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('device', 'NN'), ('ed', 'NN'), ('wherein', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('using', 'VBG'), ('head', 'JJ'), ('posture', 'NN'), ('model', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('using', 'VBG'), ('gaze', 'JJ'), ('information', 'NN'), ('model', 'NN'), ('based', 'VBN'), ('characteristics', 'NNS'), ('eye', 'NN'), ('region', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('device', 'NN'), ('ed', 'NN'), ('wherein', 'WRB'), ('head', 'NN'), ('posture', 'NN'), ('model', 'NN'), ('obtained', 'VBD'), ('training', 'VBG'), ('acquiring', 'VBG'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('wherein', 'NN'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('includes', 'VBZ'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('includes', 'VBZ'), ('correspondence', 'NN'), ('sample', 'NN'), ('face', 'VBP'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('mean', 'NN'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('sample', 'NN'), ('face', 'NN'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('sample', 'JJ'), ('face', 'NN'), ('contained', 'VBD'), ('pieces', 'NNS'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('based', 'VBN'), ('mean', 'JJ'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('obtain', 'VB'), ('preprocessed', 'JJ'), ('sample', 'JJ'), ('face', 'NN'), ('setting', 'VBG'), ('preprocessed', 'JJ'), ('sample', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('head', 'JJ'), ('posture', 'NN'), ('information', 'NN'), ('first', 'RB'), ('model', 'VBZ'), ('training', 'VBG'), ('sample', 'JJ'), ('performing', 'VBG'), ('training', 'NN'), ('using', 'VBG'), ('machine', 'NN'), ('learning', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('model', 'NN'), ('training', 'NN'), ('samples', 'NNS'), ('obtain', 'VB'), ('head', 'JJ'), ('posture', 'NN'), ('model', 'NN'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('model', 'NN'), ('obtained', 'VBD'), ('training', 'VBG'), ('acquiring', 'VBG'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('wherein', 'JJ'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('includes', 'VBZ'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'JJ'), ('data', 'NNS'), ('includes', 'VBZ'), ('correspondence', 'NN'), ('sample', 'NN'), ('eye', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('mean', 'NN'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('sample', 'NN'), ('eye', 'NN'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('sample', 'NN'), ('eye', 'NN'), ('contained', 'VBD'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('based', 'VBN'), ('mean', 'JJ'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('obtain', 'VB'), ('preprocessed', 'JJ'), ('sample', 'NN'), ('eye', 'NN'), ('setting', 'VBG'), ('preprocessed', 'JJ'), ('sample', 'NN'), ('eye', 'NN'), ('corresponding', 'VBG'), ('gaze', 'JJ'), ('information', 'NN'), ('second', 'JJ'), ('model', 'NN'), ('training', 'VBG'), ('sample', 'JJ'), ('performing', 'VBG'), ('training', 'NN'), ('using', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('second', 'JJ'), ('model', 'NN'), ('training', 'NN'), ('samples', 'NNS'), ('obtain', 'VB'), ('gaze', 'JJ'), ('information', 'NN'), ('model', 'NN'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('angle', 'NN'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('comprises', 'VBZ'), ('pitch', 'VBP'), ('angle', 'NN'), ('yaw', 'NN'), ('angle', 'NN'), ('roll', 'NN'), ('angle', 'NN'), ('wherein', 'NN'), ('pitch', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('x-axis', 'JJ'), ('yaw', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('y-axis', 'JJ'), ('roll', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('z-axis', 'JJ'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('payment', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('configured', 'VBN'), ('instructions', 'NNS'), ('executable', 'JJ'), ('one', 'CD'), ('cause', 'NN'), ('one', 'CD'), ('perform', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('acquiring', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('target', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('wherein', 'IN'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('includes', 'VBZ'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('target', 'NN'), ('willingness', 'NN'), ('pay', 'NN'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('including', 'VBG'), ('determining', 'VBG'), ('whether', 'IN'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('less', 'RBR'), ('angle', 'JJ'), ('threshold', 'NN'), ('wherein', 'NN'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('includes', 'VBZ'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('greater', 'JJR'), ('probability', 'NN'), ('threshold', 'NN'), ('wherein', 'NN'), ('gaze', 'JJ'), ('information', 'NN'), ('includes', 'VBZ'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('less', 'RBR'), ('angle', 'JJ'), ('threshold', 'NN'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('greater', 'JJR'), ('probability', 'NN'), ('threshold', 'VBD'), ('determining', 'VBG'), ('target', 'NN'), ('willingness', 'JJ'), ('pay', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('target', 'NN'), ('willingness', 'JJ'), ('pay', 'NN'), ('completing', 'VBG'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('storage', 'NN'), ('medium', 'NN'), ('ed', 'VBZ'), ('wherein', 'JJ'), ('completing', 'VBG'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('comprises', 'VBZ'), ('triggering', 'VBG'), ('performing', 'VBG'), ('payment', 'NN'), ('initiating', 'NN'), ('operation', 'NN'), ('acquire', 'VB'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('triggering', 'VBG'), ('performing', 'VBG'), ('payment', 'NN'), ('confirmation', 'NN'), ('operation', 'NN'), ('complete', 'JJ'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('payment', 'NN'), ('account', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('target', 'NN'), ('storage', 'NN'), ('medium', 'NN'), ('ed', 'NN'), ('wherein', 'IN'), ('determining', 'VBG'), ('whether', 'IN'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('comprises', 'NNS'), ('determining', 'VBG'), ('whether', 'IN'), ('current', 'JJ'), ('corresponding', 'VBG'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('consistent', 'JJ'), ('target', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('current', 'JJ'), ('consistent', 'JJ'), ('target', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('target', 'NN'), ('willingness', 'NN'), ('pay', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('storage', 'NN'), ('medium', 'NN'), ('ed', 'NN'), ('wherein', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('using', 'VBG'), ('head', 'JJ'), ('posture', 'NN'), ('model', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('using', 'VBG'), ('gaze', 'JJ'), ('information', 'NN'), ('model', 'NN'), ('based', 'VBN'), ('characteristics', 'NNS'), ('eye', 'NN'), ('region', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('storage', 'NN'), ('medium', 'NN'), ('ed', 'NN'), ('wherein', 'WRB'), ('head', 'NN'), ('posture', 'NN'), ('model', 'NN'), ('obtained', 'VBD'), ('training', 'VBG'), ('acquiring', 'VBG'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('wherein', 'NN'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('includes', 'VBZ'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('includes', 'VBZ'), ('correspondence', 'NN'), ('sample', 'NN'), ('face', 'VBP'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('mean', 'NN'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('sample', 'NN'), ('face', 'NN'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('sample', 'JJ'), ('face', 'NN'), ('contained', 'VBD'), ('pieces', 'NNS'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('based', 'VBN'), ('mean', 'JJ'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('obtain', 'VB'), ('preprocessed', 'JJ'), ('sample', 'JJ'), ('face', 'NN'), ('setting', 'VBG'), ('preprocessed', 'JJ'), ('sample', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('head', 'JJ'), ('posture', 'NN'), ('information', 'NN'), ('first', 'RB'), ('model', 'VBZ'), ('training', 'VBG'), ('sample', 'JJ'), ('performing', 'VBG'), ('training', 'NN'), ('using', 'VBG'), ('machine', 'NN'), ('learning', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('model', 'NN'), ('training', 'NN'), ('samples', 'NNS'), ('obtain', 'VB'), ('head', 'JJ'), ('posture', 'NN'), ('model', 'NN'), ('wherein', 'NN'), ('gaze', 'JJ'), ('information', 'NN'), ('model', 'NN'), ('obtained', 'VBD'), ('training', 'VBG'), ('acquiring', 'VBG'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('wherein', 'JJ'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('includes', 'VBZ'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'JJ'), ('data', 'NNS'), ('includes', 'VBZ'), ('correspondence', 'NN'), ('sample', 'NN'), ('eye', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('mean', 'NN'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('sample', 'NN'), ('eye', 'NN'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('sample', 'NN'), ('eye', 'NN'), ('contained', 'VBD'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('based', 'VBN'), ('mean', 'JJ'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('obtain', 'VB'), ('preprocessed', 'JJ'), ('sample', 'NN'), ('eye', 'NN'), ('setting', 'VBG'), ('preprocessed', 'JJ'), ('sample', 'NN'), ('eye', 'NN'), ('corresponding', 'VBG'), ('gaze', 'JJ'), ('information', 'NN'), ('second', 'JJ'), ('model', 'NN'), ('training', 'VBG'), ('sample', 'JJ'), ('performing', 'VBG'), ('training', 'NN'), ('using', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('based', 'VBN'), ('second', 'JJ'), ('model', 'NN'), ('training', 'NN'), ('samples', 'NNS'), ('obtain', 'VB'), ('gaze', 'JJ'), ('information', 'NN'), ('model', 'NN'), ('storage', 'NN'), ('medium', 'NN'), ('ed', 'NN'), ('wherein', 'NN'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('comprises', 'VBZ'), ('pitch', 'VBP'), ('angle', 'NN'), ('yaw', 'NN'), ('angle', 'NN'), ('roll', 'NN'), ('angle', 'NN'), ('wherein', 'NN'), ('pitch', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('x-axis', 'JJ'), ('yaw', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('y-axis', 'JJ'), ('roll', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('z-axis', 'JJ'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('motion', 'NN'), ('detection', 'NN'), ('module', 'NN'), ('motion', 'NN'), ('subject', 'NN'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('assigning', 'VBG'), ('unique', 'JJ'), ('session', 'NN'), ('identification', 'NN'), ('number', 'NN'), ('subject', 'JJ'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('generating', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('assessing', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('determining', 'VBG'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('identifying', 'VBG'), ('intent', 'NN'), ('subject', 'JJ'), ('authorizing', 'VBG'), ('access', 'NN'), ('point', 'NN'), ('entry', 'NN'), ('based', 'VBN'), ('determined', 'VBN'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('intent', 'NN'), ('subject', 'JJ'), ('comprising', 'VBG'), ('determining', 'VBG'), ('one', 'CD'), ('additional', 'JJ'), ('subjects', 'NNS'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('assigning', 'VBG'), ('unique', 'JJ'), ('session', 'NN'), ('identification', 'NN'), ('number', 'NN'), ('one', 'CD'), ('additional', 'JJ'), ('subjects', 'NNS'), ('detected', 'VBN'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('wherein', 'IN'), ('assessing', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprises', 'VBZ'), ('assessing', 'VBG'), ('whether', 'IN'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('object', 'NN'), ('equates', 'NNS'), ('predetermined', 'VBD'), ('metric', 'JJ'), ('quality', 'NN'), ('upon', 'IN'), ('determining', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('object', 'VBP'), ('inferior', 'JJ'), ('predetermined', 'VBN'), ('metric', 'JJ'), ('quality', 'NN'), ('discarding', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('generating', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('whether', 'IN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('photographic', 'JJ'), ('upon', 'IN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('photographic', 'JJ'), ('generating', 'VBG'), ('warning', 'VBG'), ('restrict', 'JJ'), ('access', 'NN'), ('point', 'NN'), ('entry', 'NN'), ('comprising', 'VBG'), ('conducing', 'VBG'), ('incremental', 'JJ'), ('training', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('wherein', 'NN'), ('conducing', 'VBG'), ('incremental', 'JJ'), ('training', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprises', 'NNS'), ('capturing', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('converting', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('first', 'RB'), ('numeric', 'JJ'), ('vector', 'NN'), ('capturing', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('converting', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('second', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('calculating', 'VBG'), ('weighted', 'VBD'), ('mean', 'JJ'), ('first', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('second', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('wherein', 'NN'), ('weighted', 'VBD'), ('mean', 'JJ'), ('represents', 'VBZ'), ('change', 'VBP'), ('facial', 'JJ'), ('area', 'NN'), ('storing', 'VBG'), ('weighted', 'JJ'), ('mean', 'JJ'), ('database', 'NN'), ('wherein', 'NN'), ('determining', 'VBG'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprises', 'VBZ'), ('comparing', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('stored', 'VBD'), ('database', 'NN'), ('authenticating', 'VBG'), ('subject', 'JJ'), ('wherein', 'NN'), ('identifying', 'VBG'), ('intent', 'NN'), ('subject', 'NN'), ('comprises', 'VBZ'), ('upon', 'IN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('bounding', 'VBG'), ('box', 'NN'), ('commencing', 'VBG'), ('authentication', 'NN'), ('subject', 'JJ'), ('calculating', 'VBG'), ('directional', 'JJ'), ('vector', 'NN'), ('face', 'NN'), ('subject', 'JJ'), ('determine', 'JJ'), ('intent', 'NN'), ('subject', 'JJ'), ('gain', 'NN'), ('access', 'NN'), ('point', 'NN'), ('entry', 'NN'), ('based', 'VBN'), ('directional', 'JJ'), ('vector', 'NN'), ('face', 'NN'), ('subject', 'JJ'), ('granting', 'VBG'), ('access', 'NN'), ('point', 'NN'), ('entry', 'NN'), ('based', 'VBN'), ('authentication', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('determining', 'VBG'), ('intent', 'NN'), ('subject', 'JJ'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('program', 'NN'), ('instructions', 'NNS'), ('stored', 'VBD'), ('thereon', 'JJ'), ('response', 'NN'), ('execution', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('cause', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('perform', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('motion', 'NN'), ('subject', 'NN'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('assigning', 'VBG'), ('unique', 'JJ'), ('session', 'NN'), ('identification', 'NN'), ('number', 'NN'), ('subject', 'JJ'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('generating', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('assessing', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('determining', 'VBG'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('identifying', 'VBG'), ('intent', 'NN'), ('subject', 'JJ'), ('authorizing', 'VBG'), ('access', 'NN'), ('point', 'NN'), ('entry', 'NN'), ('based', 'VBN'), ('determined', 'VBN'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('intent', 'NN'), ('subject', 'JJ'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('determining', 'VBG'), ('one', 'CD'), ('additional', 'JJ'), ('subjects', 'NNS'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('assigning', 'VBG'), ('unique', 'JJ'), ('session', 'NN'), ('identification', 'NN'), ('number', 'NN'), ('one', 'CD'), ('additional', 'JJ'), ('subjects', 'NNS'), ('detected', 'VBN'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('assessing', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprises', 'VBZ'), ('assessing', 'VBG'), ('whether', 'IN'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('object', 'NN'), ('equates', 'NNS'), ('predetermined', 'VBD'), ('metric', 'JJ'), ('quality', 'NN'), ('upon', 'IN'), ('determining', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('object', 'VBP'), ('inferior', 'JJ'), ('predetermined', 'VBN'), ('metric', 'JJ'), ('quality', 'NN'), ('discarding', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('generating', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('whether', 'IN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('photographic', 'JJ'), ('upon', 'IN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('photographic', 'JJ'), ('generating', 'VBG'), ('warning', 'VBG'), ('restrict', 'JJ'), ('access', 'NN'), ('access', 'NN'), ('point', 'NN'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('conducing', 'VBG'), ('incremental', 'JJ'), ('training', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('conducing', 'VBG'), ('incremental', 'JJ'), ('training', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprises', 'NNS'), ('capturing', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('converting', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('first', 'RB'), ('numeric', 'JJ'), ('vector', 'NN'), ('capturing', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('converting', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('second', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('calculating', 'VBG'), ('weighted', 'VBD'), ('mean', 'JJ'), ('first', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('second', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('wherein', 'NN'), ('weighted', 'VBD'), ('mean', 'JJ'), ('represents', 'VBZ'), ('change', 'VBP'), ('facial', 'JJ'), ('area', 'NN'), ('storing', 'VBG'), ('weighted', 'JJ'), ('mean', 'JJ'), ('database', 'NN'), ('apparatus', 'NN'), ('face', 'NN'), ('comprising', 'VBG'), ('memory', 'NN'), ('store', 'NN'), ('computer', 'NN'), ('program', 'NN'), ('instructions', 'NNS'), ('computer', 'NN'), ('program', 'NN'), ('instructions', 'NNS'), ('executed', 'VBD'), ('cause', 'NN'), ('perform', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('motion', 'NN'), ('subject', 'NN'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('assigning', 'VBG'), ('unique', 'JJ'), ('session', 'NN'), ('identification', 'NN'), ('number', 'NN'), ('subject', 'JJ'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('generating', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('assessing', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('determining', 'VBG'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('identifying', 'VBG'), ('intent', 'NN'), ('subject', 'JJ'), ('authorizing', 'VBG'), ('access', 'NN'), ('point', 'NN'), ('entry', 'NN'), ('based', 'VBN'), ('determined', 'VBN'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('intent', 'NN'), ('subject', 'JJ'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('determining', 'VBG'), ('one', 'CD'), ('additional', 'JJ'), ('subjects', 'NNS'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('assigning', 'VBG'), ('unique', 'JJ'), ('session', 'NN'), ('identification', 'NN'), ('number', 'NN'), ('one', 'CD'), ('additional', 'JJ'), ('subjects', 'NNS'), ('detected', 'VBN'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('apparatus', 'VBP'), ('wherein', 'NN'), ('assessing', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprises', 'VBZ'), ('assessing', 'VBG'), ('whether', 'IN'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('object', 'NN'), ('equates', 'NNS'), ('predetermined', 'VBD'), ('metric', 'JJ'), ('quality', 'NN'), ('upon', 'IN'), ('determining', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('object', 'VBP'), ('inferior', 'JJ'), ('predetermined', 'VBN'), ('metric', 'JJ'), ('quality', 'NN'), ('discarding', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('generating', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('whether', 'IN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('photographic', 'JJ'), ('upon', 'IN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('photographic', 'JJ'), ('generating', 'VBG'), ('warning', 'VBG'), ('restrict', 'JJ'), ('access', 'NN'), ('access', 'NN'), ('point', 'NN'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('conducing', 'VBG'), ('incremental', 'JJ'), ('training', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('apparatus', 'NN'), ('wherein', 'NN'), ('conducing', 'VBG'), ('incremental', 'JJ'), ('training', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprises', 'NNS'), ('capturing', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('converting', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('first', 'RB'), ('numeric', 'JJ'), ('vector', 'NN'), ('capturing', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('converting', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('second', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('calculating', 'VBG'), ('weighted', 'VBD'), ('mean', 'JJ'), ('first', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('second', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('wherein', 'NN'), ('weighted', 'VBD'), ('mean', 'JJ'), ('represents', 'VBZ'), ('change', 'VBP'), ('facial', 'JJ'), ('area', 'NN'), ('storing', 'VBG'), ('weighted', 'JJ'), ('mean', 'JJ'), ('database', 'NN'), ('robot', 'NN'), ('comprising', 'VBG'), ('body', 'NN'), ('configured', 'VBD'), ('rotate', 'JJ'), ('tilt', 'NN'), ('camera', 'NN'), ('coupled', 'VBN'), ('body', 'NN'), ('configured', 'JJ'), ('rotate', 'NN'), ('tilt', 'NN'), ('according', 'VBG'), ('rotate', 'NN'), ('tilt', 'NN'), ('body', 'NN'), ('wherein', 'JJ'), ('camera', 'NN'), ('configured', 'VBD'), ('acquire', 'VB'), ('video', 'NN'), ('space', 'NN'), ('face', 'NN'), ('unit', 'NN'), ('configured', 'VBD'), ('recognize', 'JJ'), ('respective', 'JJ'), ('one', 'CD'), ('persons', 'NNS'), ('video', 'VBP'), ('unit', 'NN'), ('configured', 'VBD'), ('track', 'JJ'), ('motion', 'NN'), ('recognized', 'VBD'), ('one', 'CD'), ('persons', 'NNS'), ('controller', 'NN'), ('configured', 'VBD'), ('calculate', 'JJ'), ('respective', 'JJ'), ('size', 'NN'), ('one', 'CD'), ('persons', 'NNS'), ('select', 'VBP'), ('first', 'JJ'), ('person', 'NN'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('based', 'VBN'), ('calculated', 'JJ'), ('sizes', 'NNS'), ('control', 'NN'), ('least', 'VBP'), ('one', 'CD'), ('direction', 'NN'), ('rotation', 'NN'), ('camera', 'NN'), ('angle', 'NN'), ('tilt', 'NN'), ('camera', 'NN'), ('focal', 'JJ'), ('distance', 'NN'), ('camera', 'NN'), ('based', 'VBN'), ('tracked', 'JJ'), ('motion', 'NN'), ('recognized', 'VBD'), ('face', 'NN'), ('first', 'RB'), ('person', 'NN'), ('robot', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('control', 'JJ'), ('direction', 'NN'), ('rotation', 'NN'), ('camera', 'NN'), ('angle', 'NN'), ('tilt', 'NN'), ('camera', 'NN'), ('achieve', 'VBP'), ('particular', 'JJ'), ('camera', 'NN'), ('relative', 'JJ'), ('face', 'NN'), ('first', 'RB'), ('person', 'NN'), ('control', 'NN'), ('focal', 'JJ'), ('distance', 'NN'), ('camera', 'NN'), ('comparing', 'VBG'), ('respective', 'JJ'), ('sizes', 'NNS'), ('face', 'VBP'), ('first', 'JJ'), ('person', 'NN'), ('motion', 'NN'), ('first', 'RB'), ('person', 'NN'), ('robot', 'JJ'), ('wherein', 'NN'), ('particular', 'JJ'), ('occurs', 'NNS'), ('camera', 'VBP'), ('general', 'JJ'), ('direction', 'NN'), ('face', 'NN'), ('first', 'RB'), ('person', 'NN'), ('robot', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('normalize', 'JJ'), ('sizes', 'NNS'), ('one', 'CD'), ('persons', 'NNS'), ('based', 'VBN'), ('interocular', 'JJ'), ('distance', 'NN'), ('select', 'NN'), ('first', 'RB'), ('person', 'NN'), ('based', 'VBN'), ('normalized', 'JJ'), ('sizes', 'NNS'), ('one', 'CD'), ('persons', 'NNS'), ('robot', 'VBP'), ('wherein', 'JJ'), ('controller', 'NN'), ('configured', 'VBD'), ('select', 'JJ'), ('person', 'NN'), ('largest', 'JJS'), ('face', 'NN'), ('size', 'NN'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('first', 'JJ'), ('person', 'NN'), ('robot', 'NN'), ('comprising', 'VBG'), ('microphone', 'NN'), ('configured', 'VBD'), ('receive', 'JJ'), ('spoken', 'NN'), ('audio', 'NN'), ('present', 'JJ'), ('space', 'NN'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('select', 'JJ'), ('first', 'JJ'), ('person', 'NN'), ('based', 'VBN'), ('received', 'VBN'), ('spoken', 'JJ'), ('audio', 'JJ'), ('robot', 'NN'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('control', 'NN'), ('gain', 'NN'), ('microphone', 'NN'), ('comparing', 'VBG'), ('respective', 'JJ'), ('sizes', 'NNS'), ('face', 'VBP'), ('first', 'JJ'), ('person', 'NN'), ('motion', 'NN'), ('first', 'RB'), ('person', 'NN'), ('robot', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('calculate', 'JJ'), ('position', 'NN'), ('spoken', 'VBN'), ('audio', 'NN'), ('provided', 'VBD'), ('select', 'JJ'), ('first', 'JJ'), ('person', 'NN'), ('based', 'VBN'), ('whether', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('position', 'NN'), ('voice', 'NN'), ('signal', 'NN'), ('provided', 'VBD'), ('robot', 'JJ'), ('wherein', 'JJ'), ('controller', 'NN'), ('configured', 'VBD'), ('select', 'JJ'), ('second', 'JJ'), ('person', 'NN'), ('first', 'JJ'), ('person', 'NN'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('second', 'JJ'), ('person', 'NN'), ('located', 'VBN'), ('position', 'NN'), ('spoken', 'VBN'), ('audio', 'NN'), ('provided', 'VBD'), ('robot', 'JJ'), ('wherein', 'JJ'), ('controller', 'NN'), ('configured', 'VBD'), ('select', 'JJ'), ('second', 'JJ'), ('person', 'NN'), ('largest', 'JJS'), ('face', 'NN'), ('size', 'NN'), ('first', 'JJ'), ('person', 'NN'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('none', 'NN'), ('one', 'CD'), ('persons', 'NNS'), ('located', 'VBN'), ('position', 'NN'), ('spoken', 'VBN'), ('audio', 'NN'), ('provided', 'VBD'), ('robot', 'JJ'), ('wherein', 'JJ'), ('controller', 'NN'), ('configured', 'VBD'), ('select', 'JJ'), ('second', 'JJ'), ('person', 'NN'), ('largest', 'JJS'), ('face', 'NN'), ('size', 'NN'), ('first', 'JJ'), ('person', 'NN'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('persons', 'NNS'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('located', 'VBN'), ('position', 'NN'), ('spoken', 'VBN'), ('audio', 'NN'), ('provided', 'VBD'), ('robot', 'NN'), ('comprising', 'VBG'), ('speaker', 'NN'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('control', 'NN'), ('volume', 'NN'), ('speaker', 'NN'), ('comparing', 'VBG'), ('respective', 'JJ'), ('sizes', 'NNS'), ('face', 'VBP'), ('first', 'JJ'), ('person', 'NN'), ('motion', 'NN'), ('first', 'RB'), ('person', 'NN'), ('robot', 'JJ'), ('wherein', 'NN'), ('body', 'NN'), ('configured', 'VBD'), ('rotate', 'JJ'), ('lateral', 'JJ'), ('direction', 'NN'), ('tilt', 'VBD'), ('vertical', 'JJ'), ('direction', 'NN'), ('comprising', 'VBG'), ('camera', 'NN'), ('coupled', 'VBN'), ('body', 'NN'), ('configured', 'JJ'), ('rotate', 'NN'), ('tilt', 'NN'), ('wherein', 'NN'), ('camera', 'NN'), ('configured', 'VBD'), ('acquire', 'VB'), ('video', 'NN'), ('space', 'NN'), ('within', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('positioned', 'VBD'), ('configured', 'JJ'), ('recognize', 'NN'), ('respective', 'JJ'), ('one', 'CD'), ('persons', 'NNS'), ('video', 'VBP'), ('track', 'JJ'), ('motion', 'NN'), ('recognized', 'VBD'), ('one', 'CD'), ('persons', 'NNS'), ('calculate', 'VBP'), ('respective', 'JJ'), ('size', 'NN'), ('one', 'CD'), ('persons', 'NNS'), ('select', 'VBP'), ('first', 'JJ'), ('person', 'NN'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('based', 'VBN'), ('calculated', 'JJ'), ('sizes', 'NNS'), ('control', 'NN'), ('least', 'VBP'), ('one', 'CD'), ('direction', 'NN'), ('rotation', 'NN'), ('camera', 'NN'), ('angle', 'NN'), ('tilt', 'NN'), ('camera', 'NN'), ('focal', 'JJ'), ('distance', 'NN'), ('camera', 'NN'), ('based', 'VBN'), ('tracked', 'JJ'), ('motion', 'NN'), ('recognized', 'VBD'), ('face', 'NN'), ('first', 'RB'), ('person', 'NN'), ('comprising', 'VBG'), ('acquiring', 'VBG'), ('camera', 'NN'), ('video', 'NN'), ('space', 'NN'), ('within', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('positioned', 'VBD'), ('respective', 'JJ'), ('one', 'CD'), ('persons', 'NNS'), ('video', 'JJ'), ('motion', 'NN'), ('recognized', 'VBD'), ('one', 'CD'), ('persons', 'NNS'), ('calculating', 'VBG'), ('respective', 'JJ'), ('size', 'NN'), ('one', 'CD'), ('persons', 'NNS'), ('selecting', 'VBG'), ('first', 'JJ'), ('person', 'NN'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('based', 'VBN'), ('calculated', 'JJ'), ('sizes', 'NNS'), ('controlling', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('direction', 'NN'), ('rotation', 'NN'), ('camera', 'NN'), ('angle', 'NN'), ('tilt', 'NN'), ('camera', 'NN'), ('focal', 'JJ'), ('distance', 'NN'), ('camera', 'NN'), ('based', 'VBN'), ('tracked', 'JJ'), ('motion', 'NN'), ('recognized', 'VBD'), ('face', 'NN'), ('first', 'RB'), ('person', 'NN'), ('inferring', 'VBG'), ('topics', 'NNS'), ('multimodal', 'VBP'), ('file', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('multimodal', 'NN'), ('file', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('entities', 'NNS'), ('multimodal', 'VBP'), ('file', 'NN'), ('linking', 'VBG'), ('set', 'VBN'), ('entities', 'NNS'), ('produce', 'VBP'), ('set', 'VBN'), ('linked', 'VBN'), ('entities', 'NNS'), ('obtaining', 'VBG'), ('reference', 'NN'), ('information', 'NN'), ('set', 'VBN'), ('entities', 'NNS'), ('based', 'VBN'), ('least', 'JJS'), ('reference', 'NN'), ('information', 'NN'), ('generating', 'VBG'), ('graph', 'NN'), ('set', 'VBN'), ('linked', 'VBN'), ('entities', 'NNS'), ('graph', 'VBP'), ('comprising', 'VBG'), ('nodes', 'NNS'), ('edges', 'NNS'), ('based', 'VBN'), ('least', 'JJS'), ('nodes', 'JJ'), ('edges', 'NNS'), ('graph', 'VBP'), ('determining', 'VBG'), ('clusters', 'NNS'), ('graph', 'VBP'), ('based', 'VBN'), ('least', 'JJS'), ('clusters', 'NNS'), ('graph', 'VBP'), ('identifying', 'VBG'), ('topic', 'NN'), ('candidates', 'NNS'), ('extracting', 'VBG'), ('features', 'NNS'), ('clusters', 'NNS'), ('graph', 'VBP'), ('based', 'VBN'), ('least', 'JJS'), ('extracted', 'JJ'), ('features', 'NNS'), ('selecting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('among', 'IN'), ('topic', 'JJ'), ('candidates', 'NNS'), ('represent', 'VBP'), ('least', 'JJS'), ('one', 'CD'), ('cluster', 'NN'), ('indexing', 'VBG'), ('multimodal', 'NNS'), ('file', 'RB'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('wherein', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('comprises', 'VBZ'), ('video', 'JJ'), ('portion', 'NN'), ('audio', 'NN'), ('portion', 'NN'), ('wherein', 'IN'), ('extracting', 'VBG'), ('set', 'NN'), ('entities', 'NNS'), ('multimodal', 'VBP'), ('file', 'NN'), ('comprises', 'NNS'), ('detecting', 'VBG'), ('objects', 'NNS'), ('video', 'JJ'), ('portion', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('detecting', 'VBG'), ('text', 'JJ'), ('audio', 'JJ'), ('portion', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('wherein', 'NN'), ('detecting', 'VBG'), ('objects', 'NNS'), ('comprises', 'VBZ'), ('performing', 'VBG'), ('face', 'NN'), ('wherein', 'NN'), ('detecting', 'VBG'), ('text', 'JJ'), ('comprises', 'NNS'), ('performing', 'VBG'), ('speech', 'NN'), ('text', 'NN'), ('process', 'NN'), ('comprising', 'VBG'), ('identifying', 'VBG'), ('language', 'NN'), ('used', 'VBN'), ('audio', 'JJ'), ('portion', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('wherein', 'NN'), ('performing', 'VBG'), ('speech', 'JJ'), ('text', 'NN'), ('process', 'NN'), ('comprises', 'VBZ'), ('performing', 'VBG'), ('speech', 'NN'), ('text', 'NN'), ('process', 'NN'), ('identified', 'VBN'), ('language', 'NN'), ('comprising', 'VBG'), ('translating', 'VBG'), ('detected', 'VBN'), ('text', 'JJ'), ('comprising', 'VBG'), ('determining', 'VBG'), ('significant', 'JJ'), ('clusters', 'NNS'), ('insignificant', 'JJ'), ('clusters', 'NNS'), ('determined', 'VBD'), ('clusters', 'NNS'), ('wherein', 'VBP'), ('extracting', 'VBG'), ('features', 'NNS'), ('clusters', 'NNS'), ('graph', 'VBP'), ('comprises', 'NNS'), ('extracting', 'VBG'), ('features', 'NNS'), ('significant', 'JJ'), ('clusters', 'NNS'), ('graph', 'VBP'), ('wherein', 'RB'), ('extracting', 'VBG'), ('features', 'NNS'), ('clusters', 'NNS'), ('graph', 'VBP'), ('comprises', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('process', 'NN'), ('selected', 'VBN'), ('list', 'NN'), ('consisting', 'VBG'), ('determining', 'VBG'), ('graph', 'JJ'), ('diameter', 'NN'), ('determining', 'VBG'), ('jaccard', 'JJ'), ('coefficient', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'JJ'), ('represent', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('cluster', 'NN'), ('comprises', 'NNS'), ('based', 'VBN'), ('least', 'RB'), ('extracted', 'JJ'), ('features', 'NNS'), ('mapping', 'VBG'), ('topic', 'NN'), ('candidates', 'NNS'), ('probability', 'NN'), ('interval', 'VBP'), ('based', 'VBN'), ('least', 'JJS'), ('mapping', 'VBG'), ('ranking', 'VBG'), ('topic', 'NN'), ('candidates', 'NNS'), ('within', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('cluster', 'NN'), ('selecting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('ranking', 'JJ'), ('comprising', 'VBG'), ('translating', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('wherein', 'NN'), ('indexing', 'VBG'), ('multimodal', 'NNS'), ('file', 'RB'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('comprises', 'VBZ'), ('indexing', 'VBG'), ('multimodal', 'NNS'), ('file', 'RB'), ('least', 'JJS'), ('one', 'CD'), ('translated', 'VBN'), ('topicid', 'NN'), ('system', 'NN'), ('inferring', 'VBG'), ('topics', 'NNS'), ('multimodal', 'VBP'), ('file', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('entity', 'NN'), ('extraction', 'NN'), ('component', 'NN'), ('comprising', 'VBG'), ('object', 'JJ'), ('detection', 'NN'), ('component', 'NN'), ('speech', 'NN'), ('text', 'JJ'), ('component', 'NN'), ('operative', 'JJ'), ('extract', 'NN'), ('set', 'VBN'), ('entities', 'NNS'), ('multimodal', 'VBP'), ('file', 'NN'), ('comprising', 'VBG'), ('video', 'JJ'), ('portion', 'NN'), ('audio', 'NN'), ('portion', 'NN'), ('entity', 'NN'), ('linking', 'VBG'), ('component', 'JJ'), ('operative', 'JJ'), ('link', 'NN'), ('extracted', 'VBD'), ('set', 'JJ'), ('entities', 'NNS'), ('produce', 'VBP'), ('set', 'VBN'), ('linked', 'VBN'), ('entities', 'NNS'), ('information', 'NN'), ('retrieval', 'NN'), ('component', 'NN'), ('operative', 'JJ'), ('obtain', 'VB'), ('reference', 'NN'), ('information', 'NN'), ('extracted', 'VBD'), ('set', 'NN'), ('entities', 'NNS'), ('graphing', 'VBG'), ('analysis', 'NN'), ('component', 'NN'), ('operative', 'JJ'), ('generate', 'NN'), ('graph', 'NN'), ('set', 'VBN'), ('linked', 'VBN'), ('entities', 'NNS'), ('graph', 'VBP'), ('comprising', 'VBG'), ('nodes', 'NNS'), ('edges', 'NNS'), ('based', 'VBN'), ('least', 'JJS'), ('nodes', 'JJ'), ('edges', 'NNS'), ('graph', 'VBP'), ('determine', 'JJ'), ('clusters', 'NNS'), ('graph', 'VBP'), ('based', 'VBN'), ('least', 'JJS'), ('clusters', 'NNS'), ('graph', 'VBP'), ('identify', 'VB'), ('topic', 'NN'), ('candidates', 'NNS'), ('extract', 'JJ'), ('features', 'NNS'), ('clusters', 'NNS'), ('graph', 'VBP'), ('topicid', 'JJ'), ('selection', 'NN'), ('component', 'NN'), ('operative', 'JJ'), ('rank', 'NN'), ('topic', 'NN'), ('candidates', 'NNS'), ('within', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('cluster', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('ranking', 'JJ'), ('select', 'JJ'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('among', 'IN'), ('topic', 'JJ'), ('candidates', 'NNS'), ('represent', 'VBP'), ('least', 'JJS'), ('one', 'CD'), ('cluster', 'NN'), ('video', 'NN'), ('indexer', 'VBP'), ('operative', 'JJ'), ('index', 'NN'), ('multimodal', 'NNS'), ('file', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('object', 'JJ'), ('detection', 'NN'), ('component', 'NN'), ('operative', 'JJ'), ('perform', 'NN'), ('face', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('speech', 'NN'), ('text', 'NN'), ('component', 'NN'), ('operative', 'JJ'), ('extract', 'JJ'), ('entity', 'NN'), ('information', 'NN'), ('least', 'JJS'), ('two', 'CD'), ('different', 'JJ'), ('languages', 'NNS'), ('one', 'CD'), ('computer', 'NN'), ('storage', 'NN'), ('devices', 'NNS'), ('computer-executable', 'JJ'), ('instructions', 'NNS'), ('stored', 'VBD'), ('thereon', 'NN'), ('inferring', 'VBG'), ('topics', 'NNS'), ('multimodal', 'JJ'), ('file', 'NN'), ('execution', 'NN'), ('computer', 'NN'), ('cause', 'VBP'), ('computer', 'NN'), ('perform', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('multimodal', 'NN'), ('file', 'NN'), ('comprising', 'VBG'), ('video', 'JJ'), ('portion', 'NN'), ('audio', 'NN'), ('portion', 'NN'), ('extracting', 'VBG'), ('set', 'NN'), ('entities', 'NNS'), ('multimodal', 'VBP'), ('file', 'NN'), ('wherein', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('entities', 'NNS'), ('multimodal', 'VBP'), ('file', 'NN'), ('comprises', 'NNS'), ('detecting', 'VBG'), ('objects', 'NNS'), ('video', 'JJ'), ('portion', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('face', 'NN'), ('detecting', 'VBG'), ('text', 'JJ'), ('audio', 'JJ'), ('portion', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('speech', 'NN'), ('text', 'NN'), ('process', 'NN'), ('disambiguating', 'VBG'), ('among', 'IN'), ('set', 'VBN'), ('detected', 'VBN'), ('entity', 'NN'), ('names', 'NNS'), ('linking', 'VBG'), ('set', 'NN'), ('entities', 'NNS'), ('produce', 'VBP'), ('set', 'VBN'), ('linked', 'VBN'), ('entities', 'NNS'), ('obtaining', 'VBG'), ('reference', 'NN'), ('information', 'NN'), ('set', 'VBN'), ('entities', 'NNS'), ('based', 'VBN'), ('least', 'JJS'), ('reference', 'NN'), ('information', 'NN'), ('generating', 'VBG'), ('graph', 'NN'), ('set', 'VBN'), ('linked', 'VBN'), ('entities', 'NNS'), ('graph', 'VBP'), ('comprising', 'VBG'), ('nodes', 'NNS'), ('edges', 'NNS'), ('based', 'VBN'), ('least', 'JJS'), ('nodes', 'JJ'), ('edges', 'NNS'), ('graph', 'VBP'), ('determining', 'VBG'), ('clusters', 'NNS'), ('graph', 'VBP'), ('determining', 'VBG'), ('significant', 'JJ'), ('clusters', 'NNS'), ('insignificant', 'JJ'), ('clusters', 'NNS'), ('determined', 'VBD'), ('clusters', 'NNS'), ('based', 'VBN'), ('least', 'JJS'), ('significant', 'JJ'), ('clusters', 'NNS'), ('graph', 'VBP'), ('identifying', 'VBG'), ('topic', 'NN'), ('candidates', 'NNS'), ('extracting', 'VBG'), ('features', 'NNS'), ('significant', 'JJ'), ('clusters', 'NNS'), ('graph', 'VBP'), ('based', 'VBN'), ('least', 'JJS'), ('extracted', 'JJ'), ('features', 'NNS'), ('mapping', 'VBG'), ('topic', 'NN'), ('candidates', 'NNS'), ('probability', 'NN'), ('interval', 'VBP'), ('based', 'VBN'), ('least', 'JJS'), ('mapping', 'VBG'), ('ranking', 'VBG'), ('topic', 'NN'), ('candidates', 'NNS'), ('within', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('significant', 'JJ'), ('cluster', 'NN'), ('based', 'VBN'), ('ranking', 'VBG'), ('selecting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('among', 'IN'), ('topic', 'JJ'), ('candidates', 'NNS'), ('represent', 'VBP'), ('least', 'JJS'), ('one', 'CD'), ('significant', 'JJ'), ('cluster', 'NN'), ('indexing', 'VBG'), ('multimodal', 'NNS'), ('file', 'RB'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('one', 'CD'), ('computer', 'NN'), ('storage', 'NN'), ('devices', 'NNS'), ('wherein', 'VBP'), ('operations', 'NNS'), ('comprise', 'VBP'), ('identifying', 'VBG'), ('language', 'NN'), ('used', 'VBN'), ('audio', 'JJ'), ('portion', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('detecting', 'VBG'), ('text', 'JJ'), ('audio', 'JJ'), ('portion', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('speech', 'NN'), ('text', 'NN'), ('process', 'NN'), ('comprises', 'VBZ'), ('performing', 'VBG'), ('speech', 'NN'), ('text', 'NN'), ('process', 'NN'), ('identified', 'VBD'), ('language权利要求', 'JJ'), ('、', 'NNP'), ('一种人脸识别方法其特征在于包括', 'NNP'), ('通过第一摄像头获取第一人脸图像', 'NNP'), ('提取所述第一人脸图像的第一人脸特征', 'NNP'), ('将所述第一人脸特征与预先存储的第二人脸特征进行对比获得参考相似度所述第', 'NNP'), ('二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得所述第二摄像头与所述第', 'NNP'), ('一摄像头属于不同类型的摄像头', 'NNP'), ('根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否对应相同人。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于', 'NNP'), ('所述第一摄像头为热成像摄像头所述第二摄像头为可见光摄像头', 'NNP'), ('或者所述第一摄像头为可见光摄像头所述第一摄像头为热成像摄像头。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('或', 'NNP'), ('所述的方法其特征在于所述根据所述参考相似度确定所', 'NNP'), ('述第一人脸特征与所述第二人脸特征是否对应相同人包括', 'NNP'), ('根据所述参考相似度、', 'NNP'), ('参考误报率以及相似度阈值确定所述第一人脸特征与所述第二', 'NNP'), ('人脸特征是否对应相同人其中不同的误报率对应不同的相似度阈值。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('或', 'NNP'), ('所述的方法其特征在于所述根据所述参考相似度确定所', 'NNP'), ('述第一人脸特征与所述第二人脸特征是否对应相同人包括', 'NNP'), ('根据所述参考相似度以及阈值信息确定归一化后的参考相似度', 'NNP'), ('根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否对', 'NNP'), ('应相同人。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('-任一项所述的方法其特征在于所述提取所述第一人脸图像的', 'NNP'), ('第_人脸特征包括', 'NNP'), ('将所述第一人脸图像输入预先训练完成的神经网络通过所述神经网络输出所述第一', 'NNP'), ('人脸图像的第一人脸特征其中所述神经网络基于第一类型图像样本和第二类型图像样', 'NNP'), ('本训练得到所述第一类型图像样本和所述第二类型图像样本由不同类型的摄像头拍摄得', 'NNP'), ('到且所述第一类型图像样本和所述第二类型图像样本中包括人脸。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于所述神经网络基于所述第一类型图像', 'NNP'), ('样本、', 'NNP'), ('所述第二类型图像样本和混合类型图像样本训练得到所述混合类型图像样本由所', 'NNP'), ('述第一类型图像样本和所述第二类型图像样本配对而得。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('-任一项所述的方法其特征在于所述第一摄像头包括车载摄像', 'NNP'), ('头所述通过第一摄像头获取第一人脸图像包括', 'NNP'), ('通过所述车载摄像头获取所述第一人脸图像所述第一人脸图像包括车辆的用车人的', 'NNP'), ('人脸图像。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于所述用车人包括驾驶所述车辆的人、', 'NNP'), ('乘坐所述车辆的人、', 'NNP'), ('对所述车辆进行修理的人、', 'NNP'), ('给所述车辆加油的人以及控制所述车辆的', 'NNP'), ('人中的一项或多项。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于所述用车人包括驾驶所述车辆的人', 'NNP'), ('所述通过所述车载摄像头获取所述第一人脸图像包括', 'NNP'), ('在接收到触发指令的情况下通过所述车载摄像头获取所述第一人脸图像', 'NNP'), ('或者在所述车辆运行时通过所述车载摄像头获取所述第一人脸图像', 'NNP'), ('或者在所述车辆的运行速度达到参考速度的情况下通过所述车载摄像头获取所述', 'NNP'), ('第一人脸图像。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('-任一项所述的方法其特征在于所述第二人脸图像为对所述', 'NNP'), ('用车人进行人脸注册的图像所述将所述第一人脸特征与预先存储的第二人脸特征进行对', 'NNP'), ('比之前所述方法还包括', 'NNP'), ('通过所述第二摄像头获取所述第二人脸图像', 'NNP'), ('提取所述第二人脸图像的第二人脸特征', 'NNP'), ('保存所述第二人脸图像的第二人脸特征。', 'NNP'), ('、', 'NNP'), ('一种神经网络训练方法其特征在于包括', 'NNP'), ('获取第一类型图像样本和第二类型图像样本所述第一类型图像样本和所述第二类型', 'NNP'), ('图像样本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述第二类型图像样', 'NNP'), ('本中包括人脸', 'NNP'), ('根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于所述根据所述第一类型图像样本和所', 'NNP'), ('述第二类型图像样本训练神经网络包括', 'NNP'), ('将所述第一类型图像样本和所述第二类型图像样本配对得到所述第一类型图像样本', 'NNP'), ('和所述第二类型图像样本的混合类型图像样本', 'NNP'), ('根据所述第一类型图像样本、', 'NNP'), ('所述第二类型图像样本和所述混合类型图像样本训练', 'NNP'), ('所述神经网络。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于所述根据所述第一类型图像样本、', 'NNP'), ('所述第二类型图像样本和所述混合类型图像样本训练所述神经网络包括', 'NNP'), ('通过所述神经网络获取所述第一类型图像样本的人脸预测结果、', 'NNP'), ('所述第二类型图像样', 'NNP'), ('本的人脸预测结果和所述混合类型图像样本的人脸预测结果', 'NNP'), ('根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、', 'NNP'), ('所述第二类型图', 'NNP'), ('像样本的人脸预测结果和人脸标注结果之间的差异、', 'NNP'), ('以及所述混合类型图像样本的人脸预', 'NNP'), ('测结果和人脸标注结果的差异训练所述神经网络。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于所述神经网络中包括第一分类器、', 'NNP'), ('第二分类器和混合分类器所述通过所述神经网络获取所述第一类型图像样本的人脸预测', 'NNP'), ('结果、', 'NNP'), ('所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果', 'NNP'), ('包括', 'NNP'), ('将所述第一类型图像样本的人脸特征输入至所述第一分类器中得到所述第一类型图', 'NNP'), ('像样本的人脸预测结果', 'NNP'), ('将所述第二类型图像样本的人脸特征输入至所述第二分类器中得到所述第二类型图', 'NNP'), ('像样本的人脸预测结果', 'NNP'), ('将所述混合类型图像样本的人脸特征输入至所述混合分类器中得到所述混合类型图', 'NNP'), ('像样本的人脸预测结果。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于所述方法还包括', 'NNP'), ('在训练完成的所述神经网络中去除所述第一分类器、', 'NNP'), ('所述第二分类器和所述混合分类', 'NNP'), ('器得到用于进行人脸识别的神经网络。', 'NNP'), ('、', 'NNP'), ('一种人脸识别装置其特征在于包括', 'NNP'), ('第一获取单元用于通过第一摄像头获取第一人脸图像', 'NNP'), ('第一提取单元用于提取所述第一人脸图像的第一人脸特征', 'NNP'), ('对比单元用于将所述第一人脸特征与预先存储的第二人脸特征进行对比获得参考', 'NNP'), ('相似度所述第二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得所述第二', 'NNP'), ('摄像头与所述第一摄像头属于不同类型的摄像头', 'NNP'), ('确定单元用于根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否', 'NNP'), ('对应相同人。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于', 'NNP'), ('所述第一摄像头为热成像摄像头所述第二摄像头为可见光摄像头', 'NNP'), ('或者所述第一摄像头为可见光摄像头所述第一摄像头为热成像摄像头。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('或', 'NNP'), ('所述的装置其特征在于', 'NNP'), ('所述确定单元具体用于根据所述参考相似度、', 'NNP'), ('参考误报率以及相似度阈值确定所述', 'NNP'), ('第一人脸特征与所述第二人脸特征是否对应相同人其中不同的误报率对应不同的相似', 'NNP'), ('度阈值。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('或', 'NNP'), ('所述的装置其特征在于', 'NNP'), ('所述确定单元具体用于根据所述参考相似度以及阈值信息确定归一化后的参考相似', 'NNP'), ('度以及根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否', 'NNP'), ('对应相同人。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('-任_项所述的装置其特征在于', 'NNP'), ('所述第一提取单元具体用于将所述第一人脸图像输入预先训练完成的神经网络通', 'NNP'), ('过所述神经网络输出所述第一人脸图像的第一人脸特征其中所述神经网络基于第一类', 'NNP'), ('型图像样本和第二类型图像样本训练得到所述第一类型图像样本和所述第二类型图像样', 'NNP'), ('本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述第二类型图像样本中包', 'NNP'), ('括人脸。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于所述神经网络基于所述第一类型图', 'NNP'), ('像样本、', 'NNP'), ('所述第二类型图像样本和混合类型图像样本训练得到所述混合类型图像样本由', 'NNP'), ('所述第一类型图像样本和所述第二类型图像样本配对而得。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('-任一项所述的装置其特征在于所述第一摄像头包括车载', 'NNP'), ('摄像头', 'NNP'), ('所述第一获取单元具体用于通过所述车载摄像头获取所述第一人脸图像所述第一', 'NNP'), ('人脸图像包括车辆的用车人的人脸图像。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于所述用车人包括驾驶所述车辆的人、', 'NNP'), ('乘坐所述车辆的人、', 'NNP'), ('对所述车辆进行修理的人、', 'NNP'), ('给所述车辆加油的人以及控制所述车辆的', 'NNP'), ('人中的一项或多项。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于所述用车人包括驾驶所述车辆的人', 'NNP'), ('所述第一获取单元具体用于在接收到触发指令的情况下通过所述车载摄像头获取所述', 'NNP'), ('第一人脸图像', 'NNP'), ('或者所述第一获取单元具体用于在所述车辆运行时通过所述车载摄像头获取所', 'NNP'), ('述第', 'NNP'), ('_人脸图像', 'NNP'), ('或者所述第一获取单元具体用于在所述车辆的运行速度达到参考速度的情况下', 'NNP'), ('通过所述车载摄像头获取所述第一人脸图像。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('-任一项所述的装置其特征在于所述第二人脸图像为对所', 'NNP'), ('述用车人进行人脸注册的图像所述装置还包括', 'NNP'), ('第二获取单元用于通过所述第二摄像头获取所述第二人脸图像', 'NNP'), ('第二提取单元用于提取所述第二人脸图像的第二人脸特征', 'NNP'), ('保存单元用于保存所述第二人脸图像的第二人脸特征。', 'NNP'), ('、', 'NNP'), ('一种神经网络训练装置其特征在于包括', 'NNP'), ('获取单元用于获取第一类型图像样本和第二类型图像样本所述第一类型图像样本', 'NNP'), ('和所述第二类型图像样本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述', 'NNP'), ('第二类型图像样本中包括人脸', 'NNP'), ('训练单元用于根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于所述训练单元包括', 'NNP'), ('配对子单元用于将所述第一类型图像样本和所述第二类型图像样本配对得到所述', 'NNP'), ('第一类型图像样本和所述第二类型图像样本的混合类型图像样本', 'NNP'), ('训练子单元用于根据所述第一类型图像样本、', 'NNP'), ('所述第二类型图像样本和所述混合类', 'NNP'), ('型图像样本训练所述神经网络。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于', 'NNP'), ('所述训练子单元具体用于通过所述神经网络获取所述第一类型图像样本的人脸预测', 'NNP'), ('结果、', 'NNP'), ('所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果', 'NNP'), ('以及根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、', 'NNP'), ('所述第二类型图', 'NNP'), ('像样本的人脸预测结果和人脸标注结果之间的差异、', 'NNP'), ('以及所述混合类型图像样本的人脸预', 'NNP'), ('测结果和人脸标注结果的差异训练所述神经网络。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于所述神经网络中包括第一分类器、', 'NNP'), ('第二分类器和混合分类器', 'NNP'), ('所述训练子单元具体用于将所述第一类型图像样本的人脸特征输入至所述第一分类', 'NNP'), ('器中得到所述第一类型图像样本的人脸预测结果以及将所述第二类型图像样本的人脸', 'NNP'), ('特征输入至所述第二分类器中得到所述第二类型图像样本的人脸预测结果以及将所述', 'NNP'), ('混合类型图像样本的人脸特征输入至所述混合分类器中得到所述混合类型图像样本的人', 'NNP'), ('脸预测结果。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于所述装置还包括', 'NNP'), ('神经网络应用单元用于在训练完成的所述神经网络中去除所述第一分类器、', 'NNP'), ('所述第', 'NNP'), ('二分类器和所述混合分类器得到用于进行人脸识别的神经网络。', 'NNP'), ('、', 'NNP'), ('一种电子设备其特征在于包括处理器和存储器所述处理器和所述存储器耦', 'NNP'), ('合其中所述存储器用于存储程序指令所述程序指令被所述处理器执行时使所述处', 'NNP'), ('理器执行权利要求', 'NNP'), ('-任一项所述的方法和或使所述处理器执行权利要求', 'NNP'), ('-任一', 'NNP'), ('项所述的方法。', 'NNP'), ('、', 'NNP'), ('一种计算机可读存储介质其特征在于所述计算机可读存储介质中存储有计算', 'NNP'), ('机程序所述计算机程序包括程序指令所述程序指令当被处理器执行时使所述处理器', 'NNP'), ('执行权利要求', 'NNP'), ('-任一项所述的方法和或使所述处理器执行权利要求', 'NNP'), ('-任一项所', 'NNP'), ('述的方法。', 'NNP'), ('system', 'NN'), ('alerting', 'VBG'), ('vision', 'NN'), ('impairment', 'NN'), ('said', 'VBD'), ('system', 'NN'), ('comprising', 'VBG'), ('processing', 'VBG'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('receiving', 'VBG'), ('scene', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('scene', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('environment', 'NN'), ('identifying', 'VBG'), ('scene', 'NN'), ('data', 'NNS'), ('certain', 'JJ'), ('consumer', 'NN'), ('identifying', 'VBG'), ('event', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('upon', 'IN'), ('identification', 'NN'), ('event', 'NN'), ('sending', 'VBG'), ('notification', 'NN'), ('relating', 'VBG'), ('vision', 'NN'), ('impairment', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('sensing', 'VBG'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('detecting', 'VBG'), ('scene', 'NN'), ('data', 'NNS'), ('system', 'NN'), ('wherein', 'NN'), ('said', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('sensing', 'VBG'), ('unit', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('least', 'JJS'), ('one', 'CD'), ('imaging', 'VBG'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('capturing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('least', 'JJS'), ('portion', 'NN'), ('consumer', 'NN'), (\"'s\", 'POS'), ('body', 'NN'), ('least', 'VBD'), ('one', 'CD'), ('motion', 'NN'), ('detector', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('detecting', 'VBG'), ('consumer', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('motion', 'NN'), ('consumer', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('eye', 'NN'), ('tracker', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('eye', 'NN'), ('motion', 'NN'), ('consumer', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('imaging', 'VBG'), ('unit', 'NN'), ('comprises', 'VBZ'), ('cameras', 'NNS'), ('placed', 'VBD'), ('different', 'JJ'), ('heights', 'NNS'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('sensing', 'VBG'), ('unit', 'NN'), ('accommodated', 'VBD'), ('optical', 'JJ'), ('digital', 'JJ'), ('eyewear', 'NN'), ('frame', 'NN'), ('display', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('processing', 'NN'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('identifying', 'VBG'), ('consumer', 'NN'), (\"'s\", 'POS'), ('condition', 'NN'), ('said', 'VBD'), ('consumer', 'NN'), (\"'s\", 'POS'), ('condition', 'NN'), ('comprising', 'VBG'), ('consumer', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('consumer', 'NN'), (\"'s\", 'POS'), ('position', 'NN'), ('location', 'NN'), ('relative', 'JJ'), ('least', 'JJS'), ('one', 'CD'), ('object', 'JJ'), ('consumer', 'NN'), (\"'s\", 'POS'), ('environment', 'NN'), ('said', 'VBD'), ('consumer', 'NN'), ('data', 'NNS'), ('comprises', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('consumer', 'NN'), (\"'s\", 'POS'), ('face', 'NN'), ('eyewear', 'JJ'), ('posture', 'NN'), ('position', 'NN'), ('sound', 'NN'), ('motion', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('event', 'NN'), ('comprises', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('position', 'NN'), ('head', 'NN'), ('increase', 'NN'), ('decrease', 'NN'), ('viewing', 'VBG'), ('distance', 'NN'), ('consumer', 'NN'), ('viewed', 'VBD'), ('object', 'JJ'), ('changing', 'VBG'), ('position', 'NN'), ('eyeglasses', 'NNS'), ('worn', 'JJ'), ('consumer', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('event', 'NN'), ('identified', 'VBN'), ('identifying', 'JJ'), ('feature', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('performing', 'VBG'), ('bruckner', 'JJ'), ('test', 'NN'), ('performing', 'VBG'), ('hirschberg', 'JJ'), ('test', 'NN'), ('measuring', 'VBG'), ('blink', 'NN'), ('count', 'NN'), ('frequency', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('feature', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('comprises', 'NNS'), ('squinting', 'VBG'), ('head', 'NN'), ('certain', 'JJ'), ('distances', 'NNS'), ('object', 'VBP'), ('consumer', 'NN'), (\"'s\", 'POS'), ('eyes', 'NNS'), ('certain', 'JJ'), ('position', 'NN'), ('eyeglasses', 'VBZ'), ('consumer', 'NN'), (\"'s\", 'POS'), ('face', 'NN'), ('strabismus', 'NN'), ('cataracts', 'VBZ'), ('reflections', 'NNS'), ('eye', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('notification', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('data', 'NN'), ('indicative', 'NN'), ('identified', 'VBN'), ('event', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('identified', 'JJ'), ('consumer', 'NN'), ('ophthalmologic', 'NN'), ('recommendations', 'NNS'), ('based', 'VBN'), ('identified', 'JJ'), ('event', 'NN'), ('lack', 'NN'), ('events', 'NNS'), ('appointment', 'JJ'), ('vision', 'NN'), ('test', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('processing', 'NN'), ('unit', 'NN'), ('comprises', 'VBZ'), ('memory', 'NN'), ('storing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('reference', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('notification', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('follow-up', 'JJ'), ('notification', 'NN'), ('system', 'NN'), ('wherein', 'NN'), ('said', 'VBD'), ('processing', 'NN'), ('unit', 'NN'), ('configured', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('identifying', 'VBG'), ('event', 'NN'), ('upon', 'IN'), ('comparison', 'NN'), ('detected', 'VBN'), ('data', 'NNS'), ('reference', 'NN'), ('data', 'NNS'), ('determining', 'VBG'), ('probability', 'NN'), ('vision', 'NN'), ('impairment', 'JJ'), ('consumer', 'NN'), ('based', 'VBN'), ('comparison', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('processing', 'NN'), ('unit', 'NN'), ('comprises', 'VBZ'), ('communication', 'NN'), ('interface', 'NN'), ('configured', 'VBD'), ('sending', 'VBG'), ('notification', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('identified', 'JJ'), ('consumer', 'NN'), ('third', 'NNP'), ('party', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('processing', 'NN'), ('unit', 'NN'), ('configured', 'VBD'), ('providing', 'VBG'), ('frame', 'NN'), ('recommendation', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('memory', 'NN'), ('configured', 'VBD'), ('storing', 'JJ'), ('database', 'NN'), ('including', 'VBG'), ('multiplicity', 'NN'), ('data', 'NNS'), ('sets', 'NNS'), ('related', 'JJ'), ('spectacle', 'NN'), ('frame', 'NN'), ('models', 'NNS'), ('sizes', 'VBZ'), ('system', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('said', 'VBD'), ('processing', 'VBG'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('correlate', 'NN'), ('frames', 'NNS'), ('parameters', 'NNS'), ('ophthalmic', 'VBP'), ('prescriptions', 'NNS'), ('system', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('said', 'VBD'), ('processing', 'VBG'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('correlate', 'NN'), ('frames', 'NNS'), ('parameters', 'NNS'), ('facial', 'JJ'), ('features', 'NNS'), ('system', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('said', 'VBD'), ('processing', 'VBG'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('correlate', 'NN'), ('frames', 'NNS'), ('parameters', 'NNS'), ('eyewear', 'VBP'), ('preferences', 'NNS'), ('system', 'NN'), ('according', 'VBG'), ('comprising', 'VBG'), ('server', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('computer', 'NN'), ('entity', 'NN'), ('linked', 'VBN'), ('server', 'RB'), ('via', 'IN'), ('network', 'NN'), ('wherein', 'NN'), ('said', 'VBD'), ('network', 'NN'), ('configured', 'VBD'), ('receive', 'JJ'), ('respond', 'NN'), ('requests', 'NNS'), ('sent', 'VBD'), ('across', 'IN'), ('network', 'NN'), ('transmitting', 'VBG'), ('one', 'CD'), ('modules', 'NNS'), ('computer', 'NN'), ('executable', 'JJ'), ('program', 'NN'), ('instructions', 'NNS'), ('displayable', 'JJ'), ('data', 'NNS'), ('network', 'NN'), ('connected', 'VBN'), ('computer', 'NN'), ('platform', 'NN'), ('response', 'NN'), ('request', 'NN'), ('wherein', 'NN'), ('said', 'VBD'), ('modules', 'NNS'), ('include', 'VBP'), ('modules', 'NNS'), ('configured', 'VBD'), ('receive', 'JJ'), ('transmit', 'NN'), ('information', 'NN'), ('transmitting', 'VBG'), ('frame', 'NN'), ('recommendation', 'NN'), ('optical', 'JJ'), ('lens', 'VBZ'), ('option', 'NN'), ('recommendation', 'NN'), ('based', 'VBN'), ('received', 'VBN'), ('information', 'NN'), ('display', 'NN'), ('network', 'NN'), ('connected', 'VBN'), ('computer', 'NN'), ('platform', 'NN'), ('computer', 'NN'), ('program', 'NN'), ('instructions', 'NNS'), ('stored', 'VBD'), ('local', 'JJ'), ('storage', 'NN'), ('executed', 'VBD'), ('processing', 'VBG'), ('unit', 'NN'), ('cause', 'NN'), ('processing', 'VBG'), ('unit', 'NN'), ('receive', 'JJ'), ('data', 'NNS'), ('indicative', 'JJ'), ('scene', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('environment', 'NN'), ('identify', 'VB'), ('data', 'NNS'), ('certain', 'JJ'), ('consumer', 'NN'), ('identify', 'VB'), ('event', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('upon', 'IN'), ('identification', 'NN'), ('event', 'NN'), ('send', 'VB'), ('notification', 'NN'), ('relating', 'VBG'), ('vision', 'NN'), ('impairment', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('stored', 'VBD'), ('tangible', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('library', 'JJ'), ('software', 'NN'), ('modules', 'NNS'), ('cause', 'VBP'), ('computer', 'NN'), ('executing', 'VBG'), ('prompt', 'JJ'), ('information', 'NN'), ('pertinent', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('eyeglasses', 'VBZ'), ('recommendation', 'NN'), ('optical', 'JJ'), ('lens', 'VBZ'), ('option', 'NN'), ('recommendation', 'NN'), ('store', 'NN'), ('said', 'VBD'), ('information', 'NN'), ('display', 'NN'), ('eyewear', 'VBP'), ('recommendations', 'NNS'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('wherein', 'NN'), ('said', 'VBD'), ('library', 'JJ'), ('comprises', 'NNS'), ('module', 'NN'), ('frame', 'NN'), ('selection', 'NN'), ('point', 'NN'), ('sales', 'NNS'), ('advertising', 'VBG'), ('computer', 'NN'), ('platform', 'NN'), ('facilitating', 'VBG'), ('eye', 'NN'), ('glasses', 'NNS'), ('marketing', 'VBG'), ('selection', 'NN'), ('comprising', 'VBG'), ('camera', 'NN'), ('configured', 'VBD'), ('execute', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('instructions', 'NNS'), ('cause', 'VBP'), ('take', 'VB'), ('consumer', 'NN'), ('identify', 'NN'), ('certain', 'JJ'), ('consumer', 'NN'), ('identify', 'VB'), ('event', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('upon', 'IN'), ('identification', 'NN'), ('event', 'NN'), ('sending', 'VBG'), ('notification', 'NN'), ('relating', 'VBG'), ('vision', 'NN'), ('impairment', 'JJ'), ('local', 'JJ'), ('storage', 'NN'), ('executable', 'JJ'), ('instructions', 'NNS'), ('carrying', 'VBG'), ('storage', 'NN'), ('information', 'NN'), ('alerting', 'VBG'), ('vision', 'NN'), ('impairment', 'NN'), ('said', 'VBD'), ('comprising', 'VBG'), ('identifying', 'VBG'), ('certain', 'JJ'), ('individual', 'JJ'), ('scene', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('scene', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('environment', 'NN'), ('identifying', 'VBG'), ('event', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('upon', 'IN'), ('identification', 'NN'), ('event', 'NN'), ('sending', 'VBG'), ('notification', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('data', 'NNS'), ('indicative', 'JJ'), ('scene', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('retail', 'JJ'), ('environment', 'NN'), ('wherein', 'NN'), ('detecting', 'VBG'), ('data', 'NNS'), ('indicative', 'JJ'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('capturing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('detecting', 'VBG'), ('data', 'NNS'), ('indicative', 'JJ'), ('motion', 'NN'), ('consumer', 'NN'), ('eye', 'NN'), ('motion', 'NN'), ('consumer', 'NN'), ('wherein', 'IN'), ('capturing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('comprises', 'VBZ'), ('continuously', 'RB'), ('recording', 'VBG'), ('scene', 'NN'), ('one', 'CD'), ('comprising', 'NN'), ('identifying', 'VBG'), ('data', 'NNS'), ('consumer', 'NN'), (\"'\", 'POS'), ('condition', 'NN'), ('including', 'VBG'), ('data', 'NNS'), ('indicative', 'JJ'), ('consumer', 'NN'), (\"'s\", 'POS'), ('position', 'NN'), ('location', 'NN'), ('relative', 'JJ'), ('consumer', 'NN'), (\"'s\", 'POS'), ('environment', 'NN'), ('said', 'VBD'), ('data', 'NNS'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), (\"'s\", 'POS'), ('face', 'NN'), ('posture', 'NN'), ('position', 'NN'), ('sound', 'NN'), ('motion', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('event', 'NN'), ('comprises', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('position', 'NN'), ('head', 'NN'), ('increase', 'NN'), ('decrease', 'NN'), ('viewing', 'VBG'), ('distance', 'NN'), ('consumer', 'NN'), ('viewed', 'VBD'), ('object', 'JJ'), ('changing', 'VBG'), ('position', 'NN'), ('eyeglasses', 'NNS'), ('worn', 'JJ'), ('consumer', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('identifying', 'VBG'), ('event', 'NN'), ('comprises', 'NNS'), ('identifying', 'VBG'), ('feature', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('performing', 'VBG'), ('bruckner', 'JJ'), ('test', 'NN'), ('performing', 'VBG'), ('hirschberg', 'JJ'), ('test', 'NN'), ('measuring', 'VBG'), ('blink', 'NN'), ('countfrequency', 'NN'), ('wherein', 'JJ'), ('feature', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('comprises', 'NNS'), ('squinting', 'VBG'), ('head', 'NN'), ('certain', 'JJ'), ('distances', 'NNS'), ('object', 'VBP'), ('consumer', 'NN'), (\"'s\", 'POS'), ('eyes', 'NNS'), ('certain', 'JJ'), ('position', 'NN'), ('eyeglasses', 'VBZ'), ('consumer', 'NN'), (\"'s\", 'POS'), ('face', 'NN'), ('strabismus', 'NN'), ('cataracts', 'VBZ'), ('reflections', 'NNS'), ('eye', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('identifying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('retail', 'JJ'), ('environment', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('receiving', 'VBG'), ('data', 'NNS'), ('characterizing', 'VBG'), ('retail', 'JJ'), ('environment', 'NN'), ('performing', 'VBG'), ('face', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('sending', 'VBG'), ('notification', 'NN'), ('comprising', 'VBG'), ('sending', 'VBG'), ('notification', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('identified', 'JJ'), ('consumer', 'NN'), ('third', 'JJ'), ('party', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('notification', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('data', 'NN'), ('indicative', 'NN'), ('identified', 'VBN'), ('event', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('identified', 'JJ'), ('consumer', 'NN'), ('ophthalmologic', 'NN'), ('recommendations', 'NNS'), ('based', 'VBN'), ('identified', 'JJ'), ('event', 'NN'), ('lack', 'NN'), ('events', 'NNS'), ('appointment', 'JJ'), ('vision', 'NN'), ('test', 'NN'), ('one', 'CD'), ('comprising', 'VBG'), ('storing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('reference', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('notification', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('follow-up', 'JJ'), ('notification', 'NN'), ('comprising', 'VBG'), ('identifying', 'VBG'), ('event', 'NN'), ('upon', 'IN'), ('comparison', 'NN'), ('detected', 'VBN'), ('data', 'NNS'), ('reference', 'NN'), ('data', 'NNS'), ('determining', 'VBG'), ('probability', 'NN'), ('vision', 'NN'), ('impairment', 'JJ'), ('consumer', 'NN'), ('based', 'VBN'), ('comparison', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('intended', 'VBN'), ('stored', 'JJ'), ('memory', 'NN'), ('unit', 'NN'), ('computer', 'NN'), ('system', 'NN'), ('removable', 'JJ'), ('memory', 'NN'), ('medium', 'NN'), ('adapted', 'VBD'), ('cooperate', 'JJ'), ('reader', 'NN'), ('unit', 'NN'), ('comprising', 'VBG'), ('instructions', 'NNS'), ('implementing', 'VBG'), ('according', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagging_c = nltk.pos_tag(tokenized_vector_c)\n",
    "print(pos_tagging_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5c5701c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('electronic', 'JJ'), ('apparatus', 'NN'), ('including', 'VBG'), ('capturing', 'VBG'), ('storage', 'NN'), ('operation', 'NN'), ('method', 'NN'), ('thereof', 'NN'), ('provided', 'VBD'), ('capturing', 'VBG'), ('captures', 'NNS'), ('storage', 'NN'), ('records', 'NNS'), ('modules', 'NNS'), ('coupled', 'VBD'), ('capturing', 'VBG'), ('storage', 'NN'), ('configured', 'VBD'), ('configure', 'NN'), ('capturing', 'VBG'), ('capture', 'NN'), ('head', 'NN'), ('perform', 'NN'), ('obtain', 'VB'), ('detect', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('within', 'IN'), ('estimate', 'JJ'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'NN'), ('according', 'VBG'), ('facial', 'JJ'), ('landmarks', 'NN'), ('calculate', 'NN'), ('gaze', 'NN'), ('position', 'NN'), ('gazes', 'VBZ'), ('screen', 'JJ'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'JJ'), ('rotation', 'NN'), ('reference', 'NN'), ('angle', 'NN'), ('predetermined', 'VBD'), ('calibration', 'NN'), ('positions', 'NNS'), ('configure', 'VBP'), ('screen', 'JJ'), ('display', 'NN'), ('corresponding', 'VBG'), ('visual', 'JJ'), ('effect', 'NN'), ('according', 'VBG'), ('gaze', 'NN'), ('positionthe', 'NN'), ('present', 'JJ'), ('disclosure', 'NN'), ('provides', 'VBZ'), ('product', 'NN'), ('thereof', 'NN'), ('adopts', 'NNS'), ('fusion', 'VBP'), ('method', 'JJ'), ('perform', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('computations', 'NNS'), ('technical', 'JJ'), ('effects', 'NNS'), ('present', 'JJ'), ('disclosure', 'NN'), ('include', 'VBP'), ('fewer', 'JJR'), ('computations', 'NNS'), ('less', 'RBR'), ('power', 'NN'), ('consumptiona', 'NN'), ('method', 'NN'), ('detecting', 'VBG'), ('body', 'NN'), ('information', 'NN'), ('passengers', 'NNS'), ('vehicle', 'NN'), ('based', 'VBN'), ('humans', 'NNS'), (\"'\", 'POS'), ('status', 'NN'), ('provided', 'VBD'), ('method', 'NN'), ('includes', 'VBZ'), ('steps', 'NNS'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('inputting', 'JJ'), ('interior', 'JJ'), ('vehicle', 'NN'), ('face', 'NN'), ('network', 'NN'), ('detect', 'JJ'), ('faces', 'VBZ'), ('passengers', 'NNS'), ('output', 'NN'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('inputting', 'VBG'), ('interior', 'NN'), ('body', 'NN'), ('network', 'NN'), ('detect', 'JJ'), ('bodies', 'NNS'), ('output', 'NN'), ('body-part', 'JJ'), ('length', 'NN'), ('information', 'NN'), ('b', 'NN'), ('retrieving', 'VBG'), ('specific', 'JJ'), ('height', 'JJ'), ('mapping', 'NN'), ('information', 'NN'), ('referring', 'VBG'), ('height', 'NN'), ('mapping', 'VBG'), ('table', 'JJ'), ('ratios', 'NNS'), ('segment', 'NN'), ('body', 'NN'), ('portions', 'NNS'), ('human', 'JJ'), ('groups', 'NNS'), ('heights', 'NNS'), ('per', 'IN'), ('human', 'JJ'), ('groups', 'NNS'), ('acquiring', 'VBG'), ('specific', 'JJ'), ('height', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('retrieving', 'VBG'), ('specific', 'JJ'), ('weight', 'NN'), ('mapping', 'VBG'), ('information', 'NN'), ('weight', 'NN'), ('mapping', 'NN'), ('table', 'JJ'), ('correlations', 'NNS'), ('heights', 'NNS'), ('weights', 'NNS'), ('per', 'IN'), ('human', 'JJ'), ('groups', 'NNS'), ('acquiring', 'VBG'), ('weight', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('referring', 'VBG'), ('specific', 'JJ'), ('heighttechniques', 'NNS'), ('related', 'VBN'), ('improved', 'JJ'), ('video', 'NN'), ('coding', 'VBG'), ('based', 'VBN'), ('face', 'NN'), ('detection', 'NN'), ('region', 'NN'), ('extraction', 'NN'), ('tracking', 'VBG'), ('discussed', 'VBN'), ('techniques', 'NNS'), ('may', 'MD'), ('include', 'VB'), ('performing', 'VBG'), ('facial', 'JJ'), ('search', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('determine', 'JJ'), ('candidate', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('testing', 'VBG'), ('candidate', 'NN'), ('based', 'VBN'), ('skin', 'JJ'), ('tone', 'NN'), ('information', 'NN'), ('determine', 'NN'), ('valid', 'JJ'), ('invalid', 'JJ'), ('rejecting', 'NN'), ('invalid', 'JJ'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('valid', 'JJ'), ('generate', 'NN'), ('coded', 'VBD'), ('bitstreama', 'NN'), ('method', 'NN'), ('managing', 'VBG'), ('smart', 'JJ'), ('database', 'NN'), ('stores', 'NNS'), ('facial', 'JJ'), ('face', 'NN'), ('provided', 'VBD'), ('method', 'NN'), ('includes', 'VBZ'), ('steps', 'NNS'), ('managing', 'VBG'), ('counting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('specific', 'JJ'), ('person', 'NN'), ('smart', 'JJ'), ('database', 'NN'), ('new', 'JJ'), ('facial', 'JJ'), ('continuously', 'RB'), ('stored', 'VBN'), ('determining', 'VBG'), ('whether', 'IN'), ('first', 'JJ'), ('counted', 'VBN'), ('value', 'NN'), ('representing', 'VBG'), ('count', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('satisfies', 'NNS'), ('first', 'RB'), ('set', 'VBN'), ('value', 'NN'), ('b', 'NN'), ('first', 'RB'), ('counted', 'VBD'), ('value', 'NN'), ('satisfies', 'NNS'), ('first', 'RB'), ('set', 'VBN'), ('value', 'NN'), ('inputting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('generate', 'VBP'), ('quality', 'NN'), ('scores', 'NNS'), ('specific', 'JJ'), ('facial', 'JJ'), ('aggregation', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('second', 'NN'), ('counted', 'VBD'), ('value', 'NN'), ('representing', 'VBG'), ('count', 'NN'), ('specific', 'JJ'), ('quality', 'NN'), ('scores', 'NNS'), ('among', 'IN'), ('quality', 'NN'), ('scores', 'NNS'), ('highest', 'JJS'), ('counting', 'VBG'), ('thereof', 'JJ'), ('satisfies', 'NNS'), ('second', 'VBP'), ('set', 'VBN'), ('value', 'NN'), ('deleting', 'VBG'), ('part', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('uncounted', 'JJ'), ('quality', 'NN'), ('scores', 'NNS'), ('smart', 'VBP'), ('databasea', 'NN'), ('capable', 'JJ'), ('determining', 'VBG'), ('algorithms', 'NN'), ('applied', 'VBD'), ('regions', 'NNS'), ('interest', 'NN'), ('within', 'IN'), ('digital', 'JJ'), ('representations', 'NNS'), ('presented', 'VBD'), ('preprocessing', 'VBG'), ('module', 'NN'), ('utilizes', 'IN'), ('one', 'CD'), ('feature', 'NN'), ('identification', 'NN'), ('algorithms', 'FW'), ('determine', 'NN'), ('regions', 'NNS'), ('interest', 'NN'), ('based', 'VBN'), ('feature', 'NN'), ('density', 'NN'), ('preprocessing', 'VBG'), ('modules', 'NNS'), ('leverages', 'VBZ'), ('feature', 'VBP'), ('density', 'NN'), ('signature', 'NN'), ('region', 'NN'), ('determine', 'NN'), ('diverse', 'NN'), ('modules', 'NNS'), ('operate', 'VBP'), ('region', 'NN'), ('interest', 'NN'), ('specific', 'JJ'), ('embodiment', 'NN'), ('focuses', 'NNS'), ('structured', 'VBD'), ('documents', 'NNS'), ('also', 'RB'), ('presented', 'VBD'), ('disclosed', 'JJ'), ('approach', 'NN'), ('enhanced', 'VBD'), ('addition', 'NN'), ('object', 'NN'), ('classifier', 'NN'), ('classifies', 'NNS'), ('types', 'VBZ'), ('objects', 'NNS'), ('found', 'VBN'), ('regions', 'NNS'), ('interestdisclosed', 'VBN'), ('mobile', 'JJ'), ('terminal', 'NN'), ('mobile', 'JJ'), ('terminal', 'NN'), ('may', 'MD'), ('include', 'VB'), ('front', 'JJ'), ('camera', 'NN'), ('obtaining', 'VBG'), ('face', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('tilted', 'VBD'), ('certain', 'JJ'), ('angle', 'NN'), ('disposed', 'VBD'), ('adjacent', 'JJ'), ('front', 'JJ'), ('camera', 'NN'), ('obtain', 'VB'), ('metadata', 'JJ'), ('face', 'NN'), ('controller', 'NN'), ('obtaining', 'VBG'), ('distance', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('front', 'NN'), ('camera', 'NN'), ('distance', 'NN'), ('enabling', 'VBG'), ('area', 'NN'), ('overlap', 'IN'), ('region', 'NN'), ('first', 'JJ'), ('region', 'NN'), ('representing', 'VBG'), ('range', 'NN'), ('photographable', 'JJ'), ('front', 'NN'), ('camera', 'NN'), ('overlaps', 'VBZ'), ('second', 'JJ'), ('region', 'NN'), ('representing', 'VBG'), ('range', 'NN'), ('photographable', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('maximumthis', 'NN'), ('disclosure', 'NN'), ('provides', 'VBZ'), ('methods', 'NNS'), ('apparatus', 'RB'), ('including', 'VBG'), ('computer', 'NN'), ('programs', 'NNS'), ('encoded', 'VBD'), ('computer', 'NN'), ('storage', 'NN'), ('media', 'NNS'), ('intelligent', 'JJ'), ('routing', 'VBG'), ('notifications', 'NNS'), ('related', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('one', 'CD'), ('aspect', 'JJ'), ('smart', 'JJ'), ('television', 'NN'), ('tv', 'NN'), ('implemented', 'VBD'), ('track', 'NN'), (\"'s\", 'POS'), ('tv', 'NN'), ('watching', 'VBG'), ('behavior', 'JJ'), ('anticipate', 'NN'), ('programming', 'NN'), ('based', 'VBN'), ('behavior', 'JJ'), ('aspects', 'NNS'), ('smart', 'JJ'), ('tv', 'NN'), ('implemented', 'VBD'), ('detect', 'NN'), (\"'s\", 'POS'), ('presence', 'NN'), ('based', 'VBN'), ('detection', 'NN'), ('automatically', 'RB'), ('change', 'JJ'), ('tv', 'NN'), ('channel', 'NN'), ('media', 'NNS'), ('programming', 'VBG'), ('analyzed', 'VBN'), ('desirable', 'JJ'), ('aspects', 'NNS'), ('smart', 'JJ'), ('tv', 'NN'), ('implemented', 'VBD'), ('transmit', 'NN'), ('notification', 'NN'), ('instructions', 'NNS'), ('electronic', 'JJ'), ('within', 'IN'), ('network', 'NN'), ('attempt', 'NN'), ('alert', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('additionally', 'RB'), ('smart', 'JJ'), ('tv', 'NN'), ('implemented', 'VBD'), ('transmit', 'NN'), ('detection', 'NN'), ('instructions', 'NNS'), ('electronic', 'JJ'), ('within', 'IN'), ('network', 'NN'), ('whereby', 'WRB'), ('electronic', 'JJ'), ('attempt', 'NN'), ('detect', 'NN'), (\"'s\", 'POS'), ('presence', 'NN'), ('voice', 'NN'), ('configured', 'VBD'), ('output', 'NN'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('including', 'VBG'), ('pixels', 'NNS'), ('pixel', 'JJ'), ('corresponds', 'VBZ'), ('one', 'CD'), ('sensors', 'NNS'), ('sensor', 'VBP'), ('array', 'JJ'), ('camera', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('depth', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('light', 'JJ'), ('sub-band', 'JJ'), ('spectral', 'JJ'), ('illuminators', 'NNS'), ('camera', 'VBP'), ('face', 'NN'), ('machine', 'NN'), ('previously', 'RB'), ('trained', 'VBN'), ('set', 'VBN'), ('labeled', 'JJ'), ('training', 'NN'), ('depth+multi-spectral', 'JJ'), ('structure', 'NN'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('confidence', 'NN'), ('value', 'NN'), ('indicating', 'VBG'), ('likelihood', 'JJ'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('includes', 'VBZ'), ('faceembodiments', 'NNS'), ('present', 'JJ'), ('disclosure', 'NN'), ('relate', 'NN'), ('processing', 'NN'), ('method', 'NN'), ('apparatus', 'NN'), ('electronic', 'JJ'), ('method', 'NN'), ('includes', 'VBZ'), ('acquiring', 'VBG'), ('photo', 'NN'), ('album', 'NN'), ('obtained', 'VBD'), ('face', 'NN'), ('clustering', 'VBG'), ('collecting', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('respective', 'JJ'), ('photo', 'NN'), ('album', 'NN'), ('acquiring', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('selecting', 'VBG'), ('cover', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('taking', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('setting', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('photo', 'NN'), ('albumtechniques', 'NNS'), ('described', 'VBD'), ('herein', 'JJ'), ('provide', 'IN'), ('location-based', 'JJ'), ('access', 'NN'), ('control', 'NN'), ('secured', 'VBD'), ('resources', 'NNS'), ('generally', 'RB'), ('described', 'VBN'), ('configurations', 'NNS'), ('disclosed', 'VBD'), ('herein', 'RBR'), ('enable', 'JJ'), ('dynamically', 'RB'), ('modify', 'JJ'), ('access', 'NN'), ('secured', 'VBD'), ('resources', 'NNS'), ('based', 'VBN'), ('one', 'CD'), ('location-related', 'JJ'), ('actions', 'NNS'), ('example', 'NN'), ('techniques', 'NNS'), ('disclosed', 'VBN'), ('herein', 'RB'), ('enable', 'JJ'), ('computing', 'VBG'), ('control', 'NN'), ('access', 'NN'), ('resources', 'NNS'), ('computing', 'VBG'), ('display', 'NN'), ('secured', 'VBN'), ('locations', 'NNS'), ('secured', 'VBN'), ('data', 'NNS'), ('configurations', 'NNS'), ('techniques', 'NNS'), ('disclosed', 'VBD'), ('herein', 'RBR'), ('enable', 'JJ'), ('controlled', 'JJ'), ('access', 'NN'), ('secured', 'VBD'), ('resources', 'NNS'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('invitation', 'NN'), ('associated', 'VBN'), ('location', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicating', 'VBG'), ('location', 'NN'), ('one', 'CD'), ('embodiment', 'NN'), ('provides', 'VBZ'), ('method', 'JJ'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('piece', 'NN'), ('content', 'NN'), ('salient', 'NN'), ('moments', 'NNS'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('method', 'NN'), ('comprises', 'NNS'), ('based', 'VBN'), ('salient', 'JJ'), ('moments', 'NNS'), ('data', 'NNS'), ('determining', 'VBG'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('method', 'NN'), ('comprises', 'VBZ'), ('displaying', 'VBG'), ('viewport', 'NN'), ('display', 'NN'), ('movement', 'NN'), ('viewport', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('path', 'NN'), ('playback', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('method', 'NN'), ('comprises', 'VBZ'), ('generating', 'VBG'), ('augmentation', 'NN'), ('salient', 'JJ'), ('moment', 'NN'), ('occurring', 'VBG'), ('piece', 'NN'), ('content', 'NN'), ('presenting', 'VBG'), ('augmentation', 'NN'), ('viewport', 'NN'), ('portion', 'NN'), ('playback', 'NN'), ('augmentation', 'NN'), ('comprises', 'VBZ'), ('interactive', 'JJ'), ('hint', 'NN'), ('guiding', 'VBG'), ('viewport', 'NN'), ('salient', 'NN'), ('momenta', 'VBD'), ('computer-implemented', 'JJ'), ('method', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('provided', 'VBD'), ('facial', 'JJ'), ('method', 'NN'), ('includes', 'VBZ'), ('receiving', 'VBG'), ('method', 'NN'), ('also', 'RB'), ('includes', 'VBZ'), ('extracting', 'JJ'), ('feature', 'NN'), ('extractor', 'NN'), ('utilizing', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('cnn', 'NNS'), ('enlarged', 'VBD'), ('intra-class', 'JJ'), ('variance', 'NN'), ('long-tail', 'JJ'), ('classes', 'NNS'), ('feature', 'VBP'), ('vectors', 'NNS'), ('method', 'VBP'), ('additionally', 'RB'), ('includes', 'VBZ'), ('generating', 'JJ'), ('feature', 'NN'), ('generator', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vectors', 'NNS'), ('feature', 'VBP'), ('vectors', 'NNS'), ('method', 'VBP'), ('includes', 'VBZ'), ('classifying', 'VBG'), ('utilizing', 'VBG'), ('fully', 'RB'), ('connected', 'VBN'), ('classifier', 'JJR'), ('identity', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vector', 'NN'), ('method', 'NN'), ('also', 'RB'), ('includes', 'VBZ'), ('control', 'NN'), ('operation', 'NN'), ('-based', 'VBD'), ('machine', 'NN'), ('react', 'NN'), ('accordance', 'NN'), ('identitysome', 'JJ'), ('embodiments', 'NNS'), ('invention', 'NN'), ('provide', 'VBP'), ('efficient', 'JJ'), ('expressive', 'JJ'), ('machine-trained', 'JJ'), ('networks', 'NNS'), ('performing', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('machine-trained', 'JJ'), ('mt', 'NN'), ('networks', 'NNS'), ('embodiments', 'NNS'), ('use', 'VBP'), ('novel', 'JJ'), ('processing', 'NN'), ('nodes', 'NNS'), ('novel', 'JJ'), ('activation', 'NN'), ('functions', 'NNS'), ('allow', 'VBP'), ('mt', 'NN'), ('network', 'NN'), ('efficiently', 'RB'), ('define', 'VBZ'), ('fewer', 'JJR'), ('processing', 'NN'), ('node', 'NN'), ('layers', 'NNS'), ('complex', 'JJ'), ('mathematical', 'JJ'), ('expression', 'NN'), ('solves', 'VBZ'), ('particular', 'JJ'), ('problem', 'NN'), ('eg', 'NN'), ('face', 'NN'), ('speech', 'NN'), ('etc', 'FW'), ('embodiments', 'NNS'), ('activation', 'NN'), ('function', 'NN'), ('eg', 'FW'), ('cup', 'NN'), ('function', 'NN'), ('used', 'VBN'), ('numerous', 'JJ'), ('processing', 'VBG'), ('nodes', 'NNS'), ('mt', 'JJ'), ('network', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('activation', 'NN'), ('function', 'NN'), ('configured', 'VBD'), ('differently', 'RB'), ('different', 'JJ'), ('processing', 'VBG'), ('nodes', 'NNS'), ('different', 'JJ'), ('nodes', 'NNS'), ('emulate', 'VBP'), ('implement', 'JJ'), ('two', 'CD'), ('different', 'JJ'), ('functions', 'NNS'), ('eg', 'VBP'), ('two', 'CD'), ('boolean', 'JJ'), ('logical', 'JJ'), ('operators', 'NNS'), ('xor', 'VBP'), ('activation', 'NN'), ('function', 'NN'), ('embodiments', 'NNS'), ('periodic', 'JJ'), ('function', 'NN'), ('configured', 'VBD'), ('implement', 'JJ'), ('different', 'JJ'), ('functions', 'NNS'), ('eg', 'VBP'), ('different', 'JJ'), ('sinusoidal', 'JJ'), ('functionsmethods', 'NNS'), ('may', 'MD'), ('provide', 'VB'), ('facial', 'JJ'), ('least', 'JJS'), ('one', 'CD'), ('input', 'NN'), ('utilizing', 'VBG'), ('hierarchical', 'JJ'), ('feature', 'NN'), ('learning', 'VBG'), ('pair-wise', 'JJ'), ('receptive', 'JJ'), ('field', 'NN'), ('theory', 'NN'), ('may', 'MD'), ('used', 'VBN'), ('input', 'VB'), ('generate', 'JJ'), ('pre-processed', 'JJ'), ('multi-channel', 'JJ'), ('channels', 'NNS'), ('pre-processed', 'JJ'), ('may', 'MD'), ('activated', 'VB'), ('based', 'VBN'), ('amount', 'NN'), ('feature', 'NN'), ('rich', 'JJ'), ('details', 'NNS'), ('within', 'IN'), ('channels', 'NNS'), ('similarly', 'RB'), ('local', 'JJ'), ('patches', 'NNS'), ('may', 'MD'), ('activated', 'VB'), ('based', 'VBN'), ('discriminant', 'NN'), ('within', 'IN'), ('local', 'JJ'), ('patches', 'NNS'), ('may', 'MD'), ('extracted', 'VB'), ('local', 'JJ'), ('patches', 'NNS'), ('discriminant', 'NN'), ('may', 'MD'), ('selected', 'VBN'), ('order', 'NN'), ('perform', 'NN'), ('feature', 'NN'), ('matching', 'VBG'), ('pair', 'JJ'), ('sets', 'NNS'), ('may', 'MD'), ('utilize', 'VB'), ('patch', 'NN'), ('feature', 'NN'), ('pooling', 'VBG'), ('pair-wise', 'JJ'), ('matching', 'JJ'), ('large-scale', 'JJ'), ('training', 'NN'), ('order', 'NN'), ('quickly', 'RB'), ('accurately', 'RB'), ('perform', 'JJ'), ('facial', 'JJ'), ('low', 'JJ'), ('cost', 'NN'), ('memory', 'NN'), ('computationa', 'NN'), ('method', 'NN'), ('controlling', 'VBG'), ('terminal', 'NN'), ('provided', 'VBD'), ('terminal', 'JJ'), ('includes', 'VBZ'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('acquired', 'VBD'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('motion', 'NN'), ('parameter', 'NN'), ('terminal', 'NN'), ('obtained', 'VBD'), ('processing', 'NN'), ('acquired', 'VBD'), ('controlled', 'VBN'), ('performed', 'NNS'), ('based', 'VBN'), ('motion', 'NN'), ('parameter', 'NN'), ('equal', 'JJ'), ('less', 'RBR'), ('preset', 'JJ'), ('parameter', 'NN'), ('threshold', 'NN'), ('skipped', 'VBD'), ('based', 'VBN'), ('motion', 'NN'), ('parameter', 'NN'), ('greater', 'JJR'), ('preset', 'NN'), ('parameter', 'NN'), ('thresholda', 'VBD'), ('drive-through', 'JJ'), ('order', 'NN'), ('processing', 'NN'), ('method', 'NN'), ('apparatus', 'NN'), ('disclosed', 'VBD'), ('drive-through', 'JJ'), ('order', 'NN'), ('processing', 'NN'), ('method', 'NN'), ('includes', 'VBZ'), ('receiving', 'VBG'), ('customer', 'NN'), ('information', 'NN'), ('detected', 'VBN'), ('vision', 'NN'), ('providing', 'VBG'), ('product', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('processing', 'VBG'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('according', 'VBG'), ('present', 'JJ'), ('disclosure', 'NN'), ('possible', 'JJ'), ('rapidly', 'RB'), ('process', 'JJ'), ('order', 'NN'), ('using', 'VBG'), ('customer', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('using', 'VBG'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('ai', 'JJ'), ('model', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('g', 'JJ'), ('networkan', 'JJ'), ('processing', 'NN'), ('method', 'NN'), ('performed', 'VBD'), ('computing', 'VBG'), ('includes', 'VBZ'), ('identifying', 'VBG'), ('using', 'VBG'), ('face', 'NN'), ('one', 'CD'), ('faces', 'VBZ'), ('face', 'NN'), ('corresponding', 'VBG'), ('respective', 'JJ'), ('person', 'NN'), ('captured', 'VBD'), ('first', 'RB'), ('identified', 'VBN'), ('face', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('selecting', 'VBG'), ('tiles', 'NNS'), ('first', 'RB'), ('tile', 'JJ'), ('matches', 'NNS'), ('face', 'VBP'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'JJ'), ('accordance', 'NN'), ('predefined', 'VBD'), ('correspondence', 'NN'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('set', 'VBN'), ('pre-stored', 'JJ'), ('description', 'NN'), ('parameters', 'NNS'), ('first', 'RB'), ('tile', 'IN'), ('generating', 'VBG'), ('second', 'JJ'), ('covering', 'VBG'), ('faces', 'VBZ'), ('respective', 'JJ'), ('persons', 'NNS'), ('first', 'RB'), ('corresponding', 'VBG'), ('first', 'JJ'), ('tiles', 'NNS'), ('sharing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('predefined', 'VBN'), ('order', 'NN'), ('via', 'IN'), ('group', 'NN'), ('chat', 'WP'), ('sessionin', 'VBZ'), ('one', 'CD'), ('embodiment', 'NN'), ('artificial', 'JJ'), ('reality', 'NN'), ('determines', 'NNS'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('tracking', 'VBG'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'NN'), ('eye', 'NN'), ('tracking', 'VBG'), ('associated', 'VBN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('worn', 'VBN'), ('artificial', 'JJ'), ('reality', 'NN'), ('receives', 'NNS'), ('first', 'RB'), ('inputs', 'RB'), ('associated', 'VBN'), ('body', 'NN'), ('determines', 'VBZ'), ('region', 'NN'), ('looking', 'VBG'), ('within', 'IN'), ('field', 'NN'), ('view', 'NN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('received', 'VBD'), ('first', 'JJ'), ('inputs', 'NNS'), ('determines', 'NNS'), ('vergence', 'NN'), ('distance', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('region', 'NN'), ('looking', 'VBG'), ('locations', 'NNS'), ('one', 'CD'), ('objects', 'VBZ'), ('scene', 'NN'), ('displayed', 'VBD'), ('head-mounted', 'JJ'), ('display', 'NN'), ('adjusts', 'VBZ'), ('one', 'CD'), ('configurations', 'NNS'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('vergence', 'NN'), ('distance', 'NN'), ('computer-implemented', 'JJ'), ('method', 'NN'), ('provided', 'VBD'), ('-based', 'JJ'), ('self-guided', 'JJ'), ('object', 'JJ'), ('detection', 'NN'), ('method', 'NN'), ('includes', 'VBZ'), ('receiving', 'VBG'), ('set', 'VBN'), ('respective', 'JJ'), ('grid', 'JJ'), ('thereon', 'NN'), ('labeled', 'VBD'), ('regarding', 'VBG'), ('respective', 'JJ'), ('object', 'NN'), ('detected', 'VBD'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('method', 'NN'), ('includes', 'VBZ'), ('training', 'VBG'), ('grid-based', 'JJ'), ('object', 'JJ'), ('detector', 'NN'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('method', 'NN'), ('also', 'RB'), ('includes', 'VBZ'), ('determining', 'VBG'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('applying', 'VBG'), ('local', 'JJ'), ('segmentation', 'NN'), ('method', 'NN'), ('additionally', 'RB'), ('includes', 'VBZ'), ('training', 'VBG'), ('region-based', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('rcnn', 'NN'), ('joint', 'NN'), ('object', 'JJ'), ('localization', 'NN'), ('object', 'NN'), ('using', 'VBG'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('input', 'NN'), ('rcnna', 'NN'), ('method', 'NN'), ('face', 'NN'), ('comprising', 'VBG'), ('multiple', 'JJ'), ('phases', 'NNS'), ('implemented', 'VBN'), ('parallel', 'JJ'), ('architecture', 'NN'), ('first', 'JJ'), ('phase', 'NN'), ('normalization', 'NN'), ('phase', 'NN'), ('whereby', 'WRB'), ('captured', 'VBD'), ('normalized', 'JJ'), ('size', 'NN'), ('orientation', 'NN'), ('illumination', 'NN'), ('stored', 'VBD'), ('preexisting', 'JJ'), ('database', 'NN'), ('second', 'JJ'), ('phase', 'NN'), ('feature', 'NN'), ('extractiondistance', 'NN'), ('matrix', 'NN'), ('phase', 'NN'), ('distance', 'NN'), ('matrix', 'NN'), ('generated', 'VBD'), ('captured', 'JJ'), ('coarse', 'JJ'), ('phase', 'NN'), ('generated', 'VBD'), ('distance', 'NN'), ('matrix', 'NNS'), ('compared', 'VBN'), ('distance', 'NN'), ('matrices', 'NNS'), ('database', 'VBP'), ('using', 'VBG'), ('euclidean', 'JJ'), ('distance', 'NN'), ('matches', 'NNS'), ('create', 'VBP'), ('candidate', 'NN'), ('lists', 'NNS'), ('detailed', 'VBD'), ('phase', 'NN'), ('multiple', 'JJ'), ('face', 'NN'), ('algorithms', 'NN'), ('applied', 'JJ'), ('candidate', 'NN'), ('lists', 'NNS'), ('produce', 'VBP'), ('final', 'JJ'), ('result', 'NN'), ('distance', 'NN'), ('matrices', 'NNS'), ('normalized', 'JJ'), ('database', 'NN'), ('may', 'MD'), ('broken', 'VB'), ('parallel', 'JJ'), ('lists', 'NNS'), ('parallelization', 'VBP'), ('feature', 'NN'), ('extractiondistance', 'NN'), ('matrix', 'NN'), ('phase', 'NN'), ('candidate', 'NN'), ('lists', 'NNS'), ('may', 'MD'), ('also', 'RB'), ('grouped', 'VB'), ('according', 'VBG'), ('dissimilarity', 'NN'), ('algorithm', 'NN'), ('parallel', 'NN'), ('processing', 'NN'), ('detailed', 'JJ'), ('phasean', 'JJ'), ('imaging', 'VBG'), ('including', 'VBG'), ('pixel', 'JJ'), ('matrix', 'NN'), ('provided', 'VBD'), ('pixel', 'JJ'), ('matrix', 'NN'), ('includes', 'VBZ'), ('phase', 'JJ'), ('detection', 'NN'), ('pixels', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('performs', 'NNS'), ('autofocusing', 'VBG'), ('according', 'VBG'), ('pixel', 'NN'), ('data', 'NNS'), ('phase', 'NN'), ('detection', 'NN'), ('pixels', 'NNS'), ('determines', 'VBZ'), ('operating', 'VBG'), ('resolution', 'NN'), ('regular', 'JJ'), ('pixels', 'NNS'), ('according', 'VBG'), ('autofocused', 'JJ'), ('pixel', 'NN'), ('data', 'NNS'), ('phase', 'NN'), ('detection', 'NN'), ('pixels', 'NNS'), ('wherein', 'VBP'), ('phase', 'JJ'), ('detection', 'NN'), ('pixels', 'NNS'), ('always-on', 'JJ'), ('pixels', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('selectively', 'RB'), ('turned', 'VBD'), ('autofocusing', 'VBG'), ('accomplishedan', 'NN'), ('apparatus', 'NN'), ('includes', 'VBZ'), ('first', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('providing', 'VBG'), ('first', 'JJ'), ('object', 'JJ'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('second', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('providing', 'VBG'), ('second', 'JJ'), ('object', 'JJ'), ('second', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('different', 'JJ'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('generator', 'NN'), ('generates', 'NNS'), ('first', 'RB'), ('depth', 'VB'), ('map', 'NN'), ('first', 'RB'), ('based', 'VBN'), ('first', 'JJ'), ('second', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('generator', 'NN'), ('generates', 'VBZ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('second', 'NN'), ('based', 'VBN'), ('first', 'RB'), ('second', 'JJ'), ('first', 'JJ'), ('depth', 'NN'), ('mapmethods', 'NNS'), ('apparatus', 'VBP'), ('including', 'VBG'), ('computer', 'NN'), ('programs', 'NNS'), ('encoded', 'VBD'), ('computer', 'NN'), ('storage', 'NN'), ('media', 'NNS'), ('payment', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('provided', 'VBD'), ('one', 'CD'), ('methods', 'NNS'), ('includes', 'VBZ'), ('acquiring', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('target', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('wherein', 'IN'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('includes', 'VBZ'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('target', 'NN'), ('willingness', 'NN'), ('pay', 'NN'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('including', 'VBG'), ('determining', 'VBG'), ('whether', 'IN'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('less', 'RBR'), ('angle', 'JJ'), ('threshold', 'NN'), ('whether', 'IN'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('greater', 'JJR'), ('probability', 'NN'), ('threshold', 'JJ'), ('response', 'NN'), ('determining', 'VBG'), ('target', 'NN'), ('willingness', 'JJ'), ('pay', 'NN'), ('completing', 'VBG'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('novel', 'JJ'), ('method', 'NN'), ('apparatus', 'NN'), ('face', 'NN'), ('authentication', 'NN'), ('disclosed', 'VBD'), ('disclosed', 'VBN'), ('method', 'NN'), ('comprises', 'VBZ'), ('detecting', 'VBG'), ('motion', 'NN'), ('subject', 'NN'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('assigning', 'VBG'), ('unique', 'JJ'), ('session', 'NN'), ('identification', 'NN'), ('number', 'NN'), ('subject', 'JJ'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('generating', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('assessing', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('conducing', 'VBG'), ('incremental', 'JJ'), ('training', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('determining', 'VBG'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('identifying', 'VBG'), ('intent', 'NN'), ('subject', 'JJ'), ('authorizing', 'VBG'), ('access', 'NN'), ('point', 'NN'), ('entry', 'NN'), ('based', 'VBN'), ('determined', 'VBN'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('intent', 'NN'), ('subjectdisclosed', 'VBN'), ('herein', 'NN'), ('robot', 'VBZ'), ('electronic', 'JJ'), ('acquiring', 'VBG'), ('video', 'NN'), ('method', 'NN'), ('acquiring', 'VBG'), ('video', 'NN'), ('using', 'VBG'), ('robot', 'JJ'), ('robot', 'NN'), ('includes', 'VBZ'), ('camera', 'NN'), ('configured', 'VBD'), ('rotate', 'JJ'), ('lateral', 'JJ'), ('direction', 'NN'), ('tilt', 'VBD'), ('vertical', 'JJ'), ('direction', 'NN'), ('controls', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('direction', 'NN'), ('rotation', 'NN'), ('camera', 'NN'), ('angle', 'NN'), ('tilt', 'NN'), ('camera', 'NN'), ('focal', 'JJ'), ('distance', 'NN'), ('camera', 'NN'), ('tracking', 'VBG'), ('video', 'NN'), ('acquired', 'VBD'), ('cameras', 'NNS'), ('methods', 'NNS'), ('disclosed', 'VBD'), ('inferring', 'VBG'), ('topics', 'NNS'), ('file', 'NN'), ('containing', 'VBG'), ('audio', 'JJ'), ('video', 'NN'), ('example', 'NN'), ('multimodal', 'JJ'), ('multimedia', 'NN'), ('file', 'NN'), ('order', 'NN'), ('facilitate', 'NN'), ('video', 'NN'), ('indexing', 'VBG'), ('set', 'VBN'), ('entities', 'NNS'), ('extracted', 'VBD'), ('file', 'NN'), ('linked', 'VBN'), ('produce', 'VBP'), ('graph', 'JJ'), ('reference', 'NN'), ('information', 'NN'), ('also', 'RB'), ('obtained', 'VBD'), ('set', 'JJ'), ('entities', 'NNS'), ('entities', 'NNS'), ('may', 'MD'), ('drawn', 'VB'), ('example', 'NN'), ('wikipedia', 'NN'), ('categories', 'NNS'), ('large', 'JJ'), ('ontological', 'JJ'), ('data', 'NNS'), ('sources', 'NNS'), ('analysis', 'NN'), ('graph', 'NN'), ('using', 'VBG'), ('unsupervised', 'JJ'), ('learning', 'VBG'), ('permits', 'NNS'), ('determining', 'VBG'), ('clusters', 'NNS'), ('graph', 'VBP'), ('extracting', 'VBG'), ('clusters', 'NNS'), ('possibly', 'RB'), ('using', 'VBG'), ('supervised', 'VBN'), ('learning', 'VBG'), ('provides', 'VBZ'), ('selection', 'NN'), ('topic', 'NN'), ('identifiers', 'NNS'), ('topic', 'VBP'), ('identifiers', 'NNS'), ('used', 'VBD'), ('indexing', 'VBG'), ('filea', 'JJ'), ('face', 'NN'), ('method', 'NN'), ('neural', 'JJ'), ('network', 'NN'), ('training', 'VBG'), ('method', 'NN'), ('apparatus', 'NN'), ('electronic', 'JJ'), ('method', 'NN'), ('comprises', 'NNS'), ('obtaining', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('means', 'VBZ'), ('first', 'JJ'), ('camera', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('feature', 'NN'), ('first', 'JJ'), ('face', 'NN'), ('comparing', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('feature', 'NN'), ('pre-stored', 'JJ'), ('second', 'JJ'), ('face', 'NN'), ('feature', 'NN'), ('obtain', 'VB'), ('reference', 'NN'), ('similarity', 'NN'), ('second', 'JJ'), ('face', 'NN'), ('feature', 'NN'), ('obtained', 'VBN'), ('extracting', 'JJ'), ('feature', 'JJ'), ('second', 'JJ'), ('face', 'NN'), ('obtained', 'VBN'), ('second', 'JJ'), ('camera', 'NN'), ('second', 'JJ'), ('camera', 'NN'), ('first', 'RB'), ('camera', 'VB'), ('different', 'JJ'), ('types', 'NNS'), ('cameras', 'NNS'), ('determining', 'VBG'), ('according', 'VBG'), ('reference', 'NN'), ('similarity', 'NN'), ('whether', 'IN'), ('first', 'JJ'), ('face', 'NN'), ('feature', 'NN'), ('second', 'JJ'), ('face', 'NN'), ('feature', 'NN'), ('correspond', 'NN'), ('person', 'NN'), ('present', 'JJ'), ('invention', 'NN'), ('discloses', 'VBZ'), ('technique', 'JJ'), ('alerting', 'VBG'), ('vision', 'NN'), ('impairment', 'NN'), ('comprises', 'VBZ'), ('processing', 'VBG'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('receiving', 'VBG'), ('scene', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('scene', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('environment', 'NN'), ('identifying', 'VBG'), ('scene', 'NN'), ('data', 'NNS'), ('certain', 'JJ'), ('consumer', 'NN'), ('identifying', 'VBG'), ('event', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('upon', 'IN'), ('identification', 'NN'), ('event', 'NN'), ('sending', 'VBG'), ('notification', 'NN'), ('relating', 'VBG'), ('vision', 'NN'), ('impairment', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "cleaned_POS_text_a = []\n",
    "\n",
    "for tuple in pos_tagging_a:\n",
    "    # POS tagged text is a list of tuples, where the first element tuple[0] is a token and the second one tuple[1] is\n",
    "    # the Part of Speech. If the POS has length == 1, the token is punctuation, otherwise it is not, and we insert it\n",
    "    # in the list cleaned_POS_text\n",
    "    if len(tuple[1]) > 1:\n",
    "        cleaned_POS_text_a.append(tuple)\n",
    "        \n",
    "print(cleaned_POS_text_a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5cc2273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('configured', 'VBN'), ('make', 'VBP'), ('screen', 'JJ'), ('display', 'NN'), ('frames', 'NNS'), ('comprising', 'VBG'), ('capturing', 'VBG'), ('device', 'NN'), ('storage', 'NN'), ('device', 'NN'), ('storing', 'VBG'), ('modules', 'NNS'), ('coupled', 'VBN'), ('capturing', 'VBG'), ('device', 'NN'), ('storage', 'NN'), ('device', 'NN'), ('configured', 'VBD'), ('execute', 'JJ'), ('modules', 'NNS'), ('storage', 'NN'), ('device', 'NN'), ('configure', 'NN'), ('screen', 'NN'), ('display', 'NN'), ('marker', 'NN'), ('objects', 'NNS'), ('predetermined', 'VBD'), ('positions', 'NNS'), ('configure', 'NN'), ('capturing', 'VBG'), ('device', 'NN'), ('capture', 'NN'), ('first', 'JJ'), ('head', 'NN'), ('looking', 'VBG'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('perform', 'VBP'), ('first', 'JJ'), ('face', 'NN'), ('operations', 'NNS'), ('first', 'RB'), ('head', 'VBP'), ('obtain', 'VB'), ('first', 'JJ'), ('face', 'NN'), ('regions', 'NNS'), ('corresponding', 'VBG'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('detect', 'VBP'), ('first', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('corresponding', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('regions', 'NNS'), ('calculate', 'VBP'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('looking', 'VBG'), ('predetermined', 'VBD'), ('positions', 'NNS'), ('according', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('configure', 'VBP'), ('capturing', 'VBG'), ('device', 'NN'), ('capture', 'NN'), ('second', 'JJ'), ('head', 'NN'), ('perform', 'VB'), ('second', 'JJ'), ('head', 'NN'), ('obtain', 'VB'), ('second', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('detect', 'JJ'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('within', 'IN'), ('second', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('estimate', 'NN'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('calculate', 'VBP'), ('gaze', 'JJ'), ('position', 'NN'), ('screen', 'NN'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'JJ'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('predetermined', 'VBD'), ('positions', 'NNS'), ('configure', 'NN'), ('screen', 'NN'), ('display', 'NN'), ('corresponding', 'VBG'), ('visual', 'JJ'), ('effect', 'NN'), ('according', 'VBG'), ('gaze', 'NN'), ('position', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('gaze', 'JJ'), ('position', 'NN'), ('comprises', 'NNS'), ('first', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('first', 'RB'), ('axial', 'JJ'), ('direction', 'NN'), ('second', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('second', 'JJ'), ('axial', 'JJ'), ('direction', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('head', 'NN'), ('posture', 'NN'), ('angles', 'VBZ'), ('comprise', 'RB'), ('head', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('head', 'NN'), ('yaw', 'NN'), ('angle', 'JJ'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('comprise', 'VBP'), ('first', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('second', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('first', 'RB'), ('yaw', 'RB'), ('angle', 'JJ'), ('second', 'JJ'), ('yaw', 'NN'), ('angle', 'NN'), ('corresponding', 'VBG'), ('predetermined', 'VBN'), ('positions', 'NNS'), ('according', 'VBG'), ('wherein', 'NN'), ('performs', 'NNS'), ('interpolation', 'NN'), ('operation', 'NN'), ('extrapolation', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('first', 'JJ'), ('yaw', 'NN'), ('angle', 'JJ'), ('second', 'JJ'), ('yaw', 'NN'), ('angle', 'NN'), ('first', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('yaw', 'JJ'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('second', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('second', 'JJ'), ('yaw', 'JJ'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('head', 'VBP'), ('yaw', 'RB'), ('angle', 'JJ'), ('thereby', 'RB'), ('obtaining', 'VBG'), ('first', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('gaze', 'JJ'), ('position', 'NN'), ('performs', 'NNS'), ('interpolation', 'NN'), ('operation', 'NN'), ('extrapolation', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('first', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('second', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('third', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('fourth', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('second', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('head', 'VBP'), ('pitch', 'NN'), ('angle', 'NN'), ('thereby', 'RB'), ('obtaining', 'VBG'), ('second', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('gaze', 'NN'), ('position', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('calculates', 'NNS'), ('first', 'RB'), ('viewing', 'VBG'), ('distances', 'NNS'), ('screen', 'NN'), ('according', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NN'), ('estimates', 'NNS'), ('second', 'JJ'), ('viewing', 'VBG'), ('distance', 'NN'), ('screen', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('adjusts', 'VBZ'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('gaze', 'VBP'), ('position', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('viewing', 'NN'), ('distance', 'NN'), ('first', 'RB'), ('viewing', 'VBG'), ('distances', 'NNS'), ('according', 'VBG'), ('wherein', 'JJ'), ('maps', 'NNS'), ('two-dimensional', 'JJ'), ('position', 'NN'), ('coordinates', 'NNS'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('plane', 'NN'), ('coordinate', 'NN'), ('system', 'NN'), ('three-dimensional', 'JJ'), ('position', 'NN'), ('coordinates', 'VBZ'), ('three-dimensional', 'JJ'), ('coordinate', 'NN'), ('system', 'NN'), ('estimates', 'VBZ'), ('head', 'JJ'), ('posture', 'NN'), ('angle', 'NN'), ('according', 'VBG'), ('three-dimensional', 'JJ'), ('position', 'NN'), ('coordinates', 'NNS'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('according', 'VBG'), ('wherein', 'JJ'), ('second', 'JJ'), ('head', 'NN'), ('comprises', 'VBZ'), ('wearable', 'JJ'), ('device', 'NN'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('comprise', 'VBP'), ('third', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('covered', 'VBD'), ('wearable', 'JJ'), ('device', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('second', 'JJ'), ('head', 'NN'), ('comprises', 'VBZ'), ('wearable', 'JJ'), ('device', 'NN'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('comprise', 'VBP'), ('one', 'CD'), ('simulated', 'VBN'), ('landmarks', 'NN'), ('marked', 'VBD'), ('wearable', 'JJ'), ('device', 'NN'), ('operating', 'VBG'), ('adapted', 'VBD'), ('comprising', 'VBG'), ('capturing', 'VBG'), ('device', 'NN'), ('making', 'VBG'), ('screen', 'JJ'), ('display', 'NN'), ('frames', 'NNS'), ('comprising', 'VBG'), ('configuring', 'VBG'), ('screen', 'JJ'), ('display', 'NN'), ('marker', 'NN'), ('objects', 'NNS'), ('predetermined', 'VBD'), ('positions', 'NNS'), ('configuring', 'VBG'), ('capturing', 'VBG'), ('device', 'NN'), ('capture', 'NN'), ('first', 'JJ'), ('head', 'NN'), ('looking', 'VBG'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('performing', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('operations', 'NNS'), ('first', 'RB'), ('head', 'VBP'), ('obtain', 'VB'), ('first', 'JJ'), ('face', 'NN'), ('regions', 'NNS'), ('corresponding', 'VBG'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('detecting', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('corresponding', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('regions', 'NNS'), ('calculating', 'VBG'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('looking', 'VBG'), ('predetermined', 'VBD'), ('positions', 'NNS'), ('according', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('configuring', 'VBG'), ('capturing', 'VBG'), ('device', 'JJ'), ('capture', 'NN'), ('second', 'JJ'), ('head', 'NN'), ('performing', 'VBG'), ('second', 'JJ'), ('head', 'NN'), ('obtain', 'VB'), ('second', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('detecting', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('within', 'IN'), ('second', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('estimating', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('calculating', 'VBG'), ('gaze', 'JJ'), ('position', 'NN'), ('screen', 'NN'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'JJ'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('predetermined', 'VBD'), ('positions', 'NNS'), ('configuring', 'VBG'), ('screen', 'NN'), ('display', 'NN'), ('corresponding', 'VBG'), ('visual', 'JJ'), ('effect', 'NN'), ('according', 'VBG'), ('gaze', 'JJ'), ('position', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('gaze', 'JJ'), ('position', 'NN'), ('comprises', 'NNS'), ('first', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('first', 'RB'), ('axial', 'JJ'), ('direction', 'NN'), ('second', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('second', 'JJ'), ('axial', 'JJ'), ('direction', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('head', 'NN'), ('posture', 'NN'), ('angles', 'VBZ'), ('comprise', 'RB'), ('head', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('head', 'NN'), ('yaw', 'NN'), ('angle', 'JJ'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('comprise', 'VBP'), ('first', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('second', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('first', 'RB'), ('yaw', 'RB'), ('angle', 'JJ'), ('second', 'JJ'), ('yaw', 'NN'), ('angle', 'NN'), ('corresponding', 'VBG'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('operation', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('step', 'NN'), ('calculating', 'VBG'), ('gaze', 'JJ'), ('position', 'NN'), ('screen', 'NN'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'JJ'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('predetermined', 'VBD'), ('positions', 'NNS'), ('comprises', 'NNS'), ('performing', 'VBG'), ('interpolation', 'NN'), ('operation', 'NN'), ('extrapolation', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('first', 'JJ'), ('yaw', 'NN'), ('angle', 'JJ'), ('second', 'JJ'), ('yaw', 'NN'), ('angle', 'NN'), ('first', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('yaw', 'JJ'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('second', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('second', 'JJ'), ('yaw', 'JJ'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('head', 'VBP'), ('yaw', 'RB'), ('angle', 'JJ'), ('thereby', 'RB'), ('obtaining', 'VBG'), ('first', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('gaze', 'JJ'), ('position', 'NN'), ('performing', 'VBG'), ('interpolation', 'NN'), ('operation', 'NN'), ('extrapolation', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('first', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('second', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('third', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('fourth', 'JJ'), ('position', 'NN'), ('corresponding', 'VBG'), ('second', 'JJ'), ('pitch', 'NN'), ('angle', 'NN'), ('among', 'IN'), ('predetermined', 'JJ'), ('positions', 'NNS'), ('head', 'VBP'), ('pitch', 'NN'), ('angle', 'NN'), ('thereby', 'RB'), ('obtaining', 'VBG'), ('second', 'JJ'), ('coordinate', 'NN'), ('value', 'NN'), ('gaze', 'JJ'), ('position', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('comprises', 'NNS'), ('calculating', 'VBG'), ('first', 'JJ'), ('viewing', 'VBG'), ('distances', 'NNS'), ('screen', 'NN'), ('according', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('estimating', 'VBG'), ('second', 'JJ'), ('viewing', 'VBG'), ('distance', 'NN'), ('screen', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('adjusting', 'VBG'), ('rotation', 'NN'), ('reference', 'NN'), ('angles', 'NNS'), ('gaze', 'VBP'), ('position', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('viewing', 'NN'), ('distance', 'NN'), ('first', 'RB'), ('viewing', 'VBG'), ('distances', 'NNS'), ('operation', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('comprises', 'NNS'), ('mapping', 'VBG'), ('two-dimensional', 'JJ'), ('position', 'NN'), ('coordinates', 'NNS'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('plane', 'NN'), ('coordinate', 'NN'), ('system', 'NN'), ('three-dimensional', 'JJ'), ('position', 'NN'), ('coordinates', 'VBZ'), ('three-dimensional', 'JJ'), ('coordinate', 'NN'), ('system', 'NN'), ('estimating', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('angle', 'IN'), ('according', 'VBG'), ('three-dimensional', 'JJ'), ('position', 'NN'), ('coordinates', 'NNS'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('operation', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('second', 'JJ'), ('head', 'NN'), ('comprises', 'VBZ'), ('wearable', 'JJ'), ('device', 'NN'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('comprise', 'VBP'), ('third', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('covered', 'VBD'), ('wearable', 'JJ'), ('device', 'NN'), ('operation', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('second', 'JJ'), ('head', 'NN'), ('comprises', 'VBZ'), ('wearable', 'JJ'), ('device', 'NN'), ('second', 'JJ'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('comprise', 'VBP'), ('one', 'CD'), ('simulated', 'VBN'), ('landmarks', 'NN'), ('marked', 'VBD'), ('wearable', 'JJ'), ('device', 'NN'), ('computation', 'NN'), ('applied', 'VBD'), ('computing', 'VBG'), ('system', 'NN'), ('wherein', 'VBD'), ('computing', 'VBG'), ('system', 'NN'), ('comprises', 'VBZ'), ('control', 'VB'), ('unit', 'NN'), ('computation', 'NN'), ('group', 'NN'), ('general', 'JJ'), ('storage', 'NN'), ('unit', 'NN'), ('wherein', 'VBZ'), ('control', 'JJ'), ('unit', 'NN'), ('comprises', 'VBZ'), ('first', 'JJ'), ('memory', 'NN'), ('decoding', 'VBG'), ('logic', 'JJ'), ('controller', 'NN'), ('wherein', 'NN'), ('computation', 'NN'), ('group', 'NN'), ('comprises', 'VBZ'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('general', 'JJ'), ('storage', 'NN'), ('unit', 'NN'), ('configured', 'VBD'), ('store', 'NN'), ('data', 'NNS'), ('computation', 'NN'), ('comprises', 'VBZ'), ('receiving', 'VBG'), ('controller', 'NN'), ('first', 'RB'), ('level', 'JJ'), ('instruction', 'NN'), ('sequence', 'NN'), ('partitioning', 'VBG'), ('decoding', 'VBG'), ('logic', 'JJ'), ('first', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequence', 'NN'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('creating', 'VBG'), ('controller', 'NN'), ('threads', 'NNS'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('allocating', 'VBG'), ('controller', 'NN'), ('independent', 'JJ'), ('register', 'NN'), ('well', 'RB'), ('configuring', 'VBG'), ('independent', 'JJ'), ('addressing', 'VBG'), ('function', 'NN'), ('thread', 'NN'), ('threads', 'NNS'), ('wherein', 'VBP'), ('integer', 'JJ'), ('greater', 'JJR'), ('equal', 'JJ'), ('obtaining', 'VBG'), ('group', 'NN'), ('controller', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('obtaining', 'VBG'), ('corresponding', 'VBG'), ('fusion', 'NN'), ('computation', 'NN'), ('manner', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('according', 'VBG'), ('computation', 'NN'), ('types', 'NNS'), ('adopting', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('fusion', 'NN'), ('computation', 'NN'), ('manner', 'NN'), ('call', 'NN'), ('threads', 'NNS'), ('performing', 'VBG'), ('computations', 'NNS'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('obtain', 'VB'), ('final', 'JJ'), ('result', 'NN'), ('wherein', 'NN'), ('obtaining', 'VBG'), ('group', 'NN'), ('controller', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('obtaining', 'VBG'), ('corresponding', 'VBG'), ('fusion', 'NN'), ('computation', 'NN'), ('manner', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('according', 'VBG'), ('computation', 'NN'), ('types', 'NNS'), ('adopting', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('fusion', 'NN'), ('computation', 'NN'), ('manner', 'NN'), ('call', 'NN'), ('threads', 'NNS'), ('performing', 'VBG'), ('computations', 'NNS'), ('second', 'JJ'), ('instruction', 'NN'), ('sequences', 'NNS'), ('obtain', 'VB'), ('final', 'JJ'), ('result', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('represent', 'JJ'), ('computation', 'NN'), ('operations', 'NNS'), ('type', 'VBP'), ('group', 'NN'), ('controller', 'NN'), ('calls', 'VBZ'), ('combined', 'VBN'), ('computation', 'NN'), ('manner', 'NN'), ('single', 'JJ'), ('instruction', 'NN'), ('multiple', 'NN'), ('data', 'NNS'), ('type', 'NN'), ('combination', 'NN'), ('single', 'JJ'), ('instruction', 'NN'), ('multiple', 'JJ'), ('threads', 'NNS'), ('uses', 'VBZ'), ('threads', 'NNS'), ('perform', 'NN'), ('combined', 'VBN'), ('computation', 'NN'), ('manner', 'NN'), ('obtain', 'VB'), ('final', 'JJ'), ('result', 'NN'), ('includes', 'VBZ'), ('partitioning', 'VBG'), ('decoding', 'VBG'), ('logic', 'JJ'), ('threads', 'NNS'), ('n', 'RB'), ('wraps', 'VBP'), ('allocating', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('converting', 'VBG'), ('group', 'NN'), ('controller', 'NN'), ('second', 'JJ'), ('instruction', 'NN'), ('sequences', 'NNS'), ('second', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('sending', 'VBG'), ('second', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('computing', 'VBG'), ('units', 'NNS'), ('calling', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('wraps', 'NNS'), ('allocated', 'VBD'), ('computing', 'VBG'), ('units', 'NNS'), ('second', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('fetch', 'VBP'), ('corresponding', 'VBG'), ('data', 'NNS'), ('according', 'VBG'), ('independent', 'JJ'), ('addressing', 'VBG'), ('function', 'NN'), ('performing', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('computations', 'NNS'), ('data', 'NNS'), ('obtain', 'VB'), ('intermediate', 'JJ'), ('results', 'NNS'), ('splicing', 'VBG'), ('intermediate', 'JJ'), ('results', 'NNS'), ('obtain', 'VB'), ('final', 'JJ'), ('result', 'NN'), ('wherein', 'NN'), ('obtaining', 'VBG'), ('group', 'NN'), ('controller', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('obtaining', 'VBG'), ('corresponding', 'VBG'), ('fusion', 'NN'), ('computation', 'NN'), ('manner', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('according', 'VBG'), ('computation', 'NN'), ('types', 'NNS'), ('adopting', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('fusion', 'NN'), ('computation', 'NN'), ('manner', 'NN'), ('call', 'NN'), ('threads', 'NNS'), ('performing', 'VBG'), ('computations', 'NNS'), ('second', 'JJ'), ('instruction', 'NN'), ('sequences', 'NNS'), ('obtain', 'VB'), ('final', 'JJ'), ('result', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('represent', 'JJ'), ('computation', 'NN'), ('operations', 'NNS'), ('different', 'JJ'), ('types', 'NNS'), ('group', 'NN'), ('controller', 'NN'), ('calls', 'VBZ'), ('simultaneous', 'JJ'), ('multi-threading', 'JJ'), ('threads', 'NNS'), ('perform', 'VB'), ('computations', 'NNS'), ('obtain', 'VB'), ('final', 'JJ'), ('result', 'NN'), ('includes', 'VBZ'), ('partitioning', 'VBG'), ('decoding', 'VBG'), ('logic', 'JJ'), ('threads', 'NNS'), ('n', 'RB'), ('wraps', 'VBP'), ('converting', 'VBG'), ('second', 'JJ'), ('instruction', 'NN'), ('sequences', 'NNS'), ('second', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('obtaining', 'VBG'), ('group', 'NN'), ('controller', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('supported', 'VBD'), ('computing', 'VBG'), ('units', 'NNS'), ('allocating', 'VBG'), ('controller', 'NN'), ('n', 'JJ'), ('wraps', 'NNS'), ('second', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('corresponding', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('support', 'NN'), ('computation', 'NN'), ('types', 'NNS'), ('wraps', 'VBP'), ('second', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('calling', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('wraps', 'NNS'), ('allocated', 'VBD'), ('computing', 'VBG'), ('units', 'NNS'), ('second', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('fetching', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('corresponding', 'VBG'), ('data', 'NNS'), ('performing', 'VBG'), ('computing', 'VBG'), ('units', 'NNS'), ('computations', 'NNS'), ('data', 'NNS'), ('obtain', 'VB'), ('intermediate', 'JJ'), ('results', 'NNS'), ('splicing', 'VBG'), ('intermediate', 'JJ'), ('results', 'NNS'), ('obtain', 'VB'), ('final', 'JJ'), ('result', 'NN'), ('comprising', 'VBG'), ('wrap', 'NN'), ('wraps', 'NNS'), ('blocked', 'VBD'), ('adding', 'VBG'), ('wrap', 'NN'), ('waiting', 'VBG'), ('queue', 'NN'), ('data', 'NNS'), ('wrap', 'NN'), ('already', 'RB'), ('fetched', 'VBD'), ('adding', 'VBG'), ('wrap', 'NN'), ('preparation', 'NN'), ('queue', 'NN'), ('wherein', 'WRB'), ('preparation', 'NN'), ('queue', 'NN'), ('queue', 'NN'), ('wrap', 'NN'), ('scheduled', 'VBN'), ('executing', 'VBG'), ('located', 'VBN'), ('computing', 'VBG'), ('resource', 'NN'), ('idle', 'JJ'), ('wherein', 'NN'), ('first', 'RB'), ('level', 'JJ'), ('instruction', 'NN'), ('sequence', 'NN'), ('includes', 'VBZ'), ('long', 'JJ'), ('instruction', 'NN'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequence', 'NN'), ('includes', 'VBZ'), ('instruction', 'NN'), ('sequence', 'NN'), ('wherein', 'NN'), ('computing', 'VBG'), ('system', 'NN'), ('includes', 'VBZ'), ('tree', 'JJ'), ('module', 'NN'), ('wherein', 'VBD'), ('tree', 'JJ'), ('module', 'NN'), ('includes', 'VBZ'), ('root', 'NN'), ('port', 'NN'), ('branch', 'NN'), ('ports', 'NNS'), ('wherein', 'VBP'), ('root', 'JJ'), ('port', 'NN'), ('tree', 'NN'), ('module', 'NN'), ('connected', 'VBN'), ('group', 'NN'), ('controller', 'NN'), ('branch', 'NN'), ('ports', 'NNS'), ('tree', 'VBP'), ('module', 'NN'), ('connected', 'VBN'), ('computing', 'VBG'), ('unit', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('respectively', 'RB'), ('tree', 'VBP'), ('module', 'NN'), ('configured', 'VBN'), ('forward', 'RB'), ('data', 'NN'), ('blocks', 'NNS'), ('wraps', 'VBP'), ('instruction', 'NN'), ('sequences', 'NNS'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('wherein', 'VBP'), ('tree', 'JJ'), ('module', 'NN'), ('n-ary', 'JJ'), ('tree', 'NN'), ('wherein', 'NN'), ('n', 'RB'), ('integer', 'RB'), ('greater', 'JJR'), ('equal', 'JJ'), ('wherein', 'NN'), ('computing', 'VBG'), ('system', 'NN'), ('includes', 'VBZ'), ('branch', 'NN'), ('processing', 'VBG'), ('circuit', 'NN'), ('wherein', 'NN'), ('branch', 'NN'), ('processing', 'VBG'), ('circuit', 'NN'), ('connected', 'VBN'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('branch', 'NN'), ('processing', 'VBG'), ('circuit', 'NN'), ('configured', 'VBD'), ('forward', 'RB'), ('data', 'NNS'), ('wraps', 'NNS'), ('instruction', 'NN'), ('sequences', 'NNS'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('computing', 'VBG'), ('system', 'NN'), ('comprising', 'VBG'), ('control', 'NN'), ('unit', 'NN'), ('computation', 'NN'), ('group', 'NN'), ('general', 'JJ'), ('storage', 'NN'), ('unit', 'NN'), ('wherein', 'VBZ'), ('control', 'JJ'), ('unit', 'NN'), ('includes', 'VBZ'), ('first', 'JJ'), ('memory', 'NN'), ('decoding', 'VBG'), ('logic', 'JJ'), ('controller', 'NN'), ('computation', 'NN'), ('group', 'NN'), ('includes', 'VBZ'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('general', 'JJ'), ('storage', 'NN'), ('unit', 'NN'), ('configured', 'VBD'), ('store', 'NN'), ('data', 'NNS'), ('controller', 'NN'), ('configured', 'VBD'), ('receive', 'JJ'), ('first', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequence', 'NN'), ('control', 'NN'), ('first', 'JJ'), ('memory', 'NN'), ('decoding', 'VBG'), ('logic', 'JJ'), ('decoding', 'VBG'), ('logic', 'NN'), ('configured', 'VBD'), ('partition', 'NN'), ('first', 'RB'), ('level', 'JJ'), ('instruction', 'NN'), ('sequence', 'NN'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('controller', 'VBP'), ('configured', 'JJ'), ('create', 'NN'), ('threads', 'NNS'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequences', 'NNS'), ('allocate', 'VBP'), ('independent', 'JJ'), ('register', 'NN'), ('configure', 'NN'), ('independent', 'JJ'), ('addressing', 'VBG'), ('function', 'NN'), ('thread', 'NN'), ('threads', 'NNS'), ('integer', 'VBP'), ('greater', 'JJR'), ('equal', 'JJ'), ('controller', 'NN'), ('configured', 'VBD'), ('convert', 'JJ'), ('second', 'JJ'), ('instruction', 'NN'), ('sequences', 'NNS'), ('control', 'NN'), ('signals', 'NNS'), ('sending', 'VBG'), ('group', 'NN'), ('controller', 'NN'), ('group', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('receive', 'JJ'), ('control', 'NN'), ('signals', 'NNS'), ('obtain', 'VB'), ('computational', 'JJ'), ('types', 'NNS'), ('control', 'NN'), ('signals', 'NNS'), ('divide', 'VBP'), ('threads', 'NNS'), ('n', 'RB'), ('wraps', 'VBP'), ('allocate', 'JJ'), ('n', 'JJ'), ('wraps', 'NNS'), ('control', 'NN'), ('signals', 'NNS'), ('computing', 'VBG'), ('units', 'NNS'), ('according', 'VBG'), ('computational', 'JJ'), ('types', 'NNS'), ('computing', 'VBG'), ('units', 'NNS'), ('configured', 'VBN'), ('fetch', 'RB'), ('data', 'NNS'), ('general', 'JJ'), ('storage', 'NN'), ('unit', 'NN'), ('allocated', 'VBD'), ('wraps', 'NNS'), ('control', 'NN'), ('signals', 'NNS'), ('perform', 'VBP'), ('computations', 'NNS'), ('obtain', 'VB'), ('intermediate', 'JJ'), ('result', 'NN'), ('group', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('splice', 'JJ'), ('intermediate', 'JJ'), ('results', 'NNS'), ('obtain', 'VB'), ('final', 'JJ'), ('computation', 'NN'), ('result', 'NN'), ('computing', 'VBG'), ('system', 'NN'), ('wherein', 'JJ'), ('computing', 'VBG'), ('units', 'NNS'), ('includes', 'VBZ'), ('addition', 'NN'), ('computing', 'VBG'), ('unit', 'NN'), ('multiplication', 'NN'), ('computing', 'VBG'), ('unit', 'NN'), ('activation', 'NN'), ('computing', 'VBG'), ('unit', 'NN'), ('dedicated', 'VBN'), ('computing', 'VBG'), ('unit', 'NN'), ('computing', 'VBG'), ('system', 'NN'), ('wherein', 'NN'), ('dedicated', 'VBN'), ('computing', 'VBG'), ('unit', 'NN'), ('includes', 'VBZ'), ('face', 'VBP'), ('computing', 'VBG'), ('unit', 'NN'), ('graphics', 'NNS'), ('computing', 'VBG'), ('unit', 'NN'), ('fingerprint', 'NN'), ('computing', 'VBG'), ('unit', 'NN'), ('neural', 'JJ'), ('network', 'NN'), ('computing', 'VBG'), ('unit', 'NN'), ('computing', 'VBG'), ('system', 'NN'), ('wherein', 'JJ'), ('group', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('computation', 'NN'), ('types', 'NNS'), ('control', 'NN'), ('signals', 'NNS'), ('graphics', 'NNS'), ('computations', 'NNS'), ('fingerprint', 'VBP'), ('identification', 'NN'), ('face', 'NN'), ('neural', 'JJ'), ('network', 'NN'), ('operations', 'NNS'), ('allocate', 'VBP'), ('control', 'NN'), ('signals', 'NNS'), ('face', 'VBP'), ('computing', 'VBG'), ('unit', 'NN'), ('graphics', 'NNS'), ('computing', 'VBG'), ('unit', 'NN'), ('fingerprint', 'NN'), ('computing', 'VBG'), ('unit', 'NN'), ('neural', 'JJ'), ('network', 'NN'), ('computing', 'VBG'), ('unit', 'NN'), ('respectively', 'RB'), ('computing', 'VBG'), ('system', 'NN'), ('wherein', 'VBD'), ('first', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequence', 'NN'), ('includes', 'VBZ'), ('long', 'JJ'), ('instruction', 'NN'), ('second', 'JJ'), ('level', 'NN'), ('instruction', 'NN'), ('sequence', 'NN'), ('includes', 'VBZ'), ('instruction', 'NN'), ('sequence', 'NN'), ('computing', 'VBG'), ('system', 'NN'), ('comprising', 'VBG'), ('tree', 'JJ'), ('module', 'NN'), ('wherein', 'VBD'), ('tree', 'JJ'), ('module', 'NN'), ('includes', 'VBZ'), ('root', 'NN'), ('port', 'NN'), ('branch', 'NN'), ('ports', 'NNS'), ('wherein', 'VBP'), ('root', 'JJ'), ('port', 'NN'), ('tree', 'NN'), ('module', 'NN'), ('connected', 'VBN'), ('group', 'NN'), ('controller', 'NN'), ('branch', 'NN'), ('ports', 'NNS'), ('tree', 'VBP'), ('module', 'NN'), ('connected', 'VBN'), ('computing', 'VBG'), ('unit', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('respectively', 'RB'), ('tree', 'VBP'), ('module', 'NN'), ('configured', 'VBN'), ('forward', 'RB'), ('data', 'NN'), ('blocks', 'NNS'), ('wraps', 'VBP'), ('instruction', 'NN'), ('sequences', 'NNS'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('computing', 'VBG'), ('system', 'NN'), ('wherein', 'VBD'), ('tree', 'JJ'), ('module', 'NN'), ('n-ary', 'JJ'), ('tree', 'NN'), ('wherein', 'NN'), ('n', 'RB'), ('integer', 'RB'), ('greater', 'JJR'), ('equal', 'JJ'), ('computing', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('computing', 'VBG'), ('system', 'NN'), ('includes', 'VBZ'), ('branch', 'NN'), ('processing', 'NN'), ('circuit', 'NN'), ('branch', 'NN'), ('processing', 'VBG'), ('circuit', 'NN'), ('connected', 'VBN'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('branch', 'NN'), ('processing', 'VBG'), ('circuit', 'NN'), ('configured', 'VBD'), ('forward', 'RB'), ('data', 'NNS'), ('wraps', 'NNS'), ('instruction', 'NN'), ('sequences', 'NNS'), ('group', 'NN'), ('controller', 'NN'), ('computing', 'VBG'), ('units', 'NNS'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('comprising', 'VBG'), ('non-instant', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'VBP'), ('computer', 'NN'), ('program', 'NN'), ('stored', 'VBD'), ('non-instant', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('computer', 'NN'), ('program', 'NN'), ('capable', 'JJ'), ('causing', 'VBG'), ('computer', 'NN'), ('perform', 'NN'), ('operations', 'NNS'), ('detecting', 'VBG'), ('body', 'NN'), ('information', 'NN'), ('one', 'CD'), ('passengers', 'NN'), ('vehicle', 'NN'), ('based', 'VBN'), ('humans', 'NNS'), (\"'\", 'POS'), ('status', 'NN'), ('comprising', 'VBG'), ('steps', 'NNS'), ('least', 'JJS'), ('one', 'CD'), ('interior', 'JJ'), ('interior', 'JJ'), ('vehicle', 'NN'), ('acquired', 'VBD'), ('passenger', 'JJR'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('performing', 'VBG'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('face', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('face', 'NN'), ('network', 'NN'), ('detect', 'JJ'), ('passengers', 'NNS'), ('interior', 'VBP'), ('thus', 'RB'), ('output', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('detected', 'VBD'), ('ii', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('body', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('body', 'NN'), ('network', 'NN'), ('detect', 'JJ'), ('bodies', 'NNS'), ('passengers', 'NNS'), ('interior', 'JJ'), ('thus', 'RB'), ('output', 'NN'), ('body-part', 'JJ'), ('length', 'NN'), ('information', 'NN'), ('detected', 'VBD'), ('bodies', 'NNS'), ('b', 'IN'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('performing', 'VBG'), ('process', 'NN'), ('retrieving', 'VBG'), ('specific', 'JJ'), ('height', 'JJ'), ('mapping', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('height', 'VBD'), ('mapping', 'VBG'), ('table', 'NN'), ('stores', 'NNS'), ('height', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('representing', 'VBG'), ('respective', 'JJ'), ('one', 'CD'), ('predetermined', 'VBD'), ('ratios', 'NNS'), ('one', 'CD'), ('segment', 'NN'), ('body', 'NN'), ('portions', 'NNS'), ('human', 'JJ'), ('groups', 'NNS'), ('heights', 'NNS'), ('per', 'IN'), ('human', 'JJ'), ('groups', 'NNS'), ('process', 'NN'), ('acquiring', 'VBG'), ('specific', 'JJ'), ('height', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('specific', 'JJ'), ('height', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('referring', 'VBG'), ('specific', 'JJ'), ('body-part', 'JJ'), ('length', 'NN'), ('information', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('process', 'NN'), ('retrieving', 'VBG'), ('specific', 'JJ'), ('weight', 'NN'), ('mapping', 'VBG'), ('information', 'NN'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('weight', 'VBD'), ('mapping', 'VBG'), ('table', 'NN'), ('stores', 'NNS'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('weight', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('representing', 'VBG'), ('predetermined', 'VBN'), ('correlations', 'NNS'), ('heights', 'NNS'), ('weights', 'NNS'), ('per', 'IN'), ('human', 'JJ'), ('groups', 'NNS'), ('process', 'NN'), ('acquiring', 'VBG'), ('weight', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('specific', 'JJ'), ('weight', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('referring', 'VBG'), ('specific', 'JJ'), ('height', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('wherein', 'JJ'), ('step', 'NN'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('body', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('body', 'NN'), ('network', 'NN'), ('output', 'NN'), ('one', 'CD'), ('one', 'CD'), ('channels', 'NNS'), ('corresponding', 'VBG'), ('interior', 'JJ'), ('via', 'IN'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('ii', 'JJ'), ('generate', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('one', 'CD'), ('channels', 'NNS'), ('corresponding', 'VBG'), ('via', 'IN'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('iii', 'JJ'), ('extract', 'JJ'), ('keypoints', 'NNS'), ('keypoint', 'VB'), ('heatmap', 'NN'), ('via', 'IN'), ('keypoint', 'NN'), ('detector', 'NN'), ('group', 'NN'), ('extracted', 'VBD'), ('keypoints', 'NNS'), ('referring', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('thus', 'RB'), ('generate', 'VB'), ('body', 'NN'), ('parts', 'NNS'), ('per', 'IN'), ('passengers', 'NNS'), ('result', 'VBP'), ('allow', 'IN'), ('body', 'NN'), ('network', 'NN'), ('output', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('body-part', 'JJ'), ('length', 'NN'), ('information', 'NN'), ('passengers', 'NNS'), ('referring', 'VBG'), ('body', 'NN'), ('parts', 'NNS'), ('per', 'IN'), ('passengers', 'NNS'), ('wherein', 'VBP'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('convolutional', 'JJ'), ('layer', 'NN'), ('applies', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('convolution', 'NN'), ('operation', 'NN'), ('interior', 'JJ'), ('thereby', 'RB'), ('output', 'NN'), ('wherein', 'NNS'), ('keypoint', 'VBP'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('includes', 'VBZ'), ('one', 'CD'), ('fully', 'RB'), ('convolutional', 'JJ'), ('network', 'NN'), ('×', 'NNP'), ('convolutional', 'NN'), ('layer', 'NN'), ('applies', 'VBZ'), ('fully-convolution', 'NN'), ('operation', 'NN'), ('×', 'NNP'), ('convolution', 'NN'), ('operation', 'NN'), ('thereby', 'RB'), ('generate', 'JJ'), ('keypoint', 'NN'), ('heatmap', 'NNS'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('wherein', 'VBD'), ('keypoint', 'JJ'), ('detector', 'NN'), ('connects', 'NNS'), ('referring', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('pairs', 'VBZ'), ('respectively', 'RB'), ('highest', 'JJS'), ('mutual', 'JJ'), ('connection', 'NN'), ('probabilities', 'NNS'), ('connected', 'VBN'), ('among', 'IN'), ('extracted', 'JJ'), ('keypoints', 'NNS'), ('thereby', 'RB'), ('group', 'NN'), ('extracted', 'VBD'), ('keypoints', 'NNS'), ('wherein', 'JJ'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('learned', 'VBD'), ('learning', 'JJ'), ('device', 'NN'), ('performing', 'VBG'), ('process', 'NN'), ('inputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('training', 'NN'), ('including', 'VBG'), ('one', 'CD'), ('objects', 'VBZ'), ('training', 'VBG'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('generate', 'VBP'), ('one', 'CD'), ('training', 'VBG'), ('one', 'CD'), ('channels', 'NNS'), ('applying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('convolutional', 'JJ'), ('operation', 'NN'), ('training', 'VBG'), ('ii', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('training', 'VBG'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('generate', 'VBP'), ('one', 'CD'), ('keypoint', 'NN'), ('heatmaps', 'VBZ'), ('training', 'VBG'), ('one', 'CD'), ('part', 'NN'), ('affinity', 'NN'), ('fields', 'NNS'), ('training', 'VBG'), ('one', 'CD'), ('channels', 'NNS'), ('training', 'VBG'), ('iii', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('keypoint', 'NN'), ('heatmaps', 'NNS'), ('training', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('fields', 'NNS'), ('training', 'VBG'), ('keypoint', 'NN'), ('detector', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('keypoint', 'NN'), ('detector', 'NN'), ('extract', 'JJ'), ('keypoints', 'NNS'), ('training', 'VBG'), ('keypoint', 'NN'), ('heatmaps', 'NNS'), ('training', 'VBG'), ('process', 'NN'), ('grouping', 'NN'), ('extracted', 'VBD'), ('keypoints', 'NNS'), ('training', 'VBG'), ('referring', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('fields', 'NNS'), ('training', 'VBG'), ('thereby', 'RB'), ('detect', 'JJ'), ('keypoints', 'NNS'), ('per', 'IN'), ('objects', 'NNS'), ('training', 'VBG'), ('iv', 'JJ'), ('process', 'NN'), ('allowing', 'VBG'), ('loss', 'NN'), ('layer', 'NN'), ('calculate', 'VBP'), ('one', 'CD'), ('losses', 'NNS'), ('referring', 'VBG'), ('keypoints', 'NNS'), ('per', 'IN'), ('objects', 'NNS'), ('training', 'VBG'), ('corresponding', 'VBG'), ('ground', 'NN'), ('truths', 'NNS'), ('thereby', 'RB'), ('adjust', 'VBP'), ('one', 'CD'), ('parameters', 'NNS'), ('feature', 'VBP'), ('extraction', 'NN'), ('network', 'NN'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('losses', 'NNS'), ('minimized', 'VBN'), ('backpropagation', 'NN'), ('using', 'VBG'), ('losses', 'NNS'), ('wherein', 'JJ'), ('step', 'JJ'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('face', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('face', 'NN'), ('network', 'NN'), ('detect', 'JJ'), ('passengers', 'NNS'), ('located', 'VBN'), ('interior', 'JJ'), ('via', 'IN'), ('face', 'NN'), ('detector', 'NN'), ('output', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('facial', 'JJ'), ('via', 'IN'), ('facial', 'JJ'), ('feature', 'NN'), ('classifier', 'NN'), ('wherein', 'JJ'), ('step', 'NN'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('face', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('face', 'NN'), ('network', 'NN'), ('apply', 'RB'), ('least', 'JJS'), ('one', 'CD'), ('convolution', 'NN'), ('operation', 'NN'), ('interior', 'JJ'), ('thus', 'RB'), ('output', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('map', 'NN'), ('corresponding', 'VBG'), ('interior', 'JJ'), ('via', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('convolutional', 'JJ'), ('layer', 'NN'), ('ii', 'NN'), ('output', 'NN'), ('one', 'CD'), ('proposal', 'NN'), ('boxes', 'VBZ'), ('passengers', 'NNS'), ('estimated', 'VBN'), ('located', 'JJ'), ('feature', 'NN'), ('map', 'NN'), ('via', 'IN'), ('region', 'NN'), ('proposal', 'NN'), ('network', 'NN'), ('iii', 'VBP'), ('apply', 'VB'), ('pooling', 'VBG'), ('operation', 'NN'), ('one', 'CD'), ('regions', 'NNS'), ('corresponding', 'VBG'), ('proposal', 'NN'), ('boxes', 'NNS'), ('feature', 'VBP'), ('map', 'JJ'), ('thus', 'RB'), ('output', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('vector', 'NN'), ('via', 'IN'), ('pooling', 'VBG'), ('layer', 'NN'), ('iv', 'JJ'), ('apply', 'RB'), ('fully-connected', 'JJ'), ('operation', 'NN'), ('feature', 'NN'), ('vector', 'NN'), ('thus', 'RB'), ('output', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('passengers', 'NNS'), ('corresponding', 'VBG'), ('proposal', 'NN'), ('boxes', 'NNS'), ('via', 'IN'), ('fully', 'RB'), ('connected', 'VBN'), ('layer', 'NN'), ('wherein', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('include', 'VBP'), ('ages', 'VBZ'), ('genders', 'NNS'), ('races', 'NNS'), ('corresponding', 'VBG'), ('passengers', 'NNS'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('detecting', 'VBG'), ('body', 'NN'), ('information', 'NN'), ('one', 'CD'), ('passengers', 'NN'), ('vehicle', 'NN'), ('based', 'VBN'), ('humans', 'NNS'), (\"'\", 'POS'), ('status', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('memory', 'NN'), ('stores', 'NNS'), ('instructions', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('configured', 'VBN'), ('execute', 'NN'), ('instructions', 'NNS'), ('perform', 'VB'), ('support', 'NN'), ('another', 'DT'), ('device', 'NN'), ('perform', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('interior', 'JJ'), ('interior', 'JJ'), ('vehicle', 'NN'), ('acquired', 'VBD'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('face', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('face', 'NN'), ('network', 'NN'), ('detect', 'JJ'), ('passengers', 'NNS'), ('interior', 'VBP'), ('thus', 'RB'), ('output', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('detected', 'VBD'), ('ii', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('body', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('body', 'NN'), ('network', 'NN'), ('detect', 'JJ'), ('bodies', 'NNS'), ('passengers', 'NNS'), ('interior', 'JJ'), ('thus', 'RB'), ('output', 'NN'), ('body-part', 'JJ'), ('length', 'NN'), ('information', 'NN'), ('detected', 'VBD'), ('bodies', 'NNS'), ('ii', 'JJ'), ('process', 'NN'), ('retrieving', 'VBG'), ('specific', 'JJ'), ('height', 'JJ'), ('mapping', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('height', 'VBD'), ('mapping', 'VBG'), ('table', 'NN'), ('stores', 'NNS'), ('height', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('representing', 'VBG'), ('respective', 'JJ'), ('one', 'CD'), ('predetermined', 'VBD'), ('ratios', 'NNS'), ('one', 'CD'), ('segment', 'NN'), ('body', 'NN'), ('portions', 'NNS'), ('human', 'JJ'), ('groups', 'NNS'), ('heights', 'NNS'), ('per', 'IN'), ('human', 'JJ'), ('groups', 'NNS'), ('process', 'NN'), ('acquiring', 'VBG'), ('specific', 'JJ'), ('height', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('specific', 'JJ'), ('height', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('referring', 'VBG'), ('specific', 'JJ'), ('body-part', 'JJ'), ('length', 'NN'), ('information', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('process', 'NN'), ('retrieving', 'VBG'), ('specific', 'JJ'), ('weight', 'NN'), ('mapping', 'VBG'), ('information', 'NN'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('weight', 'VBD'), ('mapping', 'VBG'), ('table', 'NN'), ('stores', 'NNS'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('weight', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('representing', 'VBG'), ('predetermined', 'VBN'), ('correlations', 'NNS'), ('heights', 'NNS'), ('weights', 'NNS'), ('per', 'IN'), ('human', 'JJ'), ('groups', 'NNS'), ('process', 'NN'), ('acquiring', 'VBG'), ('weight', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('specific', 'JJ'), ('weight', 'VBD'), ('mapping', 'VBG'), ('information', 'NN'), ('referring', 'VBG'), ('specific', 'JJ'), ('height', 'NN'), ('specific', 'JJ'), ('passenger', 'NN'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('wherein', 'NN'), ('process', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('body', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('body', 'NN'), ('network', 'NN'), ('output', 'NN'), ('one', 'CD'), ('one', 'CD'), ('channels', 'NNS'), ('corresponding', 'VBG'), ('interior', 'JJ'), ('via', 'IN'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('ii', 'JJ'), ('generate', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('one', 'CD'), ('channels', 'NNS'), ('corresponding', 'VBG'), ('via', 'IN'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('iii', 'JJ'), ('extract', 'JJ'), ('keypoints', 'NNS'), ('keypoint', 'VB'), ('heatmap', 'NN'), ('via', 'IN'), ('keypoint', 'NN'), ('detector', 'NN'), ('group', 'NN'), ('extracted', 'VBD'), ('keypoints', 'NNS'), ('referring', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('thus', 'RB'), ('generate', 'VB'), ('body', 'NN'), ('parts', 'NNS'), ('per', 'IN'), ('passengers', 'NNS'), ('result', 'VBP'), ('allow', 'IN'), ('body', 'NN'), ('network', 'NN'), ('output', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('body-part', 'JJ'), ('length', 'NN'), ('information', 'NN'), ('passengers', 'NNS'), ('referring', 'VBG'), ('body', 'NN'), ('parts', 'NNS'), ('per', 'IN'), ('passengers', 'NNS'), ('passenger', 'VBP'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('wherein', 'NN'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('includes', 'VBZ'), ('one', 'CD'), ('fully', 'RB'), ('convolutional', 'JJ'), ('network', 'NN'), ('×', 'NNP'), ('convolutional', 'NN'), ('layer', 'NN'), ('applies', 'VBZ'), ('fully-convolution', 'NN'), ('operation', 'NN'), ('×', 'NNP'), ('convolution', 'NN'), ('operation', 'NN'), ('thereby', 'RB'), ('generate', 'JJ'), ('keypoint', 'NN'), ('heatmap', 'NNS'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('wherein', 'NN'), ('keypoint', 'NN'), ('detector', 'NN'), ('connects', 'NNS'), ('referring', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('pairs', 'VBZ'), ('respectively', 'RB'), ('highest', 'JJS'), ('mutual', 'JJ'), ('connection', 'NN'), ('probabilities', 'NNS'), ('connected', 'VBN'), ('among', 'IN'), ('extracted', 'JJ'), ('keypoints', 'NNS'), ('thereby', 'RB'), ('group', 'NN'), ('extracted', 'VBD'), ('keypoints', 'NNS'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('wherein', 'NN'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('learned', 'VBD'), ('learning', 'JJ'), ('device', 'NN'), ('performing', 'VBG'), ('process', 'NN'), ('inputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('training', 'NN'), ('including', 'VBG'), ('one', 'CD'), ('objects', 'VBZ'), ('training', 'VBG'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('feature', 'NN'), ('extraction', 'NN'), ('network', 'NN'), ('generate', 'VBP'), ('one', 'CD'), ('training', 'VBG'), ('one', 'CD'), ('channels', 'NNS'), ('applying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('convolutional', 'JJ'), ('operation', 'NN'), ('training', 'VBG'), ('ii', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('training', 'VBG'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('generate', 'VBP'), ('one', 'CD'), ('keypoint', 'NN'), ('heatmaps', 'VBZ'), ('training', 'VBG'), ('one', 'CD'), ('part', 'NN'), ('affinity', 'NN'), ('fields', 'NNS'), ('training', 'VBG'), ('one', 'CD'), ('channels', 'NNS'), ('training', 'VBG'), ('iii', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('keypoint', 'NN'), ('heatmaps', 'NNS'), ('training', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('fields', 'NNS'), ('training', 'VBG'), ('keypoint', 'NN'), ('detector', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('keypoint', 'NN'), ('detector', 'NN'), ('extract', 'JJ'), ('keypoints', 'NNS'), ('training', 'VBG'), ('keypoint', 'NN'), ('heatmaps', 'NNS'), ('training', 'VBG'), ('process', 'NN'), ('grouping', 'NN'), ('extracted', 'VBD'), ('keypoints', 'NNS'), ('training', 'VBG'), ('referring', 'VBG'), ('part', 'NN'), ('affinity', 'NN'), ('fields', 'NNS'), ('training', 'VBG'), ('thereby', 'RB'), ('detect', 'JJ'), ('keypoints', 'NNS'), ('per', 'IN'), ('objects', 'NNS'), ('training', 'VBG'), ('iv', 'JJ'), ('process', 'NN'), ('allowing', 'VBG'), ('loss', 'NN'), ('layer', 'NN'), ('calculate', 'VBP'), ('one', 'CD'), ('losses', 'NNS'), ('referring', 'VBG'), ('keypoints', 'NNS'), ('per', 'IN'), ('objects', 'NNS'), ('training', 'VBG'), ('corresponding', 'VBG'), ('ground', 'NN'), ('truths', 'NNS'), ('thereby', 'RB'), ('adjust', 'VBP'), ('one', 'CD'), ('parameters', 'NNS'), ('feature', 'VBP'), ('extraction', 'NN'), ('network', 'NN'), ('keypoint', 'NN'), ('heatmap', 'NN'), ('&', 'CC'), ('part', 'NN'), ('affinity', 'NN'), ('field', 'NN'), ('extractor', 'NN'), ('losses', 'NNS'), ('minimized', 'VBN'), ('backpropagation', 'NN'), ('using', 'VBG'), ('losses', 'NNS'), ('passenger', 'NN'), ('body', 'NN'), ('information-detecting', 'JJ'), ('device', 'NN'), ('wherein', 'NN'), ('process', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('inputting', 'VBG'), ('interior', 'JJ'), ('face', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('face', 'NN'), ('network', 'NN'), ('apply', 'RB'), ('least', 'JJS'), ('one', 'CD'), ('convolution', 'NN'), ('operation', 'NN'), ('interior', 'JJ'), ('thus', 'RB'), ('output', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('map', 'NN'), ('corresponding', 'VBG'), ('interior', 'JJ'), ('via', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('convolutional', 'JJ'), ('layer', 'NN'), ('ii', 'NN'), ('output', 'NN'), ('one', 'CD'), ('proposal', 'NN'), ('boxes', 'VBZ'), ('passengers', 'NNS'), ('estimated', 'VBN'), ('located', 'JJ'), ('feature', 'NN'), ('map', 'NN'), ('via', 'IN'), ('region', 'NN'), ('proposal', 'NN'), ('network', 'NN'), ('iii', 'VBP'), ('apply', 'VB'), ('pooling', 'VBG'), ('operation', 'NN'), ('one', 'CD'), ('regions', 'NNS'), ('corresponding', 'VBG'), ('proposal', 'NN'), ('boxes', 'NNS'), ('feature', 'VBP'), ('map', 'JJ'), ('thus', 'RB'), ('output', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('vector', 'NN'), ('via', 'IN'), ('pooling', 'VBG'), ('layer', 'NN'), ('iv', 'JJ'), ('apply', 'RB'), ('fully-connected', 'JJ'), ('operation', 'NN'), ('feature', 'NN'), ('vector', 'NN'), ('thus', 'RB'), ('output', 'NN'), ('multiple', 'JJ'), ('pieces', 'NNS'), ('passenger', 'NN'), ('feature', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('passengers', 'NNS'), ('corresponding', 'VBG'), ('proposal', 'NN'), ('boxes', 'NNS'), ('via', 'IN'), ('fully', 'RB'), ('connected', 'VBN'), ('layer', 'NN'), ('computer', 'NN'), ('implemented', 'VBD'), ('performing', 'VBG'), ('video', 'NN'), ('coding', 'VBG'), ('based', 'VBN'), ('face', 'NN'), ('detection', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('video', 'NN'), ('frames', 'VBZ'), ('video', 'JJ'), ('sequence', 'NN'), ('determining', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('key', 'JJ'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('performing', 'VBG'), ('response', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('key', 'JJ'), ('frame', 'NN'), ('video', 'JJ'), ('sequence', 'NN'), ('multi-stage', 'NN'), ('facial', 'JJ'), ('search', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('predetermined', 'JJ'), ('feature', 'NN'), ('templates', 'NNS'), ('predetermined', 'VBD'), ('number', 'NN'), ('stages', 'NNS'), ('determine', 'VBP'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('testing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('regions', 'NNS'), ('based', 'VBN'), ('skin', 'JJ'), ('tone', 'NN'), ('information', 'NN'), ('determine', 'NN'), ('first', 'RB'), ('candidate', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('invalid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('rejecting', 'VBG'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('outputting', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('generate', 'NN'), ('coded', 'VBD'), ('bitstream', 'NN'), ('wherein', 'NN'), ('skin', 'VBD'), ('tone', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('skin', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('wherein', 'NN'), ('said', 'VBD'), ('testing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('regions', 'NNS'), ('based', 'VBN'), ('skin', 'JJ'), ('tone', 'NN'), ('information', 'NN'), ('performed', 'VBN'), ('response', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('key', 'JJ'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('rectangular', 'JJ'), ('region', 'NN'), ('comprising', 'VBG'), ('determining', 'VBG'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'VBD'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('pixel', 'NN'), ('accuracy', 'NN'), ('small', 'JJ'), ('block', 'NN'), ('pixels', 'NNS'), ('accuracy', 'VBP'), ('wherein', 'IN'), ('determining', 'VBG'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('generating', 'VBG'), ('enhanced', 'VBN'), ('skip', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('binarizing', 'VBG'), ('enhanced', 'VBD'), ('skip', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('overlaying', 'VBG'), ('binarized', 'VBN'), ('enhanced', 'JJ'), ('skip', 'NN'), ('probability', 'NN'), ('map', 'NN'), ('least', 'JJS'), ('portion', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('provide', 'VBP'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'IN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('comprises', 'VBZ'), ('non-key', 'JJ'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('comprising', 'VBG'), ('performing', 'VBG'), ('face', 'NN'), ('detection', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('based', 'VBN'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprising', 'VBG'), ('second', 'JJ'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('wherein', 'JJ'), ('second', 'JJ'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('location', 'NN'), ('second', 'JJ'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('displacement', 'JJ'), ('offset', 'NN'), ('respect', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprising', 'VBG'), ('determining', 'VBG'), ('displacement', 'NN'), ('offset', 'NN'), ('based', 'VBN'), ('offset', 'VBN'), ('centroid', 'JJ'), ('bounding', 'VBG'), ('box', 'NN'), ('around', 'IN'), ('skin', 'NN'), ('enhanced', 'VBN'), ('region', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('centroid', 'JJ'), ('second', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('around', 'IN'), ('second', 'JJ'), ('skin', 'NN'), ('enhanced', 'VBD'), ('region', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('wherein', 'NN'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('reducing', 'VBG'), ('quantization', 'NN'), ('parameter', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('adjusting', 'VBG'), ('lambda', 'NN'), ('value', 'NN'), ('first', 'RB'), ('candidate', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('disabling', 'VBG'), ('skip', 'NN'), ('coding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'WRB'), ('bitstream', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('hadvanced', 'VBD'), ('video', 'NN'), ('coding', 'VBG'), ('avc', 'JJ'), ('compliant', 'JJ'), ('bitstream', 'NN'), ('hhigh', 'NN'), ('efficiency', 'NN'), ('video', 'NN'), ('coding', 'VBG'), ('hevc', 'NN'), ('compliant', 'JJ'), ('bitstream', 'NN'), ('vp', 'NN'), ('compliant', 'JJ'), ('bitstream', 'NN'), ('vp', 'NN'), ('compliant', 'JJ'), ('bitstream', 'NN'), ('alliance', 'NN'), ('open', 'JJ'), ('media', 'NNS'), ('aom', 'VBP'), ('av', 'JJ'), ('compliant', 'NN'), ('bitstream', 'NN'), ('computer', 'NN'), ('implemented', 'VBD'), ('performing', 'VBG'), ('face', 'NN'), ('detection', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('sequence', 'NN'), ('video', 'NN'), ('frames', 'NNS'), ('performing', 'VBG'), ('multi-stage', 'JJ'), ('facial', 'JJ'), ('search', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('predetermined', 'JJ'), ('feature', 'NN'), ('templates', 'NNS'), ('predetermined', 'VBD'), ('number', 'NN'), ('stages', 'NNS'), ('determine', 'VBP'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('testing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('regions', 'NNS'), ('based', 'VBN'), ('skin', 'JJ'), ('tone', 'NN'), ('information', 'NN'), ('determine', 'NN'), ('first', 'RB'), ('candidate', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('invalid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('rejecting', 'VBG'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('outputting', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('processing', 'VBG'), ('providing', 'VBG'), ('index', 'NN'), ('indicative', 'JJ'), ('person', 'NN'), ('present', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'VBD'), ('sequence', 'NN'), ('video', 'NN'), ('frames', 'VBZ'), ('comprises', 'NNS'), ('sequence', 'NN'), ('surveillance', 'NN'), ('video', 'NN'), ('frames', 'NNS'), ('comprising', 'VBG'), ('performing', 'VBG'), ('face', 'NN'), ('surveillance', 'NN'), ('video', 'NN'), ('frames', 'NNS'), ('based', 'VBN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'VBD'), ('sequence', 'NN'), ('video', 'NN'), ('frames', 'VBZ'), ('comprises', 'NNS'), ('sequence', 'NN'), ('decoded', 'VBD'), ('video', 'NN'), ('frames', 'NNS'), ('comprising', 'VBG'), ('adding', 'VBG'), ('marker', 'NN'), ('corresponding', 'VBG'), ('received', 'VBD'), ('video', 'JJ'), ('frame', 'NN'), ('perform', 'NN'), ('face', 'NN'), ('received', 'VBD'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'VBD'), ('sequence', 'NN'), ('video', 'NN'), ('frames', 'NNS'), ('received', 'VBD'), ('device', 'NN'), ('login', 'NN'), ('attempt', 'NN'), ('comprising', 'VBG'), ('performing', 'VBG'), ('face', 'NN'), ('based', 'VBN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('allowing', 'VBG'), ('access', 'NN'), ('device', 'NN'), ('secured', 'VBD'), ('face', 'NN'), ('recognized', 'VBN'), ('wherein', 'JJ'), ('sequence', 'NN'), ('video', 'NN'), ('frames', 'VBZ'), ('comprises', 'NNS'), ('sequence', 'NN'), ('videoconferencing', 'VBG'), ('frames', 'NNS'), ('comprising', 'VBG'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('generate', 'NN'), ('coded', 'VBD'), ('bitstream', 'NN'), ('wherein', 'NN'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('comprises', 'VBZ'), ('encoding', 'VBG'), ('background', 'RP'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('bitstream', 'NN'), ('comprising', 'VBG'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('generate', 'NN'), ('coded', 'VBD'), ('bitstream', 'NN'), ('wherein', 'NN'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('comprises', 'VBZ'), ('including', 'VBG'), ('metadata', 'NNS'), ('corresponding', 'VBG'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('bitstream', 'NN'), ('comprising', 'VBG'), ('decoding', 'VBG'), ('coded', 'VBN'), ('bitstream', 'NN'), ('generate', 'NN'), ('decoded', 'VBD'), ('video', 'NN'), ('frame', 'NN'), ('determine', 'NN'), ('metadata', 'NN'), ('corresponding', 'VBG'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('bitstream', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('replacing', 'VBG'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('based', 'VBN'), ('decoded', 'VBN'), ('metadata', 'NNS'), ('cropping', 'VBG'), ('displaying', 'VBG'), ('data', 'NNS'), ('corresponding', 'VBG'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('based', 'VBN'), ('decoded', 'VBN'), ('metadata', 'NNS'), ('indexing', 'VBG'), ('decoded', 'VBD'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('decoded', 'VBN'), ('metadata', 'NN'), ('system', 'NN'), ('performing', 'VBG'), ('video', 'NN'), ('coding', 'VBG'), ('based', 'VBN'), ('face', 'NN'), ('detection', 'NN'), ('comprising', 'VBG'), ('memory', 'NN'), ('configured', 'VBD'), ('store', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('video', 'NN'), ('frames', 'VBZ'), ('video', 'JJ'), ('sequence', 'NN'), ('coupled', 'VBD'), ('memory', 'NN'), ('receive', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('determine', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('key', 'JJ'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('perform', 'NN'), ('response', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('key', 'JJ'), ('frame', 'NN'), ('video', 'JJ'), ('sequence', 'NN'), ('multi-stage', 'NN'), ('facial', 'JJ'), ('search', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('predetermined', 'JJ'), ('feature', 'NN'), ('templates', 'NNS'), ('predetermined', 'VBD'), ('number', 'NN'), ('stages', 'NNS'), ('determine', 'VBP'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('test', 'NN'), ('first', 'RB'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('regions', 'NNS'), ('based', 'VBN'), ('skin', 'JJ'), ('tone', 'NN'), ('information', 'NN'), ('determine', 'NN'), ('first', 'RB'), ('candidate', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('invalid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('reject', 'JJ'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('outputting', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('encode', 'FW'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('generate', 'NN'), ('coded', 'VBD'), ('bitstream', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('skin', 'VBZ'), ('tone', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('skin', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('rectangular', 'JJ'), ('region', 'NN'), ('determine', 'NN'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'VBD'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('pixel', 'NN'), ('accuracy', 'NN'), ('small', 'JJ'), ('block', 'NN'), ('pixels', 'NNS'), ('accuracy', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('determine', 'JJ'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('generate', 'NN'), ('enhanced', 'VBD'), ('skip', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('binarize', 'NN'), ('enhanced', 'VBD'), ('skip', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('overlay', 'NN'), ('binarized', 'VBD'), ('enhanced', 'JJ'), ('skip', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('least', 'JJS'), ('portion', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('provide', 'VBP'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('comprises', 'VBZ'), ('non-key', 'JJ'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('perform', 'NN'), ('face', 'NN'), ('detection', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('based', 'VBN'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('track', 'JJ'), ('second', 'JJ'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('encode', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('reduce', 'VB'), ('quantization', 'NN'), ('parameter', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('adjust', 'VBP'), ('lambda', 'NN'), ('value', 'NN'), ('first', 'RB'), ('candidate', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('disable', 'JJ'), ('skip', 'NN'), ('coding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('instructions', 'NNS'), ('response', 'NN'), ('executed', 'VBD'), ('device', 'NN'), ('cause', 'NN'), ('device', 'NN'), ('perform', 'NN'), ('video', 'NN'), ('coding', 'VBG'), ('based', 'VBN'), ('face', 'NN'), ('detection', 'NN'), ('receiving', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('video', 'NN'), ('frames', 'VBZ'), ('video', 'JJ'), ('sequence', 'NN'), ('determining', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('key', 'JJ'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('performing', 'VBG'), ('response', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('key', 'JJ'), ('frame', 'NN'), ('video', 'JJ'), ('sequence', 'NN'), ('multi-stage', 'NN'), ('facial', 'JJ'), ('search', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('predetermined', 'JJ'), ('feature', 'NN'), ('templates', 'NNS'), ('predetermined', 'VBD'), ('number', 'NN'), ('stages', 'NNS'), ('determine', 'VBP'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('testing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('regions', 'NNS'), ('based', 'VBN'), ('skin', 'JJ'), ('tone', 'NN'), ('information', 'NN'), ('determine', 'NN'), ('first', 'RB'), ('candidate', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('invalid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('rejecting', 'VBG'), ('second', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('outputting', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('generate', 'NN'), ('coded', 'VBD'), ('bitstream', 'JJ'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('skin', 'VBD'), ('tone', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('skin', 'JJ'), ('probability', 'NN'), ('map', 'VBP'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('rectangular', 'JJ'), ('region', 'NN'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('instructions', 'NNS'), ('response', 'NN'), ('executed', 'VBD'), ('device', 'NN'), ('cause', 'NN'), ('device', 'NN'), ('perform', 'NN'), ('video', 'NN'), ('coding', 'VBG'), ('based', 'VBN'), ('face', 'NN'), ('detection', 'NN'), ('determining', 'VBG'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('wherein', 'VBD'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('pixel', 'NN'), ('accuracy', 'NN'), ('small', 'JJ'), ('block', 'NN'), ('pixels', 'NNS'), ('accuracy', 'IN'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('determining', 'VBG'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('generating', 'VBG'), ('enhanced', 'VBN'), ('skip', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('binarizing', 'VBG'), ('enhanced', 'VBD'), ('skip', 'JJ'), ('probability', 'NN'), ('map', 'NN'), ('overlaying', 'VBG'), ('binarized', 'VBN'), ('enhanced', 'JJ'), ('skip', 'NN'), ('probability', 'NN'), ('map', 'NN'), ('least', 'JJS'), ('portion', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('provide', 'VBP'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('comprises', 'VBZ'), ('non-key', 'JJ'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('instructions', 'NNS'), ('response', 'NN'), ('executed', 'VBD'), ('device', 'NN'), ('cause', 'NN'), ('device', 'NN'), ('perform', 'NN'), ('video', 'NN'), ('coding', 'VBG'), ('based', 'VBN'), ('face', 'NN'), ('detection', 'NN'), ('performing', 'VBG'), ('face', 'NN'), ('detection', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('video', 'NN'), ('sequence', 'NN'), ('based', 'VBN'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('instructions', 'NNS'), ('response', 'NN'), ('executed', 'VBD'), ('device', 'NN'), ('cause', 'NN'), ('device', 'NN'), ('perform', 'NN'), ('video', 'NN'), ('coding', 'VBG'), ('based', 'VBN'), ('face', 'NN'), ('detection', 'NN'), ('second', 'JJ'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('second', 'JJ'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('free', 'JJ'), ('form', 'NN'), ('shape', 'NN'), ('face', 'NN'), ('region', 'NN'), ('video', 'NN'), ('frame', 'NN'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('encoding', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('part', 'NN'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('valid', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('reducing', 'VBG'), ('quantization', 'NN'), ('parameter', 'NN'), ('corresponding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('adjusting', 'VBG'), ('lambda', 'NN'), ('value', 'NN'), ('first', 'RB'), ('candidate', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('disabling', 'VBG'), ('skip', 'NN'), ('coding', 'VBG'), ('first', 'JJ'), ('candidate', 'NN'), ('face', 'NN'), ('region', 'NN'), ('managing', 'VBG'), ('smart', 'JJ'), ('database', 'NN'), ('stores', 'NNS'), ('facial', 'JJ'), ('face', 'NN'), ('comprising', 'VBG'), ('steps', 'NNS'), ('managing', 'VBG'), ('device', 'NN'), ('performing', 'VBG'), ('process', 'NN'), ('counting', 'VBG'), ('one', 'CD'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('specific', 'JJ'), ('person', 'NN'), ('stored', 'VBD'), ('smart', 'JJ'), ('database', 'NN'), ('new', 'JJ'), ('facial', 'JJ'), ('face', 'NN'), ('continuously', 'RB'), ('stored', 'VBD'), ('process', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('first', 'JJ'), ('counted', 'VBN'), ('value', 'NN'), ('representing', 'VBG'), ('count', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('satisfies', 'NNS'), ('preset', 'VBP'), ('first', 'RB'), ('set', 'VBN'), ('value', 'NN'), ('b', 'NN'), ('first', 'RB'), ('counted', 'VBD'), ('value', 'NN'), ('determined', 'VBD'), ('satisfying', 'VBG'), ('first', 'RB'), ('set', 'VBN'), ('value', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('performing', 'VBG'), ('process', 'NN'), ('inputting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('generate', 'VBP'), ('quality', 'NN'), ('scores', 'NNS'), ('specific', 'JJ'), ('facial', 'JJ'), ('aggregation', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('process', 'NN'), ('sorting', 'VBG'), ('quality', 'NN'), ('scores', 'NNS'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('descending', 'NN'), ('order', 'NN'), ('quality', 'NN'), ('scores', 'VBZ'), ('process', 'NN'), ('counting', 'NN'), ('sorted', 'VBN'), ('specific', 'JJ'), ('facial', 'JJ'), ('descending', 'NN'), ('order', 'NN'), ('second', 'JJ'), ('counted', 'VBN'), ('value', 'NN'), ('represents', 'VBZ'), ('number', 'NN'), ('counted', 'VBN'), ('part', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('becomes', 'NNS'), ('equal', 'JJ'), ('preset', 'JJ'), ('second', 'NN'), ('set', 'VBN'), ('value', 'NN'), ('process', 'NN'), ('deleting', 'VBG'), ('uncounted', 'JJ'), ('part', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('smart', 'JJ'), ('database', 'NN'), ('comprising', 'VBG'), ('step', 'NN'), ('c', 'RB'), ('managing', 'VBG'), ('device', 'NN'), ('performing', 'VBG'), ('process', 'NN'), ('generating', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('optimal', 'JJ'), ('feature', 'NN'), ('weighted', 'VBD'), ('summation', 'NN'), ('one', 'CD'), ('features', 'VBZ'), ('specific', 'JJ'), ('facial', 'JJ'), ('using', 'VBG'), ('counted', 'JJ'), ('part', 'NN'), ('quality', 'NN'), ('scores', 'VBZ'), ('process', 'JJ'), ('setting', 'VBG'), ('optimal', 'JJ'), ('feature', 'NN'), ('representative', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('person', 'NN'), ('wherein', 'JJ'), ('step', 'NN'), ('b', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('inputting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('cnn', 'NN'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('cnn', 'NN'), ('generate', 'NN'), ('one', 'CD'), ('features', 'VBZ'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('vector', 'NN'), ('features', 'NNS'), ('embedded', 'VBD'), ('aggregation', 'NN'), ('module', 'NN'), ('including', 'VBG'), ('least', 'JJS'), ('two', 'CD'), ('attention', 'NN'), ('blocks', 'NNS'), ('thereby', 'RB'), ('allow', 'VB'), ('aggregation', 'NN'), ('module', 'NN'), ('generate', 'VBP'), ('quality', 'NN'), ('scores', 'NNS'), ('features', 'VBZ'), ('wherein', 'JJ'), ('step', 'NN'), ('b', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('matching', 'VBG'), ('i-', 'JJ'), ('one', 'CD'), ('features', 'VBZ'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('stored', 'VBN'), ('smart', 'JJ'), ('database', 'NN'), ('i-', 'JJ'), ('quality', 'NN'), ('scores', 'NNS'), ('ii', 'VBP'), ('specific', 'JJ'), ('person', 'NN'), ('process', 'NN'), ('storing', 'VBG'), ('matched', 'VBN'), ('features', 'NNS'), ('matched', 'VBN'), ('quality', 'NN'), ('scores', 'NNS'), ('smart', 'VBP'), ('database', 'NN'), ('comprising', 'VBG'), ('step', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('performing', 'VBG'), ('one', 'CD'), ('process', 'NN'), ('learning', 'VBG'), ('face', 'NN'), ('system', 'NN'), ('using', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('specific', 'JJ'), ('person', 'NN'), ('stored', 'VBD'), ('smart', 'JJ'), ('database', 'NN'), ('ii', 'NN'), ('process', 'NN'), ('transmitting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('specific', 'JJ'), ('person', 'NN'), ('learning', 'JJ'), ('device', 'NN'), ('corresponding', 'VBG'), ('face', 'NN'), ('system', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('learning', 'VBG'), ('device', 'NN'), ('learn', 'FW'), ('face', 'NN'), ('system', 'NN'), ('using', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('wherein', 'NN'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('learned', 'VBD'), ('learning', 'JJ'), ('device', 'NN'), ('repeating', 'VBG'), ('process', 'NN'), ('inputting', 'VBG'), ('multiple', 'JJ'), ('facial', 'JJ'), ('training', 'NN'), ('corresponding', 'VBG'), ('set', 'VBN'), ('single', 'JJ'), ('face', 'NN'), ('video', 'NN'), ('single', 'JJ'), ('face', 'NN'), ('cnn', 'JJ'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('cnn', 'NN'), ('generate', 'NN'), ('one', 'CD'), ('features', 'VBZ'), ('training', 'VBG'), ('applying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('convolution', 'NN'), ('operation', 'NN'), ('facial', 'JJ'), ('training', 'NN'), ('ii', 'NN'), ('process', 'NN'), ('inputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('vector', 'NN'), ('training', 'NN'), ('features', 'NNS'), ('training', 'VBG'), ('embedded', 'JJ'), ('aggregation', 'NN'), ('module', 'NN'), ('including', 'VBG'), ('least', 'JJS'), ('two', 'CD'), ('attention', 'NN'), ('blocks', 'NNS'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('aggregation', 'NN'), ('module', 'NN'), ('generate', 'VBP'), ('quality', 'NN'), ('scores', 'NNS'), ('training', 'VBG'), ('features', 'NNS'), ('training', 'VBG'), ('aggregation', 'NN'), ('features', 'NNS'), ('training', 'VBG'), ('using', 'VBG'), ('one', 'CD'), ('attention', 'NN'), ('parameters', 'NNS'), ('learned', 'VBD'), ('previous', 'JJ'), ('iteration', 'NN'), ('iii', 'NN'), ('process', 'NN'), ('outputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('optimal', 'JJ'), ('feature', 'NN'), ('training', 'NN'), ('weighted', 'JJ'), ('summation', 'NN'), ('features', 'NNS'), ('training', 'VBG'), ('using', 'VBG'), ('quality', 'NN'), ('scores', 'NNS'), ('training', 'VBG'), ('iv', 'JJ'), ('process', 'NN'), ('updating', 'VBG'), ('attention', 'NN'), ('parameters', 'NNS'), ('learned', 'VBD'), ('previous', 'JJ'), ('iteration', 'NN'), ('least', 'JJS'), ('two', 'CD'), ('attention', 'NN'), ('blocks', 'NNS'), ('one', 'CD'), ('losses', 'NNS'), ('minimized', 'VBN'), ('outputted', 'JJ'), ('loss', 'NN'), ('layer', 'NN'), ('referring', 'VBG'), ('optimal', 'JJ'), ('feature', 'NN'), ('training', 'VBG'), ('corresponding', 'VBG'), ('ground', 'NN'), ('truth', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('managing', 'VBG'), ('smart', 'JJ'), ('database', 'NN'), ('stores', 'NNS'), ('facial', 'JJ'), ('face', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('memory', 'NN'), ('stores', 'NNS'), ('instructions', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('configured', 'VBN'), ('execute', 'NN'), ('instructions', 'NNS'), ('perform', 'VB'), ('support', 'NN'), ('another', 'DT'), ('device', 'NN'), ('perform', 'NN'), ('process', 'NN'), ('counting', 'VBG'), ('one', 'CD'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('specific', 'JJ'), ('person', 'NN'), ('stored', 'VBD'), ('smart', 'JJ'), ('database', 'NN'), ('new', 'JJ'), ('facial', 'JJ'), ('face', 'NN'), ('continuously', 'RB'), ('stored', 'VBD'), ('process', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('first', 'JJ'), ('counted', 'VBN'), ('value', 'NN'), ('representing', 'VBG'), ('count', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('satisfies', 'NNS'), ('preset', 'VBP'), ('first', 'RB'), ('set', 'VBN'), ('value', 'NN'), ('ii', 'NN'), ('first', 'RB'), ('counted', 'VBD'), ('value', 'NN'), ('determined', 'VBD'), ('satisfying', 'VBG'), ('first', 'RB'), ('set', 'VBN'), ('value', 'NN'), ('process', 'NN'), ('inputting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('generate', 'VBP'), ('quality', 'NN'), ('scores', 'NNS'), ('specific', 'JJ'), ('facial', 'JJ'), ('aggregation', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('process', 'NN'), ('sorting', 'VBG'), ('quality', 'NN'), ('scores', 'NNS'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('descending', 'NN'), ('order', 'NN'), ('quality', 'NN'), ('scores', 'VBZ'), ('process', 'NN'), ('counting', 'NN'), ('sorted', 'VBN'), ('specific', 'JJ'), ('facial', 'JJ'), ('descending', 'NN'), ('order', 'NN'), ('second', 'JJ'), ('counted', 'VBN'), ('value', 'NN'), ('represents', 'VBZ'), ('number', 'NN'), ('counted', 'VBN'), ('part', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('becomes', 'NNS'), ('equal', 'JJ'), ('preset', 'JJ'), ('second', 'NN'), ('set', 'VBN'), ('value', 'NN'), ('process', 'NN'), ('deleting', 'VBG'), ('uncounted', 'JJ'), ('part', 'NN'), ('specific', 'JJ'), ('facial', 'JJ'), ('smart', 'JJ'), ('database', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('performs', 'VBZ'), ('iii', 'JJ'), ('process', 'NN'), ('generating', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('optimal', 'JJ'), ('feature', 'NN'), ('weighted', 'VBD'), ('summation', 'NN'), ('one', 'CD'), ('features', 'VBZ'), ('specific', 'JJ'), ('facial', 'JJ'), ('using', 'VBG'), ('counted', 'JJ'), ('part', 'NN'), ('quality', 'NN'), ('scores', 'VBZ'), ('process', 'JJ'), ('setting', 'VBG'), ('optimal', 'JJ'), ('feature', 'NN'), ('representative', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('person', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('process', 'NN'), ('ii', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('inputting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('cnn', 'NN'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('cnn', 'NN'), ('generate', 'NN'), ('one', 'CD'), ('features', 'VBZ'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('process', 'NN'), ('inputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('vector', 'NN'), ('features', 'NNS'), ('embedded', 'VBD'), ('aggregation', 'NN'), ('module', 'NN'), ('including', 'VBG'), ('least', 'JJS'), ('two', 'CD'), ('attention', 'NN'), ('blocks', 'NNS'), ('thereby', 'RB'), ('allow', 'VB'), ('aggregation', 'NN'), ('module', 'NN'), ('generate', 'VBP'), ('quality', 'NN'), ('scores', 'NNS'), ('features', 'VBZ'), ('managing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('process', 'NN'), ('ii', 'NN'), ('performs', 'NNS'), ('process', 'NN'), ('matching', 'VBG'), ('i-', 'JJ'), ('one', 'CD'), ('features', 'VBZ'), ('corresponding', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('stored', 'VBN'), ('smart', 'JJ'), ('database', 'NN'), ('i-', 'JJ'), ('quality', 'NN'), ('scores', 'NNS'), ('ii', 'VBP'), ('specific', 'JJ'), ('person', 'NN'), ('process', 'NN'), ('storing', 'VBG'), ('matched', 'VBN'), ('features', 'NNS'), ('matched', 'VBN'), ('quality', 'NN'), ('scores', 'NNS'), ('smart', 'VBP'), ('database', 'NN'), ('managing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('performs', 'VBZ'), ('iv', 'VBP'), ('one', 'CD'), ('process', 'NN'), ('learning', 'VBG'), ('face', 'NN'), ('system', 'NN'), ('using', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('specific', 'JJ'), ('person', 'NN'), ('stored', 'VBD'), ('smart', 'JJ'), ('database', 'NN'), ('ii', 'NN'), ('process', 'NN'), ('transmitting', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('corresponding', 'NN'), ('specific', 'JJ'), ('person', 'NN'), ('learning', 'JJ'), ('device', 'NN'), ('corresponding', 'VBG'), ('face', 'NN'), ('system', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('learning', 'VBG'), ('device', 'NN'), ('learn', 'FW'), ('face', 'NN'), ('system', 'NN'), ('using', 'VBG'), ('specific', 'JJ'), ('facial', 'JJ'), ('managing', 'NN'), ('device', 'NN'), ('wherein', 'IN'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('learned', 'VBD'), ('learning', 'JJ'), ('device', 'NN'), ('repeating', 'VBG'), ('process', 'NN'), ('inputting', 'VBG'), ('multiple', 'JJ'), ('facial', 'JJ'), ('training', 'NN'), ('corresponding', 'VBG'), ('set', 'VBN'), ('single', 'JJ'), ('face', 'NN'), ('video', 'NN'), ('single', 'JJ'), ('face', 'NN'), ('cnn', 'JJ'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'VB'), ('cnn', 'NN'), ('generate', 'NN'), ('one', 'CD'), ('features', 'VBZ'), ('training', 'VBG'), ('applying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('convolution', 'NN'), ('operation', 'NN'), ('facial', 'JJ'), ('training', 'NN'), ('ii', 'NN'), ('process', 'NN'), ('inputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('feature', 'NN'), ('vector', 'NN'), ('training', 'NN'), ('features', 'NNS'), ('training', 'VBG'), ('embedded', 'JJ'), ('aggregation', 'NN'), ('module', 'NN'), ('including', 'VBG'), ('least', 'JJS'), ('two', 'CD'), ('attention', 'NN'), ('blocks', 'NNS'), ('neural', 'JJ'), ('aggregation', 'NN'), ('network', 'NN'), ('thereby', 'RB'), ('allow', 'JJ'), ('aggregation', 'NN'), ('module', 'NN'), ('generate', 'VBP'), ('quality', 'NN'), ('scores', 'NNS'), ('training', 'VBG'), ('features', 'NNS'), ('training', 'VBG'), ('aggregation', 'NN'), ('features', 'NNS'), ('training', 'VBG'), ('using', 'VBG'), ('one', 'CD'), ('attention', 'NN'), ('parameters', 'NNS'), ('learned', 'VBD'), ('previous', 'JJ'), ('iteration', 'NN'), ('iii', 'NN'), ('process', 'NN'), ('outputting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('optimal', 'JJ'), ('feature', 'NN'), ('training', 'NN'), ('weighted', 'JJ'), ('summation', 'NN'), ('features', 'NNS'), ('training', 'VBG'), ('using', 'VBG'), ('quality', 'NN'), ('scores', 'NNS'), ('training', 'VBG'), ('iv', 'JJ'), ('process', 'NN'), ('updating', 'VBG'), ('attention', 'NN'), ('parameters', 'NNS'), ('learned', 'VBD'), ('previous', 'JJ'), ('iteration', 'NN'), ('least', 'JJS'), ('two', 'CD'), ('attention', 'NN'), ('blocks', 'NNS'), ('one', 'CD'), ('losses', 'NNS'), ('minimized', 'VBN'), ('outputted', 'JJ'), ('loss', 'NN'), ('layer', 'NN'), ('referring', 'VBG'), ('optimal', 'JJ'), ('feature', 'NN'), ('training', 'VBG'), ('corresponding', 'VBG'), ('ground', 'NN'), ('truth', 'NN'), ('object', 'IN'), ('data', 'NNS'), ('processing', 'VBG'), ('system', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('configured', 'VBN'), ('execute', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('implementation', 'NN'), ('algorithms', 'NN'), ('stored', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('algorithm', 'JJ'), ('feature', 'NN'), ('density', 'NN'), ('selection', 'NN'), ('criteria', 'NNS'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('code', 'NN'), ('executed', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('data', 'NN'), ('preprocessing', 'NN'), ('code', 'NN'), ('comprising', 'VBG'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'RB'), ('configured', 'VBD'), ('obtain', 'VB'), ('digital', 'JJ'), ('representation', 'NN'), ('scene', 'NN'), ('scene', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('textual', 'JJ'), ('media', 'NNS'), ('generate', 'NN'), ('set', 'VBN'), ('invariant', 'JJ'), ('features', 'NNS'), ('applying', 'VBG'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'IN'), ('digital', 'JJ'), ('representation', 'NN'), ('cluster', 'NN'), ('set', 'VBN'), ('invariant', 'JJ'), ('features', 'NNS'), ('regions', 'NNS'), ('interest', 'NN'), ('digital', 'JJ'), ('representation', 'NN'), ('scene', 'NN'), ('region', 'NN'), ('interest', 'NN'), ('region', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('classify', 'VB'), ('region', 'NN'), ('classifier', 'JJR'), ('code', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('regions', 'NNS'), ('interest', 'NN'), ('according', 'VBG'), ('object', 'JJ'), ('type', 'NN'), ('function', 'NN'), ('attributes', 'VBZ'), ('derived', 'JJ'), ('region', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('digital', 'JJ'), ('representation', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('classified', 'JJ'), ('regions', 'NNS'), ('interest', 'NN'), ('corresponds', 'NNS'), ('text', 'JJ'), ('use', 'NN'), ('classification', 'NN'), ('result', 'NN'), ('corresponding', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('regions', 'NNS'), ('interest', 'NN'), ('classify', 'NN'), ('another', 'DT'), ('regions', 'NNS'), ('interest', 'NN'), ('according', 'VBG'), ('object', 'JJ'), ('type', 'NN'), ('wherein', 'NN'), ('another', 'DT'), ('regions', 'NNS'), ('interest', 'NN'), ('corresponds', 'VBZ'), ('region', 'NN'), ('interest', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('preprocessing', 'VBG'), ('code', 'NN'), ('based', 'VBN'), ('feature', 'NN'), ('density', 'NN'), ('selection', 'NN'), ('criteria', 'NNS'), ('determines', 'VBZ'), ('ocr', 'JJ'), ('algorithm', 'NN'), ('applicable', 'JJ'), ('text', 'NN'), ('algorithms', 'NN'), ('applicable', 'JJ'), ('aspects', 'NNS'), ('photographs', 'VBP'), ('logos', 'JJ'), ('system', 'NN'), ('wherein', 'NN'), ('creates', 'VBZ'), ('profile', 'IN'), ('camera-equipped', 'JJ'), ('smartphone', 'NN'), ('includes', 'VBZ'), ('information', 'NN'), ('visually', 'RB'), ('impaired', 'JJ'), ('causes', 'NNS'), ('prioritized', 'JJ'), ('execution', 'NN'), ('ocr', 'IN'), ('algorithm', 'JJ'), ('text', 'JJ'), ('reader', 'NN'), ('program', 'NN'), ('begins', 'VBZ'), ('reading', 'VBG'), ('text', 'JJ'), ('quickly', 'RB'), ('possible', 'JJ'), ('system', 'NN'), ('comprising', 'VBG'), ('audio', 'JJ'), ('tactile', 'NN'), ('feedback', 'NN'), ('mechanism', 'NN'), ('helps', 'VBZ'), ('position', 'NN'), ('smart', 'JJ'), ('phone', 'NN'), ('relative', 'JJ'), ('text', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('``', '``'), ('hold', 'VB'), ('still', 'RB'), (\"''\", \"''\"), ('audio', 'JJ'), ('feedback', 'NN'), ('signal', 'JJ'), ('sent', 'VBD'), ('text', 'NN'), ('center', 'NN'), ('captured', 'VBN'), ('scene', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('digital', 'JJ'), ('representation', 'NN'), ('comprises', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('following', 'VBG'), ('types', 'NNS'), ('digital', 'JJ'), ('data', 'NNS'), ('data', 'NNS'), ('video', 'NN'), ('data', 'NNS'), ('audio', 'RB'), ('data', 'NNS'), ('system', 'NN'), ('wherein', 'VBP'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('feature', 'NN'), ('identification', 'NN'), ('algorithms', 'IN'), ('fast', 'JJ'), ('sift', 'NN'), ('freak', 'NN'), ('brisk', 'JJ'), ('harris', 'NN'), ('daisy', 'NN'), ('mser', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('edge', 'NN'), ('detection', 'NN'), ('algorithm', 'NN'), ('corner', 'NN'), ('detection', 'NN'), ('algorithm', 'JJ'), ('saliency', 'NN'), ('map', 'NN'), ('algorithm', 'NN'), ('curve', 'NN'), ('detection', 'NN'), ('algorithm', 'IN'), ('texton', 'NN'), ('identification', 'NN'), ('algorithm', 'IN'), ('wavelets', 'NNS'), ('algorithm', 'JJ'), ('system', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('region', 'NN'), ('interest', 'NN'), ('represents', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('physical', 'JJ'), ('object', 'NN'), ('scene', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('region', 'NN'), ('interest', 'NN'), ('represents', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('textual', 'JJ'), ('media', 'NNS'), ('scene', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('region', 'NN'), ('interest', 'NN'), ('represents', 'VBZ'), ('document', 'JJ'), ('textual', 'JJ'), ('media', 'NNS'), ('system', 'NN'), ('wherein', 'JJ'), ('region', 'NN'), ('interest', 'NN'), ('represents', 'VBZ'), ('financial', 'JJ'), ('document', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('region', 'NN'), ('interest', 'NN'), ('represents', 'VBZ'), ('structured', 'VBN'), ('document', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('implementation', 'NN'), ('algorithms', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('template', 'NN'), ('driven', 'VBN'), ('algorithm', 'JJ'), ('face', 'NN'), ('algorithm', 'JJ'), ('optical', 'JJ'), ('character', 'NN'), ('algorithm', 'NN'), ('speech', 'NN'), ('algorithm', 'NN'), ('object', 'VBP'), ('algorithm', 'NN'), ('system', 'NN'), ('wherein', 'WRB'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('code', 'NN'), ('configured', 'VBD'), ('assign', 'JJ'), ('region', 'NN'), ('interest', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('algorithm', 'NN'), ('function', 'NN'), ('scene', 'NN'), ('context', 'NN'), ('derived', 'VBD'), ('digital', 'JJ'), ('representation', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('scene', 'NN'), ('context', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('types', 'NNS'), ('data', 'NNS'), ('location', 'NN'), ('position', 'NN'), ('time', 'NN'), ('identity', 'NN'), ('news', 'NN'), ('event', 'NN'), ('medical', 'JJ'), ('event', 'NN'), ('promotion', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('mobile', 'JJ'), ('device', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('implementation', 'NN'), ('algorithms', 'NN'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('code', 'NN'), ('system', 'NN'), ('wherein', 'VBP'), ('mobile', 'JJ'), ('device', 'NN'), ('comprises', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('following', 'VBG'), ('smart', 'JJ'), ('phone', 'NN'), ('tablet', 'NN'), ('wearable', 'JJ'), ('glass', 'NN'), ('toy', 'NN'), ('vehicle', 'NN'), ('computer', 'NN'), ('phablet', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('network-accessible', 'JJ'), ('server', 'NN'), ('device', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('implementation', 'NN'), ('algorithms', 'NN'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('code', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('object', 'JJ'), ('type', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('face', 'NN'), ('animal', 'JJ'), ('vehicle', 'NN'), ('document', 'NN'), ('plant', 'NN'), ('building', 'NN'), ('appliance', 'NN'), ('clothing', 'NN'), ('body', 'NN'), ('part', 'NN'), ('toy', 'NN'), ('object', 'VBP'), ('data', 'NNS'), ('processing', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('configured', 'VBN'), ('execute', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('implementation', 'NN'), ('algorithms', 'NN'), ('stored', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('algorithm', 'JJ'), ('feature', 'NN'), ('density', 'NN'), ('selection', 'NN'), ('criteria', 'NNS'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('code', 'NN'), ('executed', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('data', 'NN'), ('preprocessing', 'NN'), ('code', 'NN'), ('comprising', 'VBG'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'RB'), ('configured', 'VBD'), ('obtain', 'VB'), ('digital', 'JJ'), ('representation', 'NN'), ('scene', 'NN'), ('scene', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('textual', 'JJ'), ('media', 'NNS'), ('generate', 'NN'), ('set', 'VBN'), ('invariant', 'JJ'), ('features', 'NNS'), ('applying', 'VBG'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'IN'), ('digital', 'JJ'), ('representation', 'NN'), ('cluster', 'NN'), ('set', 'VBN'), ('invariant', 'JJ'), ('features', 'NNS'), ('regions', 'NNS'), ('interest', 'NN'), ('digital', 'JJ'), ('representation', 'NN'), ('scene', 'NN'), ('region', 'NN'), ('interest', 'NN'), ('region', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('classify', 'VB'), ('region', 'NN'), ('classifier', 'JJR'), ('code', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('regions', 'NNS'), ('interest', 'NN'), ('according', 'VBG'), ('object', 'JJ'), ('type', 'NN'), ('function', 'NN'), ('attributes', 'VBZ'), ('derived', 'JJ'), ('region', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('digital', 'JJ'), ('representation', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('classified', 'JJ'), ('regions', 'NNS'), ('interest', 'NN'), ('corresponds', 'NNS'), ('text', 'JJ'), ('use', 'NN'), ('classification', 'NN'), ('result', 'NN'), ('corresponding', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('regions', 'NNS'), ('interest', 'NN'), ('classify', 'NN'), ('another', 'DT'), ('regions', 'NNS'), ('interest', 'NN'), ('according', 'VBG'), ('object', 'JJ'), ('type', 'NN'), ('wherein', 'NN'), ('another', 'DT'), ('regions', 'NNS'), ('interest', 'NN'), ('corresponds', 'VBZ'), ('region', 'NN'), ('interest', 'NN'), ('assign', 'NN'), ('region', 'NN'), ('interest', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('algorithm', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('implementation', 'NN'), ('diverse', 'NN'), ('algorithms', 'NN'), ('function', 'NN'), ('region', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('region', 'NN'), ('interest', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('selection', 'NN'), ('criteria', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('implementation', 'NN'), ('diverse', 'NN'), ('algorithms', 'JJ'), ('configure', 'NN'), ('assigned', 'VBD'), ('algorithms', 'JJ'), ('process', 'NN'), ('respective', 'JJ'), ('regions', 'NNS'), ('interest', 'NN'), ('wherein', 'NN'), ('preprocessing', 'VBG'), ('code', 'NN'), ('based', 'VBN'), ('feature', 'NN'), ('density', 'NN'), ('selection', 'NN'), ('criteria', 'NNS'), ('determines', 'VBZ'), ('ocr', 'JJ'), ('algorithm', 'NN'), ('applicable', 'JJ'), ('text', 'NN'), ('algorithms', 'NN'), ('applicable', 'JJ'), ('aspects', 'NNS'), ('photographs', 'VBP'), ('logos', 'JJ'), ('device', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('configured', 'VBN'), ('execute', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('implementation', 'NN'), ('algorithms', 'NN'), ('stored', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('algorithm', 'JJ'), ('feature', 'NN'), ('density', 'NN'), ('selection', 'NN'), ('criteria', 'NNS'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('code', 'NN'), ('executed', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('data', 'NN'), ('preprocessing', 'NN'), ('code', 'NN'), ('comprising', 'VBG'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'RB'), ('configured', 'VBD'), ('obtain', 'VB'), ('digital', 'JJ'), ('representation', 'NN'), ('scene', 'NN'), ('scene', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('textual', 'JJ'), ('media', 'NNS'), ('generate', 'NN'), ('set', 'VBN'), ('invariant', 'JJ'), ('features', 'NNS'), ('applying', 'VBG'), ('invariant', 'JJ'), ('feature', 'NN'), ('identification', 'NN'), ('algorithm', 'IN'), ('digital', 'JJ'), ('representation', 'NN'), ('cluster', 'NN'), ('set', 'VBN'), ('invariant', 'JJ'), ('features', 'NNS'), ('regions', 'NNS'), ('interest', 'NN'), ('digital', 'JJ'), ('representation', 'NN'), ('scene', 'NN'), ('region', 'NN'), ('interest', 'NN'), ('region', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('classify', 'VB'), ('region', 'NN'), ('classifier', 'JJR'), ('code', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('regions', 'NNS'), ('interest', 'NN'), ('according', 'VBG'), ('object', 'JJ'), ('type', 'NN'), ('function', 'NN'), ('attributes', 'VBZ'), ('derived', 'JJ'), ('region', 'NN'), ('feature', 'NN'), ('density', 'NN'), ('digital', 'JJ'), ('representation', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('classified', 'JJ'), ('regions', 'NNS'), ('interest', 'NN'), ('corresponds', 'NNS'), ('text', 'JJ'), ('use', 'NN'), ('classification', 'NN'), ('result', 'NN'), ('corresponding', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('regions', 'NNS'), ('interest', 'NN'), ('classify', 'NN'), ('another', 'DT'), ('regions', 'NNS'), ('interest', 'NN'), ('according', 'VBG'), ('object', 'JJ'), ('type', 'NN'), ('wherein', 'NN'), ('another', 'DT'), ('regions', 'NNS'), ('interest', 'NN'), ('corresponds', 'VBZ'), ('region', 'NN'), ('interest', 'NN'), ('mobile', 'IN'), ('terminal', 'JJ'), ('comprising', 'VBG'), ('front', 'JJ'), ('camera', 'NN'), ('configured', 'VBD'), ('obtain', 'VB'), ('two-dimensional', 'JJ'), ('face', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('tilted', 'VBD'), ('certain', 'JJ'), ('angle', 'NN'), ('disposed', 'VBD'), ('adjacent', 'JJ'), ('front', 'JJ'), ('camera', 'NN'), ('obtain', 'VB'), ('metadata', 'JJ'), ('face', 'NN'), ('controller', 'NN'), ('obtaining', 'VBG'), ('distance', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('front', 'NN'), ('camera', 'NN'), ('distance', 'NN'), ('enabling', 'VBG'), ('area', 'NN'), ('overlap', 'IN'), ('region', 'NN'), ('first', 'JJ'), ('region', 'NN'), ('representing', 'VBG'), ('range', 'NN'), ('photographable', 'JJ'), ('front', 'NN'), ('camera', 'NN'), ('overlaps', 'VBZ'), ('second', 'JJ'), ('region', 'NN'), ('representing', 'VBG'), ('range', 'NN'), ('photographable', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('maximum', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('obtain', 'VB'), ('distance', 'NN'), ('enabling', 'VBG'), ('area', 'NN'), ('overlap', 'JJ'), ('region', 'NN'), ('maximum', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('front', 'NN'), ('camera', 'NN'), ('varying', 'VBG'), ('tilting', 'VBG'), ('angle', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('set', 'VBN'), ('distance', 'NN'), ('enabling', 'VBG'), ('area', 'NN'), ('overlap', 'JJ'), ('region', 'NN'), ('maximum', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('front', 'NN'), ('camera', 'NN'), ('tilting', 'VBG'), ('angle', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('optimal', 'JJ'), ('disposition', 'NN'), ('location', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('set', 'VBN'), ('disposition', 'NN'), ('location', 'NN'), ('front', 'NN'), ('camera', 'NN'), ('original', 'JJ'), ('point', 'NN'), ('calculates', 'NNS'), ('coordinates', 'NNS'), ('first', 'RB'), ('triangle', 'VBP'), ('representing', 'VBG'), ('first', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('field', 'NN'), ('view', 'NN'), ('front', 'JJ'), ('camera', 'NN'), ('maximum', 'JJ'), ('photographing', 'VBG'), ('distance', 'NN'), ('front', 'NN'), ('camera', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('calculate', 'JJ'), ('coordinates', 'NNS'), ('second', 'JJ'), ('triangle', 'JJ'), ('representing', 'VBG'), ('second', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('field', 'NN'), ('view', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('maximum', 'NN'), ('photographing', 'VBG'), ('distance', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('distance', 'NN'), ('front', 'JJ'), ('camera', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('tilting', 'VBG'), ('angle', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('tilted', 'VBD'), ('controller', 'NN'), ('configured', 'JJ'), ('calculate', 'NN'), ('coordinates', 'NNS'), ('third', 'JJ'), ('triangle', 'JJ'), ('representing', 'VBG'), ('third', 'JJ'), ('region', 'NN'), ('photographable', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('rotation-convert', 'JJ'), ('coordinates', 'NNS'), ('third', 'JJ'), ('triangle', 'NNS'), ('based', 'VBN'), ('tilting', 'VBG'), ('angle', 'JJ'), ('glance', 'NN'), ('sensor', 'NN'), ('calculate', 'NN'), ('coordinates', 'NNS'), ('second', 'JJ'), ('triangle', 'VBP'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('calculate', 'NN'), ('coordinates', 'NNS'), ('overlap', 'VBP'), ('region', 'NN'), ('based', 'VBN'), ('coordinates', 'NNS'), ('first', 'JJ'), ('triangle', 'JJ'), ('coordinates', 'NNS'), ('second', 'JJ'), ('triangle', 'NN'), ('calculates', 'NNS'), ('area', 'NN'), ('overlap', 'VBP'), ('region', 'NN'), ('based', 'VBN'), ('coordinates', 'NNS'), ('overlap', 'JJ'), ('region', 'NN'), ('mobile', 'IN'), ('terminal', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('generate', 'JJ'), ('three-dimensional', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('obtained', 'VBN'), ('front', 'JJ'), ('camera', 'NN'), ('metadata', 'NN'), ('obtained', 'VBD'), ('glance', 'NN'), ('sensor', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('metadata', 'NN'), ('comprises', 'VBZ'), ('one', 'CD'), ('angle', 'NN'), ('face', 'NN'), ('size', 'NN'), ('face', 'NN'), ('location', 'NN'), ('face', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('angle', 'JJ'), ('face', 'NN'), ('comprises', 'NNS'), ('angle', 'VBP'), ('face', 'NN'), ('rotated', 'VBD'), ('one', 'CD'), ('pitch', 'NN'), ('axis', 'NN'), ('roll', 'NN'), ('axis', 'NN'), ('yaw', 'NN'), ('axis', 'VBP'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('comprising', 'VBG'), ('memory', 'NN'), ('storing', 'VBG'), ('generated', 'VBD'), ('face', 'NN'), ('information', 'NN'), ('wherein', 'WRB'), ('controller', 'NN'), ('configured', 'VBD'), ('performs', 'NNS'), ('authentication', 'NN'), ('process', 'NN'), ('comparing', 'VBG'), ('stored', 'VBD'), ('face', 'NN'), ('information', 'NN'), ('face', 'NN'), ('information', 'NN'), ('obtained', 'VBN'), ('authentication', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('controlled', 'VBD'), ('permanently', 'RB'), ('activated', 'VBN'), ('low', 'JJ'), ('power', 'NN'), ('obtain', 'VB'), ('front', 'JJ'), ('metadata', 'NNS'), ('front', 'VBP'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('front', 'JJ'), ('camera', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('disposed', 'VBD'), ('line', 'NN'), ('upper', 'JJ'), ('end', 'NN'), ('mobile', 'JJ'), ('terminal', 'NN'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('wherein', 'NN'), ('glance', 'NN'), ('sensor', 'NN'), ('tilted', 'VBD'), ('one', 'CD'), ('direction', 'NN'), ('direction', 'NN'), ('direction', 'NN'), ('left', 'VBD'), ('direction', 'NN'), ('right', 'JJ'), ('direction', 'NN'), ('mobile', 'IN'), ('terminal', 'JJ'), ('wherein', 'NN'), ('metadata', 'NNS'), ('data', 'NNS'), ('changed', 'VBD'), ('mobile', 'JJ'), ('terminal', 'NN'), ('tilted', 'VBD'), ('external', 'JJ'), ('physical', 'JJ'), ('force', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('smart', 'JJ'), ('television', 'NN'), ('tv', 'NN'), ('indication', 'NN'), ('upcoming', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('wherein', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('based', 'VBN'), ('profile', 'IN'), ('identifying', 'VBG'), ('one', 'CD'), ('devices', 'NNS'), ('communication', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('including', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('microphone', 'NN'), ('camera', 'NN'), ('instructing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('identified', 'JJ'), ('device', 'NN'), ('detect', 'NN'), ('audio', 'NN'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('microphone', 'NN'), ('detect', 'JJ'), ('visual', 'JJ'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('camera', 'NN'), ('selecting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('device', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('based', 'VBN'), ('detected', 'JJ'), ('audio', 'JJ'), ('signal', 'NN'), ('detected', 'VBD'), ('visual', 'JJ'), ('signal', 'NN'), ('providing', 'VBG'), ('instructions', 'NNS'), ('selected', 'VBN'), ('device', 'NN'), ('output', 'NN'), ('notification', 'NN'), ('related', 'VBN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('wherein', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('one', 'CD'), ('live', 'JJ'), ('television', 'NN'), ('program', 'NN'), ('recorded', 'VBN'), ('television', 'NN'), ('program', 'NN'), ('broadcast', 'NN'), ('television', 'NN'), ('program', 'NN'), ('application-provided', 'JJ'), ('program', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('detected', 'JJ'), ('audio', 'JJ'), ('signal', 'NN'), ('includes', 'VBZ'), ('voice', 'NN'), ('comprising', 'VBG'), ('determining', 'VBG'), ('distance', 'NN'), ('recognized', 'VBN'), ('voice', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('distance', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('detected', 'VBN'), ('visual', 'JJ'), ('signals', 'NNS'), ('includes', 'VBZ'), ('face', 'NN'), ('wherein', 'NN'), ('face', 'NN'), ('includes', 'VBZ'), ('face', 'NN'), ('technique', 'NN'), ('comprising', 'VBG'), ('presenting', 'VBG'), ('smart', 'JJ'), ('tv', 'NN'), ('upcoming', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('favorite', 'JJ'), ('channel', 'NNS'), ('list', 'NN'), ('comprising', 'VBG'), ('obtaining', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('wherein', 'RB'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('historical', 'JJ'), ('time', 'NN'), ('historical', 'JJ'), ('date', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('obtaining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('current', 'JJ'), ('time', 'NN'), ('current', 'JJ'), ('date', 'NN'), ('processing', 'NN'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('determine', 'NN'), ('probability', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('current', 'JJ'), ('time', 'NN'), ('current', 'JJ'), ('date', 'NN'), ('presenting', 'NN'), ('favorite', 'JJ'), ('channel', 'NNS'), ('list', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('probability', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('wherein', 'JJ'), ('processing', 'NN'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('includes', 'VBZ'), ('employing', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('model', 'NN'), ('wherein', 'NN'), ('employing', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('model', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('duration', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('historical', 'JJ'), ('time', 'NN'), ('historical', 'JJ'), ('date', 'NN'), ('setting', 'VBG'), ('threshold', 'JJ'), ('time', 'NN'), ('duration', 'NN'), ('comparing', 'VBG'), ('determined', 'VBN'), ('duration', 'NN'), ('threshold', 'JJ'), ('time', 'NN'), ('duration', 'NN'), ('filtering', 'VBG'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('threshold', 'JJ'), ('time', 'NN'), ('duration', 'NN'), ('smart', 'JJ'), ('television', 'NN'), ('tv', 'NN'), ('comprising', 'VBG'), ('network', 'NN'), ('interface', 'JJ'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('communication', 'NN'), ('network', 'NN'), ('interface', 'JJ'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('capable', 'JJ'), ('executing', 'VBG'), ('-executable', 'JJ'), ('program', 'NN'), ('code', 'NN'), ('stored', 'VBD'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('cause', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('receive', 'NN'), ('indication', 'NN'), ('upcoming', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('wherein', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('based', 'VBN'), ('profile', 'JJ'), ('identify', 'VB'), ('one', 'CD'), ('devices', 'NNS'), ('communication', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('including', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('microphone', 'NN'), ('camera', 'NN'), ('instruct', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('identified', 'JJ'), ('device', 'NN'), ('detect', 'NN'), ('audio', 'NN'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('microphone', 'NN'), ('detect', 'JJ'), ('visual', 'JJ'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('camera', 'NN'), ('select', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('device', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('based', 'VBN'), ('detected', 'JJ'), ('audio', 'JJ'), ('signal', 'NN'), ('detected', 'VBD'), ('visual', 'JJ'), ('signal', 'NN'), ('provide', 'NN'), ('instructions', 'NNS'), ('selected', 'VBN'), ('device', 'NN'), ('output', 'NN'), ('notification', 'NN'), ('related', 'VBN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('detected', 'JJ'), ('audio', 'JJ'), ('signal', 'NN'), ('includes', 'VBZ'), ('voice', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('capable', 'JJ'), ('executing', 'VBG'), ('-executable', 'JJ'), ('program', 'NN'), ('code', 'NN'), ('determine', 'NN'), ('distance', 'NN'), ('recognized', 'VBN'), ('voice', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('distance', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('detected', 'VBN'), ('visual', 'JJ'), ('signals', 'NNS'), ('includes', 'VBZ'), ('detecting', 'VBG'), ('presence', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('detecting', 'VBG'), ('presence', 'NN'), ('includes', 'VBZ'), ('employing', 'VBG'), ('one', 'CD'), ('camera', 'NN'), ('microphone', 'NN'), ('fingerprint', 'NN'), ('sensor', 'NN'), ('associated', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('smart', 'JJ'), ('tv', 'NN'), ('mobile', 'NN'), ('device', 'NN'), ('smartphone', 'NN'), ('laptop', 'JJ'), ('computer', 'NN'), ('tablet', 'NN'), ('device', 'NN'), ('wearable', 'JJ'), ('device', 'NN'), ('internet', 'NN'), ('things', 'NNS'), ('iot', 'JJ'), ('device', 'JJ'), ('internet', 'NN'), ('everything', 'NN'), ('ioe', 'NN'), ('device', 'NN'), ('iot', 'NN'), ('hub', 'NN'), ('ioe', 'NN'), ('hub', 'NN'), ('smart', 'JJ'), ('television', 'NN'), ('tv', 'NN'), ('comprising', 'NN'), ('means', 'VBZ'), ('receiving', 'VBG'), ('indication', 'NN'), ('upcoming', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('wherein', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('based', 'VBN'), ('profile', 'NN'), ('means', 'VBZ'), ('identifying', 'VBG'), ('one', 'CD'), ('devices', 'NNS'), ('communication', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('including', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('microphone', 'NN'), ('camera', 'NN'), ('means', 'VBZ'), ('instructing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('identified', 'JJ'), ('device', 'NN'), ('detect', 'NN'), ('audio', 'NN'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('microphone', 'NN'), ('detect', 'JJ'), ('visual', 'JJ'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('camera', 'NN'), ('means', 'VBZ'), ('selecting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('device', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('based', 'VBN'), ('detected', 'JJ'), ('audio', 'JJ'), ('signal', 'NN'), ('detected', 'VBD'), ('visual', 'JJ'), ('signal', 'NN'), ('means', 'NNS'), ('providing', 'VBG'), ('instructions', 'NNS'), ('selected', 'VBN'), ('device', 'NN'), ('output', 'NN'), ('notification', 'NN'), ('related', 'VBN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'VBD'), ('one', 'CD'), ('devices', 'NNS'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('mobile', 'JJ'), ('device', 'NN'), ('smartphone', 'NN'), ('laptop', 'JJ'), ('computer', 'NN'), ('tablet', 'NN'), ('device', 'NN'), ('wearable', 'JJ'), ('device', 'NN'), ('internet', 'NN'), ('things', 'NNS'), ('iot', 'JJ'), ('device', 'JJ'), ('internet', 'NN'), ('everything', 'NN'), ('ioe', 'NN'), ('device', 'NN'), ('iot', 'NN'), ('hub', 'NN'), ('ioe', 'NN'), ('hub', 'NN'), ('another', 'DT'), ('smart', 'JJ'), ('tv', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('one', 'CD'), ('live', 'JJ'), ('television', 'NN'), ('program', 'NN'), ('recorded', 'VBN'), ('television', 'NN'), ('program', 'NN'), ('broadcast', 'NN'), ('television', 'NN'), ('program', 'NN'), ('application-provided', 'JJ'), ('program', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('notification', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('push', 'NN'), ('message', 'NN'), ('sms', 'JJ'), ('message', 'NN'), ('waysms', 'JJ'), ('message', 'NN'), ('audio', 'NN'), ('alert', 'NN'), ('audio', 'JJ'), ('message', 'NN'), ('email', 'JJ'), ('message', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('comprising', 'VBG'), ('presenting', 'VBG'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('favorite', 'JJ'), ('channel', 'NNS'), ('list', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('comprising', 'NN'), ('means', 'VBZ'), ('obtaining', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('wherein', 'RB'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('historical', 'JJ'), ('time', 'NN'), ('historical', 'JJ'), ('date', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('smart', 'JJ'), ('tv', 'NN'), ('means', 'NNS'), ('obtaining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('current', 'JJ'), ('time', 'NN'), ('current', 'JJ'), ('date', 'NN'), ('means', 'VBZ'), ('processing', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('determine', 'NN'), ('probability', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('smart', 'JJ'), ('tv', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('current', 'JJ'), ('time', 'NN'), ('current', 'JJ'), ('date', 'NN'), ('means', 'NNS'), ('presenting', 'VBG'), ('favorite', 'JJ'), ('channel', 'NNS'), ('list', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('probability', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('means', 'VBZ'), ('processing', 'VBG'), ('media', 'NNS'), ('programming', 'VBG'), ('viewing', 'VBG'), ('data', 'NNS'), ('includes', 'VBZ'), ('employing', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('model', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('employing', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('model', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('duration', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('smart', 'JJ'), ('tv', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('historical', 'JJ'), ('time', 'NN'), ('historical', 'JJ'), ('date', 'NN'), ('setting', 'VBG'), ('threshold', 'JJ'), ('time', 'NN'), ('duration', 'NN'), ('comparing', 'VBG'), ('determined', 'VBN'), ('duration', 'NN'), ('threshold', 'JJ'), ('time', 'NN'), ('duration', 'NN'), ('filtering', 'VBG'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('viewed', 'VBD'), ('threshold', 'JJ'), ('time', 'NN'), ('duration', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('comprising', 'NN'), ('means', 'VBZ'), ('adjusting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('volume', 'NN'), ('brightness', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('wherein', 'NN'), ('adjusting', 'VBG'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('historical', 'JJ'), ('time', 'NN'), ('historical', 'JJ'), ('date', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('comprising', 'NN'), ('means', 'VBZ'), ('restricting', 'VBG'), ('access', 'NN'), ('one', 'CD'), ('media', 'NNS'), ('programs', 'NNS'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('-executable', 'JJ'), ('program', 'NN'), ('code', 'NN'), ('configured', 'VBD'), ('cause', 'NN'), ('smart', 'JJ'), ('television', 'NN'), ('tv', 'NN'), ('receive', 'VBP'), ('indication', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('wherein', 'NN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('based', 'VBN'), ('profile', 'JJ'), ('identify', 'VB'), ('one', 'CD'), ('devices', 'NNS'), ('communication', 'NN'), ('smart', 'JJ'), ('tv', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('including', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('microphone', 'NN'), ('camera', 'NN'), ('instruct', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('identified', 'JJ'), ('device', 'NN'), ('detect', 'NN'), ('audio', 'NN'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('microphone', 'NN'), ('detect', 'JJ'), ('visual', 'JJ'), ('signals', 'NNS'), ('using', 'VBG'), ('respective', 'JJ'), ('camera', 'NN'), ('select', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('device', 'NN'), ('one', 'CD'), ('devices', 'NNS'), ('based', 'VBN'), ('detected', 'JJ'), ('audio', 'JJ'), ('signal', 'NN'), ('detected', 'VBD'), ('visual', 'JJ'), ('signal', 'NN'), ('provide', 'NN'), ('instructions', 'NNS'), ('selected', 'VBN'), ('device', 'NN'), ('output', 'NN'), ('notification', 'NN'), ('related', 'VBN'), ('upcoming', 'JJ'), ('media', 'NNS'), ('programming', 'VBG'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('detected', 'JJ'), ('audio', 'JJ'), ('signal', 'NN'), ('includes', 'VBZ'), ('voice', 'JJ'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('capable', 'JJ'), ('executing', 'VBG'), ('-executable', 'JJ'), ('program', 'NN'), ('code', 'NN'), ('determine', 'NN'), ('distance', 'NN'), ('recognized', 'VBN'), ('voice', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('distance', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('first', 'JJ'), ('device', 'NN'), ('based', 'VBN'), ('detected', 'VBN'), ('visual', 'JJ'), ('signals', 'NNS'), ('includes', 'VBZ'), ('face', 'VBP'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('face', 'NN'), ('includes', 'VBZ'), ('face', 'NN'), ('technique', 'NN'), ('camera', 'NN'), ('comprising', 'VBG'), ('sensor', 'JJ'), ('array', 'NN'), ('including', 'VBG'), ('sensors', 'NNS'), ('infrared', 'VBD'), ('ir', 'JJ'), ('illuminator', 'NN'), ('configured', 'VBD'), ('emit', 'RB'), ('active', 'JJ'), ('ir', 'NN'), ('light', 'JJ'), ('ir', 'JJ'), ('light', 'JJ'), ('sub-band', 'JJ'), ('spectral', 'JJ'), ('illuminators', 'NNS'), ('spectral', 'JJ'), ('illuminator', 'NN'), ('configured', 'VBD'), ('emit', 'RB'), ('active', 'JJ'), ('spectral', 'JJ'), ('light', 'NN'), ('different', 'JJ'), ('spectral', 'JJ'), ('light', 'NN'), ('sub-band', 'NN'), ('depth', 'NN'), ('controller', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('determine', 'JJ'), ('depth', 'NN'), ('value', 'NN'), ('sensors', 'NNS'), ('based', 'VBN'), ('active', 'JJ'), ('ir', 'NN'), ('light', 'JJ'), ('spectral', 'JJ'), ('controller', 'NN'), ('machine', 'NN'), ('configured', 'VBD'), ('sensors', 'NNS'), ('determine', 'VBP'), ('spectral', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('light', 'JJ'), ('sub-band', 'JJ'), ('spectral', 'JJ'), ('illuminators', 'NNS'), ('output', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('test', 'RBS'), ('depth+multi-spectral', 'JJ'), ('including', 'VBG'), ('pixels', 'NNS'), ('pixel', 'RB'), ('corresponding', 'VBG'), ('one', 'CD'), ('sensors', 'NNS'), ('sensor', 'VBP'), ('array', 'JJ'), ('including', 'VBG'), ('least', 'JJS'), ('depth', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('light', 'JJ'), ('sub-band', 'JJ'), ('spectral', 'JJ'), ('illuminators', 'NNS'), ('face', 'VBP'), ('machine', 'NN'), ('previously', 'RB'), ('trained', 'VBN'), ('set', 'VBN'), ('labeled', 'JJ'), ('training', 'NN'), ('depth+multi-spectral', 'JJ'), ('structure', 'NN'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('confidence', 'NN'), ('value', 'NN'), ('indicating', 'VBG'), ('likelihood', 'JJ'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('includes', 'VBZ'), ('face', 'NN'), ('camera', 'NN'), ('wherein', 'VBZ'), ('spectral', 'JJ'), ('value', 'NN'), ('calculated', 'VBD'), ('based', 'VBN'), ('depth', 'NN'), ('value', 'NN'), ('determined', 'VBD'), ('sensor', 'JJ'), ('corresponds', 'NNS'), ('pixel', 'VBP'), ('camera', 'NN'), ('wherein', 'NN'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('use', 'IN'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('determine', 'NN'), ('confidence', 'NN'), ('value', 'NN'), ('camera', 'NN'), ('wherein', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('includes', 'VBZ'), ('input', 'NN'), ('nodes', 'NNS'), ('wherein', 'VBP'), ('input', 'JJ'), ('node', 'RB'), ('configured', 'VBD'), ('receive', 'JJ'), ('pixel', 'NN'), ('value', 'NN'), ('array', 'NN'), ('corresponding', 'VBG'), ('different', 'JJ'), ('pixel', 'NN'), ('pixels', 'NNS'), ('test', 'VBP'), ('depth+multi-spectral', 'JJ'), ('wherein', 'NN'), ('pixel', 'VBZ'), ('value', 'NN'), ('array', 'NN'), ('includes', 'VBZ'), ('depth', 'JJ'), ('value', 'NN'), ('multi-spectral', 'JJ'), ('values', 'NNS'), ('pixel', 'VBP'), ('camera', 'NN'), ('wherein', 'NN'), ('multi-spectral', 'JJ'), ('values', 'NNS'), ('pixel', 'VBP'), ('include', 'VBP'), ('three', 'CD'), ('spectral', 'JJ'), ('values', 'NNS'), ('camera', 'VBP'), ('wherein', 'NN'), ('output', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('surface', 'NN'), ('normal', 'JJ'), ('pixel', 'JJ'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('wherein', 'NN'), ('pixel', 'VBZ'), ('value', 'NN'), ('array', 'NN'), ('includes', 'VBZ'), ('surface', 'NN'), ('normal', 'JJ'), ('camera', 'NN'), ('wherein', 'NN'), ('output', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('curvature', 'NN'), ('pixel', 'JJ'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('wherein', 'NN'), ('pixel', 'VBZ'), ('value', 'NN'), ('array', 'NN'), ('includes', 'VBZ'), ('curvature', 'NN'), ('camera', 'NN'), ('wherein', 'NN'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('use', 'NN'), ('models', 'NNS'), ('determine', 'VBP'), ('confidence', 'NN'), ('value', 'NN'), ('wherein', 'NN'), ('models', 'NNS'), ('includes', 'VBZ'), ('channel-specific', 'JJ'), ('models', 'NNS'), ('wherein', 'IN'), ('channel-specific', 'JJ'), ('model', 'NN'), ('configured', 'VBD'), ('process', 'JJ'), ('different', 'JJ'), ('pixel', 'NN'), ('parameter', 'NN'), ('pixels', 'NNS'), ('test', 'VBP'), ('depth+multi-spectral', 'JJ'), ('wherein', 'NN'), ('channel-specific', 'JJ'), ('model', 'NN'), ('includes', 'VBZ'), ('input', 'JJ'), ('nodes', 'NNS'), ('wherein', 'VBP'), ('channel-specific', 'JJ'), ('model', 'NN'), ('input', 'NN'), ('node', 'RB'), ('configured', 'VBD'), ('receive', 'JJ'), ('pixel', 'NN'), ('parameter', 'NN'), ('value', 'NN'), ('different', 'JJ'), ('pixel', 'NN'), ('pixels', 'NNS'), ('test', 'VBP'), ('depth+multi-spectral', 'JJ'), ('camera', 'NN'), ('wherein', 'NN'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('use', 'RB'), ('statistical', 'JJ'), ('model', 'NN'), ('determine', 'NN'), ('confidence', 'NN'), ('value', 'NN'), ('camera', 'NN'), ('wherein', 'VBZ'), ('statistical', 'JJ'), ('model', 'NN'), ('includes', 'VBZ'), ('nearest', 'JJS'), ('neighbor', 'NN'), ('algorithm', 'NN'), ('camera', 'NN'), ('wherein', 'VBZ'), ('statistical', 'JJ'), ('model', 'NN'), ('includes', 'VBZ'), ('support', 'NN'), ('vector', 'NN'), ('machine', 'NN'), ('camera', 'NN'), ('wherein', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('location', 'NN'), ('test', 'IN'), ('depth+multi-spectral', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('around', 'IN'), ('recognized', 'VBN'), ('face', 'NN'), ('camera', 'NN'), ('wherein', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('location', 'NN'), ('test', 'IN'), ('depth+multi-spectral', 'JJ'), ('identified', 'VBN'), ('two-dimensional', 'JJ'), ('facial', 'JJ'), ('feature', 'NN'), ('recognized', 'VBN'), ('face', 'NN'), ('camera', 'NN'), ('wherein', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('location', 'NN'), ('test', 'IN'), ('depth+multi-spectral', 'JJ'), ('identified', 'VBN'), ('three-dimensional', 'JJ'), ('facial', 'JJ'), ('feature', 'NN'), ('recognized', 'VBN'), ('face', 'NN'), ('camera', 'NN'), ('wherein', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('location', 'NN'), ('test', 'IN'), ('depth+multi-spectral', 'JJ'), ('identified', 'JJ'), ('spectral', 'JJ'), ('feature', 'NN'), ('recognized', 'VBN'), ('face', 'NN'), ('camera', 'NN'), ('wherein', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('pixel', 'JJ'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('confidence', 'NN'), ('value', 'NN'), ('indicating', 'VBG'), ('likelihood', 'NN'), ('pixel', 'NN'), ('included', 'VBD'), ('face', 'NN'), ('camera', 'NN'), ('wherein', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('identity', 'NN'), ('face', 'NN'), ('recognized', 'VBN'), ('test', 'IN'), ('depth+multi-spectral', 'JJ'), ('camera', 'NN'), ('wherein', 'NN'), ('sensors', 'NNS'), ('sensor', 'VBP'), ('array', 'JJ'), ('differential', 'JJ'), ('sensors', 'NNS'), ('wherein', 'VBP'), ('spectral', 'JJ'), ('value', 'NN'), ('determined', 'VBD'), ('based', 'VBN'), ('depth', 'NN'), ('value', 'NN'), ('differential', 'JJ'), ('measurement', 'JJ'), ('differential', 'NN'), ('sensor', 'NN'), ('camera', 'NN'), ('comprising', 'VBG'), ('sensor', 'JJ'), ('array', 'NN'), ('including', 'VBG'), ('sensors', 'NNS'), ('infrared', 'VBD'), ('ir', 'JJ'), ('illuminator', 'NN'), ('configured', 'VBD'), ('emit', 'RB'), ('active', 'JJ'), ('ir', 'NN'), ('light', 'JJ'), ('ir', 'JJ'), ('light', 'JJ'), ('sub-band', 'JJ'), ('spectral', 'JJ'), ('illuminators', 'NNS'), ('spectral', 'JJ'), ('illuminator', 'NN'), ('configured', 'VBD'), ('emit', 'RB'), ('active', 'JJ'), ('spectral', 'JJ'), ('light', 'NN'), ('different', 'JJ'), ('spectral', 'JJ'), ('light', 'NN'), ('sub-band', 'NN'), ('depth', 'NN'), ('controller', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('determine', 'JJ'), ('depth', 'NN'), ('value', 'NN'), ('sensors', 'NNS'), ('based', 'VBN'), ('active', 'JJ'), ('ir', 'NN'), ('light', 'JJ'), ('spectral', 'JJ'), ('controller', 'NN'), ('machine', 'NN'), ('configured', 'VBD'), ('sensors', 'NNS'), ('determine', 'VBP'), ('spectral', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('light', 'JJ'), ('sub-band', 'JJ'), ('spectral', 'JJ'), ('illuminators', 'NNS'), ('wherein', 'VBP'), ('spectral', 'JJ'), ('value', 'NN'), ('calculated', 'VBD'), ('based', 'VBN'), ('depth', 'NN'), ('value', 'NN'), ('determined', 'VBD'), ('sensor', 'JJ'), ('corresponds', 'NNS'), ('pixel', 'JJ'), ('output', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('test', 'RBS'), ('depth+multi-spectral', 'JJ'), ('including', 'VBG'), ('pixels', 'NNS'), ('pixel', 'RB'), ('corresponding', 'VBG'), ('one', 'CD'), ('sensors', 'NNS'), ('sensor', 'VBP'), ('array', 'JJ'), ('including', 'VBG'), ('least', 'JJS'), ('depth', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('value', 'NN'), ('spectral', 'JJ'), ('light', 'JJ'), ('sub-band', 'JJ'), ('spectral', 'JJ'), ('illuminators', 'NNS'), ('face', 'VBP'), ('machine', 'NN'), ('including', 'VBG'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('previously', 'RB'), ('trained', 'VBN'), ('set', 'VBN'), ('labeled', 'JJ'), ('training', 'NN'), ('depth+multi-spectral', 'JJ'), ('structure', 'NN'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('face', 'NN'), ('machine', 'NN'), ('configured', 'VBN'), ('output', 'NN'), ('confidence', 'NN'), ('value', 'NN'), ('indicating', 'VBG'), ('likelihood', 'JJ'), ('test', 'NN'), ('depth+multi-spectral', 'JJ'), ('includes', 'VBZ'), ('face', 'NN'), ('processing', 'VBG'), ('comprising', 'VBG'), ('acquiring', 'VBG'), ('photo', 'NN'), ('album', 'NN'), ('obtained', 'VBD'), ('face', 'NN'), ('clustering', 'VBG'), ('collecting', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('respective', 'JJ'), ('photo', 'NN'), ('album', 'NN'), ('acquiring', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('selecting', 'VBG'), ('cover', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('taking', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('setting', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('photo', 'NN'), ('album', 'JJ'), ('wherein', 'NN'), ('selecting', 'VBG'), ('cover', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('performing', 'VBG'), ('calculation', 'NN'), ('face', 'NN'), ('parameter', 'NN'), ('preset', 'VB'), ('way', 'NN'), ('obtain', 'VB'), ('cover', 'JJ'), ('score', 'NN'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'JJ'), ('comprises', 'VBZ'), ('acquiring', 'VBG'), ('source', 'NN'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('coming', 'VBG'), ('preset', 'VBN'), ('source', 'NN'), ('cover', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'JJ'), ('comprises', 'VBZ'), ('acquiring', 'VBG'), ('number', 'NN'), ('contained', 'VBN'), ('determining', 'VBG'), ('single-person', 'JJ'), ('according', 'VBG'), ('number', 'NN'), ('selecting', 'VBG'), ('single-person', 'JJ'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'JJ'), ('comprises', 'VBZ'), ('single-person', 'JJ'), ('photo', 'NN'), ('album', 'NN'), ('determining', 'VBG'), ('including', 'VBG'), ('two', 'CD'), ('photo', 'NN'), ('album', 'IN'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('including', 'VBG'), ('two', 'CD'), ('cover', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('face', 'NN'), ('feature', 'NN'), ('points', 'NNS'), ('face', 'VBP'), ('parameter', 'NN'), ('comprises', 'NNS'), ('face', 'VBP'), ('turning', 'VBG'), ('angle', 'RP'), ('acquiring', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('acquiring', 'VBG'), ('coordinate', 'NN'), ('values', 'NNS'), ('face', 'VBP'), ('feature', 'NN'), ('points', 'NNS'), ('determining', 'VBG'), ('distances', 'NNS'), ('angles', 'NNS'), ('face', 'VBP'), ('feature', 'NN'), ('points', 'NNS'), ('determining', 'VBG'), ('face', 'NN'), ('turning', 'VBG'), ('angle', 'RP'), ('according', 'VBG'), ('distances', 'NNS'), ('angles', 'NNS'), ('according', 'VBG'), ('wherein', 'JJ'), ('face', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('face', 'VBP'), ('ratio', 'NN'), ('acquiring', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('face', 'NN'), ('region', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('calculating', 'VBG'), ('ratio', 'NN'), ('area', 'NN'), ('face', 'NN'), ('region', 'NN'), ('area', 'NN'), ('obtain', 'VB'), ('face', 'NN'), ('ratio', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('calculating', 'VBG'), ('face', 'NN'), ('ratio', 'NN'), ('comprises', 'VBZ'), ('one', 'CD'), ('face', 'NN'), ('subtracting', 'VBG'), ('area', 'NN'), ('occupied', 'VBD'), ('face', 'NN'), ('corresponding', 'VBG'), ('photo', 'NN'), ('album', 'NN'), ('face', 'NN'), ('region', 'NN'), ('obtain', 'VB'), ('remaining', 'VBG'), ('area', 'NN'), ('calculating', 'VBG'), ('ratio', 'NN'), ('remaining', 'VBG'), ('area', 'NN'), ('area', 'NN'), ('obtain', 'VB'), ('face', 'NN'), ('ratio', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('collecting', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('respective', 'JJ'), ('photo', 'NN'), ('album', 'NN'), ('comprises', 'VBZ'), ('acquiring', 'VBG'), ('identifications', 'NNS'), ('photo', 'VBP'), ('album', 'IN'), ('extracting', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('identifications', 'NNS'), ('face', 'VBP'), ('database', 'JJ'), ('face', 'NN'), ('database', 'NN'), ('stored', 'VBD'), ('face', 'NN'), ('results', 'NNS'), ('face', 'VBP'), ('results', 'NNS'), ('including', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('processing', 'NN'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('memory', 'NN'), ('configured', 'VBD'), ('store', 'NN'), ('instructions', 'NNS'), ('executable', 'JJ'), ('wherein', 'NN'), ('configured', 'VBD'), ('run', 'VBN'), ('program', 'NN'), ('corresponding', 'NN'), ('instructions', 'NNS'), ('reading', 'VBG'), ('instructions', 'NNS'), ('stored', 'VBD'), ('memory', 'NN'), ('perform', 'NN'), ('acquiring', 'VBG'), ('photo', 'NN'), ('album', 'NN'), ('obtained', 'VBD'), ('face', 'NN'), ('clustering', 'VBG'), ('collecting', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('photo', 'NN'), ('album', 'NN'), ('acquiring', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('selecting', 'VBG'), ('cover', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('taking', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('setting', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('photo', 'NN'), ('album', 'JJ'), ('wherein', 'NN'), ('configured', 'VBD'), ('perform', 'JJ'), ('calculation', 'NN'), ('face', 'NN'), ('parameter', 'NN'), ('preset', 'VB'), ('way', 'NN'), ('obtain', 'VB'), ('cover', 'JJ'), ('score', 'NN'), ('select', 'JJ'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'NN'), ('wherein', 'NN'), ('configured', 'VBD'), ('acquire', 'VB'), ('source', 'NN'), ('select', 'NN'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('coming', 'VBG'), ('preset', 'VBN'), ('source', 'NN'), ('cover', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('configured', 'VBD'), ('acquire', 'VB'), ('number', 'NN'), ('contained', 'VBN'), ('determine', 'JJ'), ('single-person', 'JJ'), ('according', 'VBG'), ('number', 'NN'), ('select', 'JJ'), ('single-person', 'JJ'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('configured', 'VBD'), ('single-person', 'JJ'), ('photo', 'NN'), ('album', 'NN'), ('determine', 'NN'), ('including', 'VBG'), ('two', 'CD'), ('photo', 'NN'), ('album', 'NN'), ('select', 'JJ'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('including', 'VBG'), ('two', 'CD'), ('cover', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('face', 'NN'), ('feature', 'NN'), ('points', 'NNS'), ('face', 'VBP'), ('parameter', 'NN'), ('comprises', 'NNS'), ('face', 'VBP'), ('turning', 'VBG'), ('angle', 'NN'), ('configured', 'VBD'), ('acquire', 'VB'), ('coordinate', 'NN'), ('values', 'NNS'), ('face', 'VBP'), ('feature', 'NN'), ('points', 'NNS'), ('determine', 'JJ'), ('distances', 'NNS'), ('angles', 'NNS'), ('face', 'VBP'), ('feature', 'NN'), ('points', 'NNS'), ('determine', 'JJ'), ('face', 'NN'), ('turning', 'VBG'), ('angle', 'RP'), ('according', 'VBG'), ('distances', 'NNS'), ('angles', 'NNS'), ('apparatus', 'VBP'), ('according', 'VBG'), ('wherein', 'NN'), ('face', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('face', 'VBP'), ('ratio', 'NN'), ('configured', 'VBN'), ('determine', 'JJ'), ('face', 'NN'), ('region', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('calculate', 'NN'), ('ratio', 'NN'), ('area', 'NN'), ('face', 'NN'), ('region', 'NN'), ('area', 'NN'), ('obtain', 'VB'), ('face', 'NN'), ('ratio', 'NN'), ('apparatus', 'IN'), ('according', 'VBG'), ('wherein', 'NN'), ('configured', 'VBD'), ('one', 'CD'), ('face', 'NN'), ('subtract', 'JJ'), ('area', 'NN'), ('occupied', 'VBD'), ('face', 'NN'), ('corresponding', 'VBG'), ('photo', 'NN'), ('album', 'NN'), ('face', 'NN'), ('region', 'NN'), ('obtain', 'VB'), ('remaining', 'VBG'), ('area', 'NN'), ('calculate', 'NN'), ('ratio', 'NN'), ('remaining', 'VBG'), ('area', 'NN'), ('area', 'NN'), ('obtain', 'VB'), ('face', 'NN'), ('ratio', 'NN'), ('apparatus', 'IN'), ('according', 'VBG'), ('wherein', 'NN'), ('configured', 'VBD'), ('acquire', 'VB'), ('identifications', 'NNS'), ('photo', 'VB'), ('album', 'JJ'), ('extract', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('identifications', 'NNS'), ('face', 'VBP'), ('database', 'JJ'), ('face', 'NN'), ('database', 'NN'), ('stored', 'VBD'), ('face', 'NN'), ('results', 'NNS'), ('face', 'VBP'), ('results', 'NNS'), ('including', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('comprising', 'VBG'), ('memory', 'NN'), ('display', 'NN'), ('screen', 'NN'), ('input', 'NN'), ('device', 'NN'), ('connected', 'VBN'), ('via', 'IN'), ('system', 'NN'), ('bus', 'JJ'), ('wherein', 'JJ'), ('memory', 'NN'), ('stored', 'VBN'), ('computer', 'NN'), ('programs', 'NNS'), ('executed', 'VBD'), ('cause', 'NN'), ('implement', 'NN'), ('processing', 'VBG'), ('processing', 'VBG'), ('comprising', 'VBG'), ('acquiring', 'VBG'), ('photo', 'NN'), ('album', 'NN'), ('obtained', 'VBD'), ('face', 'NN'), ('clustering', 'VBG'), ('collecting', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('respective', 'JJ'), ('photo', 'NN'), ('album', 'NN'), ('acquiring', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('information', 'NN'), ('selecting', 'VBG'), ('cover', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('taking', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('setting', 'VBG'), ('face-region', 'JJ'), ('cover', 'NN'), ('photo', 'NN'), ('album', 'JJ'), ('wherein', 'NN'), ('selecting', 'VBG'), ('cover', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('performing', 'VBG'), ('calculation', 'NN'), ('face', 'NN'), ('parameter', 'NN'), ('preset', 'VB'), ('way', 'NN'), ('obtain', 'VB'), ('cover', 'JJ'), ('score', 'NN'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('cover', 'JJ'), ('comprises', 'VBZ'), ('acquiring', 'VBG'), ('source', 'NN'), ('selecting', 'VBG'), ('highest', 'JJS'), ('cover', 'NN'), ('score', 'NN'), ('coming', 'VBG'), ('preset', 'VBN'), ('source', 'NN'), ('cover', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('comprises', 'NNS'), ('least', 'JJS'), ('one', 'CD'), ('mobile', 'JJ'), ('phone', 'NN'), ('tablet', 'NN'), ('computer', 'NN'), ('personal', 'JJ'), ('digital', 'NN'), ('assistant', 'NN'), ('wearable', 'JJ'), ('device', 'NN'), ('computer-implemented', 'JJ'), ('comprising', 'NN'), ('receiving', 'VBG'), ('computing', 'VBG'), ('device', 'NN'), ('meeting', 'NN'), ('invitation', 'NN'), ('identifying', 'VBG'), ('location', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('meeting', 'NN'), ('invitation', 'NN'), ('configured', 'VBD'), ('provide', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('wherein', 'NN'), ('meeting', 'NN'), ('invitation', 'NN'), ('causes', 'VBZ'), ('system', 'NN'), ('control', 'NN'), ('pathway', 'RB'), ('allowing', 'VBG'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('providing', 'VBG'), ('based', 'VBN'), ('meeting', 'VBG'), ('invitation', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('controlling', 'VBG'), ('pathway', 'RB'), ('allowing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('physically', 'RB'), ('access', 'NN'), ('location', 'NN'), ('pathway', 'NN'), ('response', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicating', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('predetermined', 'VBN'), ('location', 'NN'), ('near', 'IN'), ('location', 'NN'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('based', 'VBN'), ('part', 'NN'), ('face', 'NN'), ('camera', 'NN'), ('system', 'NN'), ('identifying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('receiving', 'VBG'), ('positioning', 'VBG'), ('data', 'NNS'), ('face', 'NN'), ('camera', 'NN'), ('system', 'NN'), ('identifying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('pattern', 'JJ'), ('movement', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('determining', 'VBG'), ('pattern', 'JJ'), ('movement', 'NN'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('revoking', 'VBG'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('identified', 'VBD'), ('meeting', 'VBG'), ('invitation', 'NN'), ('controlling', 'VBG'), ('pathway', 'RB'), ('restrict', 'VB'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('identified', 'VBD'), ('meeting', 'VBG'), ('invitation', 'NN'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('pathway', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('pattern', 'JJ'), ('movement', 'NN'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('computer-implemented', 'JJ'), ('wherein', 'NN'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('passed', 'VBD'), ('egress', 'RB'), ('associated', 'VBN'), ('location', 'NN'), ('predetermined', 'VBN'), ('direction', 'NN'), ('computer-implemented', 'JJ'), ('wherein', 'NN'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('moved', 'VBD'), ('area', 'NN'), ('predetermined', 'VBN'), ('direction', 'NN'), ('computer-implemented', 'JJ'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('second', 'JJ'), ('pattern', 'JJ'), ('movement', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('wherein', 'NN'), ('access', 'NN'), ('secured', 'VBN'), ('data', 'NNS'), ('associated', 'VBN'), ('location', 'NN'), ('provided', 'VBN'), ('response', 'NN'), ('detecting', 'VBG'), ('second', 'JJ'), ('pattern', 'JJ'), ('movement', 'NN'), ('computer-implemented', 'JJ'), ('comprising', 'VBG'), ('collating', 'NN'), ('secured', 'VBN'), ('data', 'NNS'), ('public', 'JJ'), ('data', 'NNS'), ('generate', 'VBP'), ('resource', 'NN'), ('data', 'NNS'), ('communicating', 'VBG'), ('resource', 'NN'), ('data', 'NNS'), ('client', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('associated', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('access', 'NN'), ('location', 'NN'), ('provided', 'VBD'), ('computer-implemented', 'JJ'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('predetermined', 'VBN'), ('location', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('passes', 'NNS'), ('predetermined', 'VBD'), ('location', 'NN'), ('computer-implemented', 'JJ'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('predetermined', 'VBN'), ('location', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('passes', 'NNS'), ('predetermined', 'VBD'), ('location', 'NN'), ('near', 'IN'), ('location', 'NN'), ('predetermined', 'VBN'), ('direction', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('memory', 'NN'), ('communication', 'NN'), ('memory', 'NN'), ('computer-readable', 'JJ'), ('instructions', 'NNS'), ('stored', 'VBD'), ('thereupon', 'RB'), ('executed', 'VBN'), ('cause', 'NN'), ('receive', 'JJ'), ('meeting', 'NN'), ('invitation', 'NN'), ('indicating', 'VBG'), ('location', 'NN'), ('identity', 'NN'), ('meeting', 'NN'), ('invitation', 'NN'), ('configured', 'VBD'), ('provide', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('wherein', 'NN'), ('meeting', 'NN'), ('invitation', 'NN'), ('causes', 'VBZ'), ('system', 'NN'), ('control', 'NN'), ('pathway', 'RB'), ('allowing', 'VBG'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('provide', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('associated', 'VBN'), ('identity', 'NN'), ('access', 'NN'), ('location', 'NN'), ('controlling', 'VBG'), ('pathway', 'RB'), ('allowing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('physically', 'RB'), ('access', 'NN'), ('location', 'NN'), ('pathway', 'NN'), ('response', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicating', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('predetermined', 'VBN'), ('location', 'NN'), ('near', 'IN'), ('location', 'NN'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('based', 'VBN'), ('part', 'NN'), ('face', 'NN'), ('camera', 'NN'), ('system', 'NN'), ('identifying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('receive', 'VBP'), ('positioning', 'VBG'), ('data', 'NNS'), ('face', 'NN'), ('camera', 'NN'), ('system', 'NN'), ('identifying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('pattern', 'JJ'), ('movement', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('determine', 'NN'), ('pattern', 'JJ'), ('movement', 'NN'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('revoke', 'VBD'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('identified', 'VBD'), ('meeting', 'VBG'), ('invitation', 'NN'), ('controlling', 'VBG'), ('pathway', 'RB'), ('restrict', 'VB'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('identified', 'VBD'), ('meeting', 'VBG'), ('invitation', 'NN'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('pathway', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('pattern', 'JJ'), ('movement', 'NN'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('passed', 'VBD'), ('egress', 'RB'), ('associated', 'VBN'), ('location', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('moved', 'VBD'), ('area', 'NN'), ('predetermined', 'VBN'), ('direction', 'NN'), ('system', 'NN'), ('wherein', 'VBP'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('second', 'JJ'), ('pattern', 'JJ'), ('movement', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('wherein', 'NN'), ('access', 'NN'), ('secured', 'VBN'), ('data', 'NNS'), ('associated', 'VBN'), ('location', 'NN'), ('provided', 'VBN'), ('response', 'NN'), ('detecting', 'VBG'), ('second', 'JJ'), ('pattern', 'JJ'), ('movement', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('instructions', 'NNS'), ('cause', 'VBP'), ('collate', 'NN'), ('secured', 'VBN'), ('data', 'NNS'), ('public', 'JJ'), ('data', 'NNS'), ('generate', 'VBP'), ('resource', 'NN'), ('data', 'NNS'), ('communicate', 'VBP'), ('resource', 'NN'), ('data', 'NNS'), ('client', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('associated', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('access', 'NN'), ('location', 'NN'), ('provided', 'VBD'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('computer-executable', 'JJ'), ('instructions', 'NNS'), ('stored', 'VBD'), ('thereupon', 'RB'), ('executed', 'VBN'), ('one', 'CD'), ('computing', 'VBG'), ('device', 'NN'), ('cause', 'NN'), ('one', 'CD'), ('computing', 'NN'), ('device', 'NN'), ('receive', 'VBP'), ('meeting', 'NN'), ('invitation', 'NN'), ('indicating', 'VBG'), ('location', 'NN'), ('identity', 'NN'), ('meeting', 'NN'), ('invitation', 'NN'), ('configured', 'VBD'), ('provide', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('wherein', 'NN'), ('meeting', 'NN'), ('invitation', 'NN'), ('causes', 'VBZ'), ('system', 'NN'), ('control', 'NN'), ('pathway', 'RB'), ('allowing', 'VBG'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('provide', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('associated', 'VBN'), ('identity', 'NN'), ('access', 'NN'), ('location', 'NN'), ('controlling', 'VBG'), ('pathway', 'RB'), ('allowing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('physically', 'RB'), ('access', 'NN'), ('location', 'NN'), ('pathway', 'NN'), ('response', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicating', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('predetermined', 'VBN'), ('location', 'NN'), ('near', 'IN'), ('location', 'NN'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('based', 'VBN'), ('part', 'NN'), ('face', 'NN'), ('camera', 'NN'), ('system', 'NN'), ('identifying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('receive', 'VBP'), ('positioning', 'VBG'), ('data', 'NNS'), ('face', 'NN'), ('camera', 'NN'), ('system', 'NN'), ('identifying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('pattern', 'JJ'), ('movement', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('determine', 'NN'), ('pattern', 'JJ'), ('movement', 'NN'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('revoke', 'VBD'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('identified', 'VBD'), ('meeting', 'VBG'), ('invitation', 'NN'), ('controlling', 'VBG'), ('pathway', 'RB'), ('restrict', 'VB'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('identified', 'VBD'), ('meeting', 'VBG'), ('invitation', 'NN'), ('physical', 'JJ'), ('access', 'NN'), ('location', 'NN'), ('pathway', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('pattern', 'JJ'), ('movement', 'NN'), ('indicates', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('exited', 'VBN'), ('location', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('passed', 'VBD'), ('egress', 'RB'), ('associated', 'VBN'), ('location', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('positioning', 'VBG'), ('data', 'NNS'), ('indicates', 'VBZ'), ('second', 'JJ'), ('pattern', 'JJ'), ('movement', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('wherein', 'NN'), ('access', 'NN'), ('secured', 'VBN'), ('data', 'NNS'), ('associated', 'VBN'), ('location', 'NN'), ('provided', 'VBN'), ('response', 'NN'), ('detecting', 'VBG'), ('second', 'JJ'), ('pattern', 'JJ'), ('movement', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('instructions', 'NNS'), ('cause', 'VBP'), ('one', 'CD'), ('collate', 'NN'), ('secured', 'VBN'), ('data', 'NNS'), ('public', 'JJ'), ('data', 'NNS'), ('generate', 'VBP'), ('resource', 'NN'), ('data', 'NNS'), ('communicate', 'VBP'), ('resource', 'NN'), ('data', 'NNS'), ('client', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('associated', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('invitee', 'NN'), ('access', 'NN'), ('location', 'NN'), ('provided', 'VBD'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('piece', 'NN'), ('content', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('based', 'VBN'), ('salient', 'JJ'), ('data', 'NNS'), ('determining', 'VBG'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('different', 'JJ'), ('salient', 'JJ'), ('events', 'NNS'), ('occurring', 'VBG'), ('piece', 'NN'), ('content', 'NN'), ('different', 'JJ'), ('times', 'NNS'), ('playback', 'VBP'), ('piece', 'NN'), ('content', 'NN'), ('providing', 'VBG'), ('viewport', 'NN'), ('display', 'NN'), ('device', 'NN'), ('wherein', 'VBD'), ('movement', 'NN'), ('viewport', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('salient', 'NN'), ('data', 'NNS'), ('playback', 'NN'), ('detecting', 'VBG'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('included', 'VBD'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('providing', 'VBG'), ('indication', 'JJ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('viewport', 'NN'), ('playback', 'NN'), ('wherein', 'NN'), ('salient', 'NN'), ('data', 'NNS'), ('identifies', 'NNS'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('indicates', 'VBZ'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('corresponding', 'VBG'), ('point', 'NN'), ('location', 'NN'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('corresponding', 'VBG'), ('time', 'NN'), ('salient', 'JJ'), ('event', 'NN'), ('occurs', 'VBZ'), ('playback', 'RB'), ('wherein', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('indicates', 'VBZ'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('corresponding', 'VBG'), ('type', 'JJ'), ('salient', 'JJ'), ('event', 'NN'), ('corresponding', 'VBG'), ('strength', 'NN'), ('value', 'NN'), ('salient', 'JJ'), ('event', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('controls', 'NNS'), ('movement', 'NN'), ('viewport', 'NN'), ('put', 'VBD'), ('different', 'JJ'), ('salient', 'JJ'), ('events', 'NNS'), ('view', 'VBP'), ('viewport', 'RB'), ('different', 'JJ'), ('times', 'NNS'), ('playback', 'VBP'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('visual', 'JJ'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('audio', 'NN'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('content', 'JJ'), ('consumption', 'NN'), ('experience', 'NN'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('wherein', 'WRB'), ('salient', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('salient', 'JJ'), ('event', 'NN'), ('detected', 'VBD'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('face', 'NN'), ('facial', 'JJ'), ('emotion', 'NN'), ('object', 'JJ'), ('motion', 'NN'), ('metadata', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('wherein', 'WRB'), ('salient', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('salient', 'JJ'), ('event', 'NN'), ('detected', 'VBD'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('interaction', 'NN'), ('indication', 'NN'), ('wherein', 'WRB'), ('indication', 'NN'), ('comprises', 'VBZ'), ('interactive', 'JJ'), ('hint', 'NN'), ('response', 'NN'), ('detecting', 'VBG'), ('interaction', 'NN'), ('adapting', 'VBG'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('based', 'VBN'), ('interaction', 'NN'), ('wherein', 'WRB'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('providing', 'VBG'), ('updated', 'JJ'), ('viewport', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('display', 'NN'), ('device', 'NN'), ('wherein', 'VBD'), ('movement', 'NN'), ('updated', 'VBN'), ('viewport', 'NN'), ('based', 'VBN'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('salient', 'NN'), ('data', 'NNS'), ('playback', 'VBP'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('controls', 'NNS'), ('movement', 'NN'), ('updated', 'VBN'), ('viewport', 'NN'), ('put', 'VBD'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('view', 'NN'), ('updated', 'VBD'), ('viewport', 'NN'), ('comprising', 'VBG'), ('changing', 'VBG'), ('weight', 'NN'), ('assigned', 'VBD'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'JJ'), ('type', 'JJ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('wherein', 'JJ'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'JJ'), ('type', 'JJ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('-readable', 'JJ'), ('memory', 'NN'), ('device', 'NN'), ('storing', 'VBG'), ('instructions', 'NNS'), ('executed', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('causes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('perform', 'NN'), ('operations', 'NNS'), ('including', 'VBG'), ('receiving', 'VBG'), ('piece', 'NN'), ('content', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('based', 'VBN'), ('salient', 'JJ'), ('data', 'NNS'), ('determining', 'VBG'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('different', 'JJ'), ('salient', 'JJ'), ('events', 'NNS'), ('occurring', 'VBG'), ('piece', 'NN'), ('content', 'NN'), ('different', 'JJ'), ('times', 'NNS'), ('playback', 'VBP'), ('piece', 'NN'), ('content', 'NN'), ('providing', 'VBG'), ('viewport', 'NN'), ('display', 'NN'), ('device', 'NN'), ('wherein', 'VBD'), ('movement', 'NN'), ('viewport', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('salient', 'NN'), ('data', 'NNS'), ('playback', 'NN'), ('detecting', 'VBG'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('included', 'VBD'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('providing', 'VBG'), ('indication', 'JJ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('viewport', 'NN'), ('playback', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('identifies', 'NNS'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('indicates', 'VBZ'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('corresponding', 'VBG'), ('point', 'NN'), ('location', 'NN'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('corresponding', 'VBG'), ('time', 'NN'), ('salient', 'JJ'), ('event', 'NN'), ('occurs', 'VBZ'), ('playback', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('indicates', 'VBZ'), ('salient', 'JJ'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('corresponding', 'VBG'), ('type', 'JJ'), ('salient', 'JJ'), ('event', 'NN'), ('corresponding', 'VBG'), ('strength', 'NN'), ('value', 'NN'), ('salient', 'NN'), ('event', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('generated', 'VBD'), ('offline', 'JJ'), ('server', 'NN'), ('system', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('visual', 'JJ'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('audio', 'NN'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('content', 'JJ'), ('consumption', 'NN'), ('experience', 'NN'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('wherein', 'WRB'), ('salient', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('salient', 'JJ'), ('event', 'NN'), ('detected', 'VBD'), ('system', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('following', 'VBG'), ('face', 'NN'), ('facial', 'JJ'), ('emotion', 'NN'), ('object', 'JJ'), ('motion', 'NN'), ('metadata', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('wherein', 'WRB'), ('salient', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('salient', 'JJ'), ('event', 'NN'), ('detected', 'VBD'), ('system', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('interaction', 'NN'), ('indication', 'NN'), ('wherein', 'WRB'), ('indication', 'NN'), ('comprises', 'VBZ'), ('interactive', 'JJ'), ('hint', 'NN'), ('response', 'NN'), ('detecting', 'VBG'), ('interaction', 'NN'), ('adapting', 'VBG'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('based', 'VBN'), ('interaction', 'NN'), ('wherein', 'WRB'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('providing', 'VBG'), ('updated', 'JJ'), ('viewport', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('display', 'NN'), ('device', 'NN'), ('wherein', 'VBD'), ('movement', 'NN'), ('updated', 'VBN'), ('viewport', 'NN'), ('based', 'VBN'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('salient', 'NN'), ('data', 'NNS'), ('playback', 'VBP'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('controls', 'NNS'), ('movement', 'NN'), ('updated', 'VBN'), ('viewport', 'NN'), ('put', 'VBD'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('view', 'NN'), ('updated', 'VBN'), ('viewport', 'NN'), ('system', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('changing', 'VBG'), ('weight', 'NN'), ('assigned', 'VBD'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'JJ'), ('type', 'JJ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('one', 'CD'), ('salient', 'NN'), ('events', 'NNS'), ('piece', 'VBP'), ('content', 'JJ'), ('type', 'JJ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('including', 'VBG'), ('instructions', 'NNS'), ('perform', 'VBP'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('piece', 'NN'), ('content', 'JJ'), ('salient', 'NN'), ('data', 'NNS'), ('piece', 'NN'), ('content', 'NN'), ('based', 'VBN'), ('salient', 'JJ'), ('data', 'NNS'), ('determining', 'VBG'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('different', 'JJ'), ('salient', 'JJ'), ('events', 'NNS'), ('occurring', 'VBG'), ('piece', 'NN'), ('content', 'NN'), ('different', 'JJ'), ('times', 'NNS'), ('playback', 'VBP'), ('piece', 'NN'), ('content', 'NN'), ('providing', 'VBG'), ('viewport', 'NN'), ('display', 'NN'), ('device', 'NN'), ('wherein', 'VBD'), ('movement', 'NN'), ('viewport', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('salient', 'NN'), ('data', 'NNS'), ('playback', 'NN'), ('detecting', 'VBG'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('included', 'VBD'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('providing', 'VBG'), ('indication', 'JJ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('viewport', 'NN'), ('playback', 'NN'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('interaction', 'NN'), ('indication', 'NN'), ('wherein', 'WRB'), ('indication', 'NN'), ('comprises', 'VBZ'), ('interactive', 'JJ'), ('hint', 'NN'), ('response', 'NN'), ('detecting', 'VBG'), ('interaction', 'NN'), ('adapting', 'VBG'), ('first', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('based', 'VBN'), ('interaction', 'NN'), ('wherein', 'WRB'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('includes', 'VBZ'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('providing', 'VBG'), ('updated', 'JJ'), ('viewport', 'NN'), ('piece', 'NN'), ('content', 'NN'), ('display', 'NN'), ('device', 'NN'), ('wherein', 'VBD'), ('movement', 'NN'), ('updated', 'VBN'), ('viewport', 'NN'), ('based', 'VBN'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('salient', 'NN'), ('data', 'NNS'), ('playback', 'VBP'), ('second', 'JJ'), ('path', 'NN'), ('viewport', 'NN'), ('controls', 'NNS'), ('movement', 'NN'), ('updated', 'VBN'), ('viewport', 'NN'), ('put', 'VBD'), ('additional', 'JJ'), ('salient', 'NN'), ('event', 'NN'), ('view', 'NN'), ('updated', 'VBD'), ('viewport', 'NN'), ('mobile', 'NN'), ('device', 'NN'), ('facial', 'JJ'), ('mobile', 'JJ'), ('device', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('cameras', 'JJ'), ('device', 'NN'), ('memory', 'NN'), ('coupled', 'VBN'), ('device', 'NN'), ('processing', 'NN'), ('system', 'NN'), ('programmed', 'VBD'), ('receive', 'JJ'), ('one', 'CD'), ('cameras', 'NN'), ('extract', 'JJ'), ('feature', 'NN'), ('extractor', 'NN'), ('utilizing', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('cnn', 'NNS'), ('enlarged', 'VBD'), ('intra-class', 'JJ'), ('variance', 'NN'), ('long-tail', 'JJ'), ('classes', 'NNS'), ('feature', 'NN'), ('vectors', 'NNS'), ('generate', 'VBP'), ('feature', 'NN'), ('generator', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vectors', 'NNS'), ('feature', 'VBP'), ('vectors', 'NNS'), ('classify', 'VBP'), ('fully', 'RB'), ('connected', 'VBN'), ('classifier', 'JJR'), ('identity', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vectors', 'NNS'), ('control', 'VBP'), ('operation', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('react', 'NN'), ('accordance', 'NN'), ('identity', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('includes', 'VBZ'), ('communication', 'NN'), ('system', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'JJ'), ('operation', 'NN'), ('tags', 'NNS'), ('video', 'VBP'), ('identity', 'NN'), ('uploads', 'NNS'), ('video', 'VBP'), ('social', 'JJ'), ('media', 'NNS'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'JJ'), ('operation', 'NN'), ('tags', 'NNS'), ('video', 'VBP'), ('identity', 'NN'), ('sends', 'NNS'), ('video', 'VBP'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'RB'), ('mobile', 'JJ'), ('device', 'NN'), ('smart', 'VBD'), ('phone', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'RB'), ('mobile', 'JJ'), ('device', 'NN'), ('body', 'NN'), ('cam', 'VBP'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('programmed', 'JJ'), ('train', 'NN'), ('feature', 'NN'), ('extractor', 'NN'), ('feature', 'NN'), ('generator', 'NN'), ('fully', 'RB'), ('connected', 'VBN'), ('classifier', 'JJ'), ('alternative', 'JJ'), ('bi-stage', 'NN'), ('strategy', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'JJ'), ('feature', 'NN'), ('extractor', 'NN'), ('shares', 'NNS'), ('covariance', 'NN'), ('matrices', 'NNS'), ('across', 'IN'), ('classes', 'NNS'), ('transfer', 'VBP'), ('intra-class', 'JJ'), ('variance', 'NN'), ('regular', 'JJ'), ('classes', 'NNS'), ('long-tail', 'JJ'), ('classes', 'NNS'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'JJ'), ('feature', 'NN'), ('generator', 'NN'), ('optimizes', 'VBZ'), ('softmax', 'JJ'), ('loss', 'NN'), ('joint', 'NN'), ('regularization', 'NN'), ('weights', 'NNS'), ('features', 'NNS'), ('magnitude', 'VBP'), ('inner', 'JJ'), ('product', 'NN'), ('weights', 'NNS'), ('features', 'NNS'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'JJ'), ('feature', 'NN'), ('extractor', 'NN'), ('averages', 'NNS'), ('feature', 'VBP'), ('vector', 'NN'), ('flipped', 'VBD'), ('feature', 'NN'), ('vector', 'NN'), ('flipped', 'VBD'), ('feature', 'NN'), ('vector', 'NN'), ('generated', 'VBD'), ('horizontally', 'RB'), ('flipped', 'VBN'), ('frame', 'VB'), ('one', 'CD'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'RB'), ('selected', 'VBN'), ('group', 'NN'), ('consisting', 'VBG'), ('video', 'NN'), ('frame', 'NN'), ('video', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'JJ'), ('communication', 'NN'), ('system', 'NN'), ('connects', 'VBZ'), ('remote', 'JJ'), ('server', 'NN'), ('includes', 'VBZ'), ('facial', 'JJ'), ('network', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'WP'), ('one', 'CD'), ('stage', 'NN'), ('alternative', 'JJ'), ('bi-stage', 'NN'), ('strategy', 'NN'), ('fixes', 'NNS'), ('feature', 'VBP'), ('extractor', 'NN'), ('applies', 'NNS'), ('feature', 'VBP'), ('generator', 'NN'), ('generate', 'VBP'), ('new', 'JJ'), ('transferred', 'VBN'), ('features', 'NNS'), ('diverse', 'JJ'), ('violate', 'JJ'), ('decision', 'NN'), ('boundary', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('recited', 'VBD'), ('wherein', 'WP'), ('one', 'CD'), ('stage', 'NN'), ('alternative', 'JJ'), ('bi-stage', 'NN'), ('strategy', 'NN'), ('fixes', 'NNS'), ('fully', 'RB'), ('connected', 'VBN'), ('classifier', 'JJR'), ('updates', 'JJ'), ('feature', 'NN'), ('extractor', 'NN'), ('feature', 'NN'), ('generator', 'NN'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('facial', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('comprising', 'VBG'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('program', 'NN'), ('instructions', 'NNS'), ('embodied', 'VBD'), ('therewith', 'JJ'), ('program', 'NN'), ('instructions', 'NNS'), ('executable', 'JJ'), ('computer', 'NN'), ('cause', 'NN'), ('computer', 'NN'), ('perform', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('device', 'NN'), ('extracting', 'VBG'), ('device', 'NN'), ('feature', 'NN'), ('extractor', 'NN'), ('utilizing', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('cnn', 'NNS'), ('enlarged', 'VBD'), ('intra-class', 'JJ'), ('variance', 'NN'), ('long-tail', 'JJ'), ('classes', 'NNS'), ('feature', 'VBP'), ('vectors', 'NNS'), ('generating', 'VBG'), ('device', 'NN'), ('feature', 'NN'), ('generator', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vectors', 'NNS'), ('feature', 'VBP'), ('vectors', 'NNS'), ('classifying', 'VBG'), ('device', 'NN'), ('utilizing', 'VBG'), ('fully', 'RB'), ('connected', 'VBN'), ('classifier', 'JJR'), ('identity', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vector', 'NN'), ('controlling', 'VBG'), ('operation', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('react', 'NN'), ('accordance', 'NN'), ('identity', 'NN'), ('computer-implemented', 'JJ'), ('facial', 'JJ'), ('mobile', 'JJ'), ('device', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('device', 'NN'), ('extracting', 'VBG'), ('device', 'NN'), ('feature', 'NN'), ('extractor', 'NN'), ('utilizing', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('cnn', 'NNS'), ('enlarged', 'VBD'), ('intra-class', 'JJ'), ('variance', 'NN'), ('long-tail', 'JJ'), ('classes', 'NNS'), ('feature', 'VBP'), ('vectors', 'NNS'), ('generating', 'VBG'), ('device', 'NN'), ('feature', 'NN'), ('generator', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vectors', 'NNS'), ('feature', 'VBP'), ('vectors', 'NNS'), ('classifying', 'VBG'), ('device', 'NN'), ('utilizing', 'VBG'), ('fully', 'RB'), ('connected', 'VBN'), ('classifier', 'JJR'), ('identity', 'NN'), ('discriminative', 'JJ'), ('feature', 'NN'), ('vector', 'NN'), ('controlling', 'VBG'), ('operation', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('react', 'NN'), ('accordance', 'NN'), ('identity', 'NN'), ('computer-implemented', 'JJ'), ('recited', 'VBD'), ('wherein', 'JJ'), ('controlling', 'VBG'), ('includes', 'VBZ'), ('tagging', 'VBG'), ('video', 'NN'), ('identity', 'NN'), ('uploading', 'VBG'), ('video', 'JJ'), ('social', 'JJ'), ('media', 'NNS'), ('computer-implemented', 'JJ'), ('recited', 'VBD'), ('wherein', 'JJ'), ('controlling', 'VBG'), ('includes', 'VBZ'), ('tagging', 'VBG'), ('video', 'NN'), ('identity', 'NN'), ('sending', 'VBG'), ('video', 'JJ'), ('computer-implemented', 'JJ'), ('recited', 'VBN'), ('wherein', 'NN'), ('extracting', 'VBG'), ('includes', 'VBZ'), ('sharing', 'VBG'), ('covariance', 'NN'), ('matrices', 'NNS'), ('across', 'IN'), ('classes', 'NNS'), ('transfer', 'VBP'), ('intra-class', 'JJ'), ('variance', 'NN'), ('regular', 'JJ'), ('classes', 'NNS'), ('long-tail', 'JJ'), ('classes', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('comprising', 'VBG'), ('non-transitory', 'JJ'), ('machine', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('storing', 'VBG'), ('machine', 'NN'), ('trained', 'VBN'), ('mt', 'JJ'), ('network', 'NN'), ('comprising', 'VBG'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('processing', 'VBG'), ('node', 'RB'), ('configured', 'VBN'), ('compute', 'NN'), ('first', 'RB'), ('output', 'NN'), ('value', 'NN'), ('combining', 'VBG'), ('set', 'VBN'), ('output', 'NN'), ('values', 'NNS'), ('set', 'VBD'), ('processing', 'VBG'), ('nodes', 'NNS'), ('use', 'VBP'), ('piecewise', 'JJ'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('compute', 'JJ'), ('second', 'JJ'), ('output', 'NN'), ('value', 'NN'), ('first', 'RB'), ('output', 'NN'), ('value', 'NN'), ('processing', 'VBG'), ('node', 'JJ'), ('wherein', 'NN'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('prior', 'RB'), ('training', 'VBG'), ('mt', 'NN'), ('network', 'NN'), ('comprises', 'NNS'), ('least', 'VBP'), ('first', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('first', 'RB'), ('slope', 'NN'), ('followed', 'VBD'), ('ii', 'JJ'), ('second', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('negative', 'JJ'), ('second', 'JJ'), ('slope', 'NN'), ('followed', 'VBD'), ('iii', 'JJ'), ('third', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('negative', 'JJ'), ('third', 'JJ'), ('slope', 'NN'), ('different', 'JJ'), ('second', 'JJ'), ('slope', 'NN'), ('followed', 'VBD'), ('iv', 'JJ'), ('fourth', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('positive', 'JJ'), ('fourth', 'JJ'), ('slope', 'NN'), ('followed', 'VBD'), ('v', 'JJ'), ('fifth', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('positive', 'JJ'), ('fifth', 'JJ'), ('slope', 'NN'), ('different', 'JJ'), ('fourth', 'JJ'), ('slope', 'NN'), ('followed', 'VBD'), ('vi', 'JJ'), ('sixth', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('sixth', 'VBD'), ('slope', 'NN'), ('wherein', 'NN'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('symmetric', 'JJ'), ('vertical', 'JJ'), ('axis', 'NN'), ('third', 'JJ'), ('fourth', 'JJ'), ('linear', 'JJ'), ('sections', 'NNS'), ('prior', 'RB'), ('training', 'VBG'), ('mt', 'NN'), ('network', 'NN'), ('content', 'JJ'), ('capturing', 'NN'), ('circuit', 'NN'), ('capturing', 'VBG'), ('content', 'JJ'), ('processing', 'NN'), ('mt', 'NN'), ('network', 'NN'), ('set', 'VBN'), ('processing', 'VBG'), ('units', 'NNS'), ('executing', 'VBG'), ('processing', 'VBG'), ('nodes', 'NNS'), ('process', 'NN'), ('content', 'NN'), ('captured', 'VBD'), ('content', 'JJ'), ('capturing', 'VBG'), ('circuit', 'NN'), ('wherein', 'NN'), ('training', 'NN'), ('set', 'VBN'), ('parameters', 'NNS'), ('define', 'VBP'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('node', 'IN'), ('first', 'JJ'), ('second', 'JJ'), ('pluralities', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('processing', 'VBG'), ('node', 'NN'), ('first', 'RB'), ('processing', 'VBG'), ('nodes', 'NNS'), ('configured', 'JJ'), ('emulate', 'JJ'), ('boolean', 'NN'), ('operator', 'NN'), ('output', 'NN'), ('value', 'NN'), ('processing', 'NN'), ('node', 'JJ'), ('range', 'NN'), ('associated', 'VBN'), ('``', '``'), (\"''\", \"''\"), ('value', 'NN'), ('set', 'VBN'), ('inputs', 'NNS'), ('processing', 'VBG'), ('node', 'NN'), ('set', 'VBN'), ('values', 'NNS'), ('range', 'VBP'), ('associated', 'VBN'), ('``', '``'), (\"''\", \"''\"), ('ii', 'NN'), ('processing', 'NN'), ('node', 'JJ'), ('second', 'JJ'), ('processing', 'NN'), ('nodes', 'NNS'), ('configured', 'VBD'), ('emulate', 'JJ'), ('boolean', 'JJ'), ('xnor', 'NN'), ('operator', 'NN'), ('output', 'NN'), ('value', 'NN'), ('processing', 'NN'), ('node', 'JJ'), ('range', 'NN'), ('associated', 'VBN'), ('``', '``'), (\"''\", \"''\"), ('set', 'NN'), ('inputs', 'NNS'), ('node', 'JJ'), ('set', 'NN'), ('values', 'NNS'), ('range', 'VBP'), ('associated', 'VBN'), ('``', '``'), (\"''\", \"''\"), ('b', 'NN'), ('set', 'VBN'), ('inputs', 'NNS'), ('node', 'JJ'), ('set', 'NN'), ('values', 'NNS'), ('range', 'VBP'), ('associated', 'VBN'), ('``', '``'), (\"''\", \"''\"), ('value', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NNS'), ('third', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('first', 'RB'), ('processing', 'VBG'), ('node', 'NN'), ('mt', 'NN'), ('network', 'NN'), ('different', 'JJ'), ('slope', 'NN'), ('third', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('second', 'JJ'), ('processing', 'NN'), ('node', 'NN'), ('mt', 'NN'), ('network', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('length', 'JJ'), ('third', 'JJ'), ('section', 'NN'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('first', 'RB'), ('processing', 'VBG'), ('node', 'NN'), ('mt', 'NN'), ('network', 'NN'), ('different', 'JJ'), ('length', 'NN'), ('third', 'JJ'), ('section', 'NN'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('second', 'JJ'), ('processing', 'NN'), ('node', 'NN'), ('mt', 'NN'), ('network', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('sets', 'VBZ'), ('parameters', 'NNS'), ('trained', 'VBD'), ('part', 'NN'), ('back', 'RB'), ('propagating', 'VBG'), ('module', 'NN'), ('back', 'RB'), ('propagating', 'JJ'), ('errors', 'NNS'), ('output', 'NN'), ('values', 'NNS'), ('later', 'RBR'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('earlier', 'RBR'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('adjusting', 'VBG'), ('set', 'NN'), ('parameters', 'NNS'), ('define', 'VBP'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('functions', 'NNS'), ('earlier', 'RBR'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('processing', 'NN'), ('node', 'JJ'), ('uses', 'NNS'), ('linear', 'JJ'), ('function', 'NN'), ('defined', 'VBD'), ('set', 'VBN'), ('parameters', 'NNS'), ('compute', 'VBP'), ('first', 'JJ'), ('output', 'NN'), ('value', 'NN'), ('processing', 'VBG'), ('node', 'CC'), ('wherein', 'JJ'), ('back', 'RB'), ('propagating', 'VBG'), ('module', 'NN'), ('back', 'RB'), ('propagates', 'VBZ'), ('errors', 'NNS'), ('output', 'NN'), ('values', 'NNS'), ('later', 'RBR'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('earlier', 'RBR'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('adjusting', 'VBG'), ('set', 'NN'), ('parameters', 'NNS'), ('define', 'VBP'), ('linear', 'JJ'), ('functions', 'NNS'), ('earlier', 'RBR'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'IN'), ('first', 'JJ'), ('processing', 'NN'), ('nodes', 'NNS'), ('emulate', 'VBP'), ('boolean', 'JJ'), ('operator', 'NN'), ('second', 'JJ'), ('processing', 'NN'), ('nodes', 'NNS'), ('emulate', 'VBP'), ('boolean', 'JJ'), ('xnor', 'NNP'), ('operator', 'NN'), ('enable', 'JJ'), ('mt', 'NN'), ('network', 'NN'), ('implement', 'JJ'), ('mathematical', 'JJ'), ('problems', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('processing', 'VBG'), ('node', 'JJ'), ('layers', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('receive', 'JJ'), ('input', 'NN'), ('values', 'NNS'), ('output', 'NN'), ('values', 'NNS'), ('processing', 'VBG'), ('nodes', 'NNS'), ('set', 'VBN'), ('prior', 'JJ'), ('layers', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('processing', 'NN'), ('node', 'JJ'), ('uses', 'NNS'), ('linear', 'JJ'), ('function', 'NN'), ('compute', 'NN'), ('first', 'RB'), ('output', 'NN'), ('value', 'NN'), ('processing', 'VBG'), ('node', 'JJ'), ('wherein', 'NN'), ('processing', 'VBG'), ('node', 'NN'), (\"'s\", 'POS'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('defined', 'VBD'), ('along', 'IN'), ('first', 'JJ'), ('second', 'JJ'), ('axes', 'NNS'), ('first', 'RB'), ('axis', 'VBP'), ('defining', 'VBG'), ('range', 'NN'), ('output', 'NN'), ('values', 'NNS'), ('processing', 'VBG'), ('node', 'NN'), (\"'s\", 'POS'), ('linear', 'JJ'), ('function', 'NN'), ('second', 'JJ'), ('axis', 'NN'), ('defining', 'VBG'), ('range', 'NN'), ('output', 'NN'), ('values', 'NNS'), ('produced', 'VBD'), ('piecewise', 'JJ'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('range', 'NN'), ('output', 'NN'), ('values', 'NNS'), ('processing', 'VBG'), ('node', 'NN'), (\"'s\", 'POS'), ('linear', 'JJ'), ('function', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('comprising', 'VBG'), ('content', 'JJ'), ('output', 'NN'), ('circuit', 'NN'), ('presenting', 'VBG'), ('output', 'NN'), ('based', 'VBN'), ('processing', 'NN'), ('content', 'NN'), ('mt', 'NN'), ('network', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('captured', 'VBD'), ('content', 'JJ'), ('one', 'CD'), ('audio', 'NN'), ('segment', 'NN'), ('wherein', 'NN'), ('presented', 'VBD'), ('output', 'NN'), ('output', 'NN'), ('display', 'VBP'), ('display', 'NN'), ('screen', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('audio', 'NN'), ('presentation', 'NN'), ('output', 'NN'), ('speaker', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('mobile', 'JJ'), ('device', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('mt', 'NN'), ('network', 'NN'), ('mt', 'NN'), ('neural', 'JJ'), ('network', 'NN'), ('processing', 'VBG'), ('nodes', 'NNS'), ('mt', 'JJ'), ('neurons', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('set', 'VBN'), ('parameters', 'NNS'), ('configured', 'VBD'), ('training', 'VBG'), ('processing', 'VBG'), ('nodes', 'NNS'), ('comprise', 'NN'), ('least', 'VBP'), ('one', 'CD'), ('negative', 'JJ'), ('second', 'JJ'), ('third', 'JJ'), ('slopes', 'NNS'), ('second', 'JJ'), ('third', 'JJ'), ('linear', 'JJ'), ('sections', 'NNS'), ('positive', 'JJ'), ('fourth', 'JJ'), ('fifth', 'JJ'), ('slopes', 'NNS'), ('fourth', 'JJ'), ('fifth', 'JJ'), ('linear', 'NN'), ('sections', 'NNS'), ('first', 'RB'), ('intercept', 'JJ'), ('second', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('second', 'JJ'), ('intercept', 'NN'), ('fifth', 'JJ'), ('linear', 'JJ'), ('section', 'NN'), ('set', 'VBN'), ('lengths', 'NNS'), ('least', 'JJS'), ('second', 'JJ'), ('third', 'JJ'), ('fourth', 'JJ'), ('fifth', 'JJ'), ('sections', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('trained', 'VBD'), ('set', 'VBN'), ('parameters', 'NNS'), ('define', 'VBP'), ('piecewise', 'NN'), ('linear', 'JJ'), ('cup', 'NN'), ('function', 'NN'), ('node', 'NN'), ('comprise', 'NN'), ('output', 'NN'), ('values', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('first', 'RB'), ('sixth', 'JJ'), ('slopes', 'NNS'), ('zerowe', 'VBP'), ('system', 'NN'), ('comprising', 'VBG'), ('memory', 'NN'), ('device', 'NN'), ('store', 'NN'), ('input', 'NN'), ('including', 'VBG'), ('input', 'JJ'), ('interface', 'NN'), ('receive', 'JJ'), ('input', 'NN'), ('pre-', 'JJ'), ('model', 'NN'), ('input', 'JJ'), ('yield', 'NN'), ('multi-channel', 'JJ'), ('feature', 'NN'), ('extractor', 'NN'), ('extract', 'NN'), ('set', 'VBN'), ('features', 'NNS'), ('based', 'VBN'), ('multi-channel', 'JJ'), ('feature', 'NN'), ('selector', 'NN'), ('select', 'VBP'), ('one', 'CD'), ('features', 'VBZ'), ('set', 'VBN'), ('features', 'NNS'), ('multi-channel', 'JJ'), ('wherein', 'VBP'), ('one', 'CD'), ('features', 'NNS'), ('selected', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('feature', 'VBP'), ('matcher', 'JJR'), ('match', 'NN'), ('one', 'CD'), ('features', 'NNS'), ('learned', 'VBD'), ('feature', 'NN'), ('set', 'VBN'), ('similarity', 'NN'), ('detector', 'NN'), ('determine', 'NN'), ('whether', 'IN'), ('one', 'CD'), ('features', 'VBZ'), ('meet', 'RBS'), ('pre-defined', 'JJ'), ('similarity', 'NN'), ('threshold', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('pre-', 'JJ'), ('activate', 'NN'), ('one', 'CD'), ('channels', 'VBZ'), ('multi-channel', 'JJ'), ('yield', 'NN'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('system', 'NN'), ('wherein', 'VBP'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('determined', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('system', 'NN'), ('wherein', 'VBD'), ('pre-', 'JJ'), ('activate', 'NN'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'VBZ'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('system', 'NN'), ('wherein', 'VBP'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'NNS'), ('determined', 'VBD'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('system', 'NN'), ('wherein', 'JJ'), ('feature', 'NN'), ('matcher', 'NN'), ('utilize', 'JJ'), ('large-scale', 'JJ'), ('data', 'NNS'), ('learning', 'VBG'), ('process', 'NN'), ('perform', 'NN'), ('feature', 'NN'), ('matching', 'VBG'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('input', 'JJ'), ('interface', 'NN'), ('receive', 'JJ'), ('input', 'NN'), ('pre-', 'JJ'), ('model', 'NN'), ('input', 'JJ'), ('yield', 'NN'), ('multi-channel', 'JJ'), ('feature', 'NN'), ('extractor', 'NN'), ('extract', 'NN'), ('set', 'VBN'), ('features', 'NNS'), ('based', 'VBN'), ('multi-channel', 'JJ'), ('feature', 'NN'), ('selector', 'NN'), ('select', 'VBP'), ('one', 'CD'), ('features', 'VBZ'), ('set', 'VBN'), ('features', 'NNS'), ('multi-channel', 'JJ'), ('wherein', 'VBP'), ('one', 'CD'), ('features', 'NNS'), ('selected', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('feature', 'VBP'), ('matcher', 'JJR'), ('match', 'NN'), ('one', 'CD'), ('features', 'NNS'), ('learned', 'VBD'), ('feature', 'NN'), ('set', 'VBN'), ('similarity', 'NN'), ('detector', 'NN'), ('determine', 'NN'), ('whether', 'IN'), ('one', 'CD'), ('features', 'VBZ'), ('meet', 'RBS'), ('pre-defined', 'JJ'), ('similarity', 'NN'), ('threshold', 'NN'), ('apparatus', 'NN'), ('wherein', 'VBD'), ('pre-', 'JJ'), ('activate', 'NN'), ('one', 'CD'), ('channels', 'VBZ'), ('multi-channel', 'JJ'), ('yield', 'NN'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('apparatus', 'VBP'), ('wherein', 'IN'), ('one', 'CD'), ('activated', 'JJ'), ('channels', 'NNS'), ('determined', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('apparatus', 'VBP'), ('wherein', 'JJ'), ('pre-', 'JJ'), ('activate', 'NN'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'VBZ'), ('one', 'CD'), ('activated', 'JJ'), ('channels', 'NNS'), ('apparatus', 'VBP'), ('wherein', 'IN'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'NNS'), ('determined', 'VBD'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('apparatus', 'VBP'), ('wherein', 'JJ'), ('feature', 'NN'), ('matcher', 'NN'), ('utilize', 'JJ'), ('large-scale', 'JJ'), ('data', 'NNS'), ('learning', 'VBG'), ('process', 'NN'), ('perform', 'NN'), ('feature', 'NN'), ('matching', 'VBG'), ('comprising', 'VBG'), ('modeling', 'VBG'), ('input', 'JJ'), ('yield', 'NN'), ('multi-channel', 'RB'), ('extracting', 'VBG'), ('set', 'VBN'), ('features', 'NNS'), ('based', 'VBN'), ('multi-channel', 'NNS'), ('selecting', 'VBG'), ('one', 'CD'), ('features', 'VBZ'), ('set', 'VBN'), ('features', 'NNS'), ('multi-channel', 'JJ'), ('wherein', 'VBP'), ('one', 'CD'), ('features', 'NNS'), ('selected', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('matching', 'VBG'), ('one', 'CD'), ('features', 'NNS'), ('learned', 'VBD'), ('feature', 'NN'), ('set', 'VBN'), ('determining', 'VBG'), ('whether', 'IN'), ('one', 'CD'), ('features', 'VBZ'), ('meet', 'RBS'), ('pre-defined', 'JJ'), ('similarity', 'NN'), ('threshold', 'NN'), ('wherein', 'NN'), ('modeling', 'VBG'), ('input', 'NN'), ('include', 'VBP'), ('activating', 'VBG'), ('one', 'CD'), ('channels', 'NNS'), ('multi-channel', 'JJ'), ('yield', 'NN'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('wherein', 'VBP'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('determined', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('wherein', 'VBP'), ('extracting', 'VBG'), ('features', 'NNS'), ('input', 'VBP'), ('include', 'VBP'), ('activating', 'VBG'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'VBZ'), ('one', 'CD'), ('activated', 'JJ'), ('channels', 'NNS'), ('wherein', 'VBP'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'NNS'), ('determined', 'VBD'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('wherein', 'VBP'), ('feature', 'NN'), ('matcher', 'NN'), ('utilizes', 'JJ'), ('large-scale', 'JJ'), ('data', 'NNS'), ('learning', 'VBG'), ('process', 'NN'), ('perform', 'NN'), ('feature', 'NN'), ('matching', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('comprising', 'VBG'), ('set', 'VBN'), ('instructions', 'NNS'), ('executed', 'VBD'), ('computing', 'VBG'), ('device', 'NN'), ('cause', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('model', 'NN'), ('input', 'JJ'), ('yield', 'NN'), ('multi-channel', 'JJ'), ('extract', 'NN'), ('set', 'VBN'), ('features', 'NNS'), ('based', 'VBN'), ('multi-channel', 'NNS'), ('select', 'VBP'), ('one', 'CD'), ('features', 'VBZ'), ('set', 'VBN'), ('features', 'NNS'), ('multi-channel', 'JJ'), ('wherein', 'NN'), ('features', 'NNS'), ('selected', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('match', 'VBP'), ('one', 'CD'), ('features', 'NNS'), ('learned', 'VBD'), ('feature', 'NN'), ('set', 'VBN'), ('determine', 'NN'), ('whether', 'IN'), ('one', 'CD'), ('features', 'VBZ'), ('meet', 'RBS'), ('pre-defined', 'JJ'), ('similarity', 'NN'), ('threshold', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('instructions', 'NNS'), ('executed', 'VBD'), ('cause', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('activate', 'VBP'), ('one', 'CD'), ('channels', 'NNS'), ('multi-channel', 'JJ'), ('yield', 'NN'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('instructions', 'NNS'), ('executed', 'VBD'), ('cause', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('determine', 'NN'), ('one', 'CD'), ('activated', 'VBN'), ('channels', 'NNS'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('extracting', 'VBG'), ('features', 'NNS'), ('input', 'VBP'), ('include', 'VBP'), ('activating', 'VBG'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'VBZ'), ('one', 'CD'), ('activated', 'JJ'), ('channels', 'NNS'), ('least', 'JJS'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'VBP'), ('one', 'CD'), ('local', 'JJ'), ('patches', 'NNS'), ('determined', 'VBD'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'JJ'), ('feature', 'NN'), ('matcher', 'NN'), ('utilize', 'JJ'), ('large-scale', 'JJ'), ('data', 'NNS'), ('learning', 'VBG'), ('process', 'NN'), ('perform', 'NN'), ('feature', 'NN'), ('matching', 'VBG'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('means', 'NNS'), ('modeling', 'VBG'), ('input', 'JJ'), ('yield', 'NN'), ('multi-channel', 'NN'), ('means', 'NNS'), ('extracting', 'VBG'), ('set', 'NN'), ('features', 'NNS'), ('based', 'VBN'), ('multi-channel', 'NNS'), ('means', 'VBZ'), ('selecting', 'VBG'), ('one', 'CD'), ('features', 'VBZ'), ('set', 'VBN'), ('features', 'NNS'), ('multi-channel', 'JJ'), ('wherein', 'VBP'), ('one', 'CD'), ('features', 'NNS'), ('selected', 'VBN'), ('based', 'VBN'), ('ability', 'NN'), ('differentiate', 'NN'), ('features', 'NNS'), ('means', 'VBZ'), ('matching', 'VBG'), ('one', 'CD'), ('features', 'NNS'), ('learned', 'VBD'), ('feature', 'NN'), ('set', 'VBN'), ('means', 'VBZ'), ('determining', 'VBG'), ('whether', 'IN'), ('one', 'CD'), ('features', 'VBZ'), ('meet', 'RBS'), ('pre-defined', 'JJ'), ('similarity', 'NN'), ('threshold', 'NN'), ('controlling', 'VBG'), ('terminal', 'JJ'), ('terminal', 'JJ'), ('comprising', 'VBG'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('comprising', 'VBG'), ('acquiring', 'VBG'), ('capturing', 'VBG'), ('apparatus', 'JJ'), ('obtaining', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('motion', 'NN'), ('parameter', 'NN'), ('terminal', 'JJ'), ('motion', 'NN'), ('parameter', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('two', 'CD'), ('parameters', 'NNS'), ('among', 'IN'), ('acceleration', 'NN'), ('angular', 'JJ'), ('velocity', 'NN'), ('motion', 'NN'), ('amplitude', 'NN'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('transmitting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('parameter', 'NN'), ('threshold', 'NN'), ('obtaining', 'VBG'), ('request', 'NN'), ('data', 'NNS'), ('management', 'NN'), ('server', 'RB'), ('parameter', 'RB'), ('threshold', 'JJ'), ('obtaining', 'VBG'), ('request', 'NN'), ('comprising', 'VBG'), ('configuration', 'NN'), ('information', 'NN'), ('terminal', 'NN'), ('receiving', 'NN'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('correspond', 'NN'), ('configuration', 'NN'), ('information', 'NN'), ('response', 'NN'), ('parameter', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('request', 'NN'), ('comparing', 'VBG'), ('two', 'CD'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('controlling', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('perform', 'NN'), ('processing', 'NN'), ('acquired', 'VBD'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'NN'), ('based', 'VBN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('respectively', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('wherein', 'VBP'), ('acquiring', 'VBG'), ('comprises', 'NNS'), ('acquiring', 'VBG'), ('real', 'JJ'), ('time', 'NN'), ('obtaining', 'VBG'), ('comprises', 'NNS'), ('obtaining', 'VBG'), ('motion', 'NN'), ('parameter', 'NN'), ('terminal', 'JJ'), ('real', 'JJ'), ('time', 'NN'), ('comprising', 'VBG'), ('response', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('motion', 'NN'), ('parameter', 'FW'), ('terminal', 'JJ'), ('response', 'NN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('obtained', 'VBN'), ('latest', 'JJS'), ('time', 'NN'), ('less', 'CC'), ('equal', 'JJ'), ('corresponding', 'NN'), ('preset', 'NN'), ('thresholds', 'NNS'), ('performing', 'VBG'), ('processing', 'NN'), ('acquired', 'VBD'), ('latest', 'JJS'), ('time', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('acquiring', 'VBG'), ('comprises', 'NNS'), ('controlling', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('turn', 'NN'), ('capturing', 'VBG'), ('apparatus', 'NNS'), ('based', 'VBN'), ('face', 'NN'), ('instruction', 'NN'), ('acquiring', 'VBG'), ('capturing', 'VBG'), ('apparatus', 'JJ'), ('face', 'NN'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('turned', 'VBD'), ('according', 'VBG'), ('wherein', 'NN'), ('controlling', 'VBG'), ('perform', 'NN'), ('processing', 'NN'), ('comprises', 'VBZ'), ('skipping', 'VBG'), ('performing', 'VBG'), ('face', 'NN'), ('acquired', 'VBD'), ('face', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'NN'), ('based', 'VBN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('respectively', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('according', 'VBG'), ('wherein', 'NN'), ('obtaining', 'VBG'), ('comprises', 'NNS'), ('least', 'JJS'), ('one', 'CD'), ('obtaining', 'VBG'), ('acceleration', 'NN'), ('terminal', 'NN'), ('using', 'VBG'), ('acceleration', 'NN'), ('sensor', 'NN'), ('obtaining', 'VBG'), ('angular', 'JJ'), ('velocity', 'NN'), ('terminal', 'NN'), ('using', 'VBG'), ('gyro', 'NN'), ('sensor', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('transmitting', 'NN'), ('comprises', 'NNS'), ('transmitting', 'VBG'), ('parameter', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('request', 'NN'), ('data', 'NNS'), ('management', 'NN'), ('server', 'NN'), ('according', 'VBG'), ('preset', 'JJ'), ('time', 'NN'), ('period', 'NN'), ('according', 'VBG'), ('comprising', 'VBG'), ('generating', 'VBG'), ('prompt', 'JJ'), ('information', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'VBD'), ('prompt', 'JJ'), ('information', 'NN'), ('used', 'VBN'), ('prompting', 'VBG'), ('terminal', 'JJ'), ('stop', 'NN'), ('moving', 'VBG'), ('according', 'VBG'), ('wherein', 'JJ'), ('motion', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('terminal', 'JJ'), ('comprising', 'VBG'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('memory', 'NN'), ('configured', 'VBD'), ('store', 'NN'), ('program', 'NN'), ('code', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('configured', 'JJ'), ('access', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('memory', 'NN'), ('operate', 'NN'), ('according', 'VBG'), ('program', 'NN'), ('code', 'NN'), ('program', 'NN'), ('code', 'NN'), ('comprising', 'VBG'), ('motion', 'NN'), ('parameter', 'NN'), ('obtaining', 'VBG'), ('code', 'NN'), ('configured', 'VBN'), ('cause', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('acquire', 'VB'), ('using', 'VBG'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('obtain', 'VB'), ('motion', 'NN'), ('parameter', 'NN'), ('terminal', 'JJ'), ('motion', 'NN'), ('parameter', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('two', 'CD'), ('parameters', 'NNS'), ('among', 'IN'), ('acceleration', 'NN'), ('angular', 'JJ'), ('velocity', 'NN'), ('motion', 'NN'), ('amplitude', 'NN'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('request', 'NN'), ('transmitting', 'VBG'), ('code', 'NN'), ('configured', 'VBN'), ('cause', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('transmit', 'NN'), ('parameter', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('request', 'NN'), ('data', 'NNS'), ('management', 'NN'), ('server', 'RB'), ('parameter', 'RB'), ('threshold', 'JJ'), ('obtaining', 'VBG'), ('request', 'NN'), ('comprising', 'VBG'), ('configuration', 'NN'), ('information', 'NN'), ('terminal', 'NN'), ('parameter', 'NN'), ('threshold', 'VBD'), ('receiving', 'VBG'), ('code', 'NN'), ('configured', 'VBN'), ('cause', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('receive', 'NN'), ('corresponding', 'NN'), ('preset', 'NN'), ('thresholds', 'NNS'), ('correspond', 'NN'), ('configuration', 'NN'), ('information', 'NN'), ('response', 'NN'), ('parameter', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('request', 'NN'), ('comparing', 'VBG'), ('code', 'NN'), ('configured', 'VBN'), ('cause', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('compare', 'NN'), ('two', 'CD'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('control', 'NN'), ('code', 'NN'), ('configured', 'VBN'), ('cause', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('perform', 'NN'), ('processing', 'NN'), ('acquired', 'VBD'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'NN'), ('based', 'VBN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('respectively', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('wherein', 'VBP'), ('motion', 'NN'), ('parameter', 'NN'), ('obtaining', 'VBG'), ('code', 'NN'), ('causes', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('acquire', 'VB'), ('real', 'JJ'), ('time', 'NN'), ('obtain', 'VB'), ('motion', 'NN'), ('parameter', 'IN'), ('terminal', 'JJ'), ('real', 'JJ'), ('time', 'NN'), ('response', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'VBN'), ('threshold', 'JJ'), ('obtain', 'VB'), ('motion', 'NN'), ('parameter', 'NN'), ('terminal', 'JJ'), ('wherein', 'NN'), ('control', 'NN'), ('code', 'NN'), ('causes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('response', 'NN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('obtained', 'VBN'), ('latest', 'JJS'), ('time', 'NN'), ('less', 'CC'), ('equal', 'JJ'), ('corresponding', 'NN'), ('preset', 'NN'), ('thresholds', 'NNS'), ('perform', 'VBP'), ('processing', 'VBG'), ('acquired', 'VBN'), ('latest', 'JJS'), ('time', 'NN'), ('terminal', 'JJ'), ('according', 'VBG'), ('wherein', 'JJ'), ('program', 'NN'), ('code', 'NN'), ('comprises', 'VBZ'), ('face', 'VBP'), ('instruction', 'NN'), ('receiving', 'VBG'), ('code', 'NN'), ('configured', 'VBN'), ('cause', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('receive', 'JJ'), ('face', 'NN'), ('instruction', 'NN'), ('wherein', 'WRB'), ('motion', 'NN'), ('parameter', 'NN'), ('obtaining', 'VBG'), ('code', 'NN'), ('causes', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('control', 'NN'), ('according', 'VBG'), ('face', 'NN'), ('instruction', 'NN'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('turn', 'NN'), ('acquire', 'VB'), ('face', 'NN'), ('using', 'VBG'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('turned', 'VBD'), ('wherein', 'JJ'), ('control', 'NN'), ('code', 'NN'), ('causes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('skip', 'NN'), ('performing', 'VBG'), ('face', 'NN'), ('acquired', 'VBD'), ('face', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'NN'), ('based', 'VBN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('respectively', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('terminal', 'JJ'), ('according', 'VBG'), ('wherein', 'JJ'), ('request', 'NN'), ('transmitting', 'VBG'), ('code', 'NN'), ('causes', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('transmit', 'NN'), ('parameter', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('request', 'NN'), ('data', 'NNS'), ('management', 'NN'), ('server', 'NN'), ('according', 'VBG'), ('preset', 'JJ'), ('time', 'NN'), ('period', 'NN'), ('terminal', 'JJ'), ('according', 'VBG'), ('wherein', 'JJ'), ('program', 'NN'), ('code', 'NN'), ('comprises', 'VBZ'), ('prompt', 'JJ'), ('information', 'NN'), ('generation', 'NN'), ('code', 'NN'), ('configured', 'VBN'), ('cause', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('generate', 'NN'), ('prompt', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'VBD'), ('prompt', 'JJ'), ('information', 'NN'), ('used', 'VBN'), ('prompting', 'VBG'), ('terminal', 'JJ'), ('stop', 'NN'), ('moving', 'VBG'), ('terminal', 'JJ'), ('according', 'VBG'), ('wherein', 'JJ'), ('motion', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('storing', 'VBG'), ('machine', 'NN'), ('instruction', 'NN'), ('executed', 'VBD'), ('one', 'CD'), ('causes', 'VBZ'), ('one', 'CD'), ('perform', 'NN'), ('obtaining', 'VBG'), ('acquired', 'VBD'), ('capturing', 'VBG'), ('apparatus', 'NN'), ('obtaining', 'VBG'), ('motion', 'NN'), ('parameter', 'NN'), ('terminal', 'JJ'), ('terminal', 'NN'), ('comprising', 'VBG'), ('capturing', 'VBG'), ('apparatus', 'JJ'), ('motion', 'NN'), ('parameter', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('two', 'CD'), ('parameters', 'NNS'), ('among', 'IN'), ('acceleration', 'NN'), ('angular', 'JJ'), ('velocity', 'NN'), ('motion', 'NN'), ('amplitude', 'NN'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('transmitting', 'VBG'), ('parameter', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('request', 'NN'), ('data', 'NNS'), ('management', 'NN'), ('server', 'RB'), ('parameter', 'RB'), ('threshold', 'JJ'), ('obtaining', 'VBG'), ('request', 'NN'), ('comprising', 'VBG'), ('configuration', 'NN'), ('information', 'NN'), ('terminal', 'NN'), ('receiving', 'NN'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('correspond', 'NN'), ('configuration', 'NN'), ('information', 'NN'), ('response', 'NN'), ('parameter', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('request', 'NN'), ('comparing', 'VBG'), ('two', 'CD'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('controlling', 'VBG'), ('perform', 'NN'), ('processing', 'NN'), ('acquired', 'VBD'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'NN'), ('based', 'VBN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('respectively', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('thresholds', 'NNS'), ('wherein', 'VBP'), ('acquiring', 'VBG'), ('comprises', 'NNS'), ('acquiring', 'VBG'), ('real', 'JJ'), ('time', 'NN'), ('obtaining', 'VBG'), ('comprises', 'NNS'), ('obtaining', 'VBG'), ('motion', 'NN'), ('parameter', 'NN'), ('terminal', 'JJ'), ('real', 'JJ'), ('time', 'NN'), ('comprising', 'VBG'), ('response', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'RB'), ('greater', 'JJR'), ('corresponding', 'VBG'), ('preset', 'NN'), ('threshold', 'VBD'), ('obtaining', 'VBG'), ('motion', 'NN'), ('parameter', 'FW'), ('terminal', 'JJ'), ('response', 'NN'), ('two', 'CD'), ('parameters', 'NNS'), ('motion', 'VBP'), ('parameter', 'NN'), ('obtained', 'VBN'), ('latest', 'JJS'), ('time', 'NN'), ('less', 'CC'), ('equal', 'JJ'), ('corresponding', 'NN'), ('preset', 'NN'), ('thresholds', 'NNS'), ('performing', 'VBG'), ('processing', 'NN'), ('acquired', 'VBD'), ('latest', 'JJS'), ('time', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('acquired', 'VBD'), ('face', 'NN'), ('processing', 'NN'), ('comprises', 'VBZ'), ('performing', 'VBG'), ('face', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('obtaining', 'VBG'), ('motion', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('obtaining', 'VBG'), ('acceleration', 'NN'), ('terminal', 'NN'), ('using', 'VBG'), ('acceleration', 'NN'), ('sensor', 'NN'), ('obtaining', 'VBG'), ('angular', 'JJ'), ('velocity', 'NN'), ('terminal', 'NN'), ('using', 'VBG'), ('gyro', 'JJ'), ('sensor', 'JJ'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('motion', 'NN'), ('parameter', 'NN'), ('comprises', 'VBZ'), ('motion', 'NN'), ('frequency', 'NN'), ('motion', 'NN'), ('time', 'NN'), ('processing', 'VBG'), ('drive-through', 'JJ'), ('order', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('customer', 'NN'), ('information', 'NN'), ('detected', 'VBN'), ('vision', 'NN'), ('providing', 'VBG'), ('product', 'NN'), ('information', 'NN'), ('customer', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('processing', 'VBG'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('receiving', 'VBG'), ('customer', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('receiving', 'VBG'), ('customer', 'NN'), ('information', 'NN'), ('associated', 'VBN'), ('vehicle', 'NN'), ('information', 'NN'), ('detected', 'VBD'), ('vehicle', 'NN'), ('receiving', 'VBG'), ('customer', 'NN'), ('information', 'NN'), ('associated', 'VBN'), ('identification', 'NN'), ('information', 'NN'), ('detected', 'VBD'), ('face', 'NN'), ('according', 'VBG'), ('comprising', 'VBG'), ('determining', 'VBG'), ('whether', 'IN'), ('customer', 'NN'), ('pre-order', 'NN'), ('customer', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('wherein', 'NN'), ('customer', 'NN'), ('determined', 'VBD'), ('pre-order', 'JJ'), ('customer', 'NN'), ('providing', 'VBG'), ('product', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('providing', 'VBG'), ('pre-order', 'JJ'), ('information', 'NN'), ('using', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('audio', 'JJ'), ('video', 'NN'), ('processing', 'VBG'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('comprises', 'VBZ'), ('providing', 'VBG'), ('information', 'NN'), ('promptly', 'RB'), ('guiding', 'VBG'), ('vehicle', 'NN'), ('pickup', 'NN'), ('stand', 'VBP'), ('using', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('audio', 'NN'), ('video', 'NN'), ('providing', 'VBG'), ('information', 'NN'), ('additional', 'JJ'), ('order', 'NN'), ('available', 'JJ'), ('according', 'VBG'), ('wherein', 'JJ'), ('product', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('recently', 'RB'), ('ordered', 'VBN'), ('product', 'NN'), ('component', 'NN'), ('frequently', 'RB'), ('ordered', 'VBD'), ('product', 'NN'), ('component', 'NN'), ('order', 'NN'), ('history', 'NN'), ('customer', 'NN'), ('information', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('receiving', 'VBG'), ('customer', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('receiving', 'VBG'), ('information', 'NN'), ('age', 'NN'), ('gender', 'NN'), ('passenger', 'NN'), ('detected', 'VBD'), ('face', 'NN'), ('providing', 'VBG'), ('product', 'NN'), ('information', 'NN'), ('customer', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('providing', 'VBG'), ('recommended', 'VBD'), ('menu', 'JJ'), ('information', 'NN'), ('differentiated', 'VBD'), ('according', 'VBG'), ('age', 'NN'), ('gender', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('processing', 'NN'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('product', 'NN'), ('component', 'NN'), ('past', 'JJ'), ('order', 'NN'), ('history', 'NN'), ('component', 'NN'), ('modified', 'VBD'), ('product', 'NN'), ('component', 'NN'), ('product', 'NN'), ('order', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('processing', 'NN'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('comprises', 'VBZ'), ('paying', 'VBG'), ('product', 'NN'), ('price', 'NN'), ('according', 'VBG'), ('biometrics-based', 'JJ'), ('authentication', 'NN'), ('communication', 'NN'), ('system', 'NN'), ('vehicle', 'NN'), ('mobile', 'IN'), ('terminal', 'JJ'), ('according', 'VBG'), ('wherein', 'JJ'), ('processing', 'NN'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('comprises', 'VBZ'), ('issuing', 'VBG'), ('payment', 'NN'), ('number', 'NN'), ('divided', 'VBN'), ('payment', 'NN'), ('performing', 'VBG'), ('divided', 'VBD'), ('payments', 'NNS'), ('according', 'VBG'), ('payment', 'NN'), ('requests', 'NNS'), ('mobile', 'VBP'), ('terminals', 'NNS'), ('payment', 'NN'), ('numbers', 'NNS'), ('inputted', 'VBD'), ('according', 'VBG'), ('wherein', 'NN'), ('processing', 'NN'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('comprises', 'VBZ'), ('accumulating', 'VBG'), ('mileage', 'NN'), ('account', 'NN'), ('corresponding', 'VBG'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('undergoing', 'JJ'), ('payment', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('processing', 'NN'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('comprises', 'VBZ'), ('suggesting', 'VBG'), ('takeout', 'RP'), ('packaging', 'VBG'), ('according', 'VBG'), ('temperature', 'NN'), ('product', 'NN'), ('atmospheric', 'JJ'), ('temperature', 'NN'), ('weather', 'NN'), ('vehicle', 'NN'), ('type', 'NN'), ('apparatus', 'NN'), ('configured', 'VBD'), ('process', 'JJ'), ('drive-through', 'JJ'), ('order', 'NN'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('transceiver', 'RB'), ('configured', 'VBN'), ('receive', 'JJ'), ('customer', 'NN'), ('information', 'NN'), ('detected', 'VBN'), ('vision', 'NN'), ('digital', 'JJ'), ('signage', 'NN'), ('configured', 'VBD'), ('provide', 'JJ'), ('product', 'NN'), ('information', 'NN'), ('customer', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('configured', 'VBD'), ('process', 'NN'), ('product', 'NN'), ('order', 'NN'), ('customer', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('transceiver', 'NN'), ('receives', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('customer', 'NN'), ('information', 'NN'), ('associated', 'VBN'), ('vehicle', 'NN'), ('information', 'NN'), ('detected', 'VBD'), ('vehicle', 'NN'), ('customer', 'NN'), ('information', 'NN'), ('associated', 'VBN'), ('identification', 'NN'), ('information', 'NN'), ('detected', 'VBD'), ('face', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('configured', 'VBD'), ('determine', 'NN'), ('whether', 'IN'), ('customer', 'NN'), ('pre-order', 'NN'), ('customer', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('customer', 'NN'), ('determined', 'VBD'), ('pre-order', 'JJ'), ('customer', 'NN'), ('perform', 'NN'), ('control', 'NN'), ('operation', 'NN'), ('provide', 'IN'), ('pre-order', 'JJ'), ('information', 'NN'), ('control', 'NN'), ('digital', 'JJ'), ('signage', 'NN'), ('output', 'NN'), ('information', 'NN'), ('promptly', 'RB'), ('guiding', 'VBG'), ('vehicle', 'NN'), ('pickup', 'NN'), ('stand', 'VB'), ('provide', 'JJ'), ('information', 'NN'), ('additional', 'JJ'), ('order', 'NN'), ('available', 'JJ'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'JJ'), ('product', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('customer', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('recently', 'RB'), ('ordered', 'VBN'), ('product', 'NN'), ('component', 'NN'), ('frequently', 'RB'), ('ordered', 'VBD'), ('product', 'NN'), ('component', 'NN'), ('order', 'NN'), ('history', 'NN'), ('customer', 'NN'), ('information', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('transceiver', 'NN'), ('configured', 'VBD'), ('receive', 'JJ'), ('information', 'NN'), ('age', 'NN'), ('gender', 'NN'), ('passenger', 'NN'), ('detected', 'VBD'), ('face', 'NN'), ('configured', 'VBN'), ('control', 'NN'), ('digital', 'JJ'), ('signage', 'NN'), ('provide', 'NN'), ('recommended', 'VBD'), ('menu', 'JJ'), ('information', 'NN'), ('differentiated', 'VBD'), ('according', 'VBG'), ('age', 'NN'), ('gender', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('configured', 'VBD'), ('determine', 'JJ'), ('product', 'NN'), ('component', 'NN'), ('past', 'JJ'), ('order', 'NN'), ('history', 'NN'), ('component', 'NN'), ('modified', 'VBD'), ('product', 'NN'), ('component', 'NN'), ('product', 'NN'), ('order', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('configured', 'JJ'), ('pay', 'NN'), ('product', 'NN'), ('price', 'NN'), ('according', 'VBG'), ('biometrics-based', 'JJ'), ('authentication', 'NN'), ('communication', 'NN'), ('system', 'NN'), ('vehicle', 'NN'), ('mobile', 'JJ'), ('terminal', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NN'), ('configured', 'VBD'), ('issue', 'NN'), ('payment', 'NN'), ('number', 'NN'), ('divided', 'VBN'), ('payment', 'NN'), ('perform', 'NN'), ('divided', 'VBD'), ('payments', 'NNS'), ('according', 'VBG'), ('requests', 'NNS'), ('mobile', 'JJ'), ('terminals', 'NNS'), ('payment', 'NN'), ('numbers', 'NNS'), ('inputted', 'VBD'), ('apparatus', 'RP'), ('according', 'VBG'), ('wherein', 'NN'), ('configured', 'VBD'), ('accumulate', 'JJ'), ('mileage', 'NN'), ('account', 'NN'), ('corresponding', 'VBG'), ('mobile', 'JJ'), ('terminal', 'JJ'), ('undergoing', 'JJ'), ('payment', 'NN'), ('apparatus', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('configured', 'VBD'), ('control', 'NN'), ('digital', 'JJ'), ('signage', 'NN'), ('suggest', 'VBP'), ('takeout', 'IN'), ('packaging', 'VBG'), ('according', 'VBG'), ('temperature', 'NN'), ('product', 'NN'), ('atmospheric', 'JJ'), ('temperature', 'NN'), ('weather', 'NN'), ('vehicle', 'NN'), ('type', 'NN'), ('information', 'NN'), ('processing', 'NN'), ('performed', 'VBD'), ('computing', 'VBG'), ('device', 'NN'), ('one', 'CD'), ('memory', 'NN'), ('storing', 'VBG'), ('programs', 'NNS'), ('executed', 'VBD'), ('one', 'CD'), ('comprising', 'VBG'), ('identifying', 'VBG'), ('using', 'VBG'), ('face', 'NN'), ('one', 'CD'), ('face', 'NN'), ('corresponding', 'VBG'), ('respective', 'JJ'), ('person', 'NN'), ('captured', 'VBD'), ('first', 'RB'), ('identified', 'VBN'), ('face', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('selecting', 'VBG'), ('tiles', 'NNS'), ('first', 'RB'), ('tile', 'JJ'), ('matches', 'NNS'), ('face', 'VBP'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'JJ'), ('accordance', 'NN'), ('predefined', 'VBD'), ('correspondence', 'NN'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('set', 'VBN'), ('pre-stored', 'JJ'), ('description', 'NN'), ('parameters', 'NNS'), ('first', 'RB'), ('tile', 'IN'), ('generating', 'VBG'), ('second', 'JJ'), ('covering', 'VBG'), ('respective', 'JJ'), ('persons', 'NNS'), ('first', 'RB'), ('corresponding', 'VBG'), ('first', 'JJ'), ('tiles', 'NNS'), ('sharing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('predefined', 'VBN'), ('order', 'NN'), ('via', 'IN'), ('group', 'NN'), ('chat', 'WP'), ('session', 'NN'), ('wherein', 'VBD'), ('first', 'JJ'), ('second', 'JJ'), ('displayed', 'VBN'), ('group', 'NN'), ('chat', 'DT'), ('session', 'NN'), ('one', 'CD'), ('time', 'NN'), ('one', 'CD'), ('two', 'CD'), ('replaced', 'VBD'), ('two', 'CD'), ('periodically', 'RB'), ('wherein', 'VBP'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('includes', 'VBZ'), ('determining', 'VBG'), ('one', 'CD'), ('descriptive', 'JJ'), ('labels', 'NNS'), ('corresponding', 'VBG'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('using', 'VBG'), ('first', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('trained', 'VBD'), ('facial', 'JJ'), ('corresponding', 'VBG'), ('descriptive', 'JJ'), ('labels', 'NNS'), ('wherein', 'VBP'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('includes', 'VBZ'), ('determining', 'VBG'), ('identity', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('based', 'VBN'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('locating', 'VBG'), ('respective', 'JJ'), ('profile', 'NN'), ('information', 'NN'), ('first', 'RB'), ('person', 'NN'), ('based', 'VBN'), ('determined', 'VBN'), ('identity', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('using', 'VBG'), ('one', 'CD'), ('characteristics', 'NNS'), ('respective', 'JJ'), ('profile', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('person', 'NN'), ('set', 'VBN'), ('profile', 'NN'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('first', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('tiles', 'VBZ'), ('dynamic', 'JJ'), ('tile', 'JJ'), ('least', 'JJS'), ('second', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('tiles', 'VBZ'), ('static', 'JJ'), ('tile', 'NN'), ('including', 'VBG'), ('receiving', 'VBG'), ('comments', 'NNS'), ('different', 'JJ'), ('group', 'NN'), ('chat', 'WP'), ('session', 'NN'), ('comment', 'NN'), ('including', 'VBG'), ('descriptive', 'JJ'), ('term', 'NN'), ('respective', 'JJ'), ('person', 'NN'), ('identified', 'VBD'), ('first', 'JJ'), ('choosing', 'VBG'), ('descriptive', 'JJ'), ('label', 'NN'), ('respective', 'JJ'), ('person', 'NN'), ('according', 'VBG'), ('comments', 'NNS'), ('updating', 'VBG'), ('second', 'JJ'), ('adding', 'VBG'), ('descriptive', 'JJ'), ('label', 'NN'), ('adjacent', 'NN'), ('first', 'RB'), ('tile', 'RB'), ('respective', 'JJ'), ('person', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('information', 'NN'), ('processing', 'VBG'), ('comprising', 'VBG'), ('one', 'CD'), ('memory', 'NN'), ('storing', 'VBG'), ('instructions', 'NNS'), ('executed', 'VBD'), ('one', 'CD'), ('cause', 'NN'), ('perform', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('identifying', 'VBG'), ('using', 'VBG'), ('face', 'NN'), ('one', 'CD'), ('face', 'NN'), ('corresponding', 'VBG'), ('respective', 'JJ'), ('person', 'NN'), ('captured', 'VBD'), ('first', 'RB'), ('identified', 'VBN'), ('face', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('selecting', 'VBG'), ('tiles', 'NNS'), ('first', 'RB'), ('tile', 'JJ'), ('matches', 'NNS'), ('face', 'VBP'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'JJ'), ('accordance', 'NN'), ('predefined', 'VBD'), ('correspondence', 'NN'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('set', 'VBN'), ('pre-stored', 'JJ'), ('description', 'NN'), ('parameters', 'NNS'), ('first', 'RB'), ('tile', 'IN'), ('generating', 'VBG'), ('second', 'JJ'), ('covering', 'VBG'), ('respective', 'JJ'), ('persons', 'NNS'), ('first', 'RB'), ('corresponding', 'VBG'), ('first', 'JJ'), ('tiles', 'NNS'), ('sharing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('predefined', 'VBN'), ('order', 'NN'), ('via', 'IN'), ('group', 'NN'), ('chat', 'WP'), ('session', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'VBD'), ('first', 'JJ'), ('second', 'JJ'), ('displayed', 'VBN'), ('group', 'NN'), ('chat', 'DT'), ('session', 'NN'), ('one', 'CD'), ('time', 'NN'), ('one', 'CD'), ('two', 'CD'), ('replaced', 'VBD'), ('two', 'CD'), ('periodically', 'RB'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'RB'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('includes', 'VBZ'), ('determining', 'VBG'), ('one', 'CD'), ('descriptive', 'JJ'), ('labels', 'NNS'), ('corresponding', 'VBG'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('using', 'VBG'), ('first', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('trained', 'VBD'), ('facial', 'JJ'), ('corresponding', 'VBG'), ('descriptive', 'JJ'), ('labels', 'NNS'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'RB'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('includes', 'VBZ'), ('determining', 'VBG'), ('identity', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('based', 'VBN'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('locating', 'VBG'), ('respective', 'JJ'), ('profile', 'NN'), ('information', 'NN'), ('first', 'RB'), ('person', 'NN'), ('based', 'VBN'), ('determined', 'VBN'), ('identity', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('using', 'VBG'), ('one', 'CD'), ('characteristics', 'NNS'), ('respective', 'JJ'), ('profile', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('person', 'NN'), ('set', 'VBN'), ('profile', 'NN'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('least', 'VBD'), ('first', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('tiles', 'VBZ'), ('dynamic', 'JJ'), ('tile', 'JJ'), ('least', 'JJS'), ('second', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('tiles', 'VBZ'), ('static', 'JJ'), ('tile', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('wherein', 'NN'), ('operations', 'NNS'), ('include', 'VBP'), ('receiving', 'VBG'), ('comments', 'NNS'), ('different', 'JJ'), ('group', 'NN'), ('chat', 'WP'), ('session', 'NN'), ('comment', 'NN'), ('including', 'VBG'), ('descriptive', 'JJ'), ('term', 'NN'), ('respective', 'JJ'), ('person', 'NN'), ('identified', 'VBD'), ('first', 'JJ'), ('choosing', 'VBG'), ('descriptive', 'JJ'), ('label', 'NN'), ('respective', 'JJ'), ('person', 'NN'), ('according', 'VBG'), ('comments', 'NNS'), ('updating', 'VBG'), ('second', 'JJ'), ('adding', 'VBG'), ('descriptive', 'JJ'), ('label', 'NN'), ('adjacent', 'NN'), ('first', 'RB'), ('tile', 'RB'), ('respective', 'JJ'), ('person', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('storing', 'VBG'), ('instructions', 'NNS'), ('executed', 'VBD'), ('computing', 'VBG'), ('device', 'NN'), ('one', 'CD'), ('cause', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('perform', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('identifying', 'VBG'), ('using', 'VBG'), ('face', 'NN'), ('one', 'CD'), ('face', 'NN'), ('corresponding', 'VBG'), ('respective', 'JJ'), ('person', 'NN'), ('captured', 'VBD'), ('first', 'RB'), ('identified', 'VBN'), ('face', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('selecting', 'VBG'), ('tiles', 'NNS'), ('first', 'RB'), ('tile', 'JJ'), ('matches', 'NNS'), ('face', 'VBP'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'JJ'), ('accordance', 'NN'), ('predefined', 'VBD'), ('correspondence', 'NN'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('set', 'VBN'), ('pre-stored', 'JJ'), ('description', 'NN'), ('parameters', 'NNS'), ('first', 'RB'), ('tile', 'IN'), ('generating', 'VBG'), ('second', 'JJ'), ('covering', 'VBG'), ('respective', 'JJ'), ('persons', 'NNS'), ('first', 'RB'), ('corresponding', 'VBG'), ('first', 'JJ'), ('tiles', 'NNS'), ('sharing', 'VBG'), ('first', 'JJ'), ('second', 'JJ'), ('predefined', 'VBN'), ('order', 'NN'), ('via', 'IN'), ('group', 'NN'), ('chat', 'DT'), ('session', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('second', 'JJ'), ('displayed', 'VBN'), ('group', 'NN'), ('chat', 'DT'), ('session', 'NN'), ('one', 'CD'), ('time', 'NN'), ('one', 'CD'), ('two', 'CD'), ('replaced', 'VBD'), ('two', 'CD'), ('periodically', 'RB'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('includes', 'VBZ'), ('determining', 'VBG'), ('one', 'CD'), ('descriptive', 'JJ'), ('labels', 'NNS'), ('corresponding', 'VBG'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('using', 'VBG'), ('first', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('trained', 'VBD'), ('facial', 'JJ'), ('corresponding', 'VBG'), ('descriptive', 'JJ'), ('labels', 'NNS'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('profile', 'JJ'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('person', 'NN'), ('first', 'RB'), ('includes', 'VBZ'), ('determining', 'VBG'), ('identity', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('based', 'VBN'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('locating', 'VBG'), ('respective', 'JJ'), ('profile', 'NN'), ('information', 'NN'), ('first', 'RB'), ('person', 'NN'), ('based', 'VBN'), ('determined', 'VBN'), ('identity', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('using', 'VBG'), ('one', 'CD'), ('characteristics', 'NNS'), ('respective', 'JJ'), ('profile', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('person', 'NN'), ('set', 'VBN'), ('profile', 'NN'), ('parameters', 'NNS'), ('corresponding', 'VBG'), ('identified', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('person', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'NN'), ('least', 'VBD'), ('first', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('tiles', 'VBZ'), ('dynamic', 'JJ'), ('tile', 'JJ'), ('least', 'JJS'), ('second', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('tiles', 'VBZ'), ('static', 'JJ'), ('tile', 'IN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('wherein', 'JJ'), ('operations', 'NNS'), ('include', 'VBP'), ('receiving', 'VBG'), ('comments', 'NNS'), ('different', 'JJ'), ('group', 'NN'), ('chat', 'WP'), ('session', 'NN'), ('comment', 'NN'), ('including', 'VBG'), ('descriptive', 'JJ'), ('term', 'NN'), ('respective', 'JJ'), ('person', 'NN'), ('identified', 'VBD'), ('first', 'JJ'), ('choosing', 'VBG'), ('descriptive', 'JJ'), ('label', 'NN'), ('respective', 'JJ'), ('person', 'NN'), ('according', 'VBG'), ('comments', 'NNS'), ('updating', 'VBG'), ('second', 'JJ'), ('adding', 'VBG'), ('descriptive', 'JJ'), ('label', 'NN'), ('adjacent', 'NN'), ('first', 'RB'), ('tile', 'RB'), ('respective', 'JJ'), ('person', 'NN'), ('comprising', 'VBG'), ('computing', 'VBG'), ('system', 'NN'), ('determining', 'VBG'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'NN'), ('wherein', 'NN'), ('eye', 'NN'), ('system', 'NN'), ('associated', 'VBN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('worn', 'NNS'), ('based', 'VBN'), ('determination', 'NN'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'VBN'), ('computer', 'NN'), ('system', 'NN'), ('performing', 'VBG'), ('receiving', 'VBG'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('estimating', 'VBG'), ('region', 'NN'), ('looking', 'VBG'), ('within', 'IN'), ('field', 'NN'), ('view', 'NN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('received', 'VBD'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('determining', 'VBG'), ('vergence', 'NN'), ('distance', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('estimated', 'VBN'), ('region', 'NN'), ('looking', 'VBG'), ('locations', 'NNS'), ('one', 'CD'), ('objects', 'VBZ'), ('scene', 'NN'), ('displayed', 'VBD'), ('head-mounted', 'JJ'), ('display', 'NN'), ('adjusting', 'VBG'), ('one', 'CD'), ('configurations', 'NNS'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('vergence', 'NN'), ('distance', 'NN'), ('wherein', 'VBD'), ('one', 'CD'), ('configurations', 'NNS'), ('head-mounted', 'JJ'), ('display', 'NN'), ('comprise', 'NN'), ('one', 'CD'), ('rendering', 'NN'), ('position', 'NN'), ('display', 'NN'), ('screen', 'JJ'), ('position', 'NN'), ('optics', 'NNS'), ('block', 'VBP'), ('comprising', 'VBG'), ('determining', 'VBG'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('second', 'JJ'), ('performance', 'NN'), ('threshold', 'VBD'), ('receiving', 'VBG'), ('eye', 'NN'), ('data', 'NNS'), ('eye', 'NN'), ('system', 'NN'), ('determining', 'VBG'), ('vergence', 'NN'), ('distance', 'NN'), ('based', 'VBN'), ('eye', 'NN'), ('data', 'NNS'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('one', 'CD'), ('second', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBD'), ('one', 'CD'), ('displaying', 'NN'), ('elements', 'NNS'), ('scene', 'VBP'), ('displayed', 'VBN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('determining', 'VBG'), ('vergence', 'NN'), ('distance', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('eye', 'NN'), ('data', 'NNS'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('one', 'CD'), ('second', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBD'), ('one', 'CD'), ('displaying', 'NN'), ('elements', 'NNS'), ('scene', 'VBP'), ('comprising', 'VBG'), ('feeding', 'VBG'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('fusion', 'NN'), ('algorithm', 'IN'), ('wherein', 'JJ'), ('fusion', 'NN'), ('algorithm', 'NN'), ('assigns', 'NNS'), ('weight', 'VBD'), ('score', 'RB'), ('input', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'VBZ'), ('determining', 'VBG'), ('vergence', 'NN'), ('distance', 'NN'), ('using', 'VBG'), ('fusion', 'NN'), ('algorithm', 'NNS'), ('based', 'VBN'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('determining', 'VBG'), ('z-depth', 'JJ'), ('display', 'NN'), ('screen', 'NN'), ('confidence', 'NN'), ('score', 'NN'), ('based', 'VBN'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('comprising', 'VBG'), ('comparing', 'VBG'), ('confidence', 'NN'), ('score', 'NN'), ('confidence', 'NN'), ('level', 'NN'), ('threshold', 'JJ'), ('response', 'NN'), ('determination', 'NN'), ('confidence', 'NN'), ('score', 'NN'), ('confidence', 'NN'), ('level', 'NN'), ('threshold', 'VBD'), ('feeding', 'VBG'), ('one', 'CD'), ('second', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBD'), ('one', 'CD'), ('displaying', 'NN'), ('elements', 'NNS'), ('scene', 'JJ'), ('fusion', 'NN'), ('algorithm', 'IN'), ('determining', 'VBG'), ('z-depth', 'JJ'), ('display', 'NN'), ('screen', 'NN'), ('using', 'VBG'), ('fusion', 'NN'), ('algorithm', 'NNS'), ('based', 'VBN'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('one', 'CD'), ('second', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBD'), ('one', 'CD'), ('displaying', 'NN'), ('elements', 'NNS'), ('scene', 'VBP'), ('comparing', 'VBG'), ('comparing', 'VBG'), ('fusion', 'NN'), ('algorithm', 'NN'), ('confidence', 'NN'), ('scores', 'NNS'), ('associated', 'VBN'), ('combinations', 'NNS'), ('inputs', 'NNS'), ('determining', 'VBG'), ('fusion', 'NN'), ('algorithm', 'IN'), ('z-depth', 'JJ'), ('display', 'NN'), ('screen', 'NN'), ('based', 'VBN'), ('combination', 'NN'), ('inputs', 'NNS'), ('associated', 'VBN'), ('highest', 'JJS'), ('confidence', 'NN'), ('score', 'NN'), ('wherein', 'VBD'), ('z-depth', 'JJ'), ('confidence', 'NN'), ('score', 'NN'), ('determined', 'VBD'), ('fusion', 'NN'), ('algorithm', 'IN'), ('using', 'VBG'), ('piecewise', 'NN'), ('comparison', 'NN'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'VBZ'), ('one', 'CD'), ('second', 'NN'), ('inputs', 'VBZ'), ('wherein', 'JJ'), ('z-depth', 'JJ'), ('confidence', 'NN'), ('score', 'NN'), ('determined', 'VBD'), ('based', 'VBN'), ('correlation', 'NN'), ('two', 'CD'), ('inputs', 'NNS'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'VBZ'), ('one', 'CD'), ('second', 'NN'), ('inputs', 'VBZ'), ('wherein', 'JJ'), ('fusion', 'NN'), ('algorithm', 'NN'), ('comprises', 'VBZ'), ('machine', 'NN'), ('learning', 'VBG'), ('ml', 'JJ'), ('algorithm', 'JJ'), ('wherein', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('ml', 'JJ'), ('algorithm', 'JJ'), ('determines', 'NNS'), ('combination', 'NN'), ('first', 'RB'), ('inputs', 'VBZ'), ('fed', 'JJ'), ('fusion', 'NN'), ('algorithm', 'VBD'), ('wherein', 'WP'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('comprise', 'NN'), ('one', 'CD'), ('hand', 'NN'), ('position', 'NN'), ('hand', 'NN'), ('direction', 'NN'), ('hand', 'NN'), ('movement', 'NN'), ('hand', 'NN'), ('gesture', 'NN'), ('head', 'NN'), ('position', 'NN'), ('head', 'NN'), ('direction', 'NN'), ('head', 'NN'), ('movement', 'NN'), ('head', 'NN'), ('gesture', 'NN'), ('gaze', 'NN'), ('angle', 'VBP'), ('rea', 'NN'), ('body', 'NN'), ('gesture', 'NN'), ('body', 'NN'), ('posture', 'NN'), ('body', 'NN'), ('movement', 'NN'), ('behavior', 'NN'), ('weighted', 'VBD'), ('combination', 'NN'), ('one', 'CD'), ('related', 'JJ'), ('parameters', 'NNS'), ('wherein', 'VBP'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('received', 'VBD'), ('one', 'CD'), ('controller', 'NN'), ('sensor', 'NN'), ('camera', 'NN'), ('microphone', 'NN'), ('accelerometer', 'NN'), ('headset', 'VBN'), ('worn', 'VBP'), ('mobile', 'JJ'), ('device', 'NN'), ('wherein', 'VBD'), ('one', 'CD'), ('second', 'NN'), ('inputs', 'NNS'), ('associated', 'VBD'), ('one', 'CD'), ('displaying', 'NN'), ('elements', 'NNS'), ('comprise', 'VBP'), ('one', 'CD'), ('z-buffer', 'NN'), ('value', 'NN'), ('associated', 'VBN'), ('displaying', 'VBG'), ('element', 'NN'), ('displaying', 'VBG'), ('element', 'NN'), ('marked', 'VBD'), ('developer', 'NN'), ('analysis', 'NN'), ('result', 'NN'), ('shape', 'NN'), ('displaying', 'VBG'), ('element', 'JJ'), ('face', 'NN'), ('result', 'NN'), ('object', 'VBP'), ('result', 'NN'), ('person', 'NN'), ('identified', 'VBD'), ('displaying', 'VBG'), ('content', 'NN'), ('object', 'NN'), ('identified', 'VBD'), ('displaying', 'VBG'), ('content', 'JJ'), ('correlation', 'NN'), ('two', 'CD'), ('displaying', 'VBG'), ('elements', 'NNS'), ('weighted', 'VBN'), ('combination', 'NN'), ('one', 'CD'), ('second', 'NN'), ('inputs', 'VBZ'), ('comprising', 'VBG'), ('determining', 'VBG'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('second', 'JJ'), ('performance', 'NN'), ('threshold', 'VBD'), ('receiving', 'VBG'), ('one', 'CD'), ('second', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBD'), ('one', 'CD'), ('displaying', 'NN'), ('elements', 'NNS'), ('scene', 'VBP'), ('displayed', 'VBN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('determining', 'VBG'), ('vergence', 'NN'), ('distance', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('one', 'CD'), ('second', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBD'), ('one', 'CD'), ('displaying', 'NN'), ('elements', 'NNS'), ('wherein', 'VBP'), ('determining', 'VBG'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('second', 'JJ'), ('performance', 'NN'), ('threshold', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('eye', 'NN'), ('system', 'NN'), ('exist', 'VBP'), ('fails', 'NNS'), ('provide', 'VBP'), ('eye', 'NN'), ('data', 'NNS'), ('wherein', 'VBD'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('comprises', 'VBZ'), ('one', 'CD'), ('accuracy', 'NN'), ('parameter', 'NN'), ('eye', 'NN'), ('system', 'NN'), ('precision', 'NN'), ('parameter', 'NN'), ('eye', 'NN'), ('system', 'NN'), ('value', 'NN'), ('parameter', 'NN'), ('eye', 'NN'), ('system', 'NN'), ('detectability', 'NN'), ('pupil', 'VBP'), ('metric', 'JJ'), ('based', 'VBN'), ('one', 'CD'), ('parameters', 'NNS'), ('associated', 'JJ'), ('parameter', 'NN'), ('change', 'NN'), ('parameter', 'NN'), ('changing', 'VBG'), ('trend', 'NN'), ('data', 'NNS'), ('availability', 'NN'), ('weighted', 'VBD'), ('combination', 'NN'), ('one', 'CD'), ('performance', 'NN'), ('related', 'JJ'), ('parameters', 'NNS'), ('wherein', 'VBP'), ('one', 'CD'), ('parameters', 'NNS'), ('associated', 'VBD'), ('comprise', 'NN'), ('one', 'CD'), ('eye', 'NN'), ('distance', 'NN'), ('pupil', 'JJ'), ('position', 'NN'), ('pupil', 'NN'), ('status', 'NN'), ('correlation', 'NN'), ('two', 'CD'), ('pupils', 'NNS'), ('head', 'VBP'), ('size', 'NN'), ('position', 'NN'), ('headset', 'VBN'), ('worn', 'JJ'), ('angle', 'NN'), ('headset', 'NN'), ('worn', 'JJ'), ('direction', 'NN'), ('headset', 'NN'), ('worn', 'JJ'), ('alignment', 'JJ'), ('eyes', 'NNS'), ('weighted', 'VBD'), ('combination', 'NN'), ('one', 'CD'), ('related', 'JJ'), ('parameters', 'NNS'), ('associated', 'VBD'), ('wherein', 'JJ'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'NN'), ('comprises', 'VBZ'), ('one', 'CD'), ('pre-determined', 'JJ'), ('value', 'NN'), ('pre-determined', 'JJ'), ('range', 'NN'), ('state', 'NN'), ('data', 'NNS'), ('changing', 'VBG'), ('speed', 'NN'), ('data', 'NNS'), ('trend', 'NN'), ('data', 'NNS'), ('change', 'VBP'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('media', 'NNS'), ('embodying', 'VBG'), ('software', 'NN'), ('operable', 'JJ'), ('executed', 'VBN'), ('computing', 'NN'), ('system', 'NN'), ('determine', 'JJ'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'NN'), ('wherein', 'NN'), ('eye', 'NN'), ('system', 'NN'), ('associated', 'VBN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('worn', 'NNS'), ('based', 'VBN'), ('determination', 'NN'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'JJ'), ('media', 'NNS'), ('embodying', 'VBG'), ('software', 'NN'), ('operable', 'JJ'), ('executed', 'VBN'), ('computing', 'VBG'), ('system', 'NN'), ('receive', 'VBP'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('estimate', 'NN'), ('region', 'NN'), ('looking', 'VBG'), ('within', 'IN'), ('field', 'NN'), ('view', 'NN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('received', 'VBD'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('determine', 'JJ'), ('vergence', 'NN'), ('distance', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('estimated', 'VBN'), ('region', 'NN'), ('looking', 'VBG'), ('locations', 'NNS'), ('one', 'CD'), ('objects', 'VBZ'), ('scene', 'NN'), ('displayed', 'VBD'), ('head-mounted', 'JJ'), ('display', 'NN'), ('adjust', 'VBP'), ('one', 'CD'), ('configurations', 'NNS'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('vergence', 'NN'), ('distance', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('one', 'CD'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('media', 'NNS'), ('embodying', 'VBG'), ('instructions', 'NNS'), ('one', 'CD'), ('coupled', 'VBN'), ('storage', 'NN'), ('media', 'NNS'), ('operable', 'JJ'), ('execute', 'JJ'), ('instructions', 'NNS'), ('determine', 'VBP'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'NN'), ('wherein', 'NN'), ('eye', 'NN'), ('system', 'NN'), ('associated', 'VBN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('worn', 'NNS'), ('based', 'VBN'), ('determination', 'NN'), ('performance', 'NN'), ('metric', 'JJ'), ('eye', 'NN'), ('system', 'NN'), ('first', 'JJ'), ('performance', 'NN'), ('threshold', 'NN'), ('system', 'NN'), ('configured', 'VBD'), ('receive', 'JJ'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('estimate', 'NN'), ('region', 'NN'), ('looking', 'VBG'), ('within', 'IN'), ('field', 'NN'), ('view', 'NN'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('received', 'VBD'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('determine', 'JJ'), ('vergence', 'NN'), ('distance', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('inputs', 'NNS'), ('associated', 'VBN'), ('body', 'NN'), ('estimated', 'VBN'), ('region', 'NN'), ('looking', 'VBG'), ('locations', 'NNS'), ('one', 'CD'), ('objects', 'VBZ'), ('scene', 'NN'), ('displayed', 'VBD'), ('head-mounted', 'JJ'), ('display', 'NN'), ('adjust', 'VBP'), ('one', 'CD'), ('configurations', 'NNS'), ('head-mounted', 'JJ'), ('display', 'NN'), ('based', 'VBN'), ('determined', 'JJ'), ('vergence', 'NN'), ('distance', 'NN'), ('computer-implemented', 'JJ'), ('-based', 'VBD'), ('self-guided', 'JJ'), ('object', 'JJ'), ('detection', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('device', 'NN'), ('set', 'VBN'), ('respective', 'JJ'), ('grid', 'JJ'), ('thereon', 'NN'), ('labeled', 'VBD'), ('regarding', 'VBG'), ('respective', 'JJ'), ('object', 'NN'), ('detected', 'VBD'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('training', 'NN'), ('device', 'NN'), ('grid-based', 'JJ'), ('object', 'NN'), ('detector', 'NN'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('determining', 'VBG'), ('device', 'NN'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('applying', 'VBG'), ('local', 'JJ'), ('segmentation', 'NN'), ('training', 'NN'), ('device', 'NN'), ('region-based', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('rcnn', 'NN'), ('joint', 'NN'), ('object', 'JJ'), ('localization', 'NN'), ('object', 'NN'), ('classification', 'NN'), ('using', 'VBG'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('input', 'NN'), ('rcnn', 'VBD'), ('computer-implemented', 'JJ'), ('comprising', 'NN'), ('performing', 'VBG'), ('action', 'NN'), ('responsive', 'JJ'), ('object', 'NN'), ('localization', 'NN'), ('object', 'JJ'), ('classification', 'NN'), ('respective', 'JJ'), ('new', 'JJ'), ('object', 'JJ'), ('new', 'JJ'), ('rcnn', 'NN'), ('applied', 'VBN'), ('computer-implemented', 'JJ'), ('wherein', 'JJ'), ('action', 'NN'), ('comprises', 'VBZ'), ('autonomously', 'RB'), ('controlling', 'VBG'), ('motor', 'NN'), ('vehicle', 'NN'), ('avoid', 'VBP'), ('collision', 'NN'), ('new', 'JJ'), ('object', 'JJ'), ('responsive', 'JJ'), ('object', 'NN'), ('localization', 'NN'), ('object', 'JJ'), ('classification', 'NN'), ('respective', 'JJ'), ('new', 'JJ'), ('object', 'JJ'), ('computer-implemented', 'JJ'), ('wherein', 'NN'), ('local', 'JJ'), ('segmentation', 'NN'), ('performed', 'VBD'), ('using', 'VBG'), ('self-similarity', 'JJ'), ('search', 'NN'), ('template', 'NN'), ('matching', 'VBG'), ('provide', 'RB'), ('respective', 'JJ'), ('bounding', 'VBG'), ('box', 'NN'), ('around', 'IN'), ('respective', 'JJ'), ('object', 'JJ'), ('set', 'VBN'), ('computer-implemented', 'JJ'), ('wherein', 'JJ'), ('local', 'JJ'), ('segmentation', 'NN'), ('applied', 'VBD'), ('segment', 'NN'), ('respective', 'JJ'), ('target', 'NN'), ('region', 'NN'), ('therein', 'IN'), ('computer-implemented', 'JJ'), ('wherein', 'JJ'), ('region-based', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('rcnn', 'NN'), ('forms', 'NNS'), ('model', 'VBP'), ('object', 'JJ'), ('training', 'NN'), ('stage', 'NN'), ('detect', 'JJ'), ('objects', 'VBZ'), ('new', 'JJ'), ('inference', 'NN'), ('stage', 'NN'), ('computer-implemented', 'JJ'), ('wherein', 'NN'), ('performed', 'VBD'), ('system', 'NN'), ('selected', 'VBN'), ('group', 'NN'), ('consisting', 'VBG'), ('surveillance', 'NN'), ('system', 'NN'), ('face', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('face', 'NN'), ('system', 'NN'), ('cancer', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('object', 'JJ'), ('system', 'NN'), ('advanced', 'VBD'), ('driver-assistance', 'NN'), ('system', 'NN'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('-based', 'VBD'), ('self-guided', 'JJ'), ('object', 'JJ'), ('detection', 'NN'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('comprising', 'VBG'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('program', 'NN'), ('instructions', 'NNS'), ('embodied', 'VBD'), ('therewith', 'JJ'), ('program', 'NN'), ('instructions', 'NNS'), ('executable', 'JJ'), ('computer', 'NN'), ('cause', 'NN'), ('computer', 'NN'), ('perform', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('device', 'NN'), ('set', 'VBN'), ('respective', 'JJ'), ('grid', 'JJ'), ('thereon', 'NN'), ('labeled', 'VBD'), ('regarding', 'VBG'), ('respective', 'JJ'), ('object', 'NN'), ('detected', 'VBD'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('training', 'NN'), ('device', 'NN'), ('grid-based', 'JJ'), ('object', 'NN'), ('detector', 'NN'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('determining', 'VBG'), ('device', 'NN'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('applying', 'VBG'), ('local', 'JJ'), ('segmentation', 'NN'), ('training', 'NN'), ('device', 'NN'), ('region-based', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('rcnn', 'NN'), ('joint', 'NN'), ('object', 'JJ'), ('localization', 'NN'), ('object', 'NN'), ('classification', 'NN'), ('using', 'VBG'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('input', 'NN'), ('rcnn', 'VBP'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('wherein', 'NN'), ('comprises', 'VBZ'), ('performing', 'VBG'), ('action', 'NN'), ('responsive', 'JJ'), ('object', 'NN'), ('localization', 'NN'), ('object', 'JJ'), ('classification', 'NN'), ('respective', 'JJ'), ('new', 'JJ'), ('object', 'JJ'), ('new', 'JJ'), ('rcnn', 'NN'), ('applied', 'VBN'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('wherein', 'VBZ'), ('action', 'NN'), ('comprises', 'VBZ'), ('autonomously', 'RB'), ('controlling', 'VBG'), ('motor', 'NN'), ('vehicle', 'NN'), ('avoid', 'VBP'), ('collision', 'NN'), ('new', 'JJ'), ('object', 'JJ'), ('responsive', 'JJ'), ('object', 'NN'), ('localization', 'NN'), ('object', 'JJ'), ('classification', 'NN'), ('respective', 'JJ'), ('new', 'JJ'), ('object', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('wherein', 'VBZ'), ('local', 'JJ'), ('segmentation', 'NN'), ('performed', 'VBD'), ('using', 'VBG'), ('self-similarity', 'JJ'), ('search', 'NN'), ('template', 'NN'), ('matching', 'VBG'), ('provide', 'RB'), ('respective', 'JJ'), ('bounding', 'VBG'), ('box', 'NN'), ('around', 'IN'), ('respective', 'JJ'), ('object', 'NN'), ('set', 'VBN'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('wherein', 'VBZ'), ('local', 'JJ'), ('segmentation', 'NN'), ('applied', 'VBD'), ('segment', 'NN'), ('respective', 'JJ'), ('target', 'NN'), ('region', 'NN'), ('therein', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('wherein', 'VBZ'), ('region-based', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('rcnn', 'NN'), ('forms', 'NNS'), ('model', 'VBP'), ('object', 'JJ'), ('training', 'NN'), ('stage', 'NN'), ('detect', 'JJ'), ('objects', 'VBZ'), ('new', 'JJ'), ('inference', 'NN'), ('stage', 'NN'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('wherein', 'NN'), ('performed', 'VBD'), ('system', 'NN'), ('selected', 'VBN'), ('group', 'NN'), ('consisting', 'VBG'), ('surveillance', 'NN'), ('system', 'NN'), ('face', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('face', 'NN'), ('system', 'NN'), ('cancer', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('object', 'JJ'), ('system', 'NN'), ('advanced', 'VBD'), ('driver-assistance', 'NN'), ('system', 'NN'), ('computer', 'NN'), ('processing', 'VBG'), ('system', 'NN'), ('-based', 'VBD'), ('self-guided', 'JJ'), ('object', 'JJ'), ('detection', 'NN'), ('comprising', 'VBG'), ('memory', 'NN'), ('device', 'NN'), ('storing', 'VBG'), ('program', 'NN'), ('code', 'NN'), ('device', 'NN'), ('running', 'VBG'), ('program', 'NN'), ('code', 'NN'), ('receive', 'VBP'), ('set', 'VBN'), ('respective', 'JJ'), ('grid', 'JJ'), ('thereon', 'NN'), ('labeled', 'VBD'), ('regarding', 'VBG'), ('respective', 'JJ'), ('object', 'NN'), ('detected', 'VBD'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('train', 'VBP'), ('grid-based', 'JJ'), ('object', 'NN'), ('detector', 'NN'), ('using', 'VBG'), ('grid', 'JJ'), ('level', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('determine', 'VBP'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('applying', 'VBG'), ('local', 'JJ'), ('segmentation', 'NN'), ('train', 'NN'), ('region-based', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('rcnn', 'NN'), ('joint', 'NN'), ('object', 'JJ'), ('localization', 'NN'), ('object', 'NN'), ('classification', 'NN'), ('using', 'VBG'), ('respective', 'JJ'), ('bounding', 'NN'), ('box', 'NN'), ('respective', 'JJ'), ('object', 'JJ'), ('input', 'NN'), ('rcnn', 'VBP'), ('computer', 'NN'), ('processing', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('device', 'NN'), ('runs', 'VBZ'), ('program', 'NN'), ('code', 'NN'), ('perform', 'VB'), ('action', 'NN'), ('responsive', 'JJ'), ('object', 'NN'), ('localization', 'NN'), ('object', 'JJ'), ('classification', 'NN'), ('respective', 'JJ'), ('new', 'JJ'), ('object', 'JJ'), ('new', 'JJ'), ('rcnn', 'NN'), ('applied', 'VBN'), ('computer', 'NN'), ('processing', 'VBG'), ('system', 'NN'), ('wherein', 'JJ'), ('action', 'NN'), ('comprises', 'VBZ'), ('autonomously', 'RB'), ('controlling', 'VBG'), ('motor', 'NN'), ('vehicle', 'NN'), ('avoid', 'VBP'), ('collision', 'NN'), ('new', 'JJ'), ('object', 'JJ'), ('responsive', 'JJ'), ('object', 'NN'), ('localization', 'NN'), ('object', 'JJ'), ('classification', 'NN'), ('respective', 'JJ'), ('new', 'JJ'), ('object', 'JJ'), ('computer', 'NN'), ('processing', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('local', 'JJ'), ('segmentation', 'NN'), ('performed', 'VBD'), ('using', 'VBG'), ('self-similarity', 'JJ'), ('search', 'NN'), ('template', 'NN'), ('matching', 'VBG'), ('provide', 'RB'), ('respective', 'JJ'), ('bounding', 'VBG'), ('box', 'NN'), ('around', 'IN'), ('respective', 'JJ'), ('object', 'NN'), ('set', 'VBN'), ('computer', 'NN'), ('processing', 'VBG'), ('system', 'NN'), ('wherein', 'JJ'), ('region-based', 'JJ'), ('convolutional', 'JJ'), ('neural', 'JJ'), ('network', 'NN'), ('rcnn', 'NN'), ('forms', 'NNS'), ('model', 'VBP'), ('object', 'JJ'), ('training', 'NN'), ('stage', 'NN'), ('detect', 'JJ'), ('objects', 'VBZ'), ('new', 'JJ'), ('inference', 'NN'), ('stage', 'NN'), ('computer', 'NN'), ('processing', 'NN'), ('system', 'NN'), ('wherein', 'VBP'), ('computer', 'NN'), ('processing', 'NN'), ('system', 'NN'), ('comprised', 'VBD'), ('system', 'NN'), ('selected', 'VBN'), ('group', 'NN'), ('consisting', 'VBG'), ('surveillance', 'NN'), ('system', 'NN'), ('face', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('face', 'NN'), ('system', 'NN'), ('cancer', 'NN'), ('detection', 'NN'), ('system', 'NN'), ('object', 'JJ'), ('system', 'NN'), ('advanced', 'VBD'), ('driver-assistance', 'NN'), ('system', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('utilizing', 'VBG'), ('database', 'NN'), ('normalized', 'VBN'), ('stored', 'VBD'), ('comprising', 'VBG'), ('capturing', 'VBG'), ('using', 'VBG'), ('camera', 'NN'), ('detecting', 'VBG'), ('face', 'NN'), ('captured', 'VBD'), ('normalizing', 'JJ'), ('detected', 'JJ'), ('facial', 'JJ'), ('match', 'NN'), ('normalized', 'VBN'), ('stored', 'VBD'), ('identifying', 'VBG'), ('facial', 'JJ'), ('features', 'NNS'), ('normalized', 'VBN'), ('detected', 'JJ'), ('facial', 'JJ'), ('generating', 'VBG'), ('facial', 'JJ'), ('metrics', 'NNS'), ('facial', 'JJ'), ('features', 'NNS'), ('calculating', 'VBG'), ('euclidean', 'JJ'), ('distances', 'NNS'), ('facial', 'JJ'), ('metrics', 'NNS'), ('normalized', 'VBN'), ('detected', 'JJ'), ('facial', 'JJ'), ('corresponding', 'VBG'), ('facial', 'JJ'), ('metrics', 'NNS'), ('stored', 'VBD'), ('comparing', 'VBG'), ('euclidean', 'JJ'), ('distance', 'NN'), ('predetermined', 'VBD'), ('threshold', 'JJ'), ('responsive', 'JJ'), ('euclidean', 'JJ'), ('distance', 'NN'), ('comparison', 'NN'), ('producing', 'VBG'), ('reduced', 'JJ'), ('candidate', 'JJ'), ('list', 'NN'), ('best', 'JJS'), ('possible', 'JJ'), ('matches', 'NNS'), ('normalized', 'VBN'), ('stored', 'VBD'), ('comparing', 'VBG'), ('parallel', 'RB'), ('normalized', 'VBN'), ('detected', 'VBN'), ('facial', 'JJ'), ('normalized', 'VBN'), ('stored', 'VBD'), ('reduced', 'JJ'), ('candidate', 'NN'), ('list', 'NN'), ('utilizing', 'VBG'), ('face', 'NN'), ('algorithms', 'NN'), ('parallel', 'RB'), ('processing', 'VBG'), ('system', 'NN'), ('uses', 'VBZ'), ('different', 'JJ'), ('face', 'NN'), ('algorithm', 'VBP'), ('responsive', 'JJ'), ('comparison', 'NN'), ('producing', 'VBG'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('parallel', 'JJ'), ('subset', 'NN'), ('reduced', 'VBN'), ('candidate', 'JJ'), ('list', 'NN'), ('selecting', 'VBG'), ('final', 'JJ'), ('match', 'NN'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('using', 'VBG'), ('deep', 'JJ'), ('learning', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('face', 'NN'), ('algorithm', 'NN'), ('trained', 'VBD'), ('outputs', 'NNS'), ('individual', 'JJ'), ('face', 'NN'), ('algorithms', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('detecting', 'VBG'), ('face', 'NN'), ('captured', 'VBD'), ('comprises', 'NNS'), ('utilizing', 'VBG'), ('opencv', 'NN'), ('detect', 'JJ'), ('face', 'NN'), ('captured', 'VBD'), ('extracting', 'VBG'), ('location', 'NN'), ('eyes', 'NNS'), ('tip', 'VBP'), ('nose', 'JJ'), ('face', 'NN'), ('determining', 'VBG'), ('distance', 'NN'), ('eyes', 'NNS'), ('cropping', 'VBG'), ('face', 'NN'), ('captured', 'VBD'), ('width', 'JJ'), ('height', 'NN'), ('cropped', 'VBD'), ('face', 'NN'), ('function', 'NN'), ('distance', 'NN'), ('eyes', 'NNS'), ('rotating', 'VBG'), ('face', 'NN'), ('angle', 'NN'), ('rotation', 'NN'), ('function', 'NN'), ('distance', 'NN'), ('eyes', 'NNS'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('width', 'NN'), ('cropped', 'VBD'), ('face', 'NN'), ('times', 'NNS'), ('distance', 'JJ'), ('eyes', 'NNS'), ('height', 'VBD'), ('cropped', 'VBD'), ('face', 'NN'), ('times', 'NNS'), ('distance', 'JJ'), ('eyes', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('angle', 'NN'), ('formed', 'VBD'), ('straight', 'JJ'), ('line', 'NN'), ('joining', 'VBG'), ('eyes', 'NNS'), ('x-axis', 'JJ'), ('face', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('rotating', 'VBG'), ('face', 'NN'), ('comprises', 'NNS'), ('rotating', 'VBG'), ('face', 'NN'), ('provide', 'VBP'), ('frontal', 'JJ'), ('face', 'NN'), ('pattern', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('comprising', 'VBG'), ('step', 'NN'), ('proportionally', 'RB'), ('rescaling', 'VBG'), ('cropped', 'VBN'), ('rotated', 'VBN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('proportional', 'JJ'), ('rescaling', 'NN'), ('yields', 'NNS'), ('cropped', 'VBD'), ('rotated', 'JJ'), ('size', 'NN'), ('=', 'NN'), ('pixels', 'NNS'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'VBP'), ('facial', 'JJ'), ('features', 'NNS'), ('identified', 'VBN'), ('normalized', 'JJ'), ('detected', 'VBN'), ('facial', 'JJ'), ('comprise', 'NN'), ('pair', 'NN'), ('eyes', 'NNS'), ('tip', 'VBP'), ('nose', 'JJ'), ('mouth', 'NN'), ('center', 'NN'), ('mouth', 'NN'), ('chin', 'JJ'), ('area', 'NN'), ('comprising', 'VBG'), ('bottom', 'JJ'), ('top', 'JJ'), ('left', 'VBN'), ('landmark', 'NN'), ('top', 'JJ'), ('right', 'NN'), ('landmark', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('generating', 'VBG'), ('facial', 'JJ'), ('metrics', 'NNS'), ('comprises', 'NNS'), ('calculating', 'VBG'), ('distance', 'NN'), ('pair', 'NN'), ('eyes', 'NNS'), ('distance', 'VB'), ('eyes', 'NNS'), ('tip', 'VBP'), ('nose', 'JJ'), ('distance', 'NN'), ('equal', 'JJ'), ('width', 'NN'), ('mouth', 'NN'), ('distance', 'NN'), ('tip', 'NN'), ('nose', 'RB'), ('center', 'JJ'), ('mouth', 'NN'), ('distance', 'NN'), ('bottom', 'NN'), ('chin', 'NN'), ('center', 'NN'), ('mouth', 'NN'), ('distance', 'NN'), ('top', 'NN'), ('left', 'VBD'), ('landmark', 'NN'), ('chin', 'NN'), ('tip', 'NN'), ('nose', 'JJ'), ('distance', 'NN'), ('top', 'JJ'), ('right', 'NN'), ('landmark', 'NN'), ('chin', 'JJ'), ('tip', 'NN'), ('nose', 'RB'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('performing', 'VBG'), ('euclidean', 'JJ'), ('distance', 'NN'), ('match', 'NN'), ('comprises', 'VBZ'), ('partitioning', 'VBG'), ('normalized', 'VBN'), ('stored', 'VBN'), ('substantially', 'RB'), ('equal', 'JJ'), ('subsets', 'NNS'), ('performing', 'VBG'), ('euclidean', 'JJ'), ('distance', 'NN'), ('match', 'NN'), ('facial', 'JJ'), ('metrics', 'NNS'), ('normalized', 'VBN'), ('detected', 'JJ'), ('facial', 'JJ'), ('corresponding', 'VBG'), ('facial', 'JJ'), ('metrics', 'NNS'), ('stored', 'VBD'), ('subsets', 'NNS'), ('normalized', 'VBN'), ('stored', 'JJ'), ('separate', 'JJ'), ('parallel', 'NN'), ('processing', 'NN'), ('system', 'NN'), ('generate', 'JJ'), ('euclidean', 'JJ'), ('distance', 'NN'), ('stored', 'VBD'), ('subset', 'JJ'), ('comparing', 'VBG'), ('euclidean', 'JJ'), ('distance', 'NN'), ('predetermined', 'VBD'), ('threshold', 'JJ'), ('separate', 'JJ'), ('responsive', 'JJ'), ('euclidean', 'JJ'), ('distance', 'NN'), ('comparison', 'NN'), ('producing', 'VBG'), ('reduced', 'JJ'), ('candidate', 'JJ'), ('list', 'NN'), ('best', 'JJS'), ('possible', 'JJ'), ('matches', 'NNS'), ('normalized', 'VBN'), ('stored', 'JJ'), ('subset', 'NN'), ('combining', 'NN'), ('reduced', 'JJ'), ('candidate', 'NN'), ('lists', 'NNS'), ('subset', 'VBP'), ('produce', 'VBP'), ('single', 'JJ'), ('reduced', 'VBN'), ('candidate', 'JJ'), ('list', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('face', 'NN'), ('algorithms', 'VBP'), ('utilized', 'JJ'), ('comparing', 'NN'), ('parallel', 'RB'), ('normalized', 'VBN'), ('detected', 'VBN'), ('facial', 'JJ'), ('normalized', 'VBN'), ('stored', 'VBD'), ('reduced', 'JJ'), ('candidate', 'NN'), ('list', 'NN'), ('consists', 'VBZ'), ('face', 'VBP'), ('algorithms', 'RB'), ('selected', 'VBN'), ('group', 'NN'), ('consisting', 'VBG'), ('principle', 'JJ'), ('component', 'JJ'), ('analysis', 'NN'), ('pca-based', 'JJ'), ('algorithms', 'NN'), ('linear', 'JJ'), ('discriminant', 'JJ'), ('analysis', 'NN'), ('lda', 'NN'), ('algorithms', 'JJ'), ('independent', 'JJ'), ('component', 'NN'), ('analysis', 'NN'), ('ica', 'NN'), ('algorithms', 'IN'), ('kernel-based', 'JJ'), ('algorithms', 'JJ'), ('feature-based', 'JJ'), ('techniques', 'NNS'), ('algorithms', 'VBP'), ('based', 'VBN'), ('neural', 'JJ'), ('networks', 'NNS'), ('algorithms', 'VBP'), ('based', 'VBN'), ('transforms', 'NNS'), ('model-based', 'JJ'), ('face', 'NN'), ('algorithms', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'IN'), ('pca-based', 'JJ'), ('algorithms', 'NNS'), ('include', 'VBP'), ('eigen', 'JJ'), ('face', 'NN'), ('detection', 'NN'), ('lda', 'VBZ'), ('algorithms', 'JJ'), ('include', 'VBP'), ('fisher', 'JJ'), ('face', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('comparing', 'VBG'), ('parallel', 'RB'), ('captured', 'VBN'), ('normalized', 'JJ'), ('stored', 'VBD'), ('reduced', 'JJ'), ('candidate', 'NN'), ('list', 'NN'), ('comprises', 'VBZ'), ('partitioning', 'VBG'), ('reduced', 'VBN'), ('candidate', 'JJ'), ('list', 'NN'), ('substantially', 'RB'), ('equal', 'JJ'), ('subsets', 'NNS'), ('processing', 'VBG'), ('subset', 'NN'), ('different', 'JJ'), ('parallel', 'RB'), ('processing', 'VBG'), ('system', 'NN'), ('uses', 'VBZ'), ('unique', 'JJ'), ('face', 'NN'), ('algorithm', 'NN'), ('produce', 'VBP'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('using', 'VBG'), ('reduce', 'VB'), ('function', 'NN'), ('mapreduce', 'NN'), ('program', 'NN'), ('combine', 'NN'), ('best', 'RBS'), ('match', 'NN'), ('results', 'NNS'), ('subsets', 'NNS'), ('produce', 'VBP'), ('single', 'JJ'), ('set', 'NN'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('partitioning', 'VBG'), ('reduced', 'VBN'), ('candidate', 'JJ'), ('list', 'NN'), ('comprises', 'NNS'), ('selecting', 'VBG'), ('comprising', 'VBG'), ('subset', 'NN'), ('optimizing', 'VBG'), ('variance', 'NN'), ('according', 'VBG'), ('following', 'VBG'), ('equation', 'NN'), ('n', 'IN'), ('number', 'NN'), ('rows', 'NNS'), ('columns', 'VBP'), ('face', 'NN'), ('vector', 'NN'), ('n', 'IN'), ('number', 'NN'), ('groups', 'NNS'), ('σij', 'VBP'), ('standard', 'JJ'), ('deviation', 'NN'), ('dimension', 'NN'), ('group', 'NN'), ('j', 'VBD'), ('face', 'NN'), ('vector', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('comprising', 'VBG'), ('subset', 'NN'), ('optimizing', 'VBG'), ('variance', 'NN'), ('according', 'VBG'), ('following', 'VBG'), ('equation', 'NN'), ('dμi', 'FW'), ('μj', 'JJ'), ('euclidean', 'JJ'), ('distance', 'NN'), ('mean', 'NN'), ('group', 'NN'), ('mean', 'VBD'), ('group', 'NN'), ('j', 'NN'), ('face', 'NN'), ('vector', 'NN'), ('l', 'JJ'), ('number', 'NN'), ('group', 'NN'), ('levels', 'NNS'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('selecting', 'VBG'), ('final', 'JJ'), ('match', 'NN'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('utilizing', 'JJ'), ('deep', 'JJ'), ('learning', 'NN'), ('neural', 'JJ'), ('network', 'NN'), ('face', 'NN'), ('algorithm', 'NN'), ('comprises', 'VBZ'), ('utilizing', 'VBG'), ('either', 'DT'), ('adaboost', 'JJ'), ('machine-learning', 'JJ'), ('algorithm', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('machine-learning', 'JJ'), ('model', 'NN'), ('scalable', 'JJ'), ('parallel', 'JJ'), ('cloud-based', 'JJ'), ('face', 'NN'), ('normalizing', 'VBG'), ('detected', 'VBN'), ('facial', 'JJ'), ('match', 'NN'), ('normalized', 'VBN'), ('stored', 'VBD'), ('includes', 'VBZ'), ('normalizing', 'NN'), ('detected', 'VBN'), ('facial', 'JJ'), ('size', 'NN'), ('illumination', 'NN'), ('normalized', 'VBN'), ('stored', 'JJ'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('containing', 'VBG'), ('executable', 'JJ'), ('program', 'NN'), ('instructions', 'NNS'), ('causing', 'VBG'), ('computer', 'NN'), ('perform', 'NN'), ('face', 'NN'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('face', 'NN'), ('captured', 'VBD'), ('camera', 'NN'), ('normalizing', 'NN'), ('detected', 'VBN'), ('facial', 'JJ'), ('match', 'NN'), ('normalized', 'VBN'), ('stored', 'VBD'), ('identifying', 'VBG'), ('facial', 'JJ'), ('features', 'NNS'), ('normalized', 'VBN'), ('detected', 'JJ'), ('facial', 'JJ'), ('generating', 'VBG'), ('facial', 'JJ'), ('metrics', 'NNS'), ('facial', 'JJ'), ('features', 'NNS'), ('calculating', 'VBG'), ('euclidean', 'JJ'), ('distances', 'NNS'), ('facial', 'JJ'), ('metrics', 'NNS'), ('normalized', 'VBN'), ('detected', 'JJ'), ('facial', 'JJ'), ('corresponding', 'VBG'), ('facial', 'JJ'), ('metrics', 'NNS'), ('stored', 'VBD'), ('comparing', 'VBG'), ('euclidean', 'JJ'), ('distance', 'NN'), ('predetermined', 'VBD'), ('threshold', 'JJ'), ('responsive', 'JJ'), ('euclidean', 'JJ'), ('distance', 'NN'), ('comparison', 'NN'), ('producing', 'VBG'), ('reduced', 'JJ'), ('candidate', 'JJ'), ('list', 'NN'), ('best', 'JJS'), ('possible', 'JJ'), ('matches', 'NNS'), ('normalized', 'VBN'), ('stored', 'VBD'), ('comparing', 'VBG'), ('parallel', 'RB'), ('captured', 'VBN'), ('normalized', 'JJ'), ('stored', 'VBD'), ('reduced', 'JJ'), ('candidate', 'NN'), ('list', 'NN'), ('utilizing', 'VBG'), ('face', 'NN'), ('algorithms', 'NN'), ('parallel', 'RB'), ('processing', 'VBG'), ('system', 'NN'), ('uses', 'VBZ'), ('different', 'JJ'), ('face', 'NN'), ('algorithm', 'VBP'), ('responsive', 'JJ'), ('comparison', 'NN'), ('producing', 'VBG'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('parallel', 'JJ'), ('subset', 'NN'), ('reduced', 'VBN'), ('candidate', 'JJ'), ('list', 'NN'), ('selecting', 'VBG'), ('final', 'JJ'), ('match', 'NN'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('using', 'VBG'), ('deep', 'JJ'), ('learning', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('face', 'NN'), ('algorithm', 'NN'), ('trained', 'VBD'), ('outputs', 'NNS'), ('individual', 'JJ'), ('face', 'NN'), ('algorithms', 'IN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('containing', 'VBG'), ('executable', 'JJ'), ('program', 'NN'), ('instructions', 'NNS'), ('wherein', 'VBP'), ('face', 'NN'), ('algorithms', 'NN'), ('utilized', 'JJ'), ('comparing', 'NN'), ('parallel', 'RB'), ('normalized', 'VBN'), ('detected', 'VBN'), ('facial', 'JJ'), ('normalized', 'VBN'), ('stored', 'VBD'), ('reduced', 'JJ'), ('candidate', 'NN'), ('list', 'NN'), ('consists', 'VBZ'), ('face', 'VBP'), ('algorithms', 'RB'), ('selected', 'VBN'), ('group', 'NN'), ('consisting', 'VBG'), ('principle', 'JJ'), ('component', 'JJ'), ('analysis', 'NN'), ('pca-based', 'JJ'), ('algorithms', 'NN'), ('linear', 'JJ'), ('discriminant', 'JJ'), ('analysis', 'NN'), ('lda', 'NN'), ('algorithms', 'JJ'), ('independent', 'JJ'), ('component', 'NN'), ('analysis', 'NN'), ('ica', 'NN'), ('algorithms', 'IN'), ('kernel-based', 'JJ'), ('algorithms', 'JJ'), ('feature-based', 'JJ'), ('techniques', 'NNS'), ('algorithms', 'VBP'), ('based', 'VBN'), ('neural', 'JJ'), ('networks', 'NNS'), ('algorithms', 'VBP'), ('based', 'VBN'), ('transforms', 'NNS'), ('model-based', 'JJ'), ('face', 'NN'), ('algorithms', 'IN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('containing', 'VBG'), ('executable', 'JJ'), ('program', 'NN'), ('instructions', 'NNS'), ('wherein', 'VBP'), ('pca-based', 'JJ'), ('algorithms', 'NNS'), ('include', 'VBP'), ('eigen', 'JJ'), ('face', 'NN'), ('detection', 'NN'), ('lda', 'VBZ'), ('algorithms', 'JJ'), ('include', 'VBP'), ('fisher', 'JJ'), ('face', 'NN'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('medium', 'NN'), ('containing', 'VBG'), ('executable', 'JJ'), ('program', 'NN'), ('instructions', 'NNS'), ('selecting', 'VBG'), ('final', 'JJ'), ('match', 'NN'), ('best', 'JJS'), ('match', 'NN'), ('results', 'NNS'), ('utilizing', 'JJ'), ('deep', 'JJ'), ('learning', 'NN'), ('neural', 'JJ'), ('network', 'NN'), ('face', 'NN'), ('algorithm', 'NN'), ('comprises', 'VBZ'), ('utilizing', 'VBG'), ('either', 'DT'), ('adaboost', 'JJ'), ('machine-learning', 'JJ'), ('algorithm', 'JJ'), ('neural', 'JJ'), ('networks', 'NNS'), ('machine-learning', 'JJ'), ('model', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('comprising', 'VBG'), ('condensing', 'VBG'), ('lens', 'NNS'), ('sensor', 'NN'), ('configured', 'VBD'), ('detect', 'JJ'), ('light', 'JJ'), ('passing', 'VBG'), ('condensing', 'VBG'), ('lens', 'NNS'), ('comprising', 'VBG'), ('pixel', 'NN'), ('matrix', 'NN'), ('wherein', 'NN'), ('pixel', 'NN'), ('matrix', 'NN'), ('comprises', 'VBZ'), ('phase', 'JJ'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('configured', 'VBN'), ('turn', 'VBP'), ('phase', 'JJ'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('autofocusing', 'VBG'), ('output', 'NN'), ('autofocused', 'VBD'), ('pixel', 'JJ'), ('data', 'NNS'), ('completing', 'VBG'), ('autofocusing', 'VBG'), ('divide', 'NN'), ('autofocused', 'VBD'), ('pixel', 'NN'), ('data', 'NNS'), ('first', 'RB'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('calculate', 'NN'), ('features', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('first', 'JJ'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('wherein', 'NN'), ('features', 'VBZ'), ('comprise', 'VBP'), ('module', 'JJ'), ('widths', 'NNS'), ('finder', 'VBP'), ('pattern', 'JJ'), ('finder', 'NN'), ('pattern', 'NN'), ('predetermined', 'VBN'), ('ratio', 'JJ'), ('harr-like', 'JJ'), ('feature', 'NN'), ('gabor', 'NN'), ('feature', 'NN'), ('determine', 'NN'), ('operating', 'VBG'), ('resolution', 'NN'), ('regular', 'JJ'), ('pixels', 'NNS'), ('according', 'VBG'), ('features', 'NNS'), ('calculated', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('divided', 'VBD'), ('autofocused', 'JJ'), ('pixel', 'NN'), ('data', 'NNS'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'JJ'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NN'), ('comprises', 'VBZ'), ('first', 'JJ'), ('pixel', 'JJ'), ('second', 'JJ'), ('pixel', 'NN'), ('cover', 'NN'), ('layer', 'NN'), ('covering', 'VBG'), ('upon', 'IN'), ('first', 'JJ'), ('region', 'NN'), ('first', 'RB'), ('pixel', 'VBZ'), ('upon', 'IN'), ('second', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('pixel', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('region', 'NN'), ('mirror', 'NN'), ('symmetrical', 'JJ'), ('microlens', 'NNS'), ('aligned', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('pixel', 'JJ'), ('second', 'JJ'), ('pixel', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('region', 'NN'), ('%', 'NN'), ('%', 'NN'), ('area', 'NN'), ('single', 'JJ'), ('pixel', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('configured', 'VBD'), ('perform', 'NN'), ('autofocusing', 'VBG'), ('using', 'VBG'), ('dual', 'JJ'), ('pixel', 'NN'), ('autofocus', 'NN'), ('technique', 'NN'), ('according', 'VBG'), ('pixel', 'NN'), ('data', 'NNS'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('completing', 'VBG'), ('autofocusing', 'VBG'), ('imaging', 'JJ'), ('device', 'NN'), ('ed', 'NN'), ('wherein', 'NN'), ('configured', 'VBD'), ('divide', 'JJ'), ('pixel', 'NN'), ('data', 'NNS'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('third', 'JJ'), ('subframe', 'JJ'), ('fourth', 'JJ'), ('subframe', 'NN'), ('completing', 'VBG'), ('autofocusing', 'VBG'), ('perform', 'NN'), ('autofocusing', 'VBG'), ('according', 'VBG'), ('third', 'JJ'), ('subframe', 'JJ'), ('fourth', 'JJ'), ('subframe', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('configured', 'VBD'), ('calibrate', 'JJ'), ('brightness', 'JJ'), ('third', 'JJ'), ('subframe', 'NN'), ('fourth', 'JJ'), ('subframe', 'JJ'), ('identical', 'JJ'), ('using', 'VBG'), ('shading', 'VBG'), ('algorithm', 'JJ'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('operating', 'VBG'), ('resolution', 'NN'), ('selected', 'VBN'), ('first', 'JJ'), ('resolution', 'NN'), ('smaller', 'JJR'), ('number', 'NN'), ('regular', 'JJ'), ('pixels', 'NNS'), ('second', 'JJ'), ('resolution', 'NN'), ('larger', 'JJR'), ('first', 'JJ'), ('resolution', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'NN'), ('wherein', 'IN'), ('regular', 'JJ'), ('pixels', 'NNS'), ('turned', 'VBD'), ('autofocusing', 'VBG'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'JJ'), ('number', 'NN'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'VBZ'), ('smaller', 'JJR'), ('regular', 'JJ'), ('pixels', 'NNS'), ('imaging', 'VBG'), ('device', 'NN'), ('comprising', 'VBG'), ('condensing', 'VBG'), ('lens', 'NNS'), ('sensor', 'NN'), ('configured', 'VBD'), ('detect', 'JJ'), ('light', 'JJ'), ('passing', 'VBG'), ('condensing', 'VBG'), ('lens', 'NNS'), ('comprising', 'VBG'), ('pixel', 'NN'), ('matrix', 'NN'), ('wherein', 'NN'), ('pixel', 'NN'), ('matrix', 'NN'), ('comprises', 'VBZ'), ('phase', 'JJ'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('configured', 'VBN'), ('turn', 'VBP'), ('phase', 'JJ'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('autofocusing', 'VBG'), ('output', 'NN'), ('autofocused', 'VBD'), ('pixel', 'JJ'), ('data', 'NNS'), ('completing', 'VBG'), ('autofocusing', 'VBG'), ('divide', 'NN'), ('autofocused', 'VBD'), ('pixel', 'NN'), ('data', 'NNS'), ('first', 'RB'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('calculate', 'NN'), ('features', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('first', 'JJ'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('wherein', 'NN'), ('features', 'VBZ'), ('comprise', 'VBP'), ('module', 'JJ'), ('widths', 'NNS'), ('finder', 'VBP'), ('pattern', 'JJ'), ('finder', 'NN'), ('pattern', 'NN'), ('predetermined', 'VBN'), ('ratio', 'JJ'), ('harr-like', 'JJ'), ('feature', 'NN'), ('gabor', 'NN'), ('feature', 'NN'), ('select', 'VBP'), ('decoding', 'VBG'), ('using', 'VBG'), ('pixel', 'JJ'), ('data', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('according', 'VBG'), ('features', 'NNS'), ('calculated', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('divided', 'VBD'), ('autofocused', 'JJ'), ('pixel', 'NN'), ('data', 'NNS'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'JJ'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NN'), ('comprises', 'VBZ'), ('first', 'JJ'), ('pixel', 'JJ'), ('second', 'JJ'), ('pixel', 'NN'), ('cover', 'NN'), ('layer', 'NN'), ('covering', 'VBG'), ('upon', 'IN'), ('first', 'JJ'), ('region', 'NN'), ('first', 'RB'), ('pixel', 'VBZ'), ('upon', 'IN'), ('second', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('pixel', 'NN'), ('wherein', 'NN'), ('first', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('region', 'NN'), ('mirror', 'NN'), ('symmetrical', 'JJ'), ('microlens', 'NNS'), ('aligned', 'VBN'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('pixel', 'JJ'), ('second', 'JJ'), ('pixel', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('configured', 'VBD'), ('perform', 'NN'), ('autofocusing', 'VBG'), ('using', 'VBG'), ('dual', 'JJ'), ('pixel', 'NN'), ('autofocus', 'NN'), ('technique', 'NN'), ('according', 'VBG'), ('pixel', 'NN'), ('data', 'NNS'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('completing', 'VBG'), ('autofocusing', 'VBG'), ('imaging', 'JJ'), ('device', 'NN'), ('ed', 'NN'), ('wherein', 'NN'), ('configured', 'VBD'), ('divide', 'JJ'), ('pixel', 'NN'), ('data', 'NNS'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('third', 'JJ'), ('subframe', 'JJ'), ('fourth', 'JJ'), ('subframe', 'NN'), ('completing', 'VBG'), ('autofocusing', 'VBG'), ('calibrate', 'JJ'), ('brightness', 'JJ'), ('third', 'JJ'), ('subframe', 'NN'), ('fourth', 'JJ'), ('subframe', 'JJ'), ('identical', 'JJ'), ('using', 'VBG'), ('shading', 'VBG'), ('algorithm', 'JJ'), ('perform', 'NN'), ('autofocusing', 'VBG'), ('according', 'VBG'), ('third', 'JJ'), ('subframe', 'JJ'), ('fourth', 'JJ'), ('subframe', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('configured', 'VBD'), ('calculate', 'NN'), ('features', 'NNS'), ('using', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('rule', 'NN'), ('based', 'VBN'), ('algorithm', 'RB'), ('machine', 'NN'), ('learning', 'VBG'), ('algorithm', 'JJ'), ('imaging', 'VBG'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('decoding', 'VBG'), ('decoding', 'VBG'), ('qr', 'NN'), ('codes', 'NNS'), ('face', 'VBP'), ('operating', 'VBG'), ('imaging', 'VBG'), ('device', 'NN'), ('imaging', 'VBG'), ('device', 'NN'), ('comprising', 'VBG'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('operating', 'VBG'), ('comprising', 'VBG'), ('turning', 'VBG'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('autofocusing', 'VBG'), ('outputting', 'VBG'), ('autofocused', 'JJ'), ('frame', 'NN'), ('completing', 'VBG'), ('autofocusing', 'VBG'), ('dividing', 'VBG'), ('autofocused', 'JJ'), ('frame', 'NN'), ('acquired', 'VBD'), ('phase', 'JJ'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('first', 'RB'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('calculating', 'VBG'), ('features', 'NNS'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('wherein', 'JJ'), ('feature', 'NN'), ('comprise', 'NN'), ('module', 'NN'), ('widths', 'NNS'), ('finder', 'VBP'), ('pattern', 'JJ'), ('finder', 'NN'), ('pattern', 'NN'), ('predetermined', 'VBN'), ('ratio', 'JJ'), ('harr-like', 'JJ'), ('feature', 'NN'), ('gabor', 'NN'), ('feature', 'NN'), ('selectively', 'RB'), ('activating', 'VBG'), ('least', 'JJS'), ('part', 'NN'), ('regular', 'JJ'), ('pixels', 'NNS'), ('according', 'VBG'), ('features', 'NNS'), ('calculated', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('first', 'JJ'), ('subframe', 'JJ'), ('second', 'JJ'), ('subframe', 'NN'), ('divided', 'VBD'), ('autofocused', 'JJ'), ('frame', 'NN'), ('operating', 'VBG'), ('ed', 'NN'), ('wherein', 'NN'), ('selectively', 'RB'), ('activating', 'VBG'), ('comprises', 'NNS'), ('activating', 'VBG'), ('first', 'JJ'), ('part', 'NN'), ('regular', 'JJ'), ('pixels', 'NNS'), ('perform', 'VBP'), ('decoding', 'VBG'), ('according', 'VBG'), ('pixel', 'NN'), ('data', 'NNS'), ('first', 'JJ'), ('part', 'NN'), ('regular', 'JJ'), ('pixels', 'NNS'), ('activating', 'VBG'), ('regular', 'JJ'), ('pixels', 'NNS'), ('perform', 'VBP'), ('according', 'VBG'), ('pixel', 'NN'), ('data', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('operating', 'VBG'), ('ed', 'NN'), ('wherein', 'NN'), ('pixel', 'NN'), ('data', 'NNS'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('captured', 'VBD'), ('frame', 'JJ'), ('pixel', 'NN'), ('data', 'NNS'), ('regular', 'JJ'), ('pixels', 'NNS'), ('also', 'RB'), ('used', 'VBD'), ('performing', 'VBG'), ('decoding', 'VBG'), ('operating', 'VBG'), ('ed', 'NN'), ('wherein', 'NN'), ('decoding', 'VBG'), ('decoding', 'VBG'), ('qr', 'NN'), ('codes', 'NNS'), ('face', 'VBP'), ('operating', 'VBG'), ('ed', 'NN'), ('wherein', 'NN'), ('phase', 'NN'), ('detection', 'NN'), ('pixel', 'NN'), ('pairs', 'NNS'), ('partially', 'RB'), ('covered', 'VBD'), ('pixels', 'NNS'), ('structure', 'NN'), ('dual', 'JJ'), ('pixel', 'NN'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('first', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('configured', 'VBD'), ('obtain', 'VB'), ('first', 'JJ'), ('object', 'JJ'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('second', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('configured', 'VBD'), ('obtain', 'VB'), ('second', 'JJ'), ('object', 'JJ'), ('second', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('different', 'JJ'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('generator', 'NN'), ('configured', 'VBD'), ('generate', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('first', 'RB'), ('based', 'VBN'), ('first', 'JJ'), ('second', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('generator', 'NN'), ('configured', 'VBD'), ('generate', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('second', 'NN'), ('based', 'VBN'), ('first', 'RB'), ('second', 'JJ'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('apparatus', 'NN'), ('wherein', 'VBP'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('narrow', 'JJ'), ('angle', 'JJ'), ('second', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('wider', 'VBP'), ('angle', 'NN'), ('apparatus', 'NN'), ('wherein', 'JJ'), ('second', 'NN'), ('divided', 'VBD'), ('primary', 'JJ'), ('region', 'NN'), ('residual', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('generator', 'NN'), ('comprises', 'VBZ'), ('relationship', 'NN'), ('estimating', 'VBG'), ('module', 'NN'), ('configured', 'VBD'), ('estimate', 'NN'), ('relationship', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('residual', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('estimating', 'VBG'), ('module', 'NN'), ('configured', 'VBD'), ('estimate', 'NN'), ('depth', 'NN'), ('map', 'VBP'), ('residual', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('estimated', 'VBN'), ('relationship', 'NN'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('apparatus', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('relationship', 'NN'), ('estimating', 'VBG'), ('module', 'NN'), ('depth', 'NN'), ('map', 'NN'), ('estimating', 'VBG'), ('module', 'NN'), ('performs', 'NNS'), ('estimating', 'VBG'), ('operation', 'NN'), ('based', 'VBN'), ('neural', 'JJ'), ('network', 'NN'), ('module', 'NN'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('depth', 'JJ'), ('map', 'JJ'), ('fusion', 'NN'), ('unit', 'NN'), ('configured', 'VBD'), ('generate', 'JJ'), ('third', 'JJ'), ('depth', 'NN'), ('map', 'JJ'), ('second', 'JJ'), ('performing', 'VBG'), ('fusion', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('first', 'RB'), ('depth', 'JJ'), ('map', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('apparatus', 'NN'), ('wherein', 'NN'), ('depth', 'NN'), ('map', 'JJ'), ('fusion', 'NN'), ('unit', 'NN'), ('comprises', 'VBZ'), ('tone', 'CD'), ('mapping', 'NN'), ('module', 'NN'), ('configured', 'VBD'), ('generate', 'JJ'), ('tone-mapped', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('correspond', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('performing', 'VBG'), ('bias', 'JJ'), ('removing', 'VBG'), ('operation', 'NN'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('fusion', 'NN'), ('module', 'NN'), ('configured', 'VBD'), ('generate', 'JJ'), ('third', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('fusing', 'VBG'), ('tone-mapped', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('apparatus', 'NN'), ('wherein', 'NN'), ('depth', 'NN'), ('map', 'JJ'), ('fusion', 'NN'), ('unit', 'NN'), ('comprises', 'VBZ'), ('propagating', 'VBG'), ('module', 'NN'), ('configured', 'VBD'), ('generate', 'NN'), ('propagated', 'VBN'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('second', 'JJ'), ('iterated', 'VBD'), ('propagating', 'VBG'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('based', 'VBN'), ('first', 'RB'), ('depth', 'JJ'), ('map', 'JJ'), ('second', 'JJ'), ('fusion', 'NN'), ('module', 'NN'), ('generates', 'VBZ'), ('third', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('fusing', 'VBG'), ('tone-mapped', 'JJ'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('propagated', 'VBD'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('apparatus', 'NN'), ('wherein', 'NN'), ('depth', 'NN'), ('map', 'JJ'), ('fusion', 'NN'), ('unit', 'NN'), ('comprises', 'VBZ'), ('post-processing', 'JJ'), ('module', 'NN'), ('configured', 'VBD'), ('perform', 'JJ'), ('post-processing', 'JJ'), ('operation', 'NN'), ('third', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('generated', 'VBD'), ('fusion', 'NN'), ('module', 'NN'), ('provide', 'IN'), ('post-processed', 'JJ'), ('third', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('apparatus', 'NN'), ('wherein', 'VBD'), ('post-processing', 'JJ'), ('module', 'NN'), ('performs', 'NNS'), ('post-processing', 'JJ'), ('operation', 'NN'), ('filtering', 'VBG'), ('interface', 'NN'), ('generated', 'VBD'), ('third', 'JJ'), ('depth', 'JJ'), ('map', 'NN'), ('accordance', 'NN'), ('fusion', 'NN'), ('fusion', 'NN'), ('module', 'NN'), ('apparatus', 'IN'), ('wherein', 'JJ'), ('post-processing', 'JJ'), ('module', 'NN'), ('removes', 'VBZ'), ('artifacts', 'NNS'), ('generated', 'VBD'), ('third', 'JJ'), ('depth', 'JJ'), ('map', 'NN'), ('accordance', 'NN'), ('fusion', 'NN'), ('fusion', 'NN'), ('module', 'NN'), ('apparatus', 'NN'), ('wherein', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('generator', 'NN'), ('analyses', 'VBZ'), ('distance', 'NN'), ('relationship', 'NN'), ('first', 'JJ'), ('second', 'NN'), ('generates', 'NNS'), ('first', 'RB'), ('depth', 'VB'), ('map', 'NN'), ('first', 'RB'), ('based', 'VBN'), ('distance', 'NN'), ('relationship', 'NN'), ('processing', 'VBG'), ('electronic', 'JJ'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('obtaining', 'VBG'), ('first', 'JJ'), ('object', 'NN'), ('using', 'VBG'), ('first', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('obtaining', 'VBG'), ('second', 'JJ'), ('object', 'JJ'), ('using', 'VBG'), ('second', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('generating', 'VBG'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('first', 'RB'), ('based', 'VBN'), ('first', 'JJ'), ('second', 'JJ'), ('estimating', 'VBG'), ('relationship', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('residual', 'JJ'), ('region', 'NN'), ('second', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('second', 'JJ'), ('generating', 'VBG'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('second', 'JJ'), ('based', 'VBN'), ('estimated', 'VBN'), ('relationship', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('residual', 'JJ'), ('region', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('wherein', 'NN'), ('electronic', 'JJ'), ('apparatus', 'NN'), ('comprises', 'NNS'), ('first', 'RB'), ('camera', 'VBP'), ('module', 'NN'), ('including', 'VBG'), ('first', 'JJ'), ('lens', 'NNS'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('second', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('including', 'VBG'), ('second', 'JJ'), ('lens', 'JJ'), ('second', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('wider', 'VBP'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('wherein', 'VBP'), ('generating', 'VBG'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('comprises', 'VBZ'), ('estimating', 'VBG'), ('depth', 'NN'), ('map', 'FW'), ('residual', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('estimated', 'VBN'), ('relationship', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('residual', 'JJ'), ('region', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'JJ'), ('generating', 'VBG'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NNS'), ('based', 'VBN'), ('depth', 'JJ'), ('map', 'JJ'), ('residual', 'JJ'), ('region', 'NN'), ('first', 'RB'), ('depth', 'VBZ'), ('map', 'NN'), ('wherein', 'NN'), ('estimating', 'VBG'), ('relationship', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('second', 'NN'), ('performed', 'VBD'), ('using', 'VBG'), ('neural', 'JJ'), ('network', 'NN'), ('model', 'NN'), ('comprising', 'VBG'), ('performing', 'VBG'), ('pre-processing', 'JJ'), ('operation', 'NN'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('generating', 'VBG'), ('third', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('residual', 'JJ'), ('fusing', 'VBG'), ('second', 'JJ'), ('depth', 'JJ'), ('map', 'NN'), ('pre-processing', 'JJ'), ('operation', 'NN'), ('performed', 'VBD'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('wherein', 'IN'), ('performing', 'VBG'), ('pre-processing', 'JJ'), ('operation', 'NN'), ('comprises', 'NNS'), ('performing', 'VBG'), ('tone', 'NN'), ('mapping', 'NN'), ('operation', 'NN'), ('depth', 'NN'), ('map', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('depth', 'NN'), ('map', 'VBP'), ('residual', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('operating', 'VBG'), ('electronic', 'JJ'), ('apparatus', 'NN'), ('electronic', 'JJ'), ('apparatus', 'NN'), ('including', 'VBG'), ('first', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('providing', 'VBG'), ('first', 'JJ'), ('object', 'NN'), ('using', 'VBG'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('second', 'JJ'), ('camera', 'NN'), ('module', 'NN'), ('providing', 'VBG'), ('second', 'JJ'), ('object', 'JJ'), ('using', 'VBG'), ('second', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('wider', 'VBP'), ('first', 'JJ'), ('field', 'NN'), ('view', 'NN'), ('generating', 'VBG'), ('depth', 'NN'), ('map', 'FW'), ('second', 'JJ'), ('based', 'VBN'), ('primary', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('residual', 'JJ'), ('region', 'NN'), ('second', 'JJ'), ('operating', 'NN'), ('comprising', 'VBG'), ('generating', 'VBG'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('estimating', 'VBG'), ('relationship', 'NN'), ('first', 'JJ'), ('second', 'JJ'), ('estimating', 'VBG'), ('relationship', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('residual', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('second', 'JJ'), ('generating', 'VBG'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'JJ'), ('second', 'JJ'), ('estimating', 'VBG'), ('depth', 'NN'), ('map', 'FW'), ('second', 'JJ'), ('region', 'NN'), ('based', 'VBN'), ('estimated', 'VBN'), ('relationship', 'NN'), ('primary', 'JJ'), ('region', 'NN'), ('residual', 'JJ'), ('region', 'NN'), ('generating', 'VBG'), ('depth', 'JJ'), ('map', 'JJ'), ('second', 'NN'), ('fusing', 'VBG'), ('first', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('second', 'JJ'), ('depth', 'NN'), ('map', 'NN'), ('operation', 'NN'), ('comprising', 'VBG'), ('executing', 'VBG'), ('application', 'NN'), ('applies', 'NNS'), ('effect', 'NN'), ('second', 'NN'), ('based', 'VBN'), ('depth', 'NN'), ('map', 'JJ'), ('residual', 'JJ'), ('operation', 'NN'), ('wherein', 'WRB'), ('application', 'NN'), ('applies', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('effect', 'NN'), ('auto-focusing', 'JJ'), ('out-focusing', 'JJ'), ('forebackground', 'NN'), ('separation', 'NN'), ('face', 'NN'), ('object', 'JJ'), ('detection', 'NN'), ('within', 'IN'), ('frame', 'NN'), ('augmented', 'JJ'), ('reality', 'NN'), ('second', 'NN'), ('based', 'VBN'), ('depth', 'JJ'), ('map', 'JJ'), ('second', 'JJ'), ('payment', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('comprising', 'VBG'), ('acquiring', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('target', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('wherein', 'IN'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('includes', 'VBZ'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('target', 'NN'), ('willingness', 'NN'), ('pay', 'NN'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('including', 'VBG'), ('determining', 'VBG'), ('whether', 'IN'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('less', 'RBR'), ('angle', 'JJ'), ('threshold', 'NN'), ('wherein', 'NN'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('includes', 'VBZ'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('greater', 'JJR'), ('probability', 'NN'), ('threshold', 'NN'), ('wherein', 'NN'), ('gaze', 'JJ'), ('information', 'NN'), ('includes', 'VBZ'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('less', 'RBR'), ('angle', 'JJ'), ('threshold', 'NN'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('greater', 'JJR'), ('probability', 'NN'), ('threshold', 'VBD'), ('determining', 'VBG'), ('target', 'NN'), ('willingness', 'JJ'), ('pay', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('target', 'NN'), ('willingness', 'JJ'), ('pay', 'NN'), ('completing', 'VBG'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('ed', 'JJ'), ('wherein', 'NN'), ('completing', 'VBG'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('comprises', 'VBZ'), ('triggering', 'VBG'), ('performing', 'VBG'), ('payment', 'NN'), ('initiating', 'NN'), ('operation', 'NN'), ('acquire', 'VB'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('triggering', 'VBG'), ('performing', 'VBG'), ('payment', 'NN'), ('confirmation', 'NN'), ('operation', 'NN'), ('complete', 'JJ'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('payment', 'NN'), ('account', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('target', 'NN'), ('ed', 'JJ'), ('wherein', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('comprises', 'NNS'), ('determining', 'VBG'), ('whether', 'IN'), ('current', 'JJ'), ('corresponding', 'VBG'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('consistent', 'JJ'), ('target', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('current', 'JJ'), ('consistent', 'JJ'), ('target', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('target', 'NN'), ('willingness', 'NN'), ('pay', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('using', 'VBG'), ('head', 'JJ'), ('posture', 'NN'), ('model', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('using', 'VBG'), ('gaze', 'JJ'), ('information', 'NN'), ('model', 'NN'), ('based', 'VBN'), ('characteristics', 'NNS'), ('eye', 'NN'), ('region', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('head', 'NN'), ('posture', 'NN'), ('model', 'NN'), ('obtained', 'VBD'), ('training', 'VBG'), ('acquiring', 'VBG'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('wherein', 'NN'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('includes', 'VBZ'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('includes', 'VBZ'), ('correspondence', 'NN'), ('sample', 'NN'), ('face', 'VBP'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('mean', 'NN'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('sample', 'NN'), ('face', 'NN'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('sample', 'JJ'), ('face', 'NN'), ('contained', 'VBD'), ('pieces', 'NNS'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('based', 'VBN'), ('mean', 'JJ'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('obtain', 'VB'), ('preprocessed', 'JJ'), ('sample', 'JJ'), ('face', 'NN'), ('setting', 'VBG'), ('preprocessed', 'JJ'), ('sample', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('head', 'JJ'), ('posture', 'NN'), ('information', 'NN'), ('first', 'RB'), ('model', 'VBZ'), ('training', 'VBG'), ('sample', 'JJ'), ('performing', 'VBG'), ('training', 'NN'), ('using', 'VBG'), ('machine', 'NN'), ('learning', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('model', 'NN'), ('training', 'NN'), ('samples', 'NNS'), ('obtain', 'VB'), ('head', 'JJ'), ('posture', 'NN'), ('model', 'NN'), ('ed', 'JJ'), ('wherein', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('model', 'NN'), ('obtained', 'VBD'), ('training', 'VBG'), ('acquiring', 'VBG'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('wherein', 'JJ'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('includes', 'VBZ'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'JJ'), ('data', 'NNS'), ('includes', 'VBZ'), ('correspondence', 'NN'), ('sample', 'NN'), ('eye', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('mean', 'NN'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('sample', 'NN'), ('eye', 'NN'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('sample', 'NN'), ('eye', 'NN'), ('contained', 'VBD'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('based', 'VBN'), ('mean', 'JJ'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('obtain', 'VB'), ('preprocessed', 'JJ'), ('sample', 'NN'), ('eye', 'NN'), ('setting', 'VBG'), ('preprocessed', 'JJ'), ('sample', 'NN'), ('eye', 'NN'), ('corresponding', 'VBG'), ('gaze', 'JJ'), ('information', 'NN'), ('second', 'JJ'), ('model', 'NN'), ('training', 'VBG'), ('sample', 'JJ'), ('performing', 'VBG'), ('training', 'NN'), ('using', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('based', 'VBN'), ('second', 'JJ'), ('model', 'NN'), ('training', 'NN'), ('samples', 'NNS'), ('obtain', 'VB'), ('gaze', 'JJ'), ('information', 'NN'), ('model', 'NN'), ('ed', 'NN'), ('wherein', 'VBP'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('comprises', 'VBZ'), ('pitch', 'VBP'), ('angle', 'NN'), ('yaw', 'NN'), ('angle', 'NN'), ('roll', 'NN'), ('angle', 'NN'), ('wherein', 'NN'), ('pitch', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('x-axis', 'JJ'), ('yaw', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('y-axis', 'JJ'), ('roll', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('z-axis', 'JJ'), ('payment', 'NN'), ('device', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('comprising', 'VBG'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('storing', 'VBG'), ('instructions', 'NNS'), ('executable', 'JJ'), ('cause', 'NN'), ('device', 'NN'), ('perform', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('acquiring', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('target', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('wherein', 'IN'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('includes', 'VBZ'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('target', 'NN'), ('willingness', 'NN'), ('pay', 'NN'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('including', 'VBG'), ('determining', 'VBG'), ('whether', 'IN'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('less', 'RBR'), ('angle', 'JJ'), ('threshold', 'NN'), ('wherein', 'NN'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('includes', 'VBZ'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('greater', 'JJR'), ('probability', 'NN'), ('threshold', 'NN'), ('wherein', 'NN'), ('gaze', 'JJ'), ('information', 'NN'), ('includes', 'VBZ'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('less', 'RBR'), ('angle', 'JJ'), ('threshold', 'NN'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('greater', 'JJR'), ('probability', 'NN'), ('threshold', 'VBD'), ('determining', 'VBG'), ('target', 'NN'), ('willingness', 'JJ'), ('pay', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('target', 'NN'), ('willingness', 'JJ'), ('pay', 'NN'), ('completing', 'VBG'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('completing', 'VBG'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('comprises', 'VBZ'), ('triggering', 'VBG'), ('performing', 'VBG'), ('payment', 'NN'), ('initiating', 'NN'), ('operation', 'NN'), ('acquire', 'VB'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('triggering', 'VBG'), ('performing', 'VBG'), ('payment', 'NN'), ('confirmation', 'NN'), ('operation', 'NN'), ('complete', 'JJ'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('payment', 'NN'), ('account', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('target', 'NN'), ('device', 'NN'), ('ed', 'NN'), ('wherein', 'IN'), ('determining', 'VBG'), ('whether', 'IN'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('comprises', 'NNS'), ('determining', 'VBG'), ('whether', 'IN'), ('current', 'JJ'), ('corresponding', 'VBG'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('consistent', 'JJ'), ('target', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('current', 'JJ'), ('consistent', 'JJ'), ('target', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('target', 'NN'), ('willingness', 'NN'), ('pay', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('device', 'NN'), ('ed', 'NN'), ('wherein', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('using', 'VBG'), ('head', 'JJ'), ('posture', 'NN'), ('model', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('using', 'VBG'), ('gaze', 'JJ'), ('information', 'NN'), ('model', 'NN'), ('based', 'VBN'), ('characteristics', 'NNS'), ('eye', 'NN'), ('region', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('device', 'NN'), ('ed', 'NN'), ('wherein', 'WRB'), ('head', 'NN'), ('posture', 'NN'), ('model', 'NN'), ('obtained', 'VBD'), ('training', 'VBG'), ('acquiring', 'VBG'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('wherein', 'NN'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('includes', 'VBZ'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('includes', 'VBZ'), ('correspondence', 'NN'), ('sample', 'NN'), ('face', 'VBP'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('mean', 'NN'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('sample', 'NN'), ('face', 'NN'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('sample', 'JJ'), ('face', 'NN'), ('contained', 'VBD'), ('pieces', 'NNS'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('based', 'VBN'), ('mean', 'JJ'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('obtain', 'VB'), ('preprocessed', 'JJ'), ('sample', 'JJ'), ('face', 'NN'), ('setting', 'VBG'), ('preprocessed', 'JJ'), ('sample', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('head', 'JJ'), ('posture', 'NN'), ('information', 'NN'), ('first', 'RB'), ('model', 'VBZ'), ('training', 'VBG'), ('sample', 'JJ'), ('performing', 'VBG'), ('training', 'NN'), ('using', 'VBG'), ('machine', 'NN'), ('learning', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('model', 'NN'), ('training', 'NN'), ('samples', 'NNS'), ('obtain', 'VB'), ('head', 'JJ'), ('posture', 'NN'), ('model', 'NN'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('model', 'NN'), ('obtained', 'VBD'), ('training', 'VBG'), ('acquiring', 'VBG'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('wherein', 'JJ'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('includes', 'VBZ'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'JJ'), ('data', 'NNS'), ('includes', 'VBZ'), ('correspondence', 'NN'), ('sample', 'NN'), ('eye', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('mean', 'NN'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('sample', 'NN'), ('eye', 'NN'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('sample', 'NN'), ('eye', 'NN'), ('contained', 'VBD'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('based', 'VBN'), ('mean', 'JJ'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('obtain', 'VB'), ('preprocessed', 'JJ'), ('sample', 'NN'), ('eye', 'NN'), ('setting', 'VBG'), ('preprocessed', 'JJ'), ('sample', 'NN'), ('eye', 'NN'), ('corresponding', 'VBG'), ('gaze', 'JJ'), ('information', 'NN'), ('second', 'JJ'), ('model', 'NN'), ('training', 'VBG'), ('sample', 'JJ'), ('performing', 'VBG'), ('training', 'NN'), ('using', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('second', 'JJ'), ('model', 'NN'), ('training', 'NN'), ('samples', 'NNS'), ('obtain', 'VB'), ('gaze', 'JJ'), ('information', 'NN'), ('model', 'NN'), ('device', 'NN'), ('ed', 'FW'), ('wherein', 'NN'), ('angle', 'NN'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('comprises', 'VBZ'), ('pitch', 'VBP'), ('angle', 'NN'), ('yaw', 'NN'), ('angle', 'NN'), ('roll', 'NN'), ('angle', 'NN'), ('wherein', 'NN'), ('pitch', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('x-axis', 'JJ'), ('yaw', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('y-axis', 'JJ'), ('roll', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('z-axis', 'JJ'), ('non-transitory', 'JJ'), ('computer-readable', 'JJ'), ('storage', 'NN'), ('medium', 'NN'), ('payment', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('configured', 'VBN'), ('instructions', 'NNS'), ('executable', 'JJ'), ('one', 'CD'), ('cause', 'NN'), ('one', 'CD'), ('perform', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('acquiring', 'VBG'), ('first', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('target', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('wherein', 'IN'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('includes', 'VBZ'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('target', 'NN'), ('willingness', 'NN'), ('pay', 'NN'), ('according', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('including', 'VBG'), ('determining', 'VBG'), ('whether', 'IN'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('less', 'RBR'), ('angle', 'JJ'), ('threshold', 'NN'), ('wherein', 'NN'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('includes', 'VBZ'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('greater', 'JJR'), ('probability', 'NN'), ('threshold', 'NN'), ('wherein', 'NN'), ('gaze', 'JJ'), ('information', 'NN'), ('includes', 'VBZ'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('less', 'RBR'), ('angle', 'JJ'), ('threshold', 'NN'), ('probability', 'NN'), ('value', 'NN'), ('gazes', 'VBZ'), ('payment', 'NN'), ('screen', 'NN'), ('greater', 'JJR'), ('probability', 'NN'), ('threshold', 'VBD'), ('determining', 'VBG'), ('target', 'NN'), ('willingness', 'JJ'), ('pay', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('target', 'NN'), ('willingness', 'JJ'), ('pay', 'NN'), ('completing', 'VBG'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('storage', 'NN'), ('medium', 'NN'), ('ed', 'VBZ'), ('wherein', 'JJ'), ('completing', 'VBG'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('comprises', 'VBZ'), ('triggering', 'VBG'), ('performing', 'VBG'), ('payment', 'NN'), ('initiating', 'NN'), ('operation', 'NN'), ('acquire', 'VB'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('based', 'VBN'), ('face', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('triggering', 'VBG'), ('performing', 'VBG'), ('payment', 'NN'), ('confirmation', 'NN'), ('operation', 'NN'), ('complete', 'JJ'), ('payment', 'NN'), ('operation', 'NN'), ('based', 'VBN'), ('payment', 'NN'), ('account', 'NN'), ('information', 'NN'), ('corresponding', 'VBG'), ('target', 'NN'), ('storage', 'NN'), ('medium', 'NN'), ('ed', 'NN'), ('wherein', 'IN'), ('determining', 'VBG'), ('whether', 'IN'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('indicates', 'VBZ'), ('willingness', 'JJ'), ('pay', 'NN'), ('comprises', 'NNS'), ('determining', 'VBG'), ('whether', 'IN'), ('current', 'JJ'), ('corresponding', 'VBG'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('consistent', 'JJ'), ('target', 'NN'), ('response', 'NN'), ('determining', 'VBG'), ('current', 'JJ'), ('consistent', 'JJ'), ('target', 'NN'), ('determining', 'VBG'), ('whether', 'IN'), ('target', 'NN'), ('willingness', 'NN'), ('pay', 'NN'), ('according', 'VBG'), ('second', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('extracted', 'VBD'), ('second', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('storage', 'NN'), ('medium', 'NN'), ('ed', 'NN'), ('wherein', 'NN'), ('extracting', 'VBG'), ('first', 'JJ'), ('characteristic', 'JJ'), ('information', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('comprises', 'VBZ'), ('determining', 'VBG'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('target', 'NN'), ('using', 'VBG'), ('head', 'JJ'), ('posture', 'NN'), ('model', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('face', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('gaze', 'NN'), ('information', 'NN'), ('target', 'NN'), ('using', 'VBG'), ('gaze', 'JJ'), ('information', 'NN'), ('model', 'NN'), ('based', 'VBN'), ('characteristics', 'NNS'), ('eye', 'NN'), ('region', 'NN'), ('first', 'RB'), ('face', 'NN'), ('information', 'NN'), ('storage', 'NN'), ('medium', 'NN'), ('ed', 'NN'), ('wherein', 'WRB'), ('head', 'NN'), ('posture', 'NN'), ('model', 'NN'), ('obtained', 'VBD'), ('training', 'VBG'), ('acquiring', 'VBG'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('wherein', 'NN'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('includes', 'VBZ'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('includes', 'VBZ'), ('correspondence', 'NN'), ('sample', 'NN'), ('face', 'VBP'), ('head', 'NN'), ('posture', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('mean', 'NN'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('sample', 'NN'), ('face', 'NN'), ('pieces', 'NNS'), ('first', 'RB'), ('sample', 'JJ'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('sample', 'JJ'), ('face', 'NN'), ('contained', 'VBD'), ('pieces', 'NNS'), ('first', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('based', 'VBN'), ('mean', 'JJ'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('obtain', 'VB'), ('preprocessed', 'JJ'), ('sample', 'JJ'), ('face', 'NN'), ('setting', 'VBG'), ('preprocessed', 'JJ'), ('sample', 'JJ'), ('face', 'NN'), ('corresponding', 'VBG'), ('head', 'JJ'), ('posture', 'NN'), ('information', 'NN'), ('first', 'RB'), ('model', 'VBZ'), ('training', 'VBG'), ('sample', 'JJ'), ('performing', 'VBG'), ('training', 'NN'), ('using', 'VBG'), ('machine', 'NN'), ('learning', 'NN'), ('based', 'VBN'), ('first', 'JJ'), ('model', 'NN'), ('training', 'NN'), ('samples', 'NNS'), ('obtain', 'VB'), ('head', 'JJ'), ('posture', 'NN'), ('model', 'NN'), ('wherein', 'NN'), ('gaze', 'JJ'), ('information', 'NN'), ('model', 'NN'), ('obtained', 'VBD'), ('training', 'VBG'), ('acquiring', 'VBG'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('wherein', 'JJ'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('set', 'VBD'), ('includes', 'VBZ'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'JJ'), ('data', 'NNS'), ('includes', 'VBZ'), ('correspondence', 'NN'), ('sample', 'NN'), ('eye', 'NN'), ('gaze', 'NN'), ('information', 'NN'), ('determining', 'VBG'), ('mean', 'NN'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('sample', 'NN'), ('eye', 'NN'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('preprocessing', 'VBG'), ('sample', 'NN'), ('eye', 'NN'), ('contained', 'VBD'), ('pieces', 'NNS'), ('second', 'JJ'), ('sample', 'NN'), ('data', 'NNS'), ('based', 'VBN'), ('mean', 'JJ'), ('data', 'NNS'), ('variance', 'NN'), ('data', 'NNS'), ('obtain', 'VB'), ('preprocessed', 'JJ'), ('sample', 'NN'), ('eye', 'NN'), ('setting', 'VBG'), ('preprocessed', 'JJ'), ('sample', 'NN'), ('eye', 'NN'), ('corresponding', 'VBG'), ('gaze', 'JJ'), ('information', 'NN'), ('second', 'JJ'), ('model', 'NN'), ('training', 'VBG'), ('sample', 'JJ'), ('performing', 'VBG'), ('training', 'NN'), ('using', 'VBG'), ('machine', 'NN'), ('learning', 'VBG'), ('based', 'VBN'), ('second', 'JJ'), ('model', 'NN'), ('training', 'NN'), ('samples', 'NNS'), ('obtain', 'VB'), ('gaze', 'JJ'), ('information', 'NN'), ('model', 'NN'), ('storage', 'NN'), ('medium', 'NN'), ('ed', 'NN'), ('wherein', 'NN'), ('angle', 'JJ'), ('rotation', 'NN'), ('preset', 'VBN'), ('direction', 'NN'), ('comprises', 'VBZ'), ('pitch', 'VBP'), ('angle', 'NN'), ('yaw', 'NN'), ('angle', 'NN'), ('roll', 'NN'), ('angle', 'NN'), ('wherein', 'NN'), ('pitch', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('x-axis', 'JJ'), ('yaw', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('y-axis', 'JJ'), ('roll', 'NN'), ('angle', 'NN'), ('refers', 'NNS'), ('angle', 'VBP'), ('rotation', 'NN'), ('around', 'IN'), ('z-axis', 'JJ'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('motion', 'NN'), ('detection', 'NN'), ('module', 'NN'), ('motion', 'NN'), ('subject', 'NN'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('assigning', 'VBG'), ('unique', 'JJ'), ('session', 'NN'), ('identification', 'NN'), ('number', 'NN'), ('subject', 'JJ'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('generating', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('assessing', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('determining', 'VBG'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('identifying', 'VBG'), ('intent', 'NN'), ('subject', 'JJ'), ('authorizing', 'VBG'), ('access', 'NN'), ('point', 'NN'), ('entry', 'NN'), ('based', 'VBN'), ('determined', 'VBN'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('intent', 'NN'), ('subject', 'JJ'), ('comprising', 'VBG'), ('determining', 'VBG'), ('one', 'CD'), ('additional', 'JJ'), ('subjects', 'NNS'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('assigning', 'VBG'), ('unique', 'JJ'), ('session', 'NN'), ('identification', 'NN'), ('number', 'NN'), ('one', 'CD'), ('additional', 'JJ'), ('subjects', 'NNS'), ('detected', 'VBN'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('wherein', 'IN'), ('assessing', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprises', 'VBZ'), ('assessing', 'VBG'), ('whether', 'IN'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('object', 'NN'), ('equates', 'NNS'), ('predetermined', 'VBD'), ('metric', 'JJ'), ('quality', 'NN'), ('upon', 'IN'), ('determining', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('object', 'VBP'), ('inferior', 'JJ'), ('predetermined', 'VBN'), ('metric', 'JJ'), ('quality', 'NN'), ('discarding', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('generating', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('whether', 'IN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('photographic', 'JJ'), ('upon', 'IN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('photographic', 'JJ'), ('generating', 'VBG'), ('warning', 'VBG'), ('restrict', 'JJ'), ('access', 'NN'), ('point', 'NN'), ('entry', 'NN'), ('comprising', 'VBG'), ('conducing', 'VBG'), ('incremental', 'JJ'), ('training', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('wherein', 'NN'), ('conducing', 'VBG'), ('incremental', 'JJ'), ('training', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprises', 'NNS'), ('capturing', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('converting', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('first', 'RB'), ('numeric', 'JJ'), ('vector', 'NN'), ('capturing', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('converting', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('second', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('calculating', 'VBG'), ('weighted', 'VBD'), ('mean', 'JJ'), ('first', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('second', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('wherein', 'NN'), ('weighted', 'VBD'), ('mean', 'JJ'), ('represents', 'VBZ'), ('change', 'VBP'), ('facial', 'JJ'), ('area', 'NN'), ('storing', 'VBG'), ('weighted', 'JJ'), ('mean', 'JJ'), ('database', 'NN'), ('wherein', 'NN'), ('determining', 'VBG'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprises', 'VBZ'), ('comparing', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('stored', 'VBD'), ('database', 'NN'), ('authenticating', 'VBG'), ('subject', 'JJ'), ('wherein', 'NN'), ('identifying', 'VBG'), ('intent', 'NN'), ('subject', 'NN'), ('comprises', 'VBZ'), ('upon', 'IN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('bounding', 'VBG'), ('box', 'NN'), ('commencing', 'VBG'), ('authentication', 'NN'), ('subject', 'JJ'), ('calculating', 'VBG'), ('directional', 'JJ'), ('vector', 'NN'), ('face', 'NN'), ('subject', 'JJ'), ('determine', 'JJ'), ('intent', 'NN'), ('subject', 'JJ'), ('gain', 'NN'), ('access', 'NN'), ('point', 'NN'), ('entry', 'NN'), ('based', 'VBN'), ('directional', 'JJ'), ('vector', 'NN'), ('face', 'NN'), ('subject', 'JJ'), ('granting', 'VBG'), ('access', 'NN'), ('point', 'NN'), ('entry', 'NN'), ('based', 'VBN'), ('authentication', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('determining', 'VBG'), ('intent', 'NN'), ('subject', 'JJ'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('program', 'NN'), ('instructions', 'NNS'), ('stored', 'VBD'), ('thereon', 'JJ'), ('response', 'NN'), ('execution', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('cause', 'NN'), ('computing', 'VBG'), ('device', 'NN'), ('perform', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('motion', 'NN'), ('subject', 'NN'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('assigning', 'VBG'), ('unique', 'JJ'), ('session', 'NN'), ('identification', 'NN'), ('number', 'NN'), ('subject', 'JJ'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('generating', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('assessing', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('determining', 'VBG'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('identifying', 'VBG'), ('intent', 'NN'), ('subject', 'JJ'), ('authorizing', 'VBG'), ('access', 'NN'), ('point', 'NN'), ('entry', 'NN'), ('based', 'VBN'), ('determined', 'VBN'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('intent', 'NN'), ('subject', 'JJ'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('determining', 'VBG'), ('one', 'CD'), ('additional', 'JJ'), ('subjects', 'NNS'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('assigning', 'VBG'), ('unique', 'JJ'), ('session', 'NN'), ('identification', 'NN'), ('number', 'NN'), ('one', 'CD'), ('additional', 'JJ'), ('subjects', 'NNS'), ('detected', 'VBN'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('assessing', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprises', 'VBZ'), ('assessing', 'VBG'), ('whether', 'IN'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('object', 'NN'), ('equates', 'NNS'), ('predetermined', 'VBD'), ('metric', 'JJ'), ('quality', 'NN'), ('upon', 'IN'), ('determining', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('object', 'VBP'), ('inferior', 'JJ'), ('predetermined', 'VBN'), ('metric', 'JJ'), ('quality', 'NN'), ('discarding', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('generating', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('whether', 'IN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('photographic', 'JJ'), ('upon', 'IN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('photographic', 'JJ'), ('generating', 'VBG'), ('warning', 'VBG'), ('restrict', 'JJ'), ('access', 'NN'), ('access', 'NN'), ('point', 'NN'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('conducing', 'VBG'), ('incremental', 'JJ'), ('training', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('non-transitory', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('wherein', 'NN'), ('conducing', 'VBG'), ('incremental', 'JJ'), ('training', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprises', 'NNS'), ('capturing', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('converting', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('first', 'RB'), ('numeric', 'JJ'), ('vector', 'NN'), ('capturing', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('converting', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('second', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('calculating', 'VBG'), ('weighted', 'VBD'), ('mean', 'JJ'), ('first', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('second', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('wherein', 'NN'), ('weighted', 'VBD'), ('mean', 'JJ'), ('represents', 'VBZ'), ('change', 'VBP'), ('facial', 'JJ'), ('area', 'NN'), ('storing', 'VBG'), ('weighted', 'JJ'), ('mean', 'JJ'), ('database', 'NN'), ('apparatus', 'NN'), ('face', 'NN'), ('comprising', 'VBG'), ('memory', 'NN'), ('store', 'NN'), ('computer', 'NN'), ('program', 'NN'), ('instructions', 'NNS'), ('computer', 'NN'), ('program', 'NN'), ('instructions', 'NNS'), ('executed', 'VBD'), ('cause', 'NN'), ('perform', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('motion', 'NN'), ('subject', 'NN'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('assigning', 'VBG'), ('unique', 'JJ'), ('session', 'NN'), ('identification', 'NN'), ('number', 'NN'), ('subject', 'JJ'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('detected', 'VBD'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('generating', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'NN'), ('assessing', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('determining', 'VBG'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('identifying', 'VBG'), ('intent', 'NN'), ('subject', 'JJ'), ('authorizing', 'VBG'), ('access', 'NN'), ('point', 'NN'), ('entry', 'NN'), ('based', 'VBN'), ('determined', 'VBN'), ('identity', 'NN'), ('subject', 'NN'), ('based', 'VBN'), ('intent', 'NN'), ('subject', 'JJ'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('determining', 'VBG'), ('one', 'CD'), ('additional', 'JJ'), ('subjects', 'NNS'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('assigning', 'VBG'), ('unique', 'JJ'), ('session', 'NN'), ('identification', 'NN'), ('number', 'NN'), ('one', 'CD'), ('additional', 'JJ'), ('subjects', 'NNS'), ('detected', 'VBN'), ('within', 'IN'), ('predetermined', 'JJ'), ('area', 'NN'), ('view', 'NN'), ('apparatus', 'VBP'), ('wherein', 'NN'), ('assessing', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprises', 'VBZ'), ('assessing', 'VBG'), ('whether', 'IN'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('object', 'NN'), ('equates', 'NNS'), ('predetermined', 'VBD'), ('metric', 'JJ'), ('quality', 'NN'), ('upon', 'IN'), ('determining', 'VBG'), ('quality', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('object', 'VBP'), ('inferior', 'JJ'), ('predetermined', 'VBN'), ('metric', 'JJ'), ('quality', 'NN'), ('discarding', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('generating', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('whether', 'IN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('photographic', 'JJ'), ('upon', 'IN'), ('detecting', 'VBG'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('photographic', 'JJ'), ('generating', 'VBG'), ('warning', 'VBG'), ('restrict', 'JJ'), ('access', 'NN'), ('access', 'NN'), ('point', 'NN'), ('apparatus', 'NN'), ('comprising', 'VBG'), ('conducing', 'VBG'), ('incremental', 'JJ'), ('training', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('apparatus', 'NN'), ('wherein', 'NN'), ('conducing', 'VBG'), ('incremental', 'JJ'), ('training', 'NN'), ('facial', 'JJ'), ('area', 'NN'), ('subject', 'JJ'), ('comprises', 'NNS'), ('capturing', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('converting', 'VBG'), ('first', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('first', 'RB'), ('numeric', 'JJ'), ('vector', 'NN'), ('capturing', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('facial', 'JJ'), ('landmarks', 'NNS'), ('converting', 'VBG'), ('second', 'JJ'), ('facial', 'JJ'), ('area', 'NN'), ('second', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('calculating', 'VBG'), ('weighted', 'VBD'), ('mean', 'JJ'), ('first', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('second', 'JJ'), ('numeric', 'JJ'), ('vector', 'NN'), ('wherein', 'NN'), ('weighted', 'VBD'), ('mean', 'JJ'), ('represents', 'VBZ'), ('change', 'VBP'), ('facial', 'JJ'), ('area', 'NN'), ('storing', 'VBG'), ('weighted', 'JJ'), ('mean', 'JJ'), ('database', 'NN'), ('robot', 'NN'), ('comprising', 'VBG'), ('body', 'NN'), ('configured', 'VBD'), ('rotate', 'JJ'), ('tilt', 'NN'), ('camera', 'NN'), ('coupled', 'VBN'), ('body', 'NN'), ('configured', 'JJ'), ('rotate', 'NN'), ('tilt', 'NN'), ('according', 'VBG'), ('rotate', 'NN'), ('tilt', 'NN'), ('body', 'NN'), ('wherein', 'JJ'), ('camera', 'NN'), ('configured', 'VBD'), ('acquire', 'VB'), ('video', 'NN'), ('space', 'NN'), ('face', 'NN'), ('unit', 'NN'), ('configured', 'VBD'), ('recognize', 'JJ'), ('respective', 'JJ'), ('one', 'CD'), ('persons', 'NNS'), ('video', 'VBP'), ('unit', 'NN'), ('configured', 'VBD'), ('track', 'JJ'), ('motion', 'NN'), ('recognized', 'VBD'), ('one', 'CD'), ('persons', 'NNS'), ('controller', 'NN'), ('configured', 'VBD'), ('calculate', 'JJ'), ('respective', 'JJ'), ('size', 'NN'), ('one', 'CD'), ('persons', 'NNS'), ('select', 'VBP'), ('first', 'JJ'), ('person', 'NN'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('based', 'VBN'), ('calculated', 'JJ'), ('sizes', 'NNS'), ('control', 'NN'), ('least', 'VBP'), ('one', 'CD'), ('direction', 'NN'), ('rotation', 'NN'), ('camera', 'NN'), ('angle', 'NN'), ('tilt', 'NN'), ('camera', 'NN'), ('focal', 'JJ'), ('distance', 'NN'), ('camera', 'NN'), ('based', 'VBN'), ('tracked', 'JJ'), ('motion', 'NN'), ('recognized', 'VBD'), ('face', 'NN'), ('first', 'RB'), ('person', 'NN'), ('robot', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('control', 'JJ'), ('direction', 'NN'), ('rotation', 'NN'), ('camera', 'NN'), ('angle', 'NN'), ('tilt', 'NN'), ('camera', 'NN'), ('achieve', 'VBP'), ('particular', 'JJ'), ('camera', 'NN'), ('relative', 'JJ'), ('face', 'NN'), ('first', 'RB'), ('person', 'NN'), ('control', 'NN'), ('focal', 'JJ'), ('distance', 'NN'), ('camera', 'NN'), ('comparing', 'VBG'), ('respective', 'JJ'), ('sizes', 'NNS'), ('face', 'VBP'), ('first', 'JJ'), ('person', 'NN'), ('motion', 'NN'), ('first', 'RB'), ('person', 'NN'), ('robot', 'JJ'), ('wherein', 'NN'), ('particular', 'JJ'), ('occurs', 'NNS'), ('camera', 'VBP'), ('general', 'JJ'), ('direction', 'NN'), ('face', 'NN'), ('first', 'RB'), ('person', 'NN'), ('robot', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('normalize', 'JJ'), ('sizes', 'NNS'), ('one', 'CD'), ('persons', 'NNS'), ('based', 'VBN'), ('interocular', 'JJ'), ('distance', 'NN'), ('select', 'NN'), ('first', 'RB'), ('person', 'NN'), ('based', 'VBN'), ('normalized', 'JJ'), ('sizes', 'NNS'), ('one', 'CD'), ('persons', 'NNS'), ('robot', 'VBP'), ('wherein', 'JJ'), ('controller', 'NN'), ('configured', 'VBD'), ('select', 'JJ'), ('person', 'NN'), ('largest', 'JJS'), ('face', 'NN'), ('size', 'NN'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('first', 'JJ'), ('person', 'NN'), ('robot', 'NN'), ('comprising', 'VBG'), ('microphone', 'NN'), ('configured', 'VBD'), ('receive', 'JJ'), ('spoken', 'NN'), ('audio', 'NN'), ('present', 'JJ'), ('space', 'NN'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('select', 'JJ'), ('first', 'JJ'), ('person', 'NN'), ('based', 'VBN'), ('received', 'VBN'), ('spoken', 'JJ'), ('audio', 'JJ'), ('robot', 'NN'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('control', 'NN'), ('gain', 'NN'), ('microphone', 'NN'), ('comparing', 'VBG'), ('respective', 'JJ'), ('sizes', 'NNS'), ('face', 'VBP'), ('first', 'JJ'), ('person', 'NN'), ('motion', 'NN'), ('first', 'RB'), ('person', 'NN'), ('robot', 'JJ'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('calculate', 'JJ'), ('position', 'NN'), ('spoken', 'VBN'), ('audio', 'NN'), ('provided', 'VBD'), ('select', 'JJ'), ('first', 'JJ'), ('person', 'NN'), ('based', 'VBN'), ('whether', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('position', 'NN'), ('voice', 'NN'), ('signal', 'NN'), ('provided', 'VBD'), ('robot', 'JJ'), ('wherein', 'JJ'), ('controller', 'NN'), ('configured', 'VBD'), ('select', 'JJ'), ('second', 'JJ'), ('person', 'NN'), ('first', 'JJ'), ('person', 'NN'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('second', 'JJ'), ('person', 'NN'), ('located', 'VBN'), ('position', 'NN'), ('spoken', 'VBN'), ('audio', 'NN'), ('provided', 'VBD'), ('robot', 'JJ'), ('wherein', 'JJ'), ('controller', 'NN'), ('configured', 'VBD'), ('select', 'JJ'), ('second', 'JJ'), ('person', 'NN'), ('largest', 'JJS'), ('face', 'NN'), ('size', 'NN'), ('first', 'JJ'), ('person', 'NN'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('none', 'NN'), ('one', 'CD'), ('persons', 'NNS'), ('located', 'VBN'), ('position', 'NN'), ('spoken', 'VBN'), ('audio', 'NN'), ('provided', 'VBD'), ('robot', 'JJ'), ('wherein', 'JJ'), ('controller', 'NN'), ('configured', 'VBD'), ('select', 'JJ'), ('second', 'JJ'), ('person', 'NN'), ('largest', 'JJS'), ('face', 'NN'), ('size', 'NN'), ('first', 'JJ'), ('person', 'NN'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('persons', 'NNS'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('located', 'VBN'), ('position', 'NN'), ('spoken', 'VBN'), ('audio', 'NN'), ('provided', 'VBD'), ('robot', 'NN'), ('comprising', 'VBG'), ('speaker', 'NN'), ('wherein', 'NN'), ('controller', 'NN'), ('configured', 'VBD'), ('control', 'NN'), ('volume', 'NN'), ('speaker', 'NN'), ('comparing', 'VBG'), ('respective', 'JJ'), ('sizes', 'NNS'), ('face', 'VBP'), ('first', 'JJ'), ('person', 'NN'), ('motion', 'NN'), ('first', 'RB'), ('person', 'NN'), ('robot', 'JJ'), ('wherein', 'NN'), ('body', 'NN'), ('configured', 'VBD'), ('rotate', 'JJ'), ('lateral', 'JJ'), ('direction', 'NN'), ('tilt', 'VBD'), ('vertical', 'JJ'), ('direction', 'NN'), ('comprising', 'VBG'), ('camera', 'NN'), ('coupled', 'VBN'), ('body', 'NN'), ('configured', 'JJ'), ('rotate', 'NN'), ('tilt', 'NN'), ('wherein', 'NN'), ('camera', 'NN'), ('configured', 'VBD'), ('acquire', 'VB'), ('video', 'NN'), ('space', 'NN'), ('within', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('positioned', 'VBD'), ('configured', 'JJ'), ('recognize', 'NN'), ('respective', 'JJ'), ('one', 'CD'), ('persons', 'NNS'), ('video', 'VBP'), ('track', 'JJ'), ('motion', 'NN'), ('recognized', 'VBD'), ('one', 'CD'), ('persons', 'NNS'), ('calculate', 'VBP'), ('respective', 'JJ'), ('size', 'NN'), ('one', 'CD'), ('persons', 'NNS'), ('select', 'VBP'), ('first', 'JJ'), ('person', 'NN'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('based', 'VBN'), ('calculated', 'JJ'), ('sizes', 'NNS'), ('control', 'NN'), ('least', 'VBP'), ('one', 'CD'), ('direction', 'NN'), ('rotation', 'NN'), ('camera', 'NN'), ('angle', 'NN'), ('tilt', 'NN'), ('camera', 'NN'), ('focal', 'JJ'), ('distance', 'NN'), ('camera', 'NN'), ('based', 'VBN'), ('tracked', 'JJ'), ('motion', 'NN'), ('recognized', 'VBD'), ('face', 'NN'), ('first', 'RB'), ('person', 'NN'), ('comprising', 'VBG'), ('acquiring', 'VBG'), ('camera', 'NN'), ('video', 'NN'), ('space', 'NN'), ('within', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('positioned', 'VBD'), ('respective', 'JJ'), ('one', 'CD'), ('persons', 'NNS'), ('video', 'JJ'), ('motion', 'NN'), ('recognized', 'VBD'), ('one', 'CD'), ('persons', 'NNS'), ('calculating', 'VBG'), ('respective', 'JJ'), ('size', 'NN'), ('one', 'CD'), ('persons', 'NNS'), ('selecting', 'VBG'), ('first', 'JJ'), ('person', 'NN'), ('among', 'IN'), ('one', 'CD'), ('persons', 'NNS'), ('based', 'VBN'), ('calculated', 'JJ'), ('sizes', 'NNS'), ('controlling', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('direction', 'NN'), ('rotation', 'NN'), ('camera', 'NN'), ('angle', 'NN'), ('tilt', 'NN'), ('camera', 'NN'), ('focal', 'JJ'), ('distance', 'NN'), ('camera', 'NN'), ('based', 'VBN'), ('tracked', 'JJ'), ('motion', 'NN'), ('recognized', 'VBD'), ('face', 'NN'), ('first', 'RB'), ('person', 'NN'), ('inferring', 'VBG'), ('topics', 'NNS'), ('multimodal', 'VBP'), ('file', 'NN'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('multimodal', 'NN'), ('file', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('entities', 'NNS'), ('multimodal', 'VBP'), ('file', 'NN'), ('linking', 'VBG'), ('set', 'VBN'), ('entities', 'NNS'), ('produce', 'VBP'), ('set', 'VBN'), ('linked', 'VBN'), ('entities', 'NNS'), ('obtaining', 'VBG'), ('reference', 'NN'), ('information', 'NN'), ('set', 'VBN'), ('entities', 'NNS'), ('based', 'VBN'), ('least', 'JJS'), ('reference', 'NN'), ('information', 'NN'), ('generating', 'VBG'), ('graph', 'NN'), ('set', 'VBN'), ('linked', 'VBN'), ('entities', 'NNS'), ('graph', 'VBP'), ('comprising', 'VBG'), ('nodes', 'NNS'), ('edges', 'NNS'), ('based', 'VBN'), ('least', 'JJS'), ('nodes', 'JJ'), ('edges', 'NNS'), ('graph', 'VBP'), ('determining', 'VBG'), ('clusters', 'NNS'), ('graph', 'VBP'), ('based', 'VBN'), ('least', 'JJS'), ('clusters', 'NNS'), ('graph', 'VBP'), ('identifying', 'VBG'), ('topic', 'NN'), ('candidates', 'NNS'), ('extracting', 'VBG'), ('features', 'NNS'), ('clusters', 'NNS'), ('graph', 'VBP'), ('based', 'VBN'), ('least', 'JJS'), ('extracted', 'JJ'), ('features', 'NNS'), ('selecting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('among', 'IN'), ('topic', 'JJ'), ('candidates', 'NNS'), ('represent', 'VBP'), ('least', 'JJS'), ('one', 'CD'), ('cluster', 'NN'), ('indexing', 'VBG'), ('multimodal', 'NNS'), ('file', 'RB'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('wherein', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('comprises', 'VBZ'), ('video', 'JJ'), ('portion', 'NN'), ('audio', 'NN'), ('portion', 'NN'), ('wherein', 'IN'), ('extracting', 'VBG'), ('set', 'NN'), ('entities', 'NNS'), ('multimodal', 'VBP'), ('file', 'NN'), ('comprises', 'NNS'), ('detecting', 'VBG'), ('objects', 'NNS'), ('video', 'JJ'), ('portion', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('detecting', 'VBG'), ('text', 'JJ'), ('audio', 'JJ'), ('portion', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('wherein', 'NN'), ('detecting', 'VBG'), ('objects', 'NNS'), ('comprises', 'VBZ'), ('performing', 'VBG'), ('face', 'NN'), ('wherein', 'NN'), ('detecting', 'VBG'), ('text', 'JJ'), ('comprises', 'NNS'), ('performing', 'VBG'), ('speech', 'NN'), ('text', 'NN'), ('process', 'NN'), ('comprising', 'VBG'), ('identifying', 'VBG'), ('language', 'NN'), ('used', 'VBN'), ('audio', 'JJ'), ('portion', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('wherein', 'NN'), ('performing', 'VBG'), ('speech', 'JJ'), ('text', 'NN'), ('process', 'NN'), ('comprises', 'VBZ'), ('performing', 'VBG'), ('speech', 'NN'), ('text', 'NN'), ('process', 'NN'), ('identified', 'VBN'), ('language', 'NN'), ('comprising', 'VBG'), ('translating', 'VBG'), ('detected', 'VBN'), ('text', 'JJ'), ('comprising', 'VBG'), ('determining', 'VBG'), ('significant', 'JJ'), ('clusters', 'NNS'), ('insignificant', 'JJ'), ('clusters', 'NNS'), ('determined', 'VBD'), ('clusters', 'NNS'), ('wherein', 'VBP'), ('extracting', 'VBG'), ('features', 'NNS'), ('clusters', 'NNS'), ('graph', 'VBP'), ('comprises', 'NNS'), ('extracting', 'VBG'), ('features', 'NNS'), ('significant', 'JJ'), ('clusters', 'NNS'), ('graph', 'VBP'), ('wherein', 'RB'), ('extracting', 'VBG'), ('features', 'NNS'), ('clusters', 'NNS'), ('graph', 'VBP'), ('comprises', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('process', 'NN'), ('selected', 'VBN'), ('list', 'NN'), ('consisting', 'VBG'), ('determining', 'VBG'), ('graph', 'JJ'), ('diameter', 'NN'), ('determining', 'VBG'), ('jaccard', 'JJ'), ('coefficient', 'NN'), ('wherein', 'NN'), ('selecting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'JJ'), ('represent', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('cluster', 'NN'), ('comprises', 'NNS'), ('based', 'VBN'), ('least', 'RB'), ('extracted', 'JJ'), ('features', 'NNS'), ('mapping', 'VBG'), ('topic', 'NN'), ('candidates', 'NNS'), ('probability', 'NN'), ('interval', 'VBP'), ('based', 'VBN'), ('least', 'JJS'), ('mapping', 'VBG'), ('ranking', 'VBG'), ('topic', 'NN'), ('candidates', 'NNS'), ('within', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('cluster', 'NN'), ('selecting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('ranking', 'JJ'), ('comprising', 'VBG'), ('translating', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('wherein', 'NN'), ('indexing', 'VBG'), ('multimodal', 'NNS'), ('file', 'RB'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('comprises', 'VBZ'), ('indexing', 'VBG'), ('multimodal', 'NNS'), ('file', 'RB'), ('least', 'JJS'), ('one', 'CD'), ('translated', 'VBN'), ('topicid', 'NN'), ('system', 'NN'), ('inferring', 'VBG'), ('topics', 'NNS'), ('multimodal', 'VBP'), ('file', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('entity', 'NN'), ('extraction', 'NN'), ('component', 'NN'), ('comprising', 'VBG'), ('object', 'JJ'), ('detection', 'NN'), ('component', 'NN'), ('speech', 'NN'), ('text', 'JJ'), ('component', 'NN'), ('operative', 'JJ'), ('extract', 'NN'), ('set', 'VBN'), ('entities', 'NNS'), ('multimodal', 'VBP'), ('file', 'NN'), ('comprising', 'VBG'), ('video', 'JJ'), ('portion', 'NN'), ('audio', 'NN'), ('portion', 'NN'), ('entity', 'NN'), ('linking', 'VBG'), ('component', 'JJ'), ('operative', 'JJ'), ('link', 'NN'), ('extracted', 'VBD'), ('set', 'JJ'), ('entities', 'NNS'), ('produce', 'VBP'), ('set', 'VBN'), ('linked', 'VBN'), ('entities', 'NNS'), ('information', 'NN'), ('retrieval', 'NN'), ('component', 'NN'), ('operative', 'JJ'), ('obtain', 'VB'), ('reference', 'NN'), ('information', 'NN'), ('extracted', 'VBD'), ('set', 'NN'), ('entities', 'NNS'), ('graphing', 'VBG'), ('analysis', 'NN'), ('component', 'NN'), ('operative', 'JJ'), ('generate', 'NN'), ('graph', 'NN'), ('set', 'VBN'), ('linked', 'VBN'), ('entities', 'NNS'), ('graph', 'VBP'), ('comprising', 'VBG'), ('nodes', 'NNS'), ('edges', 'NNS'), ('based', 'VBN'), ('least', 'JJS'), ('nodes', 'JJ'), ('edges', 'NNS'), ('graph', 'VBP'), ('determine', 'JJ'), ('clusters', 'NNS'), ('graph', 'VBP'), ('based', 'VBN'), ('least', 'JJS'), ('clusters', 'NNS'), ('graph', 'VBP'), ('identify', 'VB'), ('topic', 'NN'), ('candidates', 'NNS'), ('extract', 'JJ'), ('features', 'NNS'), ('clusters', 'NNS'), ('graph', 'VBP'), ('topicid', 'JJ'), ('selection', 'NN'), ('component', 'NN'), ('operative', 'JJ'), ('rank', 'NN'), ('topic', 'NN'), ('candidates', 'NNS'), ('within', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('cluster', 'NN'), ('based', 'VBN'), ('least', 'JJS'), ('ranking', 'JJ'), ('select', 'JJ'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('among', 'IN'), ('topic', 'JJ'), ('candidates', 'NNS'), ('represent', 'VBP'), ('least', 'JJS'), ('one', 'CD'), ('cluster', 'NN'), ('video', 'NN'), ('indexer', 'VBP'), ('operative', 'JJ'), ('index', 'NN'), ('multimodal', 'NNS'), ('file', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('object', 'JJ'), ('detection', 'NN'), ('component', 'NN'), ('operative', 'JJ'), ('perform', 'NN'), ('face', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('speech', 'NN'), ('text', 'NN'), ('component', 'NN'), ('operative', 'JJ'), ('extract', 'JJ'), ('entity', 'NN'), ('information', 'NN'), ('least', 'JJS'), ('two', 'CD'), ('different', 'JJ'), ('languages', 'NNS'), ('one', 'CD'), ('computer', 'NN'), ('storage', 'NN'), ('devices', 'NNS'), ('computer-executable', 'JJ'), ('instructions', 'NNS'), ('stored', 'VBD'), ('thereon', 'NN'), ('inferring', 'VBG'), ('topics', 'NNS'), ('multimodal', 'JJ'), ('file', 'NN'), ('execution', 'NN'), ('computer', 'NN'), ('cause', 'VBP'), ('computer', 'NN'), ('perform', 'NN'), ('operations', 'NNS'), ('comprising', 'VBG'), ('receiving', 'VBG'), ('multimodal', 'NN'), ('file', 'NN'), ('comprising', 'VBG'), ('video', 'JJ'), ('portion', 'NN'), ('audio', 'NN'), ('portion', 'NN'), ('extracting', 'VBG'), ('set', 'NN'), ('entities', 'NNS'), ('multimodal', 'VBP'), ('file', 'NN'), ('wherein', 'NN'), ('extracting', 'VBG'), ('set', 'VBN'), ('entities', 'NNS'), ('multimodal', 'VBP'), ('file', 'NN'), ('comprises', 'NNS'), ('detecting', 'VBG'), ('objects', 'NNS'), ('video', 'JJ'), ('portion', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('face', 'NN'), ('detecting', 'VBG'), ('text', 'JJ'), ('audio', 'JJ'), ('portion', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('speech', 'NN'), ('text', 'NN'), ('process', 'NN'), ('disambiguating', 'VBG'), ('among', 'IN'), ('set', 'VBN'), ('detected', 'VBN'), ('entity', 'NN'), ('names', 'NNS'), ('linking', 'VBG'), ('set', 'NN'), ('entities', 'NNS'), ('produce', 'VBP'), ('set', 'VBN'), ('linked', 'VBN'), ('entities', 'NNS'), ('obtaining', 'VBG'), ('reference', 'NN'), ('information', 'NN'), ('set', 'VBN'), ('entities', 'NNS'), ('based', 'VBN'), ('least', 'JJS'), ('reference', 'NN'), ('information', 'NN'), ('generating', 'VBG'), ('graph', 'NN'), ('set', 'VBN'), ('linked', 'VBN'), ('entities', 'NNS'), ('graph', 'VBP'), ('comprising', 'VBG'), ('nodes', 'NNS'), ('edges', 'NNS'), ('based', 'VBN'), ('least', 'JJS'), ('nodes', 'JJ'), ('edges', 'NNS'), ('graph', 'VBP'), ('determining', 'VBG'), ('clusters', 'NNS'), ('graph', 'VBP'), ('determining', 'VBG'), ('significant', 'JJ'), ('clusters', 'NNS'), ('insignificant', 'JJ'), ('clusters', 'NNS'), ('determined', 'VBD'), ('clusters', 'NNS'), ('based', 'VBN'), ('least', 'JJS'), ('significant', 'JJ'), ('clusters', 'NNS'), ('graph', 'VBP'), ('identifying', 'VBG'), ('topic', 'NN'), ('candidates', 'NNS'), ('extracting', 'VBG'), ('features', 'NNS'), ('significant', 'JJ'), ('clusters', 'NNS'), ('graph', 'VBP'), ('based', 'VBN'), ('least', 'JJS'), ('extracted', 'JJ'), ('features', 'NNS'), ('mapping', 'VBG'), ('topic', 'NN'), ('candidates', 'NNS'), ('probability', 'NN'), ('interval', 'VBP'), ('based', 'VBN'), ('least', 'JJS'), ('mapping', 'VBG'), ('ranking', 'VBG'), ('topic', 'NN'), ('candidates', 'NNS'), ('within', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('significant', 'JJ'), ('cluster', 'NN'), ('based', 'VBN'), ('ranking', 'VBG'), ('selecting', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('among', 'IN'), ('topic', 'JJ'), ('candidates', 'NNS'), ('represent', 'VBP'), ('least', 'JJS'), ('one', 'CD'), ('significant', 'JJ'), ('cluster', 'NN'), ('indexing', 'VBG'), ('multimodal', 'NNS'), ('file', 'RB'), ('least', 'JJS'), ('one', 'CD'), ('topicid', 'NN'), ('one', 'CD'), ('computer', 'NN'), ('storage', 'NN'), ('devices', 'NNS'), ('wherein', 'VBP'), ('operations', 'NNS'), ('comprise', 'VBP'), ('identifying', 'VBG'), ('language', 'NN'), ('used', 'VBN'), ('audio', 'JJ'), ('portion', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('detecting', 'VBG'), ('text', 'JJ'), ('audio', 'JJ'), ('portion', 'NN'), ('multimodal', 'NN'), ('file', 'NN'), ('speech', 'NN'), ('text', 'NN'), ('process', 'NN'), ('comprises', 'VBZ'), ('performing', 'VBG'), ('speech', 'NN'), ('text', 'NN'), ('process', 'NN'), ('identified', 'VBD'), ('language权利要求', 'JJ'), ('、', 'NNP'), ('一种人脸识别方法其特征在于包括', 'NNP'), ('通过第一摄像头获取第一人脸图像', 'NNP'), ('提取所述第一人脸图像的第一人脸特征', 'NNP'), ('将所述第一人脸特征与预先存储的第二人脸特征进行对比获得参考相似度所述第', 'NNP'), ('二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得所述第二摄像头与所述第', 'NNP'), ('一摄像头属于不同类型的摄像头', 'NNP'), ('根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否对应相同人。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于', 'NNP'), ('所述第一摄像头为热成像摄像头所述第二摄像头为可见光摄像头', 'NNP'), ('或者所述第一摄像头为可见光摄像头所述第一摄像头为热成像摄像头。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('或', 'NNP'), ('所述的方法其特征在于所述根据所述参考相似度确定所', 'NNP'), ('述第一人脸特征与所述第二人脸特征是否对应相同人包括', 'NNP'), ('根据所述参考相似度、', 'NNP'), ('参考误报率以及相似度阈值确定所述第一人脸特征与所述第二', 'NNP'), ('人脸特征是否对应相同人其中不同的误报率对应不同的相似度阈值。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('或', 'NNP'), ('所述的方法其特征在于所述根据所述参考相似度确定所', 'NNP'), ('述第一人脸特征与所述第二人脸特征是否对应相同人包括', 'NNP'), ('根据所述参考相似度以及阈值信息确定归一化后的参考相似度', 'NNP'), ('根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否对', 'NNP'), ('应相同人。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('-任一项所述的方法其特征在于所述提取所述第一人脸图像的', 'NNP'), ('第_人脸特征包括', 'NNP'), ('将所述第一人脸图像输入预先训练完成的神经网络通过所述神经网络输出所述第一', 'NNP'), ('人脸图像的第一人脸特征其中所述神经网络基于第一类型图像样本和第二类型图像样', 'NNP'), ('本训练得到所述第一类型图像样本和所述第二类型图像样本由不同类型的摄像头拍摄得', 'NNP'), ('到且所述第一类型图像样本和所述第二类型图像样本中包括人脸。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于所述神经网络基于所述第一类型图像', 'NNP'), ('样本、', 'NNP'), ('所述第二类型图像样本和混合类型图像样本训练得到所述混合类型图像样本由所', 'NNP'), ('述第一类型图像样本和所述第二类型图像样本配对而得。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('-任一项所述的方法其特征在于所述第一摄像头包括车载摄像', 'NNP'), ('头所述通过第一摄像头获取第一人脸图像包括', 'NNP'), ('通过所述车载摄像头获取所述第一人脸图像所述第一人脸图像包括车辆的用车人的', 'NNP'), ('人脸图像。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于所述用车人包括驾驶所述车辆的人、', 'NNP'), ('乘坐所述车辆的人、', 'NNP'), ('对所述车辆进行修理的人、', 'NNP'), ('给所述车辆加油的人以及控制所述车辆的', 'NNP'), ('人中的一项或多项。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于所述用车人包括驾驶所述车辆的人', 'NNP'), ('所述通过所述车载摄像头获取所述第一人脸图像包括', 'NNP'), ('在接收到触发指令的情况下通过所述车载摄像头获取所述第一人脸图像', 'NNP'), ('或者在所述车辆运行时通过所述车载摄像头获取所述第一人脸图像', 'NNP'), ('或者在所述车辆的运行速度达到参考速度的情况下通过所述车载摄像头获取所述', 'NNP'), ('第一人脸图像。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('-任一项所述的方法其特征在于所述第二人脸图像为对所述', 'NNP'), ('用车人进行人脸注册的图像所述将所述第一人脸特征与预先存储的第二人脸特征进行对', 'NNP'), ('比之前所述方法还包括', 'NNP'), ('通过所述第二摄像头获取所述第二人脸图像', 'NNP'), ('提取所述第二人脸图像的第二人脸特征', 'NNP'), ('保存所述第二人脸图像的第二人脸特征。', 'NNP'), ('、', 'NNP'), ('一种神经网络训练方法其特征在于包括', 'NNP'), ('获取第一类型图像样本和第二类型图像样本所述第一类型图像样本和所述第二类型', 'NNP'), ('图像样本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述第二类型图像样', 'NNP'), ('本中包括人脸', 'NNP'), ('根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于所述根据所述第一类型图像样本和所', 'NNP'), ('述第二类型图像样本训练神经网络包括', 'NNP'), ('将所述第一类型图像样本和所述第二类型图像样本配对得到所述第一类型图像样本', 'NNP'), ('和所述第二类型图像样本的混合类型图像样本', 'NNP'), ('根据所述第一类型图像样本、', 'NNP'), ('所述第二类型图像样本和所述混合类型图像样本训练', 'NNP'), ('所述神经网络。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于所述根据所述第一类型图像样本、', 'NNP'), ('所述第二类型图像样本和所述混合类型图像样本训练所述神经网络包括', 'NNP'), ('通过所述神经网络获取所述第一类型图像样本的人脸预测结果、', 'NNP'), ('所述第二类型图像样', 'NNP'), ('本的人脸预测结果和所述混合类型图像样本的人脸预测结果', 'NNP'), ('根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、', 'NNP'), ('所述第二类型图', 'NNP'), ('像样本的人脸预测结果和人脸标注结果之间的差异、', 'NNP'), ('以及所述混合类型图像样本的人脸预', 'NNP'), ('测结果和人脸标注结果的差异训练所述神经网络。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于所述神经网络中包括第一分类器、', 'NNP'), ('第二分类器和混合分类器所述通过所述神经网络获取所述第一类型图像样本的人脸预测', 'NNP'), ('结果、', 'NNP'), ('所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果', 'NNP'), ('包括', 'NNP'), ('将所述第一类型图像样本的人脸特征输入至所述第一分类器中得到所述第一类型图', 'NNP'), ('像样本的人脸预测结果', 'NNP'), ('将所述第二类型图像样本的人脸特征输入至所述第二分类器中得到所述第二类型图', 'NNP'), ('像样本的人脸预测结果', 'NNP'), ('将所述混合类型图像样本的人脸特征输入至所述混合分类器中得到所述混合类型图', 'NNP'), ('像样本的人脸预测结果。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的方法其特征在于所述方法还包括', 'NNP'), ('在训练完成的所述神经网络中去除所述第一分类器、', 'NNP'), ('所述第二分类器和所述混合分类', 'NNP'), ('器得到用于进行人脸识别的神经网络。', 'NNP'), ('、', 'NNP'), ('一种人脸识别装置其特征在于包括', 'NNP'), ('第一获取单元用于通过第一摄像头获取第一人脸图像', 'NNP'), ('第一提取单元用于提取所述第一人脸图像的第一人脸特征', 'NNP'), ('对比单元用于将所述第一人脸特征与预先存储的第二人脸特征进行对比获得参考', 'NNP'), ('相似度所述第二人脸特征经第二摄像头获取的第二人脸图像的特征提取而得所述第二', 'NNP'), ('摄像头与所述第一摄像头属于不同类型的摄像头', 'NNP'), ('确定单元用于根据所述参考相似度确定所述第一人脸特征与所述第二人脸特征是否', 'NNP'), ('对应相同人。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于', 'NNP'), ('所述第一摄像头为热成像摄像头所述第二摄像头为可见光摄像头', 'NNP'), ('或者所述第一摄像头为可见光摄像头所述第一摄像头为热成像摄像头。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('或', 'NNP'), ('所述的装置其特征在于', 'NNP'), ('所述确定单元具体用于根据所述参考相似度、', 'NNP'), ('参考误报率以及相似度阈值确定所述', 'NNP'), ('第一人脸特征与所述第二人脸特征是否对应相同人其中不同的误报率对应不同的相似', 'NNP'), ('度阈值。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('或', 'NNP'), ('所述的装置其特征在于', 'NNP'), ('所述确定单元具体用于根据所述参考相似度以及阈值信息确定归一化后的参考相似', 'NNP'), ('度以及根据所述归一化后的参考相似度确定所述第一人脸特征与所述第二人脸特征是否', 'NNP'), ('对应相同人。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('-任_项所述的装置其特征在于', 'NNP'), ('所述第一提取单元具体用于将所述第一人脸图像输入预先训练完成的神经网络通', 'NNP'), ('过所述神经网络输出所述第一人脸图像的第一人脸特征其中所述神经网络基于第一类', 'NNP'), ('型图像样本和第二类型图像样本训练得到所述第一类型图像样本和所述第二类型图像样', 'NNP'), ('本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述第二类型图像样本中包', 'NNP'), ('括人脸。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于所述神经网络基于所述第一类型图', 'NNP'), ('像样本、', 'NNP'), ('所述第二类型图像样本和混合类型图像样本训练得到所述混合类型图像样本由', 'NNP'), ('所述第一类型图像样本和所述第二类型图像样本配对而得。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('-任一项所述的装置其特征在于所述第一摄像头包括车载', 'NNP'), ('摄像头', 'NNP'), ('所述第一获取单元具体用于通过所述车载摄像头获取所述第一人脸图像所述第一', 'NNP'), ('人脸图像包括车辆的用车人的人脸图像。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于所述用车人包括驾驶所述车辆的人、', 'NNP'), ('乘坐所述车辆的人、', 'NNP'), ('对所述车辆进行修理的人、', 'NNP'), ('给所述车辆加油的人以及控制所述车辆的', 'NNP'), ('人中的一项或多项。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于所述用车人包括驾驶所述车辆的人', 'NNP'), ('所述第一获取单元具体用于在接收到触发指令的情况下通过所述车载摄像头获取所述', 'NNP'), ('第一人脸图像', 'NNP'), ('或者所述第一获取单元具体用于在所述车辆运行时通过所述车载摄像头获取所', 'NNP'), ('述第', 'NNP'), ('_人脸图像', 'NNP'), ('或者所述第一获取单元具体用于在所述车辆的运行速度达到参考速度的情况下', 'NNP'), ('通过所述车载摄像头获取所述第一人脸图像。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('-任一项所述的装置其特征在于所述第二人脸图像为对所', 'NNP'), ('述用车人进行人脸注册的图像所述装置还包括', 'NNP'), ('第二获取单元用于通过所述第二摄像头获取所述第二人脸图像', 'NNP'), ('第二提取单元用于提取所述第二人脸图像的第二人脸特征', 'NNP'), ('保存单元用于保存所述第二人脸图像的第二人脸特征。', 'NNP'), ('、', 'NNP'), ('一种神经网络训练装置其特征在于包括', 'NNP'), ('获取单元用于获取第一类型图像样本和第二类型图像样本所述第一类型图像样本', 'NNP'), ('和所述第二类型图像样本由不同类型的摄像头拍摄得到且所述第一类型图像样本和所述', 'NNP'), ('第二类型图像样本中包括人脸', 'NNP'), ('训练单元用于根据所述第一类型图像样本和所述第二类型图像样本训练神经网络。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于所述训练单元包括', 'NNP'), ('配对子单元用于将所述第一类型图像样本和所述第二类型图像样本配对得到所述', 'NNP'), ('第一类型图像样本和所述第二类型图像样本的混合类型图像样本', 'NNP'), ('训练子单元用于根据所述第一类型图像样本、', 'NNP'), ('所述第二类型图像样本和所述混合类', 'NNP'), ('型图像样本训练所述神经网络。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于', 'NNP'), ('所述训练子单元具体用于通过所述神经网络获取所述第一类型图像样本的人脸预测', 'NNP'), ('结果、', 'NNP'), ('所述第二类型图像样本的人脸预测结果和所述混合类型图像样本的人脸预测结果', 'NNP'), ('以及根据所述第一类型图像样本的人脸预测结果和人脸标注结果的差异、', 'NNP'), ('所述第二类型图', 'NNP'), ('像样本的人脸预测结果和人脸标注结果之间的差异、', 'NNP'), ('以及所述混合类型图像样本的人脸预', 'NNP'), ('测结果和人脸标注结果的差异训练所述神经网络。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于所述神经网络中包括第一分类器、', 'NNP'), ('第二分类器和混合分类器', 'NNP'), ('所述训练子单元具体用于将所述第一类型图像样本的人脸特征输入至所述第一分类', 'NNP'), ('器中得到所述第一类型图像样本的人脸预测结果以及将所述第二类型图像样本的人脸', 'NNP'), ('特征输入至所述第二分类器中得到所述第二类型图像样本的人脸预测结果以及将所述', 'NNP'), ('混合类型图像样本的人脸特征输入至所述混合分类器中得到所述混合类型图像样本的人', 'NNP'), ('脸预测结果。', 'NNP'), ('、', 'NNP'), ('根据权利要求', 'NNP'), ('所述的装置其特征在于所述装置还包括', 'NNP'), ('神经网络应用单元用于在训练完成的所述神经网络中去除所述第一分类器、', 'NNP'), ('所述第', 'NNP'), ('二分类器和所述混合分类器得到用于进行人脸识别的神经网络。', 'NNP'), ('、', 'NNP'), ('一种电子设备其特征在于包括处理器和存储器所述处理器和所述存储器耦', 'NNP'), ('合其中所述存储器用于存储程序指令所述程序指令被所述处理器执行时使所述处', 'NNP'), ('理器执行权利要求', 'NNP'), ('-任一项所述的方法和或使所述处理器执行权利要求', 'NNP'), ('-任一', 'NNP'), ('项所述的方法。', 'NNP'), ('、', 'NNP'), ('一种计算机可读存储介质其特征在于所述计算机可读存储介质中存储有计算', 'NNP'), ('机程序所述计算机程序包括程序指令所述程序指令当被处理器执行时使所述处理器', 'NNP'), ('执行权利要求', 'NNP'), ('-任一项所述的方法和或使所述处理器执行权利要求', 'NNP'), ('-任一项所', 'NNP'), ('述的方法。', 'NNP'), ('system', 'NN'), ('alerting', 'VBG'), ('vision', 'NN'), ('impairment', 'NN'), ('said', 'VBD'), ('system', 'NN'), ('comprising', 'VBG'), ('processing', 'VBG'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('receiving', 'VBG'), ('scene', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('scene', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('environment', 'NN'), ('identifying', 'VBG'), ('scene', 'NN'), ('data', 'NNS'), ('certain', 'JJ'), ('consumer', 'NN'), ('identifying', 'VBG'), ('event', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('upon', 'IN'), ('identification', 'NN'), ('event', 'NN'), ('sending', 'VBG'), ('notification', 'NN'), ('relating', 'VBG'), ('vision', 'NN'), ('impairment', 'NN'), ('system', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('sensing', 'VBG'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('detecting', 'VBG'), ('scene', 'NN'), ('data', 'NNS'), ('system', 'NN'), ('wherein', 'NN'), ('said', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('sensing', 'VBG'), ('unit', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('least', 'JJS'), ('one', 'CD'), ('imaging', 'VBG'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('capturing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('least', 'JJS'), ('portion', 'NN'), ('consumer', 'NN'), (\"'s\", 'POS'), ('body', 'NN'), ('least', 'VBD'), ('one', 'CD'), ('motion', 'NN'), ('detector', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('detecting', 'VBG'), ('consumer', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('motion', 'NN'), ('consumer', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('eye', 'NN'), ('tracker', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('eye', 'NN'), ('motion', 'NN'), ('consumer', 'NN'), ('system', 'NN'), ('wherein', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('imaging', 'VBG'), ('unit', 'NN'), ('comprises', 'VBZ'), ('cameras', 'NNS'), ('placed', 'VBD'), ('different', 'JJ'), ('heights', 'NNS'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('sensing', 'VBG'), ('unit', 'NN'), ('accommodated', 'VBD'), ('optical', 'JJ'), ('digital', 'JJ'), ('eyewear', 'NN'), ('frame', 'NN'), ('display', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('processing', 'NN'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('identifying', 'VBG'), ('consumer', 'NN'), (\"'s\", 'POS'), ('condition', 'NN'), ('said', 'VBD'), ('consumer', 'NN'), (\"'s\", 'POS'), ('condition', 'NN'), ('comprising', 'VBG'), ('consumer', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('consumer', 'NN'), (\"'s\", 'POS'), ('position', 'NN'), ('location', 'NN'), ('relative', 'JJ'), ('least', 'JJS'), ('one', 'CD'), ('object', 'JJ'), ('consumer', 'NN'), (\"'s\", 'POS'), ('environment', 'NN'), ('said', 'VBD'), ('consumer', 'NN'), ('data', 'NNS'), ('comprises', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('consumer', 'NN'), (\"'s\", 'POS'), ('face', 'NN'), ('eyewear', 'JJ'), ('posture', 'NN'), ('position', 'NN'), ('sound', 'NN'), ('motion', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('event', 'NN'), ('comprises', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('position', 'NN'), ('head', 'NN'), ('increase', 'NN'), ('decrease', 'NN'), ('viewing', 'VBG'), ('distance', 'NN'), ('consumer', 'NN'), ('viewed', 'VBD'), ('object', 'JJ'), ('changing', 'VBG'), ('position', 'NN'), ('eyeglasses', 'NNS'), ('worn', 'JJ'), ('consumer', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('event', 'NN'), ('identified', 'VBN'), ('identifying', 'JJ'), ('feature', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('performing', 'VBG'), ('bruckner', 'JJ'), ('test', 'NN'), ('performing', 'VBG'), ('hirschberg', 'JJ'), ('test', 'NN'), ('measuring', 'VBG'), ('blink', 'NN'), ('count', 'NN'), ('frequency', 'NN'), ('system', 'NN'), ('wherein', 'JJ'), ('feature', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('comprises', 'NNS'), ('squinting', 'VBG'), ('head', 'NN'), ('certain', 'JJ'), ('distances', 'NNS'), ('object', 'VBP'), ('consumer', 'NN'), (\"'s\", 'POS'), ('eyes', 'NNS'), ('certain', 'JJ'), ('position', 'NN'), ('eyeglasses', 'VBZ'), ('consumer', 'NN'), (\"'s\", 'POS'), ('face', 'NN'), ('strabismus', 'NN'), ('cataracts', 'VBZ'), ('reflections', 'NNS'), ('eye', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('notification', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('data', 'NN'), ('indicative', 'NN'), ('identified', 'VBN'), ('event', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('identified', 'JJ'), ('consumer', 'NN'), ('ophthalmologic', 'NN'), ('recommendations', 'NNS'), ('based', 'VBN'), ('identified', 'JJ'), ('event', 'NN'), ('lack', 'NN'), ('events', 'NNS'), ('appointment', 'JJ'), ('vision', 'NN'), ('test', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('processing', 'NN'), ('unit', 'NN'), ('comprises', 'VBZ'), ('memory', 'NN'), ('storing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('reference', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('notification', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('follow-up', 'JJ'), ('notification', 'NN'), ('system', 'NN'), ('wherein', 'NN'), ('said', 'VBD'), ('processing', 'NN'), ('unit', 'NN'), ('configured', 'VBD'), ('least', 'JJS'), ('one', 'CD'), ('identifying', 'VBG'), ('event', 'NN'), ('upon', 'IN'), ('comparison', 'NN'), ('detected', 'VBN'), ('data', 'NNS'), ('reference', 'NN'), ('data', 'NNS'), ('determining', 'VBG'), ('probability', 'NN'), ('vision', 'NN'), ('impairment', 'JJ'), ('consumer', 'NN'), ('based', 'VBN'), ('comparison', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('processing', 'NN'), ('unit', 'NN'), ('comprises', 'VBZ'), ('communication', 'NN'), ('interface', 'NN'), ('configured', 'VBD'), ('sending', 'VBG'), ('notification', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('identified', 'JJ'), ('consumer', 'NN'), ('third', 'NNP'), ('party', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('processing', 'NN'), ('unit', 'NN'), ('configured', 'VBD'), ('providing', 'VBG'), ('frame', 'NN'), ('recommendation', 'NN'), ('system', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('memory', 'NN'), ('configured', 'VBD'), ('storing', 'JJ'), ('database', 'NN'), ('including', 'VBG'), ('multiplicity', 'NN'), ('data', 'NNS'), ('sets', 'NNS'), ('related', 'JJ'), ('spectacle', 'NN'), ('frame', 'NN'), ('models', 'NNS'), ('sizes', 'VBZ'), ('system', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('said', 'VBD'), ('processing', 'VBG'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('correlate', 'NN'), ('frames', 'NNS'), ('parameters', 'NNS'), ('ophthalmic', 'VBP'), ('prescriptions', 'NNS'), ('system', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('said', 'VBD'), ('processing', 'VBG'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('correlate', 'NN'), ('frames', 'NNS'), ('parameters', 'NNS'), ('facial', 'JJ'), ('features', 'NNS'), ('system', 'NN'), ('according', 'VBG'), ('wherein', 'NNS'), ('said', 'VBD'), ('processing', 'VBG'), ('unit', 'NN'), ('configured', 'VBD'), ('operable', 'JJ'), ('correlate', 'NN'), ('frames', 'NNS'), ('parameters', 'NNS'), ('eyewear', 'VBP'), ('preferences', 'NNS'), ('system', 'NN'), ('according', 'VBG'), ('comprising', 'VBG'), ('server', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('computer', 'NN'), ('entity', 'NN'), ('linked', 'VBN'), ('server', 'RB'), ('via', 'IN'), ('network', 'NN'), ('wherein', 'NN'), ('said', 'VBD'), ('network', 'NN'), ('configured', 'VBD'), ('receive', 'JJ'), ('respond', 'NN'), ('requests', 'NNS'), ('sent', 'VBD'), ('across', 'IN'), ('network', 'NN'), ('transmitting', 'VBG'), ('one', 'CD'), ('modules', 'NNS'), ('computer', 'NN'), ('executable', 'JJ'), ('program', 'NN'), ('instructions', 'NNS'), ('displayable', 'JJ'), ('data', 'NNS'), ('network', 'NN'), ('connected', 'VBN'), ('computer', 'NN'), ('platform', 'NN'), ('response', 'NN'), ('request', 'NN'), ('wherein', 'NN'), ('said', 'VBD'), ('modules', 'NNS'), ('include', 'VBP'), ('modules', 'NNS'), ('configured', 'VBD'), ('receive', 'JJ'), ('transmit', 'NN'), ('information', 'NN'), ('transmitting', 'VBG'), ('frame', 'NN'), ('recommendation', 'NN'), ('optical', 'JJ'), ('lens', 'VBZ'), ('option', 'NN'), ('recommendation', 'NN'), ('based', 'VBN'), ('received', 'VBN'), ('information', 'NN'), ('display', 'NN'), ('network', 'NN'), ('connected', 'VBN'), ('computer', 'NN'), ('platform', 'NN'), ('computer', 'NN'), ('program', 'NN'), ('instructions', 'NNS'), ('stored', 'VBD'), ('local', 'JJ'), ('storage', 'NN'), ('executed', 'VBD'), ('processing', 'VBG'), ('unit', 'NN'), ('cause', 'NN'), ('processing', 'VBG'), ('unit', 'NN'), ('receive', 'JJ'), ('data', 'NNS'), ('indicative', 'JJ'), ('scene', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('environment', 'NN'), ('identify', 'VB'), ('data', 'NNS'), ('certain', 'JJ'), ('consumer', 'NN'), ('identify', 'VB'), ('event', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('upon', 'IN'), ('identification', 'NN'), ('event', 'NN'), ('send', 'VB'), ('notification', 'NN'), ('relating', 'VBG'), ('vision', 'NN'), ('impairment', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('stored', 'VBD'), ('tangible', 'JJ'), ('computer', 'NN'), ('readable', 'JJ'), ('medium', 'NN'), ('comprising', 'VBG'), ('library', 'JJ'), ('software', 'NN'), ('modules', 'NNS'), ('cause', 'VBP'), ('computer', 'NN'), ('executing', 'VBG'), ('prompt', 'JJ'), ('information', 'NN'), ('pertinent', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('eyeglasses', 'VBZ'), ('recommendation', 'NN'), ('optical', 'JJ'), ('lens', 'VBZ'), ('option', 'NN'), ('recommendation', 'NN'), ('store', 'NN'), ('said', 'VBD'), ('information', 'NN'), ('display', 'NN'), ('eyewear', 'VBP'), ('recommendations', 'NNS'), ('computer', 'NN'), ('program', 'NN'), ('product', 'NN'), ('wherein', 'NN'), ('said', 'VBD'), ('library', 'JJ'), ('comprises', 'NNS'), ('module', 'NN'), ('frame', 'NN'), ('selection', 'NN'), ('point', 'NN'), ('sales', 'NNS'), ('advertising', 'VBG'), ('computer', 'NN'), ('platform', 'NN'), ('facilitating', 'VBG'), ('eye', 'NN'), ('glasses', 'NNS'), ('marketing', 'VBG'), ('selection', 'NN'), ('comprising', 'VBG'), ('camera', 'NN'), ('configured', 'VBD'), ('execute', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('instructions', 'NNS'), ('cause', 'VBP'), ('take', 'VB'), ('consumer', 'NN'), ('identify', 'NN'), ('certain', 'JJ'), ('consumer', 'NN'), ('identify', 'VB'), ('event', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('upon', 'IN'), ('identification', 'NN'), ('event', 'NN'), ('sending', 'VBG'), ('notification', 'NN'), ('relating', 'VBG'), ('vision', 'NN'), ('impairment', 'JJ'), ('local', 'JJ'), ('storage', 'NN'), ('executable', 'JJ'), ('instructions', 'NNS'), ('carrying', 'VBG'), ('storage', 'NN'), ('information', 'NN'), ('alerting', 'VBG'), ('vision', 'NN'), ('impairment', 'NN'), ('said', 'VBD'), ('comprising', 'VBG'), ('identifying', 'VBG'), ('certain', 'JJ'), ('individual', 'JJ'), ('scene', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('scene', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('environment', 'NN'), ('identifying', 'VBG'), ('event', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('upon', 'IN'), ('identification', 'NN'), ('event', 'NN'), ('sending', 'VBG'), ('notification', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('comprising', 'VBG'), ('detecting', 'VBG'), ('data', 'NNS'), ('indicative', 'JJ'), ('scene', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('retail', 'JJ'), ('environment', 'NN'), ('wherein', 'NN'), ('detecting', 'VBG'), ('data', 'NNS'), ('indicative', 'JJ'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('comprises', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('capturing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('detecting', 'VBG'), ('data', 'NNS'), ('indicative', 'JJ'), ('motion', 'NN'), ('consumer', 'NN'), ('eye', 'NN'), ('motion', 'NN'), ('consumer', 'NN'), ('wherein', 'IN'), ('capturing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('comprises', 'VBZ'), ('continuously', 'RB'), ('recording', 'VBG'), ('scene', 'NN'), ('one', 'CD'), ('comprising', 'NN'), ('identifying', 'VBG'), ('data', 'NNS'), ('consumer', 'NN'), (\"'\", 'POS'), ('condition', 'NN'), ('including', 'VBG'), ('data', 'NNS'), ('indicative', 'JJ'), ('consumer', 'NN'), (\"'s\", 'POS'), ('position', 'NN'), ('location', 'NN'), ('relative', 'JJ'), ('consumer', 'NN'), (\"'s\", 'POS'), ('environment', 'NN'), ('said', 'VBD'), ('data', 'NNS'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), (\"'s\", 'POS'), ('face', 'NN'), ('posture', 'NN'), ('position', 'NN'), ('sound', 'NN'), ('motion', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('said', 'VBD'), ('event', 'NN'), ('comprises', 'NNS'), ('least', 'VBP'), ('one', 'CD'), ('position', 'NN'), ('head', 'NN'), ('increase', 'NN'), ('decrease', 'NN'), ('viewing', 'VBG'), ('distance', 'NN'), ('consumer', 'NN'), ('viewed', 'VBD'), ('object', 'JJ'), ('changing', 'VBG'), ('position', 'NN'), ('eyeglasses', 'NNS'), ('worn', 'JJ'), ('consumer', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('identifying', 'VBG'), ('event', 'NN'), ('comprises', 'NNS'), ('identifying', 'VBG'), ('feature', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('performing', 'VBG'), ('bruckner', 'JJ'), ('test', 'NN'), ('performing', 'VBG'), ('hirschberg', 'JJ'), ('test', 'NN'), ('measuring', 'VBG'), ('blink', 'NN'), ('countfrequency', 'NN'), ('wherein', 'JJ'), ('feature', 'NN'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('comprises', 'NNS'), ('squinting', 'VBG'), ('head', 'NN'), ('certain', 'JJ'), ('distances', 'NNS'), ('object', 'VBP'), ('consumer', 'NN'), (\"'s\", 'POS'), ('eyes', 'NNS'), ('certain', 'JJ'), ('position', 'NN'), ('eyeglasses', 'VBZ'), ('consumer', 'NN'), (\"'s\", 'POS'), ('face', 'NN'), ('strabismus', 'NN'), ('cataracts', 'VBZ'), ('reflections', 'NNS'), ('eye', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('identifying', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('consumer', 'NN'), ('retail', 'JJ'), ('environment', 'NN'), ('comprising', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('receiving', 'VBG'), ('data', 'NNS'), ('characterizing', 'VBG'), ('retail', 'JJ'), ('environment', 'NN'), ('performing', 'VBG'), ('face', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('sending', 'VBG'), ('notification', 'NN'), ('comprising', 'VBG'), ('sending', 'VBG'), ('notification', 'NN'), ('least', 'JJS'), ('one', 'CD'), ('identified', 'JJ'), ('consumer', 'NN'), ('third', 'JJ'), ('party', 'NN'), ('one', 'CD'), ('wherein', 'NN'), ('notification', 'NN'), ('includes', 'VBZ'), ('least', 'JJS'), ('one', 'CD'), ('data', 'NN'), ('indicative', 'NN'), ('identified', 'VBN'), ('event', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('identified', 'JJ'), ('consumer', 'NN'), ('ophthalmologic', 'NN'), ('recommendations', 'NNS'), ('based', 'VBN'), ('identified', 'JJ'), ('event', 'NN'), ('lack', 'NN'), ('events', 'NNS'), ('appointment', 'JJ'), ('vision', 'NN'), ('test', 'NN'), ('one', 'CD'), ('comprising', 'VBG'), ('storing', 'VBG'), ('least', 'JJS'), ('one', 'CD'), ('reference', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('behavioral', 'JJ'), ('compensation', 'NN'), ('vision', 'NN'), ('impairment', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('notification', 'NN'), ('data', 'NNS'), ('indicative', 'JJ'), ('follow-up', 'JJ'), ('notification', 'NN'), ('comprising', 'VBG'), ('identifying', 'VBG'), ('event', 'NN'), ('upon', 'IN'), ('comparison', 'NN'), ('detected', 'VBN'), ('data', 'NNS'), ('reference', 'NN'), ('data', 'NNS'), ('determining', 'VBG'), ('probability', 'NN'), ('vision', 'NN'), ('impairment', 'JJ'), ('consumer', 'NN'), ('based', 'VBN'), ('comparison', 'JJ'), ('computer', 'NN'), ('program', 'NN'), ('intended', 'VBN'), ('stored', 'JJ'), ('memory', 'NN'), ('unit', 'NN'), ('computer', 'NN'), ('system', 'NN'), ('removable', 'JJ'), ('memory', 'NN'), ('medium', 'NN'), ('adapted', 'VBD'), ('cooperate', 'JJ'), ('reader', 'NN'), ('unit', 'NN'), ('comprising', 'VBG'), ('instructions', 'NNS'), ('implementing', 'VBG'), ('according', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "cleaned_POS_text_c = []\n",
    "\n",
    "for tuple in pos_tagging_c:\n",
    "    # POS tagged text is a list of tuples, where the first element tuple[0] is a token and the second one tuple[1] is\n",
    "    # the Part of Speech. If the POS has length == 1, the token is punctuation, otherwise it is not, and we insert it\n",
    "    # in the list cleaned_POS_text\n",
    "    if len(tuple[1]) > 1:\n",
    "        cleaned_POS_text_c.append(tuple)\n",
    "        \n",
    "print(cleaned_POS_text_c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f4cd6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('electronic', 'a'), ('apparatus', 'n'), ('including', 'v'), ('capturing', 'v'), ('storage', 'n'), ('operation', 'n'), ('method', 'n'), ('thereof', 'n'), ('provided', 'v'), ('capturing', 'v'), ('captures', 'n'), ('storage', 'n'), ('records', 'n'), ('modules', 'n'), ('coupled', 'v'), ('capturing', 'v'), ('storage', 'n'), ('configured', 'v'), ('configure', 'n'), ('capturing', 'v'), ('capture', 'n'), ('head', 'n'), ('perform', 'n'), ('obtain', 'v'), ('detect', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('within', None), ('estimate', 'a'), ('head', 'n'), ('posture', 'n'), ('angle', 'n'), ('according', 'v'), ('facial', 'a'), ('landmarks', 'n'), ('calculate', 'n'), ('gaze', 'n'), ('position', 'n'), ('gazes', 'v'), ('screen', 'a'), ('according', 'v'), ('head', 'n'), ('posture', 'n'), ('angle', 'a'), ('rotation', 'n'), ('reference', 'n'), ('angle', 'n'), ('predetermined', 'v'), ('calibration', 'n'), ('positions', 'n'), ('configure', 'v'), ('screen', 'a'), ('display', 'n'), ('corresponding', 'v'), ('visual', 'a'), ('effect', 'n'), ('according', 'v'), ('gaze', 'n'), ('positionthe', 'n'), ('present', 'a'), ('disclosure', 'n'), ('provides', 'v'), ('product', 'n'), ('thereof', 'n'), ('adopts', 'n'), ('fusion', 'v'), ('method', 'a'), ('perform', 'n'), ('machine', 'n'), ('learning', 'v'), ('computations', 'n'), ('technical', 'a'), ('effects', 'n'), ('present', 'a'), ('disclosure', 'n'), ('include', 'v'), ('fewer', 'a'), ('computations', 'n'), ('less', 'r'), ('power', 'n'), ('consumptiona', 'n'), ('method', 'n'), ('detecting', 'v'), ('body', 'n'), ('information', 'n'), ('passengers', 'n'), ('vehicle', 'n'), ('based', 'v'), ('humans', 'n'), (\"'\", None), ('status', 'n'), ('provided', 'v'), ('method', 'n'), ('includes', 'v'), ('steps', 'n'), ('passenger', 'n'), ('body', 'n'), ('information-detecting', 'a'), ('inputting', 'a'), ('interior', 'a'), ('vehicle', 'n'), ('face', 'n'), ('network', 'n'), ('detect', 'a'), ('faces', 'v'), ('passengers', 'n'), ('output', 'n'), ('passenger', 'n'), ('feature', 'n'), ('information', 'n'), ('inputting', 'v'), ('interior', 'n'), ('body', 'n'), ('network', 'n'), ('detect', 'a'), ('bodies', 'n'), ('output', 'n'), ('body-part', 'a'), ('length', 'n'), ('information', 'n'), ('b', 'n'), ('retrieving', 'v'), ('specific', 'a'), ('height', 'a'), ('mapping', 'n'), ('information', 'n'), ('referring', 'v'), ('height', 'n'), ('mapping', 'v'), ('table', 'a'), ('ratios', 'n'), ('segment', 'n'), ('body', 'n'), ('portions', 'n'), ('human', 'a'), ('groups', 'n'), ('heights', 'n'), ('per', None), ('human', 'a'), ('groups', 'n'), ('acquiring', 'v'), ('specific', 'a'), ('height', 'n'), ('specific', 'a'), ('passenger', 'n'), ('retrieving', 'v'), ('specific', 'a'), ('weight', 'n'), ('mapping', 'v'), ('information', 'n'), ('weight', 'n'), ('mapping', 'n'), ('table', 'a'), ('correlations', 'n'), ('heights', 'n'), ('weights', 'n'), ('per', None), ('human', 'a'), ('groups', 'n'), ('acquiring', 'v'), ('weight', 'n'), ('specific', 'a'), ('passenger', 'n'), ('referring', 'v'), ('specific', 'a'), ('heighttechniques', 'n'), ('related', 'v'), ('improved', 'a'), ('video', 'n'), ('coding', 'v'), ('based', 'v'), ('face', 'n'), ('detection', 'n'), ('region', 'n'), ('extraction', 'n'), ('tracking', 'v'), ('discussed', 'v'), ('techniques', 'n'), ('may', None), ('include', 'v'), ('performing', 'v'), ('facial', 'a'), ('search', 'n'), ('video', 'n'), ('frame', 'n'), ('determine', 'a'), ('candidate', 'n'), ('video', 'n'), ('frame', 'n'), ('testing', 'v'), ('candidate', 'n'), ('based', 'v'), ('skin', 'a'), ('tone', 'n'), ('information', 'n'), ('determine', 'n'), ('valid', 'a'), ('invalid', 'a'), ('rejecting', 'n'), ('invalid', 'a'), ('encoding', 'v'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('valid', 'a'), ('generate', 'n'), ('coded', 'v'), ('bitstreama', 'n'), ('method', 'n'), ('managing', 'v'), ('smart', 'a'), ('database', 'n'), ('stores', 'n'), ('facial', 'a'), ('face', 'n'), ('provided', 'v'), ('method', 'n'), ('includes', 'v'), ('steps', 'n'), ('managing', 'v'), ('counting', 'v'), ('specific', 'a'), ('facial', 'a'), ('corresponding', 'n'), ('specific', 'a'), ('person', 'n'), ('smart', 'a'), ('database', 'n'), ('new', 'a'), ('facial', 'a'), ('continuously', 'r'), ('stored', 'v'), ('determining', 'v'), ('whether', None), ('first', 'a'), ('counted', 'v'), ('value', 'n'), ('representing', 'v'), ('count', 'n'), ('specific', 'a'), ('facial', 'a'), ('satisfies', 'n'), ('first', 'r'), ('set', 'v'), ('value', 'n'), ('b', 'n'), ('first', 'r'), ('counted', 'v'), ('value', 'n'), ('satisfies', 'n'), ('first', 'r'), ('set', 'v'), ('value', 'n'), ('inputting', 'v'), ('specific', 'a'), ('facial', 'a'), ('neural', 'a'), ('aggregation', 'n'), ('network', 'n'), ('generate', 'v'), ('quality', 'n'), ('scores', 'n'), ('specific', 'a'), ('facial', 'a'), ('aggregation', 'n'), ('specific', 'a'), ('facial', 'a'), ('second', 'n'), ('counted', 'v'), ('value', 'n'), ('representing', 'v'), ('count', 'n'), ('specific', 'a'), ('quality', 'n'), ('scores', 'n'), ('among', None), ('quality', 'n'), ('scores', 'n'), ('highest', 'a'), ('counting', 'v'), ('thereof', 'a'), ('satisfies', 'n'), ('second', 'v'), ('set', 'v'), ('value', 'n'), ('deleting', 'v'), ('part', 'n'), ('specific', 'a'), ('facial', 'a'), ('corresponding', 'n'), ('uncounted', 'a'), ('quality', 'n'), ('scores', 'n'), ('smart', 'v'), ('databasea', 'n'), ('capable', 'a'), ('determining', 'v'), ('algorithms', 'n'), ('applied', 'v'), ('regions', 'n'), ('interest', 'n'), ('within', None), ('digital', 'a'), ('representations', 'n'), ('presented', 'v'), ('preprocessing', 'v'), ('module', 'n'), ('utilizes', None), ('one', None), ('feature', 'n'), ('identification', 'n'), ('algorithms', None), ('determine', 'n'), ('regions', 'n'), ('interest', 'n'), ('based', 'v'), ('feature', 'n'), ('density', 'n'), ('preprocessing', 'v'), ('modules', 'n'), ('leverages', 'v'), ('feature', 'v'), ('density', 'n'), ('signature', 'n'), ('region', 'n'), ('determine', 'n'), ('diverse', 'n'), ('modules', 'n'), ('operate', 'v'), ('region', 'n'), ('interest', 'n'), ('specific', 'a'), ('embodiment', 'n'), ('focuses', 'n'), ('structured', 'v'), ('documents', 'n'), ('also', 'r'), ('presented', 'v'), ('disclosed', 'a'), ('approach', 'n'), ('enhanced', 'v'), ('addition', 'n'), ('object', 'n'), ('classifier', 'n'), ('classifies', 'n'), ('types', 'v'), ('objects', 'n'), ('found', 'v'), ('regions', 'n'), ('interestdisclosed', 'v'), ('mobile', 'a'), ('terminal', 'n'), ('mobile', 'a'), ('terminal', 'n'), ('may', None), ('include', 'v'), ('front', 'a'), ('camera', 'n'), ('obtaining', 'v'), ('face', 'n'), ('glance', 'n'), ('sensor', 'n'), ('tilted', 'v'), ('certain', 'a'), ('angle', 'n'), ('disposed', 'v'), ('adjacent', 'a'), ('front', 'a'), ('camera', 'n'), ('obtain', 'v'), ('metadata', 'a'), ('face', 'n'), ('controller', 'n'), ('obtaining', 'v'), ('distance', 'n'), ('glance', 'n'), ('sensor', 'n'), ('front', 'n'), ('camera', 'n'), ('distance', 'n'), ('enabling', 'v'), ('area', 'n'), ('overlap', None), ('region', 'n'), ('first', 'a'), ('region', 'n'), ('representing', 'v'), ('range', 'n'), ('photographable', 'a'), ('front', 'n'), ('camera', 'n'), ('overlaps', 'v'), ('second', 'a'), ('region', 'n'), ('representing', 'v'), ('range', 'n'), ('photographable', 'a'), ('glance', 'n'), ('sensor', 'n'), ('maximumthis', 'n'), ('disclosure', 'n'), ('provides', 'v'), ('methods', 'n'), ('apparatus', 'r'), ('including', 'v'), ('computer', 'n'), ('programs', 'n'), ('encoded', 'v'), ('computer', 'n'), ('storage', 'n'), ('media', 'n'), ('intelligent', 'a'), ('routing', 'v'), ('notifications', 'n'), ('related', 'a'), ('media', 'n'), ('programming', 'v'), ('one', None), ('aspect', 'a'), ('smart', 'a'), ('television', 'n'), ('tv', 'n'), ('implemented', 'v'), ('track', 'n'), (\"'s\", None), ('tv', 'n'), ('watching', 'v'), ('behavior', 'a'), ('anticipate', 'n'), ('programming', 'n'), ('based', 'v'), ('behavior', 'a'), ('aspects', 'n'), ('smart', 'a'), ('tv', 'n'), ('implemented', 'v'), ('detect', 'n'), (\"'s\", None), ('presence', 'n'), ('based', 'v'), ('detection', 'n'), ('automatically', 'r'), ('change', 'a'), ('tv', 'n'), ('channel', 'n'), ('media', 'n'), ('programming', 'v'), ('analyzed', 'v'), ('desirable', 'a'), ('aspects', 'n'), ('smart', 'a'), ('tv', 'n'), ('implemented', 'v'), ('transmit', 'n'), ('notification', 'n'), ('instructions', 'n'), ('electronic', 'a'), ('within', None), ('network', 'n'), ('attempt', 'n'), ('alert', 'n'), ('upcoming', 'a'), ('media', 'n'), ('programming', 'v'), ('additionally', 'r'), ('smart', 'a'), ('tv', 'n'), ('implemented', 'v'), ('transmit', 'n'), ('detection', 'n'), ('instructions', 'n'), ('electronic', 'a'), ('within', None), ('network', 'n'), ('whereby', None), ('electronic', 'a'), ('attempt', 'n'), ('detect', 'n'), (\"'s\", None), ('presence', 'n'), ('voice', 'n'), ('configured', 'v'), ('output', 'n'), ('test', 'n'), ('depth+multi-spectral', 'a'), ('including', 'v'), ('pixels', 'n'), ('pixel', 'a'), ('corresponds', 'v'), ('one', None), ('sensors', 'n'), ('sensor', 'v'), ('array', 'a'), ('camera', 'n'), ('includes', 'v'), ('least', 'a'), ('depth', 'a'), ('value', 'n'), ('spectral', 'a'), ('value', 'n'), ('spectral', 'a'), ('light', 'a'), ('sub-band', 'a'), ('spectral', 'a'), ('illuminators', 'n'), ('camera', 'v'), ('face', 'n'), ('machine', 'n'), ('previously', 'r'), ('trained', 'v'), ('set', 'v'), ('labeled', 'a'), ('training', 'n'), ('depth+multi-spectral', 'a'), ('structure', 'n'), ('test', 'n'), ('depth+multi-spectral', 'a'), ('face', 'n'), ('machine', 'n'), ('configured', 'v'), ('output', 'n'), ('confidence', 'n'), ('value', 'n'), ('indicating', 'v'), ('likelihood', 'a'), ('test', 'n'), ('depth+multi-spectral', 'a'), ('includes', 'v'), ('faceembodiments', 'n'), ('present', 'a'), ('disclosure', 'n'), ('relate', 'n'), ('processing', 'n'), ('method', 'n'), ('apparatus', 'n'), ('electronic', 'a'), ('method', 'n'), ('includes', 'v'), ('acquiring', 'v'), ('photo', 'n'), ('album', 'n'), ('obtained', 'v'), ('face', 'n'), ('clustering', 'v'), ('collecting', 'v'), ('face', 'n'), ('information', 'n'), ('respective', 'a'), ('photo', 'n'), ('album', 'n'), ('acquiring', 'v'), ('face', 'n'), ('parameter', 'n'), ('according', 'v'), ('face', 'n'), ('information', 'n'), ('selecting', 'v'), ('cover', 'n'), ('according', 'v'), ('face', 'n'), ('parameter', 'n'), ('taking', 'v'), ('face-region', 'a'), ('cover', 'n'), ('setting', 'v'), ('face-region', 'a'), ('cover', 'n'), ('photo', 'n'), ('albumtechniques', 'n'), ('described', 'v'), ('herein', 'a'), ('provide', None), ('location-based', 'a'), ('access', 'n'), ('control', 'n'), ('secured', 'v'), ('resources', 'n'), ('generally', 'r'), ('described', 'v'), ('configurations', 'n'), ('disclosed', 'v'), ('herein', 'r'), ('enable', 'a'), ('dynamically', 'r'), ('modify', 'a'), ('access', 'n'), ('secured', 'v'), ('resources', 'n'), ('based', 'v'), ('one', None), ('location-related', 'a'), ('actions', 'n'), ('example', 'n'), ('techniques', 'n'), ('disclosed', 'v'), ('herein', 'r'), ('enable', 'a'), ('computing', 'v'), ('control', 'n'), ('access', 'n'), ('resources', 'n'), ('computing', 'v'), ('display', 'n'), ('secured', 'v'), ('locations', 'n'), ('secured', 'v'), ('data', 'n'), ('configurations', 'n'), ('techniques', 'n'), ('disclosed', 'v'), ('herein', 'r'), ('enable', 'a'), ('controlled', 'a'), ('access', 'n'), ('secured', 'v'), ('resources', 'n'), ('based', 'v'), ('least', 'a'), ('part', 'n'), ('invitation', 'n'), ('associated', 'v'), ('location', 'n'), ('positioning', 'v'), ('data', 'n'), ('indicating', 'v'), ('location', 'n'), ('one', None), ('embodiment', 'n'), ('provides', 'v'), ('method', 'a'), ('comprising', 'v'), ('receiving', 'v'), ('piece', 'n'), ('content', 'n'), ('salient', 'n'), ('moments', 'n'), ('data', 'n'), ('piece', 'n'), ('content', 'n'), ('method', 'n'), ('comprises', 'n'), ('based', 'v'), ('salient', 'a'), ('moments', 'n'), ('data', 'n'), ('determining', 'v'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('piece', 'n'), ('content', 'n'), ('method', 'n'), ('comprises', 'v'), ('displaying', 'v'), ('viewport', 'n'), ('display', 'n'), ('movement', 'n'), ('viewport', 'n'), ('based', 'v'), ('first', 'a'), ('path', 'n'), ('playback', 'n'), ('piece', 'n'), ('content', 'n'), ('method', 'n'), ('comprises', 'v'), ('generating', 'v'), ('augmentation', 'n'), ('salient', 'a'), ('moment', 'n'), ('occurring', 'v'), ('piece', 'n'), ('content', 'n'), ('presenting', 'v'), ('augmentation', 'n'), ('viewport', 'n'), ('portion', 'n'), ('playback', 'n'), ('augmentation', 'n'), ('comprises', 'v'), ('interactive', 'a'), ('hint', 'n'), ('guiding', 'v'), ('viewport', 'n'), ('salient', 'n'), ('momenta', 'v'), ('computer-implemented', 'a'), ('method', 'a'), ('computer', 'n'), ('program', 'n'), ('product', 'n'), ('provided', 'v'), ('facial', 'a'), ('method', 'n'), ('includes', 'v'), ('receiving', 'v'), ('method', 'n'), ('also', 'r'), ('includes', 'v'), ('extracting', 'a'), ('feature', 'n'), ('extractor', 'n'), ('utilizing', 'a'), ('convolutional', 'a'), ('neural', 'a'), ('network', 'n'), ('cnn', 'n'), ('enlarged', 'v'), ('intra-class', 'a'), ('variance', 'n'), ('long-tail', 'a'), ('classes', 'n'), ('feature', 'v'), ('vectors', 'n'), ('method', 'v'), ('additionally', 'r'), ('includes', 'v'), ('generating', 'a'), ('feature', 'n'), ('generator', 'n'), ('discriminative', 'a'), ('feature', 'n'), ('vectors', 'n'), ('feature', 'v'), ('vectors', 'n'), ('method', 'v'), ('includes', 'v'), ('classifying', 'v'), ('utilizing', 'v'), ('fully', 'r'), ('connected', 'v'), ('classifier', 'a'), ('identity', 'n'), ('discriminative', 'a'), ('feature', 'n'), ('vector', 'n'), ('method', 'n'), ('also', 'r'), ('includes', 'v'), ('control', 'n'), ('operation', 'n'), ('-based', 'v'), ('machine', 'n'), ('react', 'n'), ('accordance', 'n'), ('identitysome', 'a'), ('embodiments', 'n'), ('invention', 'n'), ('provide', 'v'), ('efficient', 'a'), ('expressive', 'a'), ('machine-trained', 'a'), ('networks', 'n'), ('performing', 'v'), ('machine', 'n'), ('learning', 'v'), ('machine-trained', 'a'), ('mt', 'n'), ('networks', 'n'), ('embodiments', 'n'), ('use', 'v'), ('novel', 'a'), ('processing', 'n'), ('nodes', 'n'), ('novel', 'a'), ('activation', 'n'), ('functions', 'n'), ('allow', 'v'), ('mt', 'n'), ('network', 'n'), ('efficiently', 'r'), ('define', 'v'), ('fewer', 'a'), ('processing', 'n'), ('node', 'n'), ('layers', 'n'), ('complex', 'a'), ('mathematical', 'a'), ('expression', 'n'), ('solves', 'v'), ('particular', 'a'), ('problem', 'n'), ('eg', 'n'), ('face', 'n'), ('speech', 'n'), ('etc', None), ('embodiments', 'n'), ('activation', 'n'), ('function', 'n'), ('eg', None), ('cup', 'n'), ('function', 'n'), ('used', 'v'), ('numerous', 'a'), ('processing', 'v'), ('nodes', 'n'), ('mt', 'a'), ('network', 'n'), ('machine', 'n'), ('learning', 'v'), ('activation', 'n'), ('function', 'n'), ('configured', 'v'), ('differently', 'r'), ('different', 'a'), ('processing', 'v'), ('nodes', 'n'), ('different', 'a'), ('nodes', 'n'), ('emulate', 'v'), ('implement', 'a'), ('two', None), ('different', 'a'), ('functions', 'n'), ('eg', 'v'), ('two', None), ('boolean', 'a'), ('logical', 'a'), ('operators', 'n'), ('xor', 'v'), ('activation', 'n'), ('function', 'n'), ('embodiments', 'n'), ('periodic', 'a'), ('function', 'n'), ('configured', 'v'), ('implement', 'a'), ('different', 'a'), ('functions', 'n'), ('eg', 'v'), ('different', 'a'), ('sinusoidal', 'a'), ('functionsmethods', 'n'), ('may', None), ('provide', 'v'), ('facial', 'a'), ('least', 'a'), ('one', None), ('input', 'n'), ('utilizing', 'v'), ('hierarchical', 'a'), ('feature', 'n'), ('learning', 'v'), ('pair-wise', 'a'), ('receptive', 'a'), ('field', 'n'), ('theory', 'n'), ('may', None), ('used', 'v'), ('input', 'v'), ('generate', 'a'), ('pre-processed', 'a'), ('multi-channel', 'a'), ('channels', 'n'), ('pre-processed', 'a'), ('may', None), ('activated', 'v'), ('based', 'v'), ('amount', 'n'), ('feature', 'n'), ('rich', 'a'), ('details', 'n'), ('within', None), ('channels', 'n'), ('similarly', 'r'), ('local', 'a'), ('patches', 'n'), ('may', None), ('activated', 'v'), ('based', 'v'), ('discriminant', 'n'), ('within', None), ('local', 'a'), ('patches', 'n'), ('may', None), ('extracted', 'v'), ('local', 'a'), ('patches', 'n'), ('discriminant', 'n'), ('may', None), ('selected', 'v'), ('order', 'n'), ('perform', 'n'), ('feature', 'n'), ('matching', 'v'), ('pair', 'a'), ('sets', 'n'), ('may', None), ('utilize', 'v'), ('patch', 'n'), ('feature', 'n'), ('pooling', 'v'), ('pair-wise', 'a'), ('matching', 'a'), ('large-scale', 'a'), ('training', 'n'), ('order', 'n'), ('quickly', 'r'), ('accurately', 'r'), ('perform', 'a'), ('facial', 'a'), ('low', 'a'), ('cost', 'n'), ('memory', 'n'), ('computationa', 'n'), ('method', 'n'), ('controlling', 'v'), ('terminal', 'n'), ('provided', 'v'), ('terminal', 'a'), ('includes', 'v'), ('capturing', 'v'), ('apparatus', 'n'), ('least', 'a'), ('one', None), ('acquired', 'v'), ('capturing', 'v'), ('apparatus', 'n'), ('motion', 'n'), ('parameter', 'n'), ('terminal', 'n'), ('obtained', 'v'), ('processing', 'n'), ('acquired', 'v'), ('controlled', 'v'), ('performed', 'n'), ('based', 'v'), ('motion', 'n'), ('parameter', 'n'), ('equal', 'a'), ('less', 'r'), ('preset', 'a'), ('parameter', 'n'), ('threshold', 'n'), ('skipped', 'v'), ('based', 'v'), ('motion', 'n'), ('parameter', 'n'), ('greater', 'a'), ('preset', 'n'), ('parameter', 'n'), ('thresholda', 'v'), ('drive-through', 'a'), ('order', 'n'), ('processing', 'n'), ('method', 'n'), ('apparatus', 'n'), ('disclosed', 'v'), ('drive-through', 'a'), ('order', 'n'), ('processing', 'n'), ('method', 'n'), ('includes', 'v'), ('receiving', 'v'), ('customer', 'n'), ('information', 'n'), ('detected', 'v'), ('vision', 'n'), ('providing', 'v'), ('product', 'n'), ('information', 'n'), ('based', 'v'), ('customer', 'n'), ('information', 'n'), ('processing', 'v'), ('product', 'n'), ('order', 'n'), ('customer', 'n'), ('according', 'v'), ('present', 'a'), ('disclosure', 'n'), ('possible', 'a'), ('rapidly', 'r'), ('process', 'a'), ('order', 'n'), ('using', 'v'), ('customer', 'n'), ('information', 'n'), ('based', 'v'), ('customer', 'n'), ('using', 'v'), ('artificial', 'a'), ('intelligence', 'n'), ('ai', 'a'), ('model', 'n'), ('machine', 'n'), ('learning', 'v'), ('g', 'a'), ('networkan', 'a'), ('processing', 'n'), ('method', 'n'), ('performed', 'v'), ('computing', 'v'), ('includes', 'v'), ('identifying', 'v'), ('using', 'v'), ('face', 'n'), ('one', None), ('faces', 'v'), ('face', 'n'), ('corresponding', 'v'), ('respective', 'a'), ('person', 'n'), ('captured', 'v'), ('first', 'r'), ('identified', 'v'), ('face', 'n'), ('extracting', 'v'), ('set', 'v'), ('profile', 'a'), ('parameters', 'n'), ('corresponding', 'v'), ('person', 'n'), ('first', 'r'), ('selecting', 'v'), ('tiles', 'n'), ('first', 'r'), ('tile', 'a'), ('matches', 'n'), ('face', 'v'), ('corresponding', 'v'), ('person', 'n'), ('first', 'a'), ('accordance', 'n'), ('predefined', 'v'), ('correspondence', 'n'), ('set', 'v'), ('profile', 'a'), ('parameters', 'n'), ('corresponding', 'v'), ('person', 'n'), ('set', 'v'), ('pre-stored', 'a'), ('description', 'n'), ('parameters', 'n'), ('first', 'r'), ('tile', None), ('generating', 'v'), ('second', 'a'), ('covering', 'v'), ('faces', 'v'), ('respective', 'a'), ('persons', 'n'), ('first', 'r'), ('corresponding', 'v'), ('first', 'a'), ('tiles', 'n'), ('sharing', 'v'), ('first', 'a'), ('second', 'a'), ('predefined', 'v'), ('order', 'n'), ('via', None), ('group', 'n'), ('chat', None), ('sessionin', 'v'), ('one', None), ('embodiment', 'n'), ('artificial', 'a'), ('reality', 'n'), ('determines', 'n'), ('performance', 'n'), ('metric', 'a'), ('eye', 'n'), ('tracking', 'v'), ('first', 'a'), ('performance', 'n'), ('threshold', 'n'), ('eye', 'n'), ('tracking', 'v'), ('associated', 'v'), ('head-mounted', 'a'), ('display', 'n'), ('worn', 'v'), ('artificial', 'a'), ('reality', 'n'), ('receives', 'n'), ('first', 'r'), ('inputs', 'r'), ('associated', 'v'), ('body', 'n'), ('determines', 'v'), ('region', 'n'), ('looking', 'v'), ('within', None), ('field', 'n'), ('view', 'n'), ('head-mounted', 'a'), ('display', 'n'), ('based', 'v'), ('received', 'v'), ('first', 'a'), ('inputs', 'n'), ('determines', 'n'), ('vergence', 'n'), ('distance', 'n'), ('based', 'v'), ('least', 'a'), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('region', 'n'), ('looking', 'v'), ('locations', 'n'), ('one', None), ('objects', 'v'), ('scene', 'n'), ('displayed', 'v'), ('head-mounted', 'a'), ('display', 'n'), ('adjusts', 'v'), ('one', None), ('configurations', 'n'), ('head-mounted', 'a'), ('display', 'n'), ('based', 'v'), ('determined', 'a'), ('vergence', 'n'), ('distance', 'n'), ('computer-implemented', 'a'), ('method', 'n'), ('provided', 'v'), ('-based', 'a'), ('self-guided', 'a'), ('object', 'a'), ('detection', 'n'), ('method', 'n'), ('includes', 'v'), ('receiving', 'v'), ('set', 'v'), ('respective', 'a'), ('grid', 'a'), ('thereon', 'n'), ('labeled', 'v'), ('regarding', 'v'), ('respective', 'a'), ('object', 'n'), ('detected', 'v'), ('using', 'v'), ('grid', 'a'), ('level', 'n'), ('label', 'n'), ('data', 'n'), ('method', 'n'), ('includes', 'v'), ('training', 'v'), ('grid-based', 'a'), ('object', 'a'), ('detector', 'n'), ('using', 'v'), ('grid', 'a'), ('level', 'n'), ('label', 'n'), ('data', 'n'), ('method', 'n'), ('also', 'r'), ('includes', 'v'), ('determining', 'v'), ('respective', 'a'), ('bounding', 'n'), ('box', 'n'), ('respective', 'a'), ('object', 'a'), ('applying', 'v'), ('local', 'a'), ('segmentation', 'n'), ('method', 'n'), ('additionally', 'r'), ('includes', 'v'), ('training', 'v'), ('region-based', 'a'), ('convolutional', 'a'), ('neural', 'a'), ('network', 'n'), ('rcnn', 'n'), ('joint', 'n'), ('object', 'a'), ('localization', 'n'), ('object', 'n'), ('using', 'v'), ('respective', 'a'), ('bounding', 'n'), ('box', 'n'), ('respective', 'a'), ('object', 'a'), ('input', 'n'), ('rcnna', 'n'), ('method', 'n'), ('face', 'n'), ('comprising', 'v'), ('multiple', 'a'), ('phases', 'n'), ('implemented', 'v'), ('parallel', 'a'), ('architecture', 'n'), ('first', 'a'), ('phase', 'n'), ('normalization', 'n'), ('phase', 'n'), ('whereby', None), ('captured', 'v'), ('normalized', 'a'), ('size', 'n'), ('orientation', 'n'), ('illumination', 'n'), ('stored', 'v'), ('preexisting', 'a'), ('database', 'n'), ('second', 'a'), ('phase', 'n'), ('feature', 'n'), ('extractiondistance', 'n'), ('matrix', 'n'), ('phase', 'n'), ('distance', 'n'), ('matrix', 'n'), ('generated', 'v'), ('captured', 'a'), ('coarse', 'a'), ('phase', 'n'), ('generated', 'v'), ('distance', 'n'), ('matrix', 'n'), ('compared', 'v'), ('distance', 'n'), ('matrices', 'n'), ('database', 'v'), ('using', 'v'), ('euclidean', 'a'), ('distance', 'n'), ('matches', 'n'), ('create', 'v'), ('candidate', 'n'), ('lists', 'n'), ('detailed', 'v'), ('phase', 'n'), ('multiple', 'a'), ('face', 'n'), ('algorithms', 'n'), ('applied', 'a'), ('candidate', 'n'), ('lists', 'n'), ('produce', 'v'), ('final', 'a'), ('result', 'n'), ('distance', 'n'), ('matrices', 'n'), ('normalized', 'a'), ('database', 'n'), ('may', None), ('broken', 'v'), ('parallel', 'a'), ('lists', 'n'), ('parallelization', 'v'), ('feature', 'n'), ('extractiondistance', 'n'), ('matrix', 'n'), ('phase', 'n'), ('candidate', 'n'), ('lists', 'n'), ('may', None), ('also', 'r'), ('grouped', 'v'), ('according', 'v'), ('dissimilarity', 'n'), ('algorithm', 'n'), ('parallel', 'n'), ('processing', 'n'), ('detailed', 'a'), ('phasean', 'a'), ('imaging', 'v'), ('including', 'v'), ('pixel', 'a'), ('matrix', 'n'), ('provided', 'v'), ('pixel', 'a'), ('matrix', 'n'), ('includes', 'v'), ('phase', 'a'), ('detection', 'n'), ('pixels', 'n'), ('regular', 'a'), ('pixels', 'n'), ('performs', 'n'), ('autofocusing', 'v'), ('according', 'v'), ('pixel', 'n'), ('data', 'n'), ('phase', 'n'), ('detection', 'n'), ('pixels', 'n'), ('determines', 'v'), ('operating', 'v'), ('resolution', 'n'), ('regular', 'a'), ('pixels', 'n'), ('according', 'v'), ('autofocused', 'a'), ('pixel', 'n'), ('data', 'n'), ('phase', 'n'), ('detection', 'n'), ('pixels', 'n'), ('wherein', 'v'), ('phase', 'a'), ('detection', 'n'), ('pixels', 'n'), ('always-on', 'a'), ('pixels', 'n'), ('regular', 'a'), ('pixels', 'n'), ('selectively', 'r'), ('turned', 'v'), ('autofocusing', 'v'), ('accomplishedan', 'n'), ('apparatus', 'n'), ('includes', 'v'), ('first', 'a'), ('camera', 'n'), ('module', 'n'), ('providing', 'v'), ('first', 'a'), ('object', 'a'), ('first', 'a'), ('field', 'n'), ('view', 'n'), ('second', 'a'), ('camera', 'n'), ('module', 'n'), ('providing', 'v'), ('second', 'a'), ('object', 'a'), ('second', 'a'), ('field', 'n'), ('view', 'n'), ('different', 'a'), ('first', 'a'), ('field', 'n'), ('view', 'n'), ('first', 'r'), ('depth', 'v'), ('map', 'n'), ('generator', 'n'), ('generates', 'n'), ('first', 'r'), ('depth', 'v'), ('map', 'n'), ('first', 'r'), ('based', 'v'), ('first', 'a'), ('second', 'a'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('generator', 'n'), ('generates', 'v'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('second', 'n'), ('based', 'v'), ('first', 'r'), ('second', 'a'), ('first', 'a'), ('depth', 'n'), ('mapmethods', 'n'), ('apparatus', 'v'), ('including', 'v'), ('computer', 'n'), ('programs', 'n'), ('encoded', 'v'), ('computer', 'n'), ('storage', 'n'), ('media', 'n'), ('payment', 'n'), ('based', 'v'), ('face', 'n'), ('provided', 'v'), ('one', None), ('methods', 'n'), ('includes', 'v'), ('acquiring', 'v'), ('first', 'a'), ('face', 'n'), ('information', 'n'), ('target', 'n'), ('extracting', 'v'), ('first', 'a'), ('characteristic', 'a'), ('information', 'n'), ('first', 'r'), ('face', 'n'), ('information', 'n'), ('wherein', None), ('first', 'a'), ('characteristic', 'a'), ('information', 'n'), ('includes', 'v'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('target', 'n'), ('gaze', 'n'), ('information', 'n'), ('target', 'n'), ('determining', 'v'), ('whether', None), ('target', 'n'), ('willingness', 'n'), ('pay', 'n'), ('according', 'v'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('target', 'n'), ('gaze', 'n'), ('information', 'n'), ('target', 'n'), ('including', 'v'), ('determining', 'v'), ('whether', None), ('angle', 'a'), ('rotation', 'n'), ('preset', 'v'), ('direction', 'n'), ('less', 'r'), ('angle', 'a'), ('threshold', 'n'), ('whether', None), ('probability', 'n'), ('value', 'n'), ('gazes', 'v'), ('payment', 'n'), ('screen', 'n'), ('greater', 'a'), ('probability', 'n'), ('threshold', 'a'), ('response', 'n'), ('determining', 'v'), ('target', 'n'), ('willingness', 'a'), ('pay', 'n'), ('completing', 'v'), ('payment', 'n'), ('operation', 'n'), ('based', 'v'), ('face', 'n'), ('novel', 'a'), ('method', 'n'), ('apparatus', 'n'), ('face', 'n'), ('authentication', 'n'), ('disclosed', 'v'), ('disclosed', 'v'), ('method', 'n'), ('comprises', 'v'), ('detecting', 'v'), ('motion', 'n'), ('subject', 'n'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('assigning', 'v'), ('unique', 'a'), ('session', 'n'), ('identification', 'n'), ('number', 'n'), ('subject', 'a'), ('detected', 'v'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('detecting', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'n'), ('detected', 'v'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('generating', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'n'), ('assessing', 'v'), ('quality', 'n'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('conducing', 'v'), ('incremental', 'a'), ('training', 'n'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('determining', 'v'), ('identity', 'n'), ('subject', 'n'), ('based', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('identifying', 'v'), ('intent', 'n'), ('subject', 'a'), ('authorizing', 'v'), ('access', 'n'), ('point', 'n'), ('entry', 'n'), ('based', 'v'), ('determined', 'v'), ('identity', 'n'), ('subject', 'n'), ('based', 'v'), ('intent', 'n'), ('subjectdisclosed', 'v'), ('herein', 'n'), ('robot', 'v'), ('electronic', 'a'), ('acquiring', 'v'), ('video', 'n'), ('method', 'n'), ('acquiring', 'v'), ('video', 'n'), ('using', 'v'), ('robot', 'a'), ('robot', 'n'), ('includes', 'v'), ('camera', 'n'), ('configured', 'v'), ('rotate', 'a'), ('lateral', 'a'), ('direction', 'n'), ('tilt', 'v'), ('vertical', 'a'), ('direction', 'n'), ('controls', 'n'), ('least', 'v'), ('one', None), ('direction', 'n'), ('rotation', 'n'), ('camera', 'n'), ('angle', 'n'), ('tilt', 'n'), ('camera', 'n'), ('focal', 'a'), ('distance', 'n'), ('camera', 'n'), ('tracking', 'v'), ('video', 'n'), ('acquired', 'v'), ('cameras', 'n'), ('methods', 'n'), ('disclosed', 'v'), ('inferring', 'v'), ('topics', 'n'), ('file', 'n'), ('containing', 'v'), ('audio', 'a'), ('video', 'n'), ('example', 'n'), ('multimodal', 'a'), ('multimedia', 'n'), ('file', 'n'), ('order', 'n'), ('facilitate', 'n'), ('video', 'n'), ('indexing', 'v'), ('set', 'v'), ('entities', 'n'), ('extracted', 'v'), ('file', 'n'), ('linked', 'v'), ('produce', 'v'), ('graph', 'a'), ('reference', 'n'), ('information', 'n'), ('also', 'r'), ('obtained', 'v'), ('set', 'a'), ('entities', 'n'), ('entities', 'n'), ('may', None), ('drawn', 'v'), ('example', 'n'), ('wikipedia', 'n'), ('categories', 'n'), ('large', 'a'), ('ontological', 'a'), ('data', 'n'), ('sources', 'n'), ('analysis', 'n'), ('graph', 'n'), ('using', 'v'), ('unsupervised', 'a'), ('learning', 'v'), ('permits', 'n'), ('determining', 'v'), ('clusters', 'n'), ('graph', 'v'), ('extracting', 'v'), ('clusters', 'n'), ('possibly', 'r'), ('using', 'v'), ('supervised', 'v'), ('learning', 'v'), ('provides', 'v'), ('selection', 'n'), ('topic', 'n'), ('identifiers', 'n'), ('topic', 'v'), ('identifiers', 'n'), ('used', 'v'), ('indexing', 'v'), ('filea', 'a'), ('face', 'n'), ('method', 'n'), ('neural', 'a'), ('network', 'n'), ('training', 'v'), ('method', 'n'), ('apparatus', 'n'), ('electronic', 'a'), ('method', 'n'), ('comprises', 'n'), ('obtaining', 'v'), ('first', 'a'), ('face', 'n'), ('means', 'v'), ('first', 'a'), ('camera', 'n'), ('extracting', 'v'), ('first', 'a'), ('face', 'n'), ('feature', 'n'), ('first', 'a'), ('face', 'n'), ('comparing', 'v'), ('first', 'a'), ('face', 'n'), ('feature', 'n'), ('pre-stored', 'a'), ('second', 'a'), ('face', 'n'), ('feature', 'n'), ('obtain', 'v'), ('reference', 'n'), ('similarity', 'n'), ('second', 'a'), ('face', 'n'), ('feature', 'n'), ('obtained', 'v'), ('extracting', 'a'), ('feature', 'a'), ('second', 'a'), ('face', 'n'), ('obtained', 'v'), ('second', 'a'), ('camera', 'n'), ('second', 'a'), ('camera', 'n'), ('first', 'r'), ('camera', 'v'), ('different', 'a'), ('types', 'n'), ('cameras', 'n'), ('determining', 'v'), ('according', 'v'), ('reference', 'n'), ('similarity', 'n'), ('whether', None), ('first', 'a'), ('face', 'n'), ('feature', 'n'), ('second', 'a'), ('face', 'n'), ('feature', 'n'), ('correspond', 'n'), ('person', 'n'), ('present', 'a'), ('invention', 'n'), ('discloses', 'v'), ('technique', 'a'), ('alerting', 'v'), ('vision', 'n'), ('impairment', 'n'), ('comprises', 'v'), ('processing', 'v'), ('unit', 'n'), ('configured', 'v'), ('operable', 'a'), ('receiving', 'v'), ('scene', 'n'), ('data', 'n'), ('indicative', 'a'), ('scene', 'n'), ('least', 'a'), ('one', None), ('consumer', 'n'), ('environment', 'n'), ('identifying', 'v'), ('scene', 'n'), ('data', 'n'), ('certain', 'a'), ('consumer', 'n'), ('identifying', 'v'), ('event', 'n'), ('indicative', 'a'), ('behavioral', 'a'), ('compensation', 'n'), ('vision', 'n'), ('impairment', 'n'), ('upon', None), ('identification', 'n'), ('event', 'n'), ('sending', 'v'), ('notification', 'n'), ('relating', 'v'), ('vision', 'n'), ('impairment', 'n')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('configured', 'v'), ('make', 'v'), ('screen', 'a'), ('display', 'n'), ('frames', 'n'), ('comprising', 'v'), ('capturing', 'v'), ('device', 'n'), ('storage', 'n'), ('device', 'n'), ('storing', 'v'), ('modules', 'n'), ('coupled', 'v'), ('capturing', 'v'), ('device', 'n'), ('storage', 'n'), ('device', 'n'), ('configured', 'v'), ('execute', 'a'), ('modules', 'n'), ('storage', 'n'), ('device', 'n'), ('configure', 'n'), ('screen', 'n'), ('display', 'n'), ('marker', 'n'), ('objects', 'n'), ('predetermined', 'v'), ('positions', 'n'), ('configure', 'n'), ('capturing', 'v'), ('device', 'n'), ('capture', 'n'), ('first', 'a'), ('head', 'n'), ('looking', 'v'), ('predetermined', 'a'), ('positions', 'n'), ('perform', 'v'), ('first', 'a'), ('face', 'n'), ('operations', 'n'), ('first', 'r'), ('head', 'v'), ('obtain', 'v'), ('first', 'a'), ('face', 'n'), ('regions', 'n'), ('corresponding', 'v'), ('predetermined', 'a'), ('positions', 'n'), ('detect', 'v'), ('first', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('corresponding', 'v'), ('first', 'a'), ('face', 'n'), ('regions', 'n'), ('calculate', 'v'), ('rotation', 'n'), ('reference', 'n'), ('angles', 'n'), ('looking', 'v'), ('predetermined', 'v'), ('positions', 'n'), ('according', 'v'), ('first', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('configure', 'v'), ('capturing', 'v'), ('device', 'n'), ('capture', 'n'), ('second', 'a'), ('head', 'n'), ('perform', 'v'), ('second', 'a'), ('head', 'n'), ('obtain', 'v'), ('second', 'a'), ('face', 'n'), ('region', 'n'), ('detect', 'a'), ('second', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('within', None), ('second', 'a'), ('face', 'n'), ('region', 'n'), ('estimate', 'n'), ('head', 'n'), ('posture', 'n'), ('angle', 'n'), ('according', 'v'), ('second', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('calculate', 'v'), ('gaze', 'a'), ('position', 'n'), ('screen', 'n'), ('according', 'v'), ('head', 'n'), ('posture', 'n'), ('angle', 'a'), ('rotation', 'n'), ('reference', 'n'), ('angles', 'n'), ('predetermined', 'v'), ('positions', 'n'), ('configure', 'n'), ('screen', 'n'), ('display', 'n'), ('corresponding', 'v'), ('visual', 'a'), ('effect', 'n'), ('according', 'v'), ('gaze', 'n'), ('position', 'n'), ('according', 'v'), ('wherein', 'n'), ('gaze', 'a'), ('position', 'n'), ('comprises', 'n'), ('first', 'a'), ('coordinate', 'n'), ('value', 'n'), ('first', 'r'), ('axial', 'a'), ('direction', 'n'), ('second', 'a'), ('coordinate', 'n'), ('value', 'n'), ('second', 'a'), ('axial', 'a'), ('direction', 'n'), ('according', 'v'), ('wherein', 'a'), ('head', 'n'), ('posture', 'n'), ('angles', 'v'), ('comprise', 'r'), ('head', 'a'), ('pitch', 'n'), ('angle', 'n'), ('head', 'n'), ('yaw', 'n'), ('angle', 'a'), ('rotation', 'n'), ('reference', 'n'), ('angles', 'n'), ('comprise', 'v'), ('first', 'a'), ('pitch', 'n'), ('angle', 'n'), ('second', 'a'), ('pitch', 'n'), ('angle', 'n'), ('first', 'r'), ('yaw', 'r'), ('angle', 'a'), ('second', 'a'), ('yaw', 'n'), ('angle', 'n'), ('corresponding', 'v'), ('predetermined', 'v'), ('positions', 'n'), ('according', 'v'), ('wherein', 'n'), ('performs', 'n'), ('interpolation', 'n'), ('operation', 'n'), ('extrapolation', 'n'), ('operation', 'n'), ('according', 'v'), ('first', 'a'), ('yaw', 'n'), ('angle', 'a'), ('second', 'a'), ('yaw', 'n'), ('angle', 'n'), ('first', 'a'), ('position', 'n'), ('corresponding', 'v'), ('first', 'a'), ('yaw', 'a'), ('angle', 'n'), ('among', None), ('predetermined', 'a'), ('positions', 'n'), ('second', 'a'), ('position', 'n'), ('corresponding', 'v'), ('second', 'a'), ('yaw', 'a'), ('angle', 'n'), ('among', None), ('predetermined', 'a'), ('positions', 'n'), ('head', 'v'), ('yaw', 'r'), ('angle', 'a'), ('thereby', 'r'), ('obtaining', 'v'), ('first', 'a'), ('coordinate', 'n'), ('value', 'n'), ('gaze', 'a'), ('position', 'n'), ('performs', 'n'), ('interpolation', 'n'), ('operation', 'n'), ('extrapolation', 'n'), ('operation', 'n'), ('according', 'v'), ('first', 'a'), ('pitch', 'n'), ('angle', 'n'), ('second', 'a'), ('pitch', 'n'), ('angle', 'n'), ('third', 'a'), ('position', 'n'), ('corresponding', 'v'), ('first', 'a'), ('pitch', 'n'), ('angle', 'n'), ('among', None), ('predetermined', 'a'), ('positions', 'n'), ('fourth', 'a'), ('position', 'n'), ('corresponding', 'v'), ('second', 'a'), ('pitch', 'n'), ('angle', 'n'), ('among', None), ('predetermined', 'a'), ('positions', 'n'), ('head', 'v'), ('pitch', 'n'), ('angle', 'n'), ('thereby', 'r'), ('obtaining', 'v'), ('second', 'a'), ('coordinate', 'n'), ('value', 'n'), ('gaze', 'n'), ('position', 'n'), ('according', 'v'), ('wherein', 'n'), ('calculates', 'n'), ('first', 'r'), ('viewing', 'v'), ('distances', 'n'), ('screen', 'n'), ('according', 'v'), ('first', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('estimates', 'n'), ('second', 'a'), ('viewing', 'v'), ('distance', 'n'), ('screen', 'n'), ('according', 'v'), ('second', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('adjusts', 'v'), ('rotation', 'n'), ('reference', 'n'), ('angles', 'n'), ('gaze', 'v'), ('position', 'n'), ('according', 'v'), ('second', 'a'), ('viewing', 'n'), ('distance', 'n'), ('first', 'r'), ('viewing', 'v'), ('distances', 'n'), ('according', 'v'), ('wherein', 'a'), ('maps', 'n'), ('two-dimensional', 'a'), ('position', 'n'), ('coordinates', 'n'), ('second', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('plane', 'n'), ('coordinate', 'n'), ('system', 'n'), ('three-dimensional', 'a'), ('position', 'n'), ('coordinates', 'v'), ('three-dimensional', 'a'), ('coordinate', 'n'), ('system', 'n'), ('estimates', 'v'), ('head', 'a'), ('posture', 'n'), ('angle', 'n'), ('according', 'v'), ('three-dimensional', 'a'), ('position', 'n'), ('coordinates', 'n'), ('second', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('according', 'v'), ('wherein', 'a'), ('second', 'a'), ('head', 'n'), ('comprises', 'v'), ('wearable', 'a'), ('device', 'n'), ('second', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('comprise', 'v'), ('third', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('covered', 'v'), ('wearable', 'a'), ('device', 'n'), ('according', 'v'), ('wherein', 'a'), ('second', 'a'), ('head', 'n'), ('comprises', 'v'), ('wearable', 'a'), ('device', 'n'), ('second', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('comprise', 'v'), ('one', None), ('simulated', 'v'), ('landmarks', 'n'), ('marked', 'v'), ('wearable', 'a'), ('device', 'n'), ('operating', 'v'), ('adapted', 'v'), ('comprising', 'v'), ('capturing', 'v'), ('device', 'n'), ('making', 'v'), ('screen', 'a'), ('display', 'n'), ('frames', 'n'), ('comprising', 'v'), ('configuring', 'v'), ('screen', 'a'), ('display', 'n'), ('marker', 'n'), ('objects', 'n'), ('predetermined', 'v'), ('positions', 'n'), ('configuring', 'v'), ('capturing', 'v'), ('device', 'n'), ('capture', 'n'), ('first', 'a'), ('head', 'n'), ('looking', 'v'), ('predetermined', 'a'), ('positions', 'n'), ('performing', 'v'), ('first', 'a'), ('face', 'n'), ('operations', 'n'), ('first', 'r'), ('head', 'v'), ('obtain', 'v'), ('first', 'a'), ('face', 'n'), ('regions', 'n'), ('corresponding', 'v'), ('predetermined', 'a'), ('positions', 'n'), ('detecting', 'v'), ('first', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('corresponding', 'v'), ('first', 'a'), ('face', 'n'), ('regions', 'n'), ('calculating', 'v'), ('rotation', 'n'), ('reference', 'n'), ('angles', 'n'), ('looking', 'v'), ('predetermined', 'v'), ('positions', 'n'), ('according', 'v'), ('first', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('configuring', 'v'), ('capturing', 'v'), ('device', 'a'), ('capture', 'n'), ('second', 'a'), ('head', 'n'), ('performing', 'v'), ('second', 'a'), ('head', 'n'), ('obtain', 'v'), ('second', 'a'), ('face', 'n'), ('region', 'n'), ('detecting', 'v'), ('second', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('within', None), ('second', 'a'), ('face', 'n'), ('region', 'n'), ('estimating', 'v'), ('head', 'n'), ('posture', 'n'), ('angle', 'n'), ('according', 'v'), ('second', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('calculating', 'v'), ('gaze', 'a'), ('position', 'n'), ('screen', 'n'), ('according', 'v'), ('head', 'n'), ('posture', 'n'), ('angle', 'a'), ('rotation', 'n'), ('reference', 'n'), ('angles', 'n'), ('predetermined', 'v'), ('positions', 'n'), ('configuring', 'v'), ('screen', 'n'), ('display', 'n'), ('corresponding', 'v'), ('visual', 'a'), ('effect', 'n'), ('according', 'v'), ('gaze', 'a'), ('position', 'n'), ('operation', 'n'), ('according', 'v'), ('wherein', 'n'), ('gaze', 'a'), ('position', 'n'), ('comprises', 'n'), ('first', 'a'), ('coordinate', 'n'), ('value', 'n'), ('first', 'r'), ('axial', 'a'), ('direction', 'n'), ('second', 'a'), ('coordinate', 'n'), ('value', 'n'), ('second', 'a'), ('axial', 'a'), ('direction', 'n'), ('operation', 'n'), ('according', 'v'), ('wherein', 'a'), ('head', 'n'), ('posture', 'n'), ('angles', 'v'), ('comprise', 'r'), ('head', 'a'), ('pitch', 'n'), ('angle', 'n'), ('head', 'n'), ('yaw', 'n'), ('angle', 'a'), ('rotation', 'n'), ('reference', 'n'), ('angles', 'n'), ('comprise', 'v'), ('first', 'a'), ('pitch', 'n'), ('angle', 'n'), ('second', 'a'), ('pitch', 'n'), ('angle', 'n'), ('first', 'r'), ('yaw', 'r'), ('angle', 'a'), ('second', 'a'), ('yaw', 'n'), ('angle', 'n'), ('corresponding', 'v'), ('predetermined', 'a'), ('positions', 'n'), ('operation', 'n'), ('according', 'v'), ('wherein', 'a'), ('step', 'n'), ('calculating', 'v'), ('gaze', 'a'), ('position', 'n'), ('screen', 'n'), ('according', 'v'), ('head', 'n'), ('posture', 'n'), ('angle', 'a'), ('rotation', 'n'), ('reference', 'n'), ('angles', 'n'), ('predetermined', 'v'), ('positions', 'n'), ('comprises', 'n'), ('performing', 'v'), ('interpolation', 'n'), ('operation', 'n'), ('extrapolation', 'n'), ('operation', 'n'), ('according', 'v'), ('first', 'a'), ('yaw', 'n'), ('angle', 'a'), ('second', 'a'), ('yaw', 'n'), ('angle', 'n'), ('first', 'a'), ('position', 'n'), ('corresponding', 'v'), ('first', 'a'), ('yaw', 'a'), ('angle', 'n'), ('among', None), ('predetermined', 'a'), ('positions', 'n'), ('second', 'a'), ('position', 'n'), ('corresponding', 'v'), ('second', 'a'), ('yaw', 'a'), ('angle', 'n'), ('among', None), ('predetermined', 'a'), ('positions', 'n'), ('head', 'v'), ('yaw', 'r'), ('angle', 'a'), ('thereby', 'r'), ('obtaining', 'v'), ('first', 'a'), ('coordinate', 'n'), ('value', 'n'), ('gaze', 'a'), ('position', 'n'), ('performing', 'v'), ('interpolation', 'n'), ('operation', 'n'), ('extrapolation', 'n'), ('operation', 'n'), ('according', 'v'), ('first', 'a'), ('pitch', 'n'), ('angle', 'n'), ('second', 'a'), ('pitch', 'n'), ('angle', 'n'), ('third', 'a'), ('position', 'n'), ('corresponding', 'v'), ('first', 'a'), ('pitch', 'n'), ('angle', 'n'), ('among', None), ('predetermined', 'a'), ('positions', 'n'), ('fourth', 'a'), ('position', 'n'), ('corresponding', 'v'), ('second', 'a'), ('pitch', 'n'), ('angle', 'n'), ('among', None), ('predetermined', 'a'), ('positions', 'n'), ('head', 'v'), ('pitch', 'n'), ('angle', 'n'), ('thereby', 'r'), ('obtaining', 'v'), ('second', 'a'), ('coordinate', 'n'), ('value', 'n'), ('gaze', 'a'), ('position', 'n'), ('operation', 'n'), ('according', 'v'), ('wherein', 'a'), ('comprises', 'n'), ('calculating', 'v'), ('first', 'a'), ('viewing', 'v'), ('distances', 'n'), ('screen', 'n'), ('according', 'v'), ('first', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('estimating', 'v'), ('second', 'a'), ('viewing', 'v'), ('distance', 'n'), ('screen', 'n'), ('according', 'v'), ('second', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('adjusting', 'v'), ('rotation', 'n'), ('reference', 'n'), ('angles', 'n'), ('gaze', 'v'), ('position', 'n'), ('according', 'v'), ('second', 'a'), ('viewing', 'n'), ('distance', 'n'), ('first', 'r'), ('viewing', 'v'), ('distances', 'n'), ('operation', 'n'), ('according', 'v'), ('wherein', 'a'), ('comprises', 'n'), ('mapping', 'v'), ('two-dimensional', 'a'), ('position', 'n'), ('coordinates', 'n'), ('second', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('plane', 'n'), ('coordinate', 'n'), ('system', 'n'), ('three-dimensional', 'a'), ('position', 'n'), ('coordinates', 'v'), ('three-dimensional', 'a'), ('coordinate', 'n'), ('system', 'n'), ('estimating', 'v'), ('head', 'n'), ('posture', 'n'), ('angle', None), ('according', 'v'), ('three-dimensional', 'a'), ('position', 'n'), ('coordinates', 'n'), ('second', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('operation', 'n'), ('according', 'v'), ('wherein', 'a'), ('second', 'a'), ('head', 'n'), ('comprises', 'v'), ('wearable', 'a'), ('device', 'n'), ('second', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('comprise', 'v'), ('third', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('covered', 'v'), ('wearable', 'a'), ('device', 'n'), ('operation', 'n'), ('according', 'v'), ('wherein', 'a'), ('second', 'a'), ('head', 'n'), ('comprises', 'v'), ('wearable', 'a'), ('device', 'n'), ('second', 'a'), ('facial', 'a'), ('landmarks', 'n'), ('comprise', 'v'), ('one', None), ('simulated', 'v'), ('landmarks', 'n'), ('marked', 'v'), ('wearable', 'a'), ('device', 'n'), ('computation', 'n'), ('applied', 'v'), ('computing', 'v'), ('system', 'n'), ('wherein', 'v'), ('computing', 'v'), ('system', 'n'), ('comprises', 'v'), ('control', 'v'), ('unit', 'n'), ('computation', 'n'), ('group', 'n'), ('general', 'a'), ('storage', 'n'), ('unit', 'n'), ('wherein', 'v'), ('control', 'a'), ('unit', 'n'), ('comprises', 'v'), ('first', 'a'), ('memory', 'n'), ('decoding', 'v'), ('logic', 'a'), ('controller', 'n'), ('wherein', 'n'), ('computation', 'n'), ('group', 'n'), ('comprises', 'v'), ('group', 'n'), ('controller', 'n'), ('computing', 'v'), ('units', 'n'), ('general', 'a'), ('storage', 'n'), ('unit', 'n'), ('configured', 'v'), ('store', 'n'), ('data', 'n'), ('computation', 'n'), ('comprises', 'v'), ('receiving', 'v'), ('controller', 'n'), ('first', 'r'), ('level', 'a'), ('instruction', 'n'), ('sequence', 'n'), ('partitioning', 'v'), ('decoding', 'v'), ('logic', 'a'), ('first', 'a'), ('level', 'n'), ('instruction', 'n'), ('sequence', 'n'), ('second', 'a'), ('level', 'n'), ('instruction', 'n'), ('sequences', 'n'), ('creating', 'v'), ('controller', 'n'), ('threads', 'n'), ('second', 'a'), ('level', 'n'), ('instruction', 'n'), ('sequences', 'n'), ('allocating', 'v'), ('controller', 'n'), ('independent', 'a'), ('register', 'n'), ('well', 'r'), ('configuring', 'v'), ('independent', 'a'), ('addressing', 'v'), ('function', 'n'), ('thread', 'n'), ('threads', 'n'), ('wherein', 'v'), ('integer', 'a'), ('greater', 'a'), ('equal', 'a'), ('obtaining', 'v'), ('group', 'n'), ('controller', 'n'), ('computation', 'n'), ('types', 'n'), ('second', 'a'), ('level', 'n'), ('instruction', 'n'), ('sequences', 'n'), ('obtaining', 'v'), ('corresponding', 'v'), ('fusion', 'n'), ('computation', 'n'), ('manner', 'n'), ('computation', 'n'), ('types', 'n'), ('according', 'v'), ('computation', 'n'), ('types', 'n'), ('adopting', 'v'), ('computing', 'v'), ('units', 'n'), ('fusion', 'n'), ('computation', 'n'), ('manner', 'n'), ('call', 'n'), ('threads', 'n'), ('performing', 'v'), ('computations', 'n'), ('second', 'a'), ('level', 'n'), ('instruction', 'n'), ('sequences', 'n'), ('obtain', 'v'), ('final', 'a'), ('result', 'n'), ('wherein', 'n'), ('obtaining', 'v'), ('group', 'n'), ('controller', 'n'), ('computation', 'n'), ('types', 'n'), ('second', 'a'), ('level', 'n'), ('instruction', 'n'), ('sequences', 'n'), ('obtaining', 'v'), ('corresponding', 'v'), ('fusion', 'n'), ('computation', 'n'), ('manner', 'n'), ('computation', 'n'), ('types', 'n'), ('according', 'v'), ('computation', 'n'), ('types', 'n'), ('adopting', 'v'), ('computing', 'v'), ('units', 'n'), ('fusion', 'n'), ('computation', 'n'), ('manner', 'n'), ('call', 'n'), ('threads', 'n'), ('performing', 'v'), ('computations', 'n'), ('second', 'a'), ('instruction', 'n'), ('sequences', 'n'), ('obtain', 'v'), ('final', 'a'), ('result', 'n'), ('computation', 'n'), ('types', 'n'), ('represent', 'a'), ('computation', 'n'), ('operations', 'n'), ('type', 'v'), ('group', 'n'), ('controller', 'n'), ('calls', 'v'), ('combined', 'v'), ('computation', 'n'), ('manner', 'n'), ('single', 'a'), ('instruction', 'n'), ('multiple', 'n'), ('data', 'n'), ('type', 'n'), ('combination', 'n'), ('single', 'a'), ('instruction', 'n'), ('multiple', 'a'), ('threads', 'n'), ('uses', 'v'), ('threads', 'n'), ('perform', 'n'), ('combined', 'v'), ('computation', 'n'), ('manner', 'n'), ('obtain', 'v'), ('final', 'a'), ('result', 'n'), ('includes', 'v'), ('partitioning', 'v'), ('decoding', 'v'), ('logic', 'a'), ('threads', 'n'), ('n', 'r'), ('wraps', 'v'), ('allocating', 'v'), ('computing', 'v'), ('units', 'n'), ('converting', 'v'), ('group', 'n'), ('controller', 'n'), ('second', 'a'), ('instruction', 'n'), ('sequences', 'n'), ('second', 'a'), ('control', 'n'), ('signals', 'n'), ('sending', 'v'), ('second', 'a'), ('control', 'n'), ('signals', 'n'), ('computing', 'v'), ('units', 'n'), ('calling', 'v'), ('computing', 'v'), ('units', 'n'), ('wraps', 'n'), ('allocated', 'v'), ('computing', 'v'), ('units', 'n'), ('second', 'a'), ('control', 'n'), ('signals', 'n'), ('fetch', 'v'), ('corresponding', 'v'), ('data', 'n'), ('according', 'v'), ('independent', 'a'), ('addressing', 'v'), ('function', 'n'), ('performing', 'v'), ('computing', 'v'), ('units', 'n'), ('computations', 'n'), ('data', 'n'), ('obtain', 'v'), ('intermediate', 'a'), ('results', 'n'), ('splicing', 'v'), ('intermediate', 'a'), ('results', 'n'), ('obtain', 'v'), ('final', 'a'), ('result', 'n'), ('wherein', 'n'), ('obtaining', 'v'), ('group', 'n'), ('controller', 'n'), ('computation', 'n'), ('types', 'n'), ('second', 'a'), ('level', 'n'), ('instruction', 'n'), ('sequences', 'n'), ('obtaining', 'v'), ('corresponding', 'v'), ('fusion', 'n'), ('computation', 'n'), ('manner', 'n'), ('computation', 'n'), ('types', 'n'), ('according', 'v'), ('computation', 'n'), ('types', 'n'), ('adopting', 'v'), ('computing', 'v'), ('units', 'n'), ('fusion', 'n'), ('computation', 'n'), ('manner', 'n'), ('call', 'n'), ('threads', 'n'), ('performing', 'v'), ('computations', 'n'), ('second', 'a'), ('instruction', 'n'), ('sequences', 'n'), ('obtain', 'v'), ('final', 'a'), ('result', 'n'), ('computation', 'n'), ('types', 'n'), ('represent', 'a'), ('computation', 'n'), ('operations', 'n'), ('different', 'a'), ('types', 'n'), ('group', 'n'), ('controller', 'n'), ('calls', 'v'), ('simultaneous', 'a'), ('multi-threading', 'a'), ('threads', 'n'), ('perform', 'v'), ('computations', 'n'), ('obtain', 'v'), ('final', 'a'), ('result', 'n'), ('includes', 'v'), ('partitioning', 'v'), ('decoding', 'v'), ('logic', 'a'), ('threads', 'n'), ('n', 'r'), ('wraps', 'v'), ('converting', 'v'), ('second', 'a'), ('instruction', 'n'), ('sequences', 'n'), ('second', 'a'), ('control', 'n'), ('signals', 'n'), ('obtaining', 'v'), ('group', 'n'), ('controller', 'n'), ('computation', 'n'), ('types', 'n'), ('supported', 'v'), ('computing', 'v'), ('units', 'n'), ('allocating', 'v'), ('controller', 'n'), ('n', 'a'), ('wraps', 'n'), ('second', 'a'), ('control', 'n'), ('signals', 'n'), ('corresponding', 'v'), ('computing', 'v'), ('units', 'n'), ('support', 'n'), ('computation', 'n'), ('types', 'n'), ('wraps', 'v'), ('second', 'a'), ('control', 'n'), ('signals', 'n'), ('calling', 'v'), ('computing', 'v'), ('units', 'n'), ('wraps', 'n'), ('allocated', 'v'), ('computing', 'v'), ('units', 'n'), ('second', 'a'), ('control', 'n'), ('signals', 'n'), ('fetching', 'v'), ('computing', 'v'), ('units', 'n'), ('corresponding', 'v'), ('data', 'n'), ('performing', 'v'), ('computing', 'v'), ('units', 'n'), ('computations', 'n'), ('data', 'n'), ('obtain', 'v'), ('intermediate', 'a'), ('results', 'n'), ('splicing', 'v'), ('intermediate', 'a'), ('results', 'n'), ('obtain', 'v'), ('final', 'a'), ('result', 'n'), ('comprising', 'v'), ('wrap', 'n'), ('wraps', 'n'), ('blocked', 'v'), ('adding', 'v'), ('wrap', 'n'), ('waiting', 'v'), ('queue', 'n'), ('data', 'n'), ('wrap', 'n'), ('already', 'r'), ('fetched', 'v'), ('adding', 'v'), ('wrap', 'n'), ('preparation', 'n'), ('queue', 'n'), ('wherein', None), ('preparation', 'n'), ('queue', 'n'), ('queue', 'n'), ('wrap', 'n'), ('scheduled', 'v'), ('executing', 'v'), ('located', 'v'), ('computing', 'v'), ('resource', 'n'), ('idle', 'a'), ('wherein', 'n'), ('first', 'r'), ('level', 'a'), ('instruction', 'n'), ('sequence', 'n'), ('includes', 'v'), ('long', 'a'), ('instruction', 'n'), ('second', 'a'), ('level', 'n'), ('instruction', 'n'), ('sequence', 'n'), ('includes', 'v'), ('instruction', 'n'), ('sequence', 'n'), ('wherein', 'n'), ('computing', 'v'), ('system', 'n'), ('includes', 'v'), ('tree', 'a'), ('module', 'n'), ('wherein', 'v'), ('tree', 'a'), ('module', 'n'), ('includes', 'v'), ('root', 'n'), ('port', 'n'), ('branch', 'n'), ('ports', 'n'), ('wherein', 'v'), ('root', 'a'), ('port', 'n'), ('tree', 'n'), ('module', 'n'), ('connected', 'v'), ('group', 'n'), ('controller', 'n'), ('branch', 'n'), ('ports', 'n'), ('tree', 'v'), ('module', 'n'), ('connected', 'v'), ('computing', 'v'), ('unit', 'n'), ('computing', 'v'), ('units', 'n'), ('respectively', 'r'), ('tree', 'v'), ('module', 'n'), ('configured', 'v'), ('forward', 'r'), ('data', 'n'), ('blocks', 'n'), ('wraps', 'v'), ('instruction', 'n'), ('sequences', 'n'), ('group', 'n'), ('controller', 'n'), ('computing', 'v'), ('units', 'n'), ('wherein', 'v'), ('tree', 'a'), ('module', 'n'), ('n-ary', 'a'), ('tree', 'n'), ('wherein', 'n'), ('n', 'r'), ('integer', 'r'), ('greater', 'a'), ('equal', 'a'), ('wherein', 'n'), ('computing', 'v'), ('system', 'n'), ('includes', 'v'), ('branch', 'n'), ('processing', 'v'), ('circuit', 'n'), ('wherein', 'n'), ('branch', 'n'), ('processing', 'v'), ('circuit', 'n'), ('connected', 'v'), ('group', 'n'), ('controller', 'n'), ('computing', 'v'), ('units', 'n'), ('branch', 'n'), ('processing', 'v'), ('circuit', 'n'), ('configured', 'v'), ('forward', 'r'), ('data', 'n'), ('wraps', 'n'), ('instruction', 'n'), ('sequences', 'n'), ('group', 'n'), ('controller', 'n'), ('computing', 'v'), ('units', 'n'), ('computing', 'v'), ('system', 'n'), ('comprising', 'v'), ('control', 'n'), ('unit', 'n'), ('computation', 'n'), ('group', 'n'), ('general', 'a'), ('storage', 'n'), ('unit', 'n'), ('wherein', 'v'), ('control', 'a'), ('unit', 'n'), ('includes', 'v'), ('first', 'a'), ('memory', 'n'), ('decoding', 'v'), ('logic', 'a'), ('controller', 'n'), ('computation', 'n'), ('group', 'n'), ('includes', 'v'), ('group', 'n'), ('controller', 'n'), ('computing', 'v'), ('units', 'n'), ('general', 'a'), ('storage', 'n'), ('unit', 'n'), ('configured', 'v'), ('store', 'n'), ('data', 'n'), ('controller', 'n'), ('configured', 'v'), ('receive', 'a'), ('first', 'a'), ('level', 'n'), ('instruction', 'n'), ('sequence', 'n'), ('control', 'n'), ('first', 'a'), ('memory', 'n'), ('decoding', 'v'), ('logic', 'a'), ('decoding', 'v'), ('logic', 'n'), ('configured', 'v'), ('partition', 'n'), ('first', 'r'), ('level', 'a'), ('instruction', 'n'), ('sequence', 'n'), ('second', 'a'), ('level', 'n'), ('instruction', 'n'), ('sequences', 'n'), ('controller', 'v'), ('configured', 'a'), ('create', 'n'), ('threads', 'n'), ('second', 'a'), ('level', 'n'), ('instruction', 'n'), ('sequences', 'n'), ('allocate', 'v'), ('independent', 'a'), ('register', 'n'), ('configure', 'n'), ('independent', 'a'), ('addressing', 'v'), ('function', 'n'), ('thread', 'n'), ('threads', 'n'), ('integer', 'v'), ('greater', 'a'), ('equal', 'a'), ('controller', 'n'), ('configured', 'v'), ('convert', 'a'), ('second', 'a'), ('instruction', 'n'), ('sequences', 'n'), ('control', 'n'), ('signals', 'n'), ('sending', 'v'), ('group', 'n'), ('controller', 'n'), ('group', 'n'), ('controller', 'n'), ('configured', 'v'), ('receive', 'a'), ('control', 'n'), ('signals', 'n'), ('obtain', 'v'), ('computational', 'a'), ('types', 'n'), ('control', 'n'), ('signals', 'n'), ('divide', 'v'), ('threads', 'n'), ('n', 'r'), ('wraps', 'v'), ('allocate', 'a'), ('n', 'a'), ('wraps', 'n'), ('control', 'n'), ('signals', 'n'), ('computing', 'v'), ('units', 'n'), ('according', 'v'), ('computational', 'a'), ('types', 'n'), ('computing', 'v'), ('units', 'n'), ('configured', 'v'), ('fetch', 'r'), ('data', 'n'), ('general', 'a'), ('storage', 'n'), ('unit', 'n'), ('allocated', 'v'), ('wraps', 'n'), ('control', 'n'), ('signals', 'n'), ('perform', 'v'), ('computations', 'n'), ('obtain', 'v'), ('intermediate', 'a'), ('result', 'n'), ('group', 'n'), ('controller', 'n'), ('configured', 'v'), ('splice', 'a'), ('intermediate', 'a'), ('results', 'n'), ('obtain', 'v'), ('final', 'a'), ('computation', 'n'), ('result', 'n'), ('computing', 'v'), ('system', 'n'), ('wherein', 'a'), ('computing', 'v'), ('units', 'n'), ('includes', 'v'), ('addition', 'n'), ('computing', 'v'), ('unit', 'n'), ('multiplication', 'n'), ('computing', 'v'), ('unit', 'n'), ('activation', 'n'), ('computing', 'v'), ('unit', 'n'), ('dedicated', 'v'), ('computing', 'v'), ('unit', 'n'), ('computing', 'v'), ('system', 'n'), ('wherein', 'n'), ('dedicated', 'v'), ('computing', 'v'), ('unit', 'n'), ('includes', 'v'), ('face', 'v'), ('computing', 'v'), ('unit', 'n'), ('graphics', 'n'), ('computing', 'v'), ('unit', 'n'), ('fingerprint', 'n'), ('computing', 'v'), ('unit', 'n'), ('neural', 'a'), ('network', 'n'), ('computing', 'v'), ('unit', 'n'), ('computing', 'v'), ('system', 'n'), ('wherein', 'a'), ('group', 'n'), ('controller', 'n'), ('configured', 'v'), ('computation', 'n'), ('types', 'n'), ('control', 'n'), ('signals', 'n'), ('graphics', 'n'), ('computations', 'n'), ('fingerprint', 'v'), ('identification', 'n'), ('face', 'n'), ('neural', 'a'), ('network', 'n'), ('operations', 'n'), ('allocate', 'v'), ('control', 'n'), ('signals', 'n'), ('face', 'v'), ('computing', 'v'), ('unit', 'n'), ('graphics', 'n'), ('computing', 'v'), ('unit', 'n'), ('fingerprint', 'n'), ('computing', 'v'), ('unit', 'n'), ('neural', 'a'), ('network', 'n'), ('computing', 'v'), ('unit', 'n'), ('respectively', 'r'), ('computing', 'v'), ('system', 'n'), ('wherein', 'v'), ('first', 'a'), ('level', 'n'), ('instruction', 'n'), ('sequence', 'n'), ('includes', 'v'), ('long', 'a'), ('instruction', 'n'), ('second', 'a'), ('level', 'n'), ('instruction', 'n'), ('sequence', 'n'), ('includes', 'v'), ('instruction', 'n'), ('sequence', 'n'), ('computing', 'v'), ('system', 'n'), ('comprising', 'v'), ('tree', 'a'), ('module', 'n'), ('wherein', 'v'), ('tree', 'a'), ('module', 'n'), ('includes', 'v'), ('root', 'n'), ('port', 'n'), ('branch', 'n'), ('ports', 'n'), ('wherein', 'v'), ('root', 'a'), ('port', 'n'), ('tree', 'n'), ('module', 'n'), ('connected', 'v'), ('group', 'n'), ('controller', 'n'), ('branch', 'n'), ('ports', 'n'), ('tree', 'v'), ('module', 'n'), ('connected', 'v'), ('computing', 'v'), ('unit', 'n'), ('computing', 'v'), ('units', 'n'), ('respectively', 'r'), ('tree', 'v'), ('module', 'n'), ('configured', 'v'), ('forward', 'r'), ('data', 'n'), ('blocks', 'n'), ('wraps', 'v'), ('instruction', 'n'), ('sequences', 'n'), ('group', 'n'), ('controller', 'n'), ('computing', 'v'), ('units', 'n'), ('computing', 'v'), ('system', 'n'), ('wherein', 'v'), ('tree', 'a'), ('module', 'n'), ('n-ary', 'a'), ('tree', 'n'), ('wherein', 'n'), ('n', 'r'), ('integer', 'r'), ('greater', 'a'), ('equal', 'a'), ('computing', 'n'), ('system', 'n'), ('wherein', 'v'), ('computing', 'v'), ('system', 'n'), ('includes', 'v'), ('branch', 'n'), ('processing', 'n'), ('circuit', 'n'), ('branch', 'n'), ('processing', 'v'), ('circuit', 'n'), ('connected', 'v'), ('group', 'n'), ('controller', 'n'), ('computing', 'v'), ('units', 'n'), ('branch', 'n'), ('processing', 'v'), ('circuit', 'n'), ('configured', 'v'), ('forward', 'r'), ('data', 'n'), ('wraps', 'n'), ('instruction', 'n'), ('sequences', 'n'), ('group', 'n'), ('controller', 'n'), ('computing', 'v'), ('units', 'n'), ('computer', 'n'), ('program', 'n'), ('product', 'n'), ('comprising', 'v'), ('non-instant', 'a'), ('computer', 'n'), ('readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('wherein', 'v'), ('computer', 'n'), ('program', 'n'), ('stored', 'v'), ('non-instant', 'a'), ('computer', 'n'), ('readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('computer', 'n'), ('program', 'n'), ('capable', 'a'), ('causing', 'v'), ('computer', 'n'), ('perform', 'n'), ('operations', 'n'), ('detecting', 'v'), ('body', 'n'), ('information', 'n'), ('one', None), ('passengers', 'n'), ('vehicle', 'n'), ('based', 'v'), ('humans', 'n'), (\"'\", None), ('status', 'n'), ('comprising', 'v'), ('steps', 'n'), ('least', 'a'), ('one', None), ('interior', 'a'), ('interior', 'a'), ('vehicle', 'n'), ('acquired', 'v'), ('passenger', 'a'), ('body', 'n'), ('information-detecting', 'a'), ('device', 'n'), ('performing', 'v'), ('process', 'n'), ('inputting', 'v'), ('interior', 'a'), ('face', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('face', 'n'), ('network', 'n'), ('detect', 'a'), ('passengers', 'n'), ('interior', 'v'), ('thus', 'r'), ('output', 'n'), ('multiple', 'a'), ('pieces', 'n'), ('passenger', 'n'), ('feature', 'n'), ('information', 'n'), ('corresponding', 'v'), ('detected', 'v'), ('ii', 'a'), ('process', 'n'), ('inputting', 'v'), ('interior', 'a'), ('body', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('body', 'n'), ('network', 'n'), ('detect', 'a'), ('bodies', 'n'), ('passengers', 'n'), ('interior', 'a'), ('thus', 'r'), ('output', 'n'), ('body-part', 'a'), ('length', 'n'), ('information', 'n'), ('detected', 'v'), ('bodies', 'n'), ('b', None), ('passenger', 'n'), ('body', 'n'), ('information-detecting', 'a'), ('device', 'n'), ('performing', 'v'), ('process', 'n'), ('retrieving', 'v'), ('specific', 'a'), ('height', 'a'), ('mapping', 'n'), ('information', 'n'), ('corresponding', 'v'), ('specific', 'a'), ('passenger', 'n'), ('feature', 'n'), ('information', 'n'), ('specific', 'a'), ('passenger', 'n'), ('height', 'v'), ('mapping', 'v'), ('table', 'n'), ('stores', 'n'), ('height', 'v'), ('mapping', 'v'), ('information', 'n'), ('representing', 'v'), ('respective', 'a'), ('one', None), ('predetermined', 'v'), ('ratios', 'n'), ('one', None), ('segment', 'n'), ('body', 'n'), ('portions', 'n'), ('human', 'a'), ('groups', 'n'), ('heights', 'n'), ('per', None), ('human', 'a'), ('groups', 'n'), ('process', 'n'), ('acquiring', 'v'), ('specific', 'a'), ('height', 'n'), ('specific', 'a'), ('passenger', 'n'), ('specific', 'a'), ('height', 'v'), ('mapping', 'v'), ('information', 'n'), ('referring', 'v'), ('specific', 'a'), ('body-part', 'a'), ('length', 'n'), ('information', 'n'), ('specific', 'a'), ('passenger', 'n'), ('process', 'n'), ('retrieving', 'v'), ('specific', 'a'), ('weight', 'n'), ('mapping', 'v'), ('information', 'n'), ('corresponding', 'v'), ('specific', 'a'), ('passenger', 'n'), ('feature', 'n'), ('information', 'n'), ('weight', 'v'), ('mapping', 'v'), ('table', 'n'), ('stores', 'n'), ('multiple', 'a'), ('pieces', 'n'), ('weight', 'v'), ('mapping', 'v'), ('information', 'n'), ('representing', 'v'), ('predetermined', 'v'), ('correlations', 'n'), ('heights', 'n'), ('weights', 'n'), ('per', None), ('human', 'a'), ('groups', 'n'), ('process', 'n'), ('acquiring', 'v'), ('weight', 'n'), ('specific', 'a'), ('passenger', 'n'), ('specific', 'a'), ('weight', 'v'), ('mapping', 'v'), ('information', 'n'), ('referring', 'v'), ('specific', 'a'), ('height', 'n'), ('specific', 'a'), ('passenger', 'n'), ('wherein', 'a'), ('step', 'n'), ('passenger', 'n'), ('body', 'n'), ('information-detecting', 'a'), ('device', 'n'), ('performs', 'n'), ('process', 'n'), ('inputting', 'v'), ('interior', 'a'), ('body', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('body', 'n'), ('network', 'n'), ('output', 'n'), ('one', None), ('one', None), ('channels', 'n'), ('corresponding', 'v'), ('interior', 'a'), ('via', None), ('feature', 'n'), ('extraction', 'n'), ('network', 'n'), ('ii', 'a'), ('generate', 'n'), ('least', 'a'), ('one', None), ('keypoint', 'n'), ('heatmap', 'n'), ('least', 'a'), ('one', None), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('one', None), ('channels', 'n'), ('corresponding', 'v'), ('via', None), ('keypoint', 'n'), ('heatmap', 'n'), ('&', None), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('extractor', 'n'), ('iii', 'a'), ('extract', 'a'), ('keypoints', 'n'), ('keypoint', 'v'), ('heatmap', 'n'), ('via', None), ('keypoint', 'n'), ('detector', 'n'), ('group', 'n'), ('extracted', 'v'), ('keypoints', 'n'), ('referring', 'v'), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('thus', 'r'), ('generate', 'v'), ('body', 'n'), ('parts', 'n'), ('per', None), ('passengers', 'n'), ('result', 'v'), ('allow', None), ('body', 'n'), ('network', 'n'), ('output', 'n'), ('multiple', 'a'), ('pieces', 'n'), ('body-part', 'a'), ('length', 'n'), ('information', 'n'), ('passengers', 'n'), ('referring', 'v'), ('body', 'n'), ('parts', 'n'), ('per', None), ('passengers', 'n'), ('wherein', 'v'), ('feature', 'n'), ('extraction', 'n'), ('network', 'n'), ('includes', 'v'), ('least', 'a'), ('one', None), ('convolutional', 'a'), ('layer', 'n'), ('applies', 'n'), ('least', 'v'), ('one', None), ('convolution', 'n'), ('operation', 'n'), ('interior', 'a'), ('thereby', 'r'), ('output', 'n'), ('wherein', 'n'), ('keypoint', 'v'), ('heatmap', 'n'), ('&', None), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('extractor', 'n'), ('includes', 'v'), ('one', None), ('fully', 'r'), ('convolutional', 'a'), ('network', 'n'), ('convolutional', 'n'), ('layer', 'n'), ('applies', 'v'), ('fully-convolution', 'n'), ('operation', 'n'), ('convolution', 'n'), ('operation', 'n'), ('thereby', 'r'), ('generate', 'a'), ('keypoint', 'n'), ('heatmap', 'n'), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('wherein', 'v'), ('keypoint', 'a'), ('detector', 'n'), ('connects', 'n'), ('referring', 'v'), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('pairs', 'v'), ('respectively', 'r'), ('highest', 'a'), ('mutual', 'a'), ('connection', 'n'), ('probabilities', 'n'), ('connected', 'v'), ('among', None), ('extracted', 'a'), ('keypoints', 'n'), ('thereby', 'r'), ('group', 'n'), ('extracted', 'v'), ('keypoints', 'n'), ('wherein', 'a'), ('feature', 'n'), ('extraction', 'n'), ('network', 'n'), ('keypoint', 'n'), ('heatmap', 'n'), ('&', None), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('extractor', 'n'), ('learned', 'v'), ('learning', 'a'), ('device', 'n'), ('performing', 'v'), ('process', 'n'), ('inputting', 'v'), ('least', 'a'), ('one', None), ('training', 'n'), ('including', 'v'), ('one', None), ('objects', 'v'), ('training', 'v'), ('feature', 'n'), ('extraction', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'a'), ('feature', 'n'), ('extraction', 'n'), ('network', 'n'), ('generate', 'v'), ('one', None), ('training', 'v'), ('one', None), ('channels', 'n'), ('applying', 'v'), ('least', 'a'), ('one', None), ('convolutional', 'a'), ('operation', 'n'), ('training', 'v'), ('ii', 'a'), ('process', 'n'), ('inputting', 'v'), ('training', 'v'), ('keypoint', 'n'), ('heatmap', 'n'), ('&', None), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('extractor', 'n'), ('thereby', 'r'), ('allow', 'a'), ('keypoint', 'n'), ('heatmap', 'n'), ('&', None), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('extractor', 'n'), ('generate', 'v'), ('one', None), ('keypoint', 'n'), ('heatmaps', 'v'), ('training', 'v'), ('one', None), ('part', 'n'), ('affinity', 'n'), ('fields', 'n'), ('training', 'v'), ('one', None), ('channels', 'n'), ('training', 'v'), ('iii', 'a'), ('process', 'n'), ('inputting', 'v'), ('keypoint', 'n'), ('heatmaps', 'n'), ('training', 'v'), ('part', 'n'), ('affinity', 'n'), ('fields', 'n'), ('training', 'v'), ('keypoint', 'n'), ('detector', 'n'), ('thereby', 'r'), ('allow', 'a'), ('keypoint', 'n'), ('detector', 'n'), ('extract', 'a'), ('keypoints', 'n'), ('training', 'v'), ('keypoint', 'n'), ('heatmaps', 'n'), ('training', 'v'), ('process', 'n'), ('grouping', 'n'), ('extracted', 'v'), ('keypoints', 'n'), ('training', 'v'), ('referring', 'v'), ('part', 'n'), ('affinity', 'n'), ('fields', 'n'), ('training', 'v'), ('thereby', 'r'), ('detect', 'a'), ('keypoints', 'n'), ('per', None), ('objects', 'n'), ('training', 'v'), ('iv', 'a'), ('process', 'n'), ('allowing', 'v'), ('loss', 'n'), ('layer', 'n'), ('calculate', 'v'), ('one', None), ('losses', 'n'), ('referring', 'v'), ('keypoints', 'n'), ('per', None), ('objects', 'n'), ('training', 'v'), ('corresponding', 'v'), ('ground', 'n'), ('truths', 'n'), ('thereby', 'r'), ('adjust', 'v'), ('one', None), ('parameters', 'n'), ('feature', 'v'), ('extraction', 'n'), ('network', 'n'), ('keypoint', 'n'), ('heatmap', 'n'), ('&', None), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('extractor', 'n'), ('losses', 'n'), ('minimized', 'v'), ('backpropagation', 'n'), ('using', 'v'), ('losses', 'n'), ('wherein', 'a'), ('step', 'a'), ('passenger', 'n'), ('body', 'n'), ('information-detecting', 'a'), ('device', 'n'), ('performs', 'n'), ('process', 'n'), ('inputting', 'v'), ('interior', 'a'), ('face', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('face', 'n'), ('network', 'n'), ('detect', 'a'), ('passengers', 'n'), ('located', 'v'), ('interior', 'a'), ('via', None), ('face', 'n'), ('detector', 'n'), ('output', 'n'), ('multiple', 'a'), ('pieces', 'n'), ('passenger', 'n'), ('feature', 'n'), ('information', 'n'), ('facial', 'a'), ('via', None), ('facial', 'a'), ('feature', 'n'), ('classifier', 'n'), ('wherein', 'a'), ('step', 'n'), ('passenger', 'n'), ('body', 'n'), ('information-detecting', 'a'), ('device', 'n'), ('performs', 'n'), ('process', 'n'), ('inputting', 'v'), ('interior', 'a'), ('face', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('face', 'n'), ('network', 'n'), ('apply', 'r'), ('least', 'a'), ('one', None), ('convolution', 'n'), ('operation', 'n'), ('interior', 'a'), ('thus', 'r'), ('output', 'n'), ('least', 'a'), ('one', None), ('feature', 'n'), ('map', 'n'), ('corresponding', 'v'), ('interior', 'a'), ('via', None), ('least', 'a'), ('one', None), ('convolutional', 'a'), ('layer', 'n'), ('ii', 'n'), ('output', 'n'), ('one', None), ('proposal', 'n'), ('boxes', 'v'), ('passengers', 'n'), ('estimated', 'v'), ('located', 'a'), ('feature', 'n'), ('map', 'n'), ('via', None), ('region', 'n'), ('proposal', 'n'), ('network', 'n'), ('iii', 'v'), ('apply', 'v'), ('pooling', 'v'), ('operation', 'n'), ('one', None), ('regions', 'n'), ('corresponding', 'v'), ('proposal', 'n'), ('boxes', 'n'), ('feature', 'v'), ('map', 'a'), ('thus', 'r'), ('output', 'n'), ('least', 'a'), ('one', None), ('feature', 'n'), ('vector', 'n'), ('via', None), ('pooling', 'v'), ('layer', 'n'), ('iv', 'a'), ('apply', 'r'), ('fully-connected', 'a'), ('operation', 'n'), ('feature', 'n'), ('vector', 'n'), ('thus', 'r'), ('output', 'n'), ('multiple', 'a'), ('pieces', 'n'), ('passenger', 'n'), ('feature', 'n'), ('information', 'n'), ('corresponding', 'v'), ('passengers', 'n'), ('corresponding', 'v'), ('proposal', 'n'), ('boxes', 'n'), ('via', None), ('fully', 'r'), ('connected', 'v'), ('layer', 'n'), ('wherein', 'n'), ('multiple', 'a'), ('pieces', 'n'), ('passenger', 'n'), ('feature', 'n'), ('information', 'n'), ('include', 'v'), ('ages', 'v'), ('genders', 'n'), ('races', 'n'), ('corresponding', 'v'), ('passengers', 'n'), ('passenger', 'n'), ('body', 'n'), ('information-detecting', 'a'), ('device', 'n'), ('detecting', 'v'), ('body', 'n'), ('information', 'n'), ('one', None), ('passengers', 'n'), ('vehicle', 'n'), ('based', 'v'), ('humans', 'n'), (\"'\", None), ('status', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('memory', 'n'), ('stores', 'n'), ('instructions', 'n'), ('least', 'v'), ('one', None), ('configured', 'v'), ('execute', 'n'), ('instructions', 'n'), ('perform', 'v'), ('support', 'n'), ('another', None), ('device', 'n'), ('perform', 'n'), ('least', 'a'), ('one', None), ('interior', 'a'), ('interior', 'a'), ('vehicle', 'n'), ('acquired', 'v'), ('process', 'n'), ('inputting', 'v'), ('interior', 'a'), ('face', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('face', 'n'), ('network', 'n'), ('detect', 'a'), ('passengers', 'n'), ('interior', 'v'), ('thus', 'r'), ('output', 'n'), ('multiple', 'a'), ('pieces', 'n'), ('passenger', 'n'), ('feature', 'n'), ('information', 'n'), ('corresponding', 'v'), ('detected', 'v'), ('ii', 'a'), ('process', 'n'), ('inputting', 'v'), ('interior', 'a'), ('body', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('body', 'n'), ('network', 'n'), ('detect', 'a'), ('bodies', 'n'), ('passengers', 'n'), ('interior', 'a'), ('thus', 'r'), ('output', 'n'), ('body-part', 'a'), ('length', 'n'), ('information', 'n'), ('detected', 'v'), ('bodies', 'n'), ('ii', 'a'), ('process', 'n'), ('retrieving', 'v'), ('specific', 'a'), ('height', 'a'), ('mapping', 'n'), ('information', 'n'), ('corresponding', 'v'), ('specific', 'a'), ('passenger', 'n'), ('feature', 'n'), ('information', 'n'), ('specific', 'a'), ('passenger', 'n'), ('height', 'v'), ('mapping', 'v'), ('table', 'n'), ('stores', 'n'), ('height', 'v'), ('mapping', 'v'), ('information', 'n'), ('representing', 'v'), ('respective', 'a'), ('one', None), ('predetermined', 'v'), ('ratios', 'n'), ('one', None), ('segment', 'n'), ('body', 'n'), ('portions', 'n'), ('human', 'a'), ('groups', 'n'), ('heights', 'n'), ('per', None), ('human', 'a'), ('groups', 'n'), ('process', 'n'), ('acquiring', 'v'), ('specific', 'a'), ('height', 'n'), ('specific', 'a'), ('passenger', 'n'), ('specific', 'a'), ('height', 'v'), ('mapping', 'v'), ('information', 'n'), ('referring', 'v'), ('specific', 'a'), ('body-part', 'a'), ('length', 'n'), ('information', 'n'), ('specific', 'a'), ('passenger', 'n'), ('process', 'n'), ('retrieving', 'v'), ('specific', 'a'), ('weight', 'n'), ('mapping', 'v'), ('information', 'n'), ('corresponding', 'v'), ('specific', 'a'), ('passenger', 'n'), ('feature', 'n'), ('information', 'n'), ('weight', 'v'), ('mapping', 'v'), ('table', 'n'), ('stores', 'n'), ('multiple', 'a'), ('pieces', 'n'), ('weight', 'v'), ('mapping', 'v'), ('information', 'n'), ('representing', 'v'), ('predetermined', 'v'), ('correlations', 'n'), ('heights', 'n'), ('weights', 'n'), ('per', None), ('human', 'a'), ('groups', 'n'), ('process', 'n'), ('acquiring', 'v'), ('weight', 'n'), ('specific', 'a'), ('passenger', 'n'), ('specific', 'a'), ('weight', 'v'), ('mapping', 'v'), ('information', 'n'), ('referring', 'v'), ('specific', 'a'), ('height', 'n'), ('specific', 'a'), ('passenger', 'n'), ('passenger', 'n'), ('body', 'n'), ('information-detecting', 'a'), ('device', 'n'), ('wherein', 'n'), ('process', 'n'), ('performs', 'n'), ('process', 'n'), ('inputting', 'v'), ('interior', 'a'), ('body', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('body', 'n'), ('network', 'n'), ('output', 'n'), ('one', None), ('one', None), ('channels', 'n'), ('corresponding', 'v'), ('interior', 'a'), ('via', None), ('feature', 'n'), ('extraction', 'n'), ('network', 'n'), ('ii', 'a'), ('generate', 'n'), ('least', 'a'), ('one', None), ('keypoint', 'n'), ('heatmap', 'n'), ('least', 'a'), ('one', None), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('one', None), ('channels', 'n'), ('corresponding', 'v'), ('via', None), ('keypoint', 'n'), ('heatmap', 'n'), ('&', None), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('extractor', 'n'), ('iii', 'a'), ('extract', 'a'), ('keypoints', 'n'), ('keypoint', 'v'), ('heatmap', 'n'), ('via', None), ('keypoint', 'n'), ('detector', 'n'), ('group', 'n'), ('extracted', 'v'), ('keypoints', 'n'), ('referring', 'v'), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('thus', 'r'), ('generate', 'v'), ('body', 'n'), ('parts', 'n'), ('per', None), ('passengers', 'n'), ('result', 'v'), ('allow', None), ('body', 'n'), ('network', 'n'), ('output', 'n'), ('multiple', 'a'), ('pieces', 'n'), ('body-part', 'a'), ('length', 'n'), ('information', 'n'), ('passengers', 'n'), ('referring', 'v'), ('body', 'n'), ('parts', 'n'), ('per', None), ('passengers', 'n'), ('passenger', 'v'), ('body', 'n'), ('information-detecting', 'a'), ('device', 'n'), ('wherein', 'n'), ('keypoint', 'n'), ('heatmap', 'n'), ('&', None), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('extractor', 'n'), ('includes', 'v'), ('one', None), ('fully', 'r'), ('convolutional', 'a'), ('network', 'n'), ('convolutional', 'n'), ('layer', 'n'), ('applies', 'v'), ('fully-convolution', 'n'), ('operation', 'n'), ('convolution', 'n'), ('operation', 'n'), ('thereby', 'r'), ('generate', 'a'), ('keypoint', 'n'), ('heatmap', 'n'), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('passenger', 'n'), ('body', 'n'), ('information-detecting', 'a'), ('device', 'n'), ('wherein', 'n'), ('keypoint', 'n'), ('detector', 'n'), ('connects', 'n'), ('referring', 'v'), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('pairs', 'v'), ('respectively', 'r'), ('highest', 'a'), ('mutual', 'a'), ('connection', 'n'), ('probabilities', 'n'), ('connected', 'v'), ('among', None), ('extracted', 'a'), ('keypoints', 'n'), ('thereby', 'r'), ('group', 'n'), ('extracted', 'v'), ('keypoints', 'n'), ('passenger', 'n'), ('body', 'n'), ('information-detecting', 'a'), ('device', 'n'), ('wherein', 'n'), ('feature', 'n'), ('extraction', 'n'), ('network', 'n'), ('keypoint', 'n'), ('heatmap', 'n'), ('&', None), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('extractor', 'n'), ('learned', 'v'), ('learning', 'a'), ('device', 'n'), ('performing', 'v'), ('process', 'n'), ('inputting', 'v'), ('least', 'a'), ('one', None), ('training', 'n'), ('including', 'v'), ('one', None), ('objects', 'v'), ('training', 'v'), ('feature', 'n'), ('extraction', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'a'), ('feature', 'n'), ('extraction', 'n'), ('network', 'n'), ('generate', 'v'), ('one', None), ('training', 'v'), ('one', None), ('channels', 'n'), ('applying', 'v'), ('least', 'a'), ('one', None), ('convolutional', 'a'), ('operation', 'n'), ('training', 'v'), ('ii', 'a'), ('process', 'n'), ('inputting', 'v'), ('training', 'v'), ('keypoint', 'n'), ('heatmap', 'n'), ('&', None), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('extractor', 'n'), ('thereby', 'r'), ('allow', 'a'), ('keypoint', 'n'), ('heatmap', 'n'), ('&', None), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('extractor', 'n'), ('generate', 'v'), ('one', None), ('keypoint', 'n'), ('heatmaps', 'v'), ('training', 'v'), ('one', None), ('part', 'n'), ('affinity', 'n'), ('fields', 'n'), ('training', 'v'), ('one', None), ('channels', 'n'), ('training', 'v'), ('iii', 'a'), ('process', 'n'), ('inputting', 'v'), ('keypoint', 'n'), ('heatmaps', 'n'), ('training', 'v'), ('part', 'n'), ('affinity', 'n'), ('fields', 'n'), ('training', 'v'), ('keypoint', 'n'), ('detector', 'n'), ('thereby', 'r'), ('allow', 'a'), ('keypoint', 'n'), ('detector', 'n'), ('extract', 'a'), ('keypoints', 'n'), ('training', 'v'), ('keypoint', 'n'), ('heatmaps', 'n'), ('training', 'v'), ('process', 'n'), ('grouping', 'n'), ('extracted', 'v'), ('keypoints', 'n'), ('training', 'v'), ('referring', 'v'), ('part', 'n'), ('affinity', 'n'), ('fields', 'n'), ('training', 'v'), ('thereby', 'r'), ('detect', 'a'), ('keypoints', 'n'), ('per', None), ('objects', 'n'), ('training', 'v'), ('iv', 'a'), ('process', 'n'), ('allowing', 'v'), ('loss', 'n'), ('layer', 'n'), ('calculate', 'v'), ('one', None), ('losses', 'n'), ('referring', 'v'), ('keypoints', 'n'), ('per', None), ('objects', 'n'), ('training', 'v'), ('corresponding', 'v'), ('ground', 'n'), ('truths', 'n'), ('thereby', 'r'), ('adjust', 'v'), ('one', None), ('parameters', 'n'), ('feature', 'v'), ('extraction', 'n'), ('network', 'n'), ('keypoint', 'n'), ('heatmap', 'n'), ('&', None), ('part', 'n'), ('affinity', 'n'), ('field', 'n'), ('extractor', 'n'), ('losses', 'n'), ('minimized', 'v'), ('backpropagation', 'n'), ('using', 'v'), ('losses', 'n'), ('passenger', 'n'), ('body', 'n'), ('information-detecting', 'a'), ('device', 'n'), ('wherein', 'n'), ('process', 'n'), ('performs', 'n'), ('process', 'n'), ('inputting', 'v'), ('interior', 'a'), ('face', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('face', 'n'), ('network', 'n'), ('apply', 'r'), ('least', 'a'), ('one', None), ('convolution', 'n'), ('operation', 'n'), ('interior', 'a'), ('thus', 'r'), ('output', 'n'), ('least', 'a'), ('one', None), ('feature', 'n'), ('map', 'n'), ('corresponding', 'v'), ('interior', 'a'), ('via', None), ('least', 'a'), ('one', None), ('convolutional', 'a'), ('layer', 'n'), ('ii', 'n'), ('output', 'n'), ('one', None), ('proposal', 'n'), ('boxes', 'v'), ('passengers', 'n'), ('estimated', 'v'), ('located', 'a'), ('feature', 'n'), ('map', 'n'), ('via', None), ('region', 'n'), ('proposal', 'n'), ('network', 'n'), ('iii', 'v'), ('apply', 'v'), ('pooling', 'v'), ('operation', 'n'), ('one', None), ('regions', 'n'), ('corresponding', 'v'), ('proposal', 'n'), ('boxes', 'n'), ('feature', 'v'), ('map', 'a'), ('thus', 'r'), ('output', 'n'), ('least', 'a'), ('one', None), ('feature', 'n'), ('vector', 'n'), ('via', None), ('pooling', 'v'), ('layer', 'n'), ('iv', 'a'), ('apply', 'r'), ('fully-connected', 'a'), ('operation', 'n'), ('feature', 'n'), ('vector', 'n'), ('thus', 'r'), ('output', 'n'), ('multiple', 'a'), ('pieces', 'n'), ('passenger', 'n'), ('feature', 'n'), ('information', 'n'), ('corresponding', 'v'), ('passengers', 'n'), ('corresponding', 'v'), ('proposal', 'n'), ('boxes', 'n'), ('via', None), ('fully', 'r'), ('connected', 'v'), ('layer', 'n'), ('computer', 'n'), ('implemented', 'v'), ('performing', 'v'), ('video', 'n'), ('coding', 'v'), ('based', 'v'), ('face', 'n'), ('detection', 'n'), ('comprising', 'v'), ('receiving', 'v'), ('video', 'n'), ('frame', 'n'), ('comprising', 'v'), ('one', None), ('video', 'n'), ('frames', 'v'), ('video', 'a'), ('sequence', 'n'), ('determining', 'v'), ('video', 'n'), ('frame', 'n'), ('key', 'a'), ('frame', 'n'), ('video', 'n'), ('sequence', 'n'), ('performing', 'v'), ('response', 'n'), ('video', 'n'), ('frame', 'n'), ('key', 'a'), ('frame', 'n'), ('video', 'a'), ('sequence', 'n'), ('multi-stage', 'n'), ('facial', 'a'), ('search', 'n'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('predetermined', 'a'), ('feature', 'n'), ('templates', 'n'), ('predetermined', 'v'), ('number', 'n'), ('stages', 'n'), ('determine', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('video', 'n'), ('frame', 'n'), ('testing', 'v'), ('first', 'a'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('regions', 'n'), ('based', 'v'), ('skin', 'a'), ('tone', 'n'), ('information', 'n'), ('determine', 'n'), ('first', 'r'), ('candidate', 'a'), ('face', 'n'), ('region', 'n'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('invalid', 'a'), ('face', 'n'), ('region', 'n'), ('rejecting', 'v'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('outputting', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('encoding', 'v'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('least', 'a'), ('part', 'n'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('generate', 'n'), ('coded', 'v'), ('bitstream', 'n'), ('wherein', 'n'), ('skin', 'v'), ('tone', 'n'), ('information', 'n'), ('comprises', 'v'), ('skin', 'a'), ('probability', 'n'), ('map', 'n'), ('wherein', 'n'), ('said', 'v'), ('testing', 'v'), ('first', 'a'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('regions', 'n'), ('based', 'v'), ('skin', 'a'), ('tone', 'n'), ('information', 'n'), ('performed', 'v'), ('response', 'n'), ('video', 'n'), ('frame', 'n'), ('key', 'a'), ('frame', 'n'), ('video', 'n'), ('sequence', 'n'), ('wherein', 'n'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('comprises', 'v'), ('rectangular', 'a'), ('region', 'n'), ('comprising', 'v'), ('determining', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('corresponding', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('wherein', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('least', 'a'), ('one', None), ('pixel', 'n'), ('accuracy', 'n'), ('small', 'a'), ('block', 'n'), ('pixels', 'n'), ('accuracy', 'v'), ('wherein', None), ('determining', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('comprises', 'v'), ('generating', 'v'), ('enhanced', 'v'), ('skip', 'a'), ('probability', 'n'), ('map', 'n'), ('corresponding', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('binarizing', 'v'), ('enhanced', 'v'), ('skip', 'a'), ('probability', 'n'), ('map', 'n'), ('overlaying', 'v'), ('binarized', 'v'), ('enhanced', 'a'), ('skip', 'n'), ('probability', 'n'), ('map', 'n'), ('least', 'a'), ('portion', 'n'), ('video', 'n'), ('frame', 'n'), ('provide', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('wherein', None), ('second', 'a'), ('video', 'n'), ('frame', 'n'), ('comprises', 'v'), ('non-key', 'a'), ('frame', 'n'), ('video', 'n'), ('sequence', 'n'), ('comprising', 'v'), ('performing', 'v'), ('face', 'n'), ('detection', 'n'), ('second', 'a'), ('video', 'n'), ('frame', 'n'), ('video', 'n'), ('sequence', 'n'), ('based', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('comprising', 'v'), ('second', 'a'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('second', 'a'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('video', 'n'), ('frame', 'n'), ('wherein', 'a'), ('second', 'a'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('comprises', 'v'), ('determining', 'v'), ('location', 'n'), ('second', 'a'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('second', 'a'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('displacement', 'a'), ('offset', 'n'), ('respect', 'n'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('comprising', 'v'), ('determining', 'v'), ('displacement', 'n'), ('offset', 'n'), ('based', 'v'), ('offset', 'v'), ('centroid', 'a'), ('bounding', 'v'), ('box', 'n'), ('around', None), ('skin', 'n'), ('enhanced', 'v'), ('region', 'n'), ('corresponding', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('second', 'a'), ('centroid', 'a'), ('second', 'a'), ('bounding', 'n'), ('box', 'n'), ('around', None), ('second', 'a'), ('skin', 'n'), ('enhanced', 'v'), ('region', 'n'), ('second', 'a'), ('video', 'n'), ('frame', 'n'), ('wherein', 'n'), ('encoding', 'v'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('least', 'a'), ('part', 'n'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('comprises', 'v'), ('least', 'a'), ('one', None), ('reducing', 'v'), ('quantization', 'n'), ('parameter', 'n'), ('corresponding', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('adjusting', 'v'), ('lambda', 'n'), ('value', 'n'), ('first', 'r'), ('candidate', 'a'), ('face', 'n'), ('region', 'n'), ('disabling', 'v'), ('skip', 'n'), ('coding', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('wherein', None), ('bitstream', 'n'), ('comprises', 'v'), ('least', 'a'), ('one', None), ('hadvanced', 'v'), ('video', 'n'), ('coding', 'v'), ('avc', 'a'), ('compliant', 'a'), ('bitstream', 'n'), ('hhigh', 'n'), ('efficiency', 'n'), ('video', 'n'), ('coding', 'v'), ('hevc', 'n'), ('compliant', 'a'), ('bitstream', 'n'), ('vp', 'n'), ('compliant', 'a'), ('bitstream', 'n'), ('vp', 'n'), ('compliant', 'a'), ('bitstream', 'n'), ('alliance', 'n'), ('open', 'a'), ('media', 'n'), ('aom', 'v'), ('av', 'a'), ('compliant', 'n'), ('bitstream', 'n'), ('computer', 'n'), ('implemented', 'v'), ('performing', 'v'), ('face', 'n'), ('detection', 'n'), ('comprising', 'v'), ('receiving', 'v'), ('video', 'n'), ('frame', 'n'), ('sequence', 'n'), ('video', 'n'), ('frames', 'n'), ('performing', 'v'), ('multi-stage', 'a'), ('facial', 'a'), ('search', 'n'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('predetermined', 'a'), ('feature', 'n'), ('templates', 'n'), ('predetermined', 'v'), ('number', 'n'), ('stages', 'n'), ('determine', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('video', 'n'), ('frame', 'n'), ('testing', 'v'), ('first', 'a'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('regions', 'n'), ('based', 'v'), ('skin', 'a'), ('tone', 'n'), ('information', 'n'), ('determine', 'n'), ('first', 'r'), ('candidate', 'a'), ('face', 'n'), ('region', 'n'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('invalid', 'a'), ('face', 'n'), ('region', 'n'), ('rejecting', 'v'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('outputting', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('processing', 'v'), ('providing', 'v'), ('index', 'n'), ('indicative', 'a'), ('person', 'n'), ('present', 'a'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('wherein', 'v'), ('sequence', 'n'), ('video', 'n'), ('frames', 'v'), ('comprises', 'n'), ('sequence', 'n'), ('surveillance', 'n'), ('video', 'n'), ('frames', 'n'), ('comprising', 'v'), ('performing', 'v'), ('face', 'n'), ('surveillance', 'n'), ('video', 'n'), ('frames', 'n'), ('based', 'v'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('wherein', 'v'), ('sequence', 'n'), ('video', 'n'), ('frames', 'v'), ('comprises', 'n'), ('sequence', 'n'), ('decoded', 'v'), ('video', 'n'), ('frames', 'n'), ('comprising', 'v'), ('adding', 'v'), ('marker', 'n'), ('corresponding', 'v'), ('received', 'v'), ('video', 'a'), ('frame', 'n'), ('perform', 'n'), ('face', 'n'), ('received', 'v'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('wherein', 'v'), ('sequence', 'n'), ('video', 'n'), ('frames', 'n'), ('received', 'v'), ('device', 'n'), ('login', 'n'), ('attempt', 'n'), ('comprising', 'v'), ('performing', 'v'), ('face', 'n'), ('based', 'v'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('allowing', 'v'), ('access', 'n'), ('device', 'n'), ('secured', 'v'), ('face', 'n'), ('recognized', 'v'), ('wherein', 'a'), ('sequence', 'n'), ('video', 'n'), ('frames', 'v'), ('comprises', 'n'), ('sequence', 'n'), ('videoconferencing', 'v'), ('frames', 'n'), ('comprising', 'v'), ('encoding', 'v'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('least', 'a'), ('part', 'n'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('generate', 'n'), ('coded', 'v'), ('bitstream', 'n'), ('wherein', 'n'), ('encoding', 'v'), ('video', 'n'), ('frame', 'n'), ('comprises', 'v'), ('encoding', 'v'), ('background', 'r'), ('region', 'n'), ('video', 'n'), ('frame', 'n'), ('bitstream', 'n'), ('comprising', 'v'), ('encoding', 'v'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('least', 'a'), ('part', 'n'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('generate', 'n'), ('coded', 'v'), ('bitstream', 'n'), ('wherein', 'n'), ('encoding', 'v'), ('video', 'n'), ('frame', 'n'), ('comprises', 'v'), ('including', 'v'), ('metadata', 'n'), ('corresponding', 'v'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('bitstream', 'n'), ('comprising', 'v'), ('decoding', 'v'), ('coded', 'v'), ('bitstream', 'n'), ('generate', 'n'), ('decoded', 'v'), ('video', 'n'), ('frame', 'n'), ('determine', 'n'), ('metadata', 'n'), ('corresponding', 'v'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('bitstream', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('replacing', 'v'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('based', 'v'), ('decoded', 'v'), ('metadata', 'n'), ('cropping', 'v'), ('displaying', 'v'), ('data', 'n'), ('corresponding', 'v'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('based', 'v'), ('decoded', 'v'), ('metadata', 'n'), ('indexing', 'v'), ('decoded', 'v'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('decoded', 'v'), ('metadata', 'n'), ('system', 'n'), ('performing', 'v'), ('video', 'n'), ('coding', 'v'), ('based', 'v'), ('face', 'n'), ('detection', 'n'), ('comprising', 'v'), ('memory', 'n'), ('configured', 'v'), ('store', 'n'), ('video', 'n'), ('frame', 'n'), ('comprising', 'v'), ('one', None), ('video', 'n'), ('frames', 'v'), ('video', 'a'), ('sequence', 'n'), ('coupled', 'v'), ('memory', 'n'), ('receive', 'n'), ('video', 'n'), ('frame', 'n'), ('determine', 'a'), ('video', 'n'), ('frame', 'n'), ('key', 'a'), ('frame', 'n'), ('video', 'n'), ('sequence', 'n'), ('perform', 'n'), ('response', 'n'), ('video', 'n'), ('frame', 'n'), ('key', 'a'), ('frame', 'n'), ('video', 'a'), ('sequence', 'n'), ('multi-stage', 'n'), ('facial', 'a'), ('search', 'n'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('predetermined', 'a'), ('feature', 'n'), ('templates', 'n'), ('predetermined', 'v'), ('number', 'n'), ('stages', 'n'), ('determine', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('video', 'n'), ('frame', 'n'), ('test', 'n'), ('first', 'r'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('regions', 'n'), ('based', 'v'), ('skin', 'a'), ('tone', 'n'), ('information', 'n'), ('determine', 'n'), ('first', 'r'), ('candidate', 'a'), ('face', 'n'), ('region', 'n'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('invalid', 'a'), ('face', 'n'), ('region', 'n'), ('reject', 'a'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('outputting', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('encode', None), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('least', 'a'), ('part', 'n'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('generate', 'n'), ('coded', 'v'), ('bitstream', 'n'), ('system', 'n'), ('wherein', 'a'), ('skin', 'v'), ('tone', 'n'), ('information', 'n'), ('comprises', 'v'), ('skin', 'a'), ('probability', 'n'), ('map', 'n'), ('system', 'n'), ('wherein', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('comprises', 'v'), ('rectangular', 'a'), ('region', 'n'), ('determine', 'n'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('corresponding', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('wherein', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('least', 'a'), ('one', None), ('pixel', 'n'), ('accuracy', 'n'), ('small', 'a'), ('block', 'n'), ('pixels', 'n'), ('accuracy', 'n'), ('system', 'n'), ('wherein', 'a'), ('determine', 'a'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('comprises', 'v'), ('generate', 'n'), ('enhanced', 'v'), ('skip', 'a'), ('probability', 'n'), ('map', 'n'), ('corresponding', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('binarize', 'n'), ('enhanced', 'v'), ('skip', 'a'), ('probability', 'n'), ('map', 'n'), ('overlay', 'n'), ('binarized', 'v'), ('enhanced', 'a'), ('skip', 'a'), ('probability', 'n'), ('map', 'n'), ('least', 'a'), ('portion', 'n'), ('video', 'n'), ('frame', 'n'), ('provide', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('system', 'n'), ('wherein', 'a'), ('second', 'a'), ('video', 'n'), ('frame', 'n'), ('comprises', 'v'), ('non-key', 'a'), ('frame', 'n'), ('video', 'n'), ('sequence', 'n'), ('perform', 'n'), ('face', 'n'), ('detection', 'n'), ('second', 'a'), ('video', 'n'), ('frame', 'n'), ('video', 'n'), ('sequence', 'n'), ('based', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('system', 'n'), ('wherein', 'a'), ('track', 'a'), ('second', 'a'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('second', 'a'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('video', 'n'), ('frame', 'n'), ('system', 'n'), ('wherein', 'a'), ('encode', 'n'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('least', 'a'), ('part', 'n'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('comprises', 'v'), ('reduce', 'v'), ('quantization', 'n'), ('parameter', 'n'), ('corresponding', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('adjust', 'v'), ('lambda', 'n'), ('value', 'n'), ('first', 'r'), ('candidate', 'a'), ('face', 'n'), ('region', 'n'), ('disable', 'a'), ('skip', 'n'), ('coding', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('least', 'a'), ('one', None), ('non-transitory', 'a'), ('machine', 'n'), ('readable', 'a'), ('medium', 'n'), ('comprising', 'v'), ('instructions', 'n'), ('response', 'n'), ('executed', 'v'), ('device', 'n'), ('cause', 'n'), ('device', 'n'), ('perform', 'n'), ('video', 'n'), ('coding', 'v'), ('based', 'v'), ('face', 'n'), ('detection', 'n'), ('receiving', 'v'), ('video', 'n'), ('frame', 'n'), ('comprising', 'v'), ('one', None), ('video', 'n'), ('frames', 'v'), ('video', 'a'), ('sequence', 'n'), ('determining', 'v'), ('video', 'n'), ('frame', 'n'), ('key', 'a'), ('frame', 'n'), ('video', 'n'), ('sequence', 'n'), ('performing', 'v'), ('response', 'n'), ('video', 'n'), ('frame', 'n'), ('key', 'a'), ('frame', 'n'), ('video', 'a'), ('sequence', 'n'), ('multi-stage', 'n'), ('facial', 'a'), ('search', 'n'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('predetermined', 'a'), ('feature', 'n'), ('templates', 'n'), ('predetermined', 'v'), ('number', 'n'), ('stages', 'n'), ('determine', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('video', 'n'), ('frame', 'n'), ('testing', 'v'), ('first', 'a'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('regions', 'n'), ('based', 'v'), ('skin', 'a'), ('tone', 'n'), ('information', 'n'), ('determine', 'n'), ('first', 'r'), ('candidate', 'a'), ('face', 'n'), ('region', 'n'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('invalid', 'a'), ('face', 'n'), ('region', 'n'), ('rejecting', 'v'), ('second', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('outputting', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('encoding', 'v'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('least', 'a'), ('part', 'n'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('generate', 'n'), ('coded', 'v'), ('bitstream', 'a'), ('non-transitory', 'a'), ('machine', 'n'), ('readable', 'a'), ('medium', 'n'), ('wherein', 'n'), ('skin', 'v'), ('tone', 'n'), ('information', 'n'), ('comprises', 'v'), ('skin', 'a'), ('probability', 'n'), ('map', 'v'), ('non-transitory', 'a'), ('machine', 'n'), ('readable', 'a'), ('medium', 'n'), ('wherein', 'n'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('comprises', 'v'), ('rectangular', 'a'), ('region', 'n'), ('machine', 'n'), ('readable', 'a'), ('medium', 'n'), ('comprising', 'v'), ('instructions', 'n'), ('response', 'n'), ('executed', 'v'), ('device', 'n'), ('cause', 'n'), ('device', 'n'), ('perform', 'n'), ('video', 'n'), ('coding', 'v'), ('based', 'v'), ('face', 'n'), ('detection', 'n'), ('determining', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('corresponding', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('wherein', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('least', 'a'), ('one', None), ('pixel', 'n'), ('accuracy', 'n'), ('small', 'a'), ('block', 'n'), ('pixels', 'n'), ('accuracy', None), ('non-transitory', 'a'), ('machine', 'n'), ('readable', 'a'), ('medium', 'n'), ('wherein', 'n'), ('determining', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('comprises', 'v'), ('generating', 'v'), ('enhanced', 'v'), ('skip', 'a'), ('probability', 'n'), ('map', 'n'), ('corresponding', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('binarizing', 'v'), ('enhanced', 'v'), ('skip', 'a'), ('probability', 'n'), ('map', 'n'), ('overlaying', 'v'), ('binarized', 'v'), ('enhanced', 'a'), ('skip', 'n'), ('probability', 'n'), ('map', 'n'), ('least', 'a'), ('portion', 'n'), ('video', 'n'), ('frame', 'n'), ('provide', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('non-transitory', 'a'), ('machine', 'n'), ('readable', 'a'), ('medium', 'n'), ('wherein', 'n'), ('second', 'a'), ('video', 'n'), ('frame', 'n'), ('comprises', 'v'), ('non-key', 'a'), ('frame', 'n'), ('video', 'n'), ('sequence', 'n'), ('machine', 'n'), ('readable', 'a'), ('medium', 'n'), ('comprising', 'v'), ('instructions', 'n'), ('response', 'n'), ('executed', 'v'), ('device', 'n'), ('cause', 'n'), ('device', 'n'), ('perform', 'n'), ('video', 'n'), ('coding', 'v'), ('based', 'v'), ('face', 'n'), ('detection', 'n'), ('performing', 'v'), ('face', 'n'), ('detection', 'n'), ('second', 'a'), ('video', 'n'), ('frame', 'n'), ('video', 'n'), ('sequence', 'n'), ('based', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('non-transitory', 'a'), ('machine', 'n'), ('readable', 'a'), ('medium', 'n'), ('machine', 'n'), ('readable', 'a'), ('medium', 'n'), ('comprising', 'v'), ('instructions', 'n'), ('response', 'n'), ('executed', 'v'), ('device', 'n'), ('cause', 'n'), ('device', 'n'), ('perform', 'n'), ('video', 'n'), ('coding', 'v'), ('based', 'v'), ('face', 'n'), ('detection', 'n'), ('second', 'a'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('second', 'a'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('free', 'a'), ('form', 'n'), ('shape', 'n'), ('face', 'n'), ('region', 'n'), ('video', 'n'), ('frame', 'n'), ('non-transitory', 'a'), ('machine', 'n'), ('readable', 'a'), ('medium', 'n'), ('wherein', 'n'), ('encoding', 'v'), ('video', 'n'), ('frame', 'n'), ('based', 'v'), ('least', 'a'), ('part', 'n'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('valid', 'a'), ('face', 'n'), ('region', 'n'), ('comprises', 'v'), ('least', 'a'), ('one', None), ('reducing', 'v'), ('quantization', 'n'), ('parameter', 'n'), ('corresponding', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('adjusting', 'v'), ('lambda', 'n'), ('value', 'n'), ('first', 'r'), ('candidate', 'a'), ('face', 'n'), ('region', 'n'), ('disabling', 'v'), ('skip', 'n'), ('coding', 'v'), ('first', 'a'), ('candidate', 'n'), ('face', 'n'), ('region', 'n'), ('managing', 'v'), ('smart', 'a'), ('database', 'n'), ('stores', 'n'), ('facial', 'a'), ('face', 'n'), ('comprising', 'v'), ('steps', 'n'), ('managing', 'v'), ('device', 'n'), ('performing', 'v'), ('process', 'n'), ('counting', 'v'), ('one', None), ('specific', 'a'), ('facial', 'a'), ('corresponding', 'n'), ('least', 'a'), ('one', None), ('specific', 'a'), ('person', 'n'), ('stored', 'v'), ('smart', 'a'), ('database', 'n'), ('new', 'a'), ('facial', 'a'), ('face', 'n'), ('continuously', 'r'), ('stored', 'v'), ('process', 'n'), ('determining', 'v'), ('whether', None), ('first', 'a'), ('counted', 'v'), ('value', 'n'), ('representing', 'v'), ('count', 'n'), ('specific', 'a'), ('facial', 'a'), ('satisfies', 'n'), ('preset', 'v'), ('first', 'r'), ('set', 'v'), ('value', 'n'), ('b', 'n'), ('first', 'r'), ('counted', 'v'), ('value', 'n'), ('determined', 'v'), ('satisfying', 'v'), ('first', 'r'), ('set', 'v'), ('value', 'n'), ('managing', 'v'), ('device', 'n'), ('performing', 'v'), ('process', 'n'), ('inputting', 'v'), ('specific', 'a'), ('facial', 'a'), ('neural', 'a'), ('aggregation', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('neural', 'a'), ('aggregation', 'n'), ('network', 'n'), ('generate', 'v'), ('quality', 'n'), ('scores', 'n'), ('specific', 'a'), ('facial', 'a'), ('aggregation', 'n'), ('specific', 'a'), ('facial', 'a'), ('process', 'n'), ('sorting', 'v'), ('quality', 'n'), ('scores', 'n'), ('corresponding', 'v'), ('specific', 'a'), ('facial', 'a'), ('descending', 'n'), ('order', 'n'), ('quality', 'n'), ('scores', 'v'), ('process', 'n'), ('counting', 'n'), ('sorted', 'v'), ('specific', 'a'), ('facial', 'a'), ('descending', 'n'), ('order', 'n'), ('second', 'a'), ('counted', 'v'), ('value', 'n'), ('represents', 'v'), ('number', 'n'), ('counted', 'v'), ('part', 'n'), ('specific', 'a'), ('facial', 'a'), ('becomes', 'n'), ('equal', 'a'), ('preset', 'a'), ('second', 'n'), ('set', 'v'), ('value', 'n'), ('process', 'n'), ('deleting', 'v'), ('uncounted', 'a'), ('part', 'n'), ('specific', 'a'), ('facial', 'a'), ('smart', 'a'), ('database', 'n'), ('comprising', 'v'), ('step', 'n'), ('c', 'r'), ('managing', 'v'), ('device', 'n'), ('performing', 'v'), ('process', 'n'), ('generating', 'v'), ('least', 'a'), ('one', None), ('optimal', 'a'), ('feature', 'n'), ('weighted', 'v'), ('summation', 'n'), ('one', None), ('features', 'v'), ('specific', 'a'), ('facial', 'a'), ('using', 'v'), ('counted', 'a'), ('part', 'n'), ('quality', 'n'), ('scores', 'v'), ('process', 'a'), ('setting', 'v'), ('optimal', 'a'), ('feature', 'n'), ('representative', 'a'), ('face', 'n'), ('corresponding', 'v'), ('specific', 'a'), ('person', 'n'), ('wherein', 'a'), ('step', 'n'), ('b', 'n'), ('managing', 'v'), ('device', 'n'), ('performs', 'n'), ('process', 'n'), ('inputting', 'v'), ('specific', 'a'), ('facial', 'a'), ('cnn', 'n'), ('neural', 'a'), ('aggregation', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('cnn', 'n'), ('generate', 'n'), ('one', None), ('features', 'v'), ('corresponding', 'v'), ('specific', 'a'), ('facial', 'a'), ('process', 'n'), ('inputting', 'v'), ('least', 'a'), ('one', None), ('feature', 'n'), ('vector', 'n'), ('features', 'n'), ('embedded', 'v'), ('aggregation', 'n'), ('module', 'n'), ('including', 'v'), ('least', 'a'), ('two', None), ('attention', 'n'), ('blocks', 'n'), ('thereby', 'r'), ('allow', 'v'), ('aggregation', 'n'), ('module', 'n'), ('generate', 'v'), ('quality', 'n'), ('scores', 'n'), ('features', 'v'), ('wherein', 'a'), ('step', 'n'), ('b', 'n'), ('managing', 'v'), ('device', 'n'), ('performs', 'n'), ('process', 'n'), ('matching', 'v'), ('i-', 'a'), ('one', None), ('features', 'v'), ('corresponding', 'v'), ('specific', 'a'), ('facial', 'a'), ('stored', 'v'), ('smart', 'a'), ('database', 'n'), ('i-', 'a'), ('quality', 'n'), ('scores', 'n'), ('ii', 'v'), ('specific', 'a'), ('person', 'n'), ('process', 'n'), ('storing', 'v'), ('matched', 'v'), ('features', 'n'), ('matched', 'v'), ('quality', 'n'), ('scores', 'n'), ('smart', 'v'), ('database', 'n'), ('comprising', 'v'), ('step', 'n'), ('managing', 'v'), ('device', 'n'), ('performing', 'v'), ('one', None), ('process', 'n'), ('learning', 'v'), ('face', 'n'), ('system', 'n'), ('using', 'v'), ('specific', 'a'), ('facial', 'a'), ('corresponding', 'n'), ('specific', 'a'), ('person', 'n'), ('stored', 'v'), ('smart', 'a'), ('database', 'n'), ('ii', 'n'), ('process', 'n'), ('transmitting', 'v'), ('specific', 'a'), ('facial', 'a'), ('corresponding', 'n'), ('specific', 'a'), ('person', 'n'), ('learning', 'a'), ('device', 'n'), ('corresponding', 'v'), ('face', 'n'), ('system', 'n'), ('thereby', 'r'), ('allow', 'v'), ('learning', 'v'), ('device', 'n'), ('learn', None), ('face', 'n'), ('system', 'n'), ('using', 'v'), ('specific', 'a'), ('facial', 'a'), ('wherein', 'n'), ('neural', 'a'), ('aggregation', 'n'), ('network', 'n'), ('learned', 'v'), ('learning', 'a'), ('device', 'n'), ('repeating', 'v'), ('process', 'n'), ('inputting', 'v'), ('multiple', 'a'), ('facial', 'a'), ('training', 'n'), ('corresponding', 'v'), ('set', 'v'), ('single', 'a'), ('face', 'n'), ('video', 'n'), ('single', 'a'), ('face', 'n'), ('cnn', 'a'), ('neural', 'a'), ('aggregation', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('cnn', 'n'), ('generate', 'n'), ('one', None), ('features', 'v'), ('training', 'v'), ('applying', 'v'), ('least', 'a'), ('one', None), ('convolution', 'n'), ('operation', 'n'), ('facial', 'a'), ('training', 'n'), ('ii', 'n'), ('process', 'n'), ('inputting', 'v'), ('least', 'a'), ('one', None), ('feature', 'n'), ('vector', 'n'), ('training', 'n'), ('features', 'n'), ('training', 'v'), ('embedded', 'a'), ('aggregation', 'n'), ('module', 'n'), ('including', 'v'), ('least', 'a'), ('two', None), ('attention', 'n'), ('blocks', 'n'), ('neural', 'a'), ('aggregation', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'a'), ('aggregation', 'n'), ('module', 'n'), ('generate', 'v'), ('quality', 'n'), ('scores', 'n'), ('training', 'v'), ('features', 'n'), ('training', 'v'), ('aggregation', 'n'), ('features', 'n'), ('training', 'v'), ('using', 'v'), ('one', None), ('attention', 'n'), ('parameters', 'n'), ('learned', 'v'), ('previous', 'a'), ('iteration', 'n'), ('iii', 'n'), ('process', 'n'), ('outputting', 'v'), ('least', 'a'), ('one', None), ('optimal', 'a'), ('feature', 'n'), ('training', 'n'), ('weighted', 'a'), ('summation', 'n'), ('features', 'n'), ('training', 'v'), ('using', 'v'), ('quality', 'n'), ('scores', 'n'), ('training', 'v'), ('iv', 'a'), ('process', 'n'), ('updating', 'v'), ('attention', 'n'), ('parameters', 'n'), ('learned', 'v'), ('previous', 'a'), ('iteration', 'n'), ('least', 'a'), ('two', None), ('attention', 'n'), ('blocks', 'n'), ('one', None), ('losses', 'n'), ('minimized', 'v'), ('outputted', 'a'), ('loss', 'n'), ('layer', 'n'), ('referring', 'v'), ('optimal', 'a'), ('feature', 'n'), ('training', 'v'), ('corresponding', 'v'), ('ground', 'n'), ('truth', 'n'), ('managing', 'v'), ('device', 'n'), ('managing', 'v'), ('smart', 'a'), ('database', 'n'), ('stores', 'n'), ('facial', 'a'), ('face', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('memory', 'n'), ('stores', 'n'), ('instructions', 'n'), ('least', 'v'), ('one', None), ('configured', 'v'), ('execute', 'n'), ('instructions', 'n'), ('perform', 'v'), ('support', 'n'), ('another', None), ('device', 'n'), ('perform', 'n'), ('process', 'n'), ('counting', 'v'), ('one', None), ('specific', 'a'), ('facial', 'a'), ('corresponding', 'n'), ('least', 'a'), ('one', None), ('specific', 'a'), ('person', 'n'), ('stored', 'v'), ('smart', 'a'), ('database', 'n'), ('new', 'a'), ('facial', 'a'), ('face', 'n'), ('continuously', 'r'), ('stored', 'v'), ('process', 'n'), ('determining', 'v'), ('whether', None), ('first', 'a'), ('counted', 'v'), ('value', 'n'), ('representing', 'v'), ('count', 'n'), ('specific', 'a'), ('facial', 'a'), ('satisfies', 'n'), ('preset', 'v'), ('first', 'r'), ('set', 'v'), ('value', 'n'), ('ii', 'n'), ('first', 'r'), ('counted', 'v'), ('value', 'n'), ('determined', 'v'), ('satisfying', 'v'), ('first', 'r'), ('set', 'v'), ('value', 'n'), ('process', 'n'), ('inputting', 'v'), ('specific', 'a'), ('facial', 'a'), ('neural', 'a'), ('aggregation', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('neural', 'a'), ('aggregation', 'n'), ('network', 'n'), ('generate', 'v'), ('quality', 'n'), ('scores', 'n'), ('specific', 'a'), ('facial', 'a'), ('aggregation', 'n'), ('specific', 'a'), ('facial', 'a'), ('process', 'n'), ('sorting', 'v'), ('quality', 'n'), ('scores', 'n'), ('corresponding', 'v'), ('specific', 'a'), ('facial', 'a'), ('descending', 'n'), ('order', 'n'), ('quality', 'n'), ('scores', 'v'), ('process', 'n'), ('counting', 'n'), ('sorted', 'v'), ('specific', 'a'), ('facial', 'a'), ('descending', 'n'), ('order', 'n'), ('second', 'a'), ('counted', 'v'), ('value', 'n'), ('represents', 'v'), ('number', 'n'), ('counted', 'v'), ('part', 'n'), ('specific', 'a'), ('facial', 'a'), ('becomes', 'n'), ('equal', 'a'), ('preset', 'a'), ('second', 'n'), ('set', 'v'), ('value', 'n'), ('process', 'n'), ('deleting', 'v'), ('uncounted', 'a'), ('part', 'n'), ('specific', 'a'), ('facial', 'a'), ('smart', 'a'), ('database', 'n'), ('managing', 'v'), ('device', 'n'), ('wherein', 'n'), ('performs', 'v'), ('iii', 'a'), ('process', 'n'), ('generating', 'v'), ('least', 'a'), ('one', None), ('optimal', 'a'), ('feature', 'n'), ('weighted', 'v'), ('summation', 'n'), ('one', None), ('features', 'v'), ('specific', 'a'), ('facial', 'a'), ('using', 'v'), ('counted', 'a'), ('part', 'n'), ('quality', 'n'), ('scores', 'v'), ('process', 'a'), ('setting', 'v'), ('optimal', 'a'), ('feature', 'n'), ('representative', 'a'), ('face', 'n'), ('corresponding', 'v'), ('specific', 'a'), ('person', 'n'), ('managing', 'v'), ('device', 'n'), ('wherein', 'n'), ('process', 'n'), ('ii', 'n'), ('performs', 'n'), ('process', 'n'), ('inputting', 'v'), ('specific', 'a'), ('facial', 'a'), ('cnn', 'n'), ('neural', 'a'), ('aggregation', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('cnn', 'n'), ('generate', 'n'), ('one', None), ('features', 'v'), ('corresponding', 'v'), ('specific', 'a'), ('facial', 'a'), ('process', 'n'), ('inputting', 'v'), ('least', 'a'), ('one', None), ('feature', 'n'), ('vector', 'n'), ('features', 'n'), ('embedded', 'v'), ('aggregation', 'n'), ('module', 'n'), ('including', 'v'), ('least', 'a'), ('two', None), ('attention', 'n'), ('blocks', 'n'), ('thereby', 'r'), ('allow', 'v'), ('aggregation', 'n'), ('module', 'n'), ('generate', 'v'), ('quality', 'n'), ('scores', 'n'), ('features', 'v'), ('managing', 'v'), ('device', 'n'), ('wherein', 'n'), ('process', 'n'), ('ii', 'n'), ('performs', 'n'), ('process', 'n'), ('matching', 'v'), ('i-', 'a'), ('one', None), ('features', 'v'), ('corresponding', 'v'), ('specific', 'a'), ('facial', 'a'), ('stored', 'v'), ('smart', 'a'), ('database', 'n'), ('i-', 'a'), ('quality', 'n'), ('scores', 'n'), ('ii', 'v'), ('specific', 'a'), ('person', 'n'), ('process', 'n'), ('storing', 'v'), ('matched', 'v'), ('features', 'n'), ('matched', 'v'), ('quality', 'n'), ('scores', 'n'), ('smart', 'v'), ('database', 'n'), ('managing', 'v'), ('device', 'n'), ('wherein', 'n'), ('performs', 'v'), ('iv', 'v'), ('one', None), ('process', 'n'), ('learning', 'v'), ('face', 'n'), ('system', 'n'), ('using', 'v'), ('specific', 'a'), ('facial', 'a'), ('corresponding', 'n'), ('specific', 'a'), ('person', 'n'), ('stored', 'v'), ('smart', 'a'), ('database', 'n'), ('ii', 'n'), ('process', 'n'), ('transmitting', 'v'), ('specific', 'a'), ('facial', 'a'), ('corresponding', 'n'), ('specific', 'a'), ('person', 'n'), ('learning', 'a'), ('device', 'n'), ('corresponding', 'v'), ('face', 'n'), ('system', 'n'), ('thereby', 'r'), ('allow', 'v'), ('learning', 'v'), ('device', 'n'), ('learn', None), ('face', 'n'), ('system', 'n'), ('using', 'v'), ('specific', 'a'), ('facial', 'a'), ('managing', 'n'), ('device', 'n'), ('wherein', None), ('neural', 'a'), ('aggregation', 'n'), ('network', 'n'), ('learned', 'v'), ('learning', 'a'), ('device', 'n'), ('repeating', 'v'), ('process', 'n'), ('inputting', 'v'), ('multiple', 'a'), ('facial', 'a'), ('training', 'n'), ('corresponding', 'v'), ('set', 'v'), ('single', 'a'), ('face', 'n'), ('video', 'n'), ('single', 'a'), ('face', 'n'), ('cnn', 'a'), ('neural', 'a'), ('aggregation', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'v'), ('cnn', 'n'), ('generate', 'n'), ('one', None), ('features', 'v'), ('training', 'v'), ('applying', 'v'), ('least', 'a'), ('one', None), ('convolution', 'n'), ('operation', 'n'), ('facial', 'a'), ('training', 'n'), ('ii', 'n'), ('process', 'n'), ('inputting', 'v'), ('least', 'a'), ('one', None), ('feature', 'n'), ('vector', 'n'), ('training', 'n'), ('features', 'n'), ('training', 'v'), ('embedded', 'a'), ('aggregation', 'n'), ('module', 'n'), ('including', 'v'), ('least', 'a'), ('two', None), ('attention', 'n'), ('blocks', 'n'), ('neural', 'a'), ('aggregation', 'n'), ('network', 'n'), ('thereby', 'r'), ('allow', 'a'), ('aggregation', 'n'), ('module', 'n'), ('generate', 'v'), ('quality', 'n'), ('scores', 'n'), ('training', 'v'), ('features', 'n'), ('training', 'v'), ('aggregation', 'n'), ('features', 'n'), ('training', 'v'), ('using', 'v'), ('one', None), ('attention', 'n'), ('parameters', 'n'), ('learned', 'v'), ('previous', 'a'), ('iteration', 'n'), ('iii', 'n'), ('process', 'n'), ('outputting', 'v'), ('least', 'a'), ('one', None), ('optimal', 'a'), ('feature', 'n'), ('training', 'n'), ('weighted', 'a'), ('summation', 'n'), ('features', 'n'), ('training', 'v'), ('using', 'v'), ('quality', 'n'), ('scores', 'n'), ('training', 'v'), ('iv', 'a'), ('process', 'n'), ('updating', 'v'), ('attention', 'n'), ('parameters', 'n'), ('learned', 'v'), ('previous', 'a'), ('iteration', 'n'), ('least', 'a'), ('two', None), ('attention', 'n'), ('blocks', 'n'), ('one', None), ('losses', 'n'), ('minimized', 'v'), ('outputted', 'a'), ('loss', 'n'), ('layer', 'n'), ('referring', 'v'), ('optimal', 'a'), ('feature', 'n'), ('training', 'v'), ('corresponding', 'v'), ('ground', 'n'), ('truth', 'n'), ('object', None), ('data', 'n'), ('processing', 'v'), ('system', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('configured', 'v'), ('execute', 'n'), ('least', 'a'), ('one', None), ('implementation', 'n'), ('algorithms', 'n'), ('stored', 'v'), ('least', 'a'), ('one', None), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('algorithm', 'a'), ('feature', 'n'), ('density', 'n'), ('selection', 'n'), ('criteria', 'n'), ('data', 'n'), ('preprocessing', 'v'), ('code', 'n'), ('executed', 'v'), ('least', 'a'), ('one', None), ('data', 'n'), ('preprocessing', 'n'), ('code', 'n'), ('comprising', 'v'), ('invariant', 'a'), ('feature', 'n'), ('identification', 'n'), ('algorithm', 'r'), ('configured', 'v'), ('obtain', 'v'), ('digital', 'a'), ('representation', 'n'), ('scene', 'n'), ('scene', 'n'), ('comprising', 'v'), ('one', None), ('textual', 'a'), ('media', 'n'), ('generate', 'n'), ('set', 'v'), ('invariant', 'a'), ('features', 'n'), ('applying', 'v'), ('invariant', 'a'), ('feature', 'n'), ('identification', 'n'), ('algorithm', None), ('digital', 'a'), ('representation', 'n'), ('cluster', 'n'), ('set', 'v'), ('invariant', 'a'), ('features', 'n'), ('regions', 'n'), ('interest', 'n'), ('digital', 'a'), ('representation', 'n'), ('scene', 'n'), ('region', 'n'), ('interest', 'n'), ('region', 'n'), ('feature', 'n'), ('density', 'n'), ('classify', 'v'), ('region', 'n'), ('classifier', 'a'), ('code', 'n'), ('least', 'a'), ('one', None), ('regions', 'n'), ('interest', 'n'), ('according', 'v'), ('object', 'a'), ('type', 'n'), ('function', 'n'), ('attributes', 'v'), ('derived', 'a'), ('region', 'n'), ('feature', 'n'), ('density', 'n'), ('digital', 'a'), ('representation', 'n'), ('wherein', 'v'), ('least', 'a'), ('one', None), ('classified', 'a'), ('regions', 'n'), ('interest', 'n'), ('corresponds', 'n'), ('text', 'a'), ('use', 'n'), ('classification', 'n'), ('result', 'n'), ('corresponding', 'v'), ('least', 'a'), ('one', None), ('regions', 'n'), ('interest', 'n'), ('classify', 'n'), ('another', None), ('regions', 'n'), ('interest', 'n'), ('according', 'v'), ('object', 'a'), ('type', 'n'), ('wherein', 'n'), ('another', None), ('regions', 'n'), ('interest', 'n'), ('corresponds', 'v'), ('region', 'n'), ('interest', 'n'), ('system', 'n'), ('wherein', 'v'), ('preprocessing', 'v'), ('code', 'n'), ('based', 'v'), ('feature', 'n'), ('density', 'n'), ('selection', 'n'), ('criteria', 'n'), ('determines', 'v'), ('ocr', 'a'), ('algorithm', 'n'), ('applicable', 'a'), ('text', 'n'), ('algorithms', 'n'), ('applicable', 'a'), ('aspects', 'n'), ('photographs', 'v'), ('logos', 'a'), ('system', 'n'), ('wherein', 'n'), ('creates', 'v'), ('profile', None), ('camera-equipped', 'a'), ('smartphone', 'n'), ('includes', 'v'), ('information', 'n'), ('visually', 'r'), ('impaired', 'a'), ('causes', 'n'), ('prioritized', 'a'), ('execution', 'n'), ('ocr', None), ('algorithm', 'a'), ('text', 'a'), ('reader', 'n'), ('program', 'n'), ('begins', 'v'), ('reading', 'v'), ('text', 'a'), ('quickly', 'r'), ('possible', 'a'), ('system', 'n'), ('comprising', 'v'), ('audio', 'a'), ('tactile', 'n'), ('feedback', 'n'), ('mechanism', 'n'), ('helps', 'v'), ('position', 'n'), ('smart', 'a'), ('phone', 'n'), ('relative', 'a'), ('text', 'n'), ('system', 'n'), ('comprising', 'v'), ('``', None), ('hold', 'v'), ('still', 'r'), (\"''\", None), ('audio', 'a'), ('feedback', 'n'), ('signal', 'a'), ('sent', 'v'), ('text', 'n'), ('center', 'n'), ('captured', 'v'), ('scene', 'n'), ('system', 'n'), ('wherein', 'a'), ('digital', 'a'), ('representation', 'n'), ('comprises', 'n'), ('least', 'v'), ('one', None), ('following', 'v'), ('types', 'n'), ('digital', 'a'), ('data', 'n'), ('data', 'n'), ('video', 'n'), ('data', 'n'), ('audio', 'r'), ('data', 'n'), ('system', 'n'), ('wherein', 'v'), ('invariant', 'a'), ('feature', 'n'), ('identification', 'n'), ('algorithm', 'n'), ('comprises', 'v'), ('least', 'a'), ('one', None), ('following', 'v'), ('feature', 'n'), ('identification', 'n'), ('algorithms', None), ('fast', 'a'), ('sift', 'n'), ('freak', 'n'), ('brisk', 'a'), ('harris', 'n'), ('daisy', 'n'), ('mser', 'n'), ('system', 'n'), ('wherein', 'v'), ('invariant', 'a'), ('feature', 'n'), ('identification', 'n'), ('algorithm', 'n'), ('includes', 'v'), ('least', 'a'), ('one', None), ('following', 'v'), ('edge', 'n'), ('detection', 'n'), ('algorithm', 'n'), ('corner', 'n'), ('detection', 'n'), ('algorithm', 'a'), ('saliency', 'n'), ('map', 'n'), ('algorithm', 'n'), ('curve', 'n'), ('detection', 'n'), ('algorithm', None), ('texton', 'n'), ('identification', 'n'), ('algorithm', None), ('wavelets', 'n'), ('algorithm', 'a'), ('system', 'n'), ('wherein', 'v'), ('least', 'a'), ('one', None), ('region', 'n'), ('interest', 'n'), ('represents', 'v'), ('least', 'a'), ('one', None), ('physical', 'a'), ('object', 'n'), ('scene', 'n'), ('system', 'n'), ('wherein', 'v'), ('least', 'a'), ('one', None), ('region', 'n'), ('interest', 'n'), ('represents', 'v'), ('least', 'a'), ('one', None), ('textual', 'a'), ('media', 'n'), ('scene', 'n'), ('system', 'n'), ('wherein', 'a'), ('region', 'n'), ('interest', 'n'), ('represents', 'v'), ('document', 'a'), ('textual', 'a'), ('media', 'n'), ('system', 'n'), ('wherein', 'a'), ('region', 'n'), ('interest', 'n'), ('represents', 'v'), ('financial', 'a'), ('document', 'n'), ('system', 'n'), ('wherein', 'a'), ('region', 'n'), ('interest', 'n'), ('represents', 'v'), ('structured', 'v'), ('document', 'n'), ('system', 'n'), ('wherein', 'v'), ('least', 'a'), ('one', None), ('implementation', 'n'), ('algorithms', 'n'), ('includes', 'v'), ('least', 'a'), ('one', None), ('following', 'v'), ('template', 'n'), ('driven', 'v'), ('algorithm', 'a'), ('face', 'n'), ('algorithm', 'a'), ('optical', 'a'), ('character', 'n'), ('algorithm', 'n'), ('speech', 'n'), ('algorithm', 'n'), ('object', 'v'), ('algorithm', 'n'), ('system', 'n'), ('wherein', None), ('data', 'n'), ('preprocessing', 'v'), ('code', 'n'), ('configured', 'v'), ('assign', 'a'), ('region', 'n'), ('interest', 'n'), ('least', 'a'), ('one', None), ('algorithm', 'n'), ('function', 'n'), ('scene', 'n'), ('context', 'n'), ('derived', 'v'), ('digital', 'a'), ('representation', 'n'), ('system', 'n'), ('wherein', 'a'), ('scene', 'n'), ('context', 'n'), ('includes', 'v'), ('least', 'a'), ('one', None), ('following', 'v'), ('types', 'n'), ('data', 'n'), ('location', 'n'), ('position', 'n'), ('time', 'n'), ('identity', 'n'), ('news', 'n'), ('event', 'n'), ('medical', 'a'), ('event', 'n'), ('promotion', 'n'), ('system', 'n'), ('comprising', 'v'), ('mobile', 'a'), ('device', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('implementation', 'n'), ('algorithms', 'n'), ('data', 'n'), ('preprocessing', 'v'), ('code', 'n'), ('system', 'n'), ('wherein', 'v'), ('mobile', 'a'), ('device', 'n'), ('comprises', 'n'), ('least', 'v'), ('one', None), ('following', 'v'), ('smart', 'a'), ('phone', 'n'), ('tablet', 'n'), ('wearable', 'a'), ('glass', 'n'), ('toy', 'n'), ('vehicle', 'n'), ('computer', 'n'), ('phablet', 'n'), ('system', 'n'), ('comprising', 'v'), ('network-accessible', 'a'), ('server', 'n'), ('device', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('implementation', 'n'), ('algorithms', 'n'), ('data', 'n'), ('preprocessing', 'v'), ('code', 'n'), ('system', 'n'), ('wherein', 'a'), ('object', 'a'), ('type', 'n'), ('includes', 'v'), ('least', 'a'), ('one', None), ('following', 'v'), ('face', 'n'), ('animal', 'a'), ('vehicle', 'n'), ('document', 'n'), ('plant', 'n'), ('building', 'n'), ('appliance', 'n'), ('clothing', 'n'), ('body', 'n'), ('part', 'n'), ('toy', 'n'), ('object', 'v'), ('data', 'n'), ('processing', 'n'), ('system', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('configured', 'v'), ('execute', 'n'), ('least', 'a'), ('one', None), ('implementation', 'n'), ('algorithms', 'n'), ('stored', 'v'), ('least', 'a'), ('one', None), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('algorithm', 'a'), ('feature', 'n'), ('density', 'n'), ('selection', 'n'), ('criteria', 'n'), ('data', 'n'), ('preprocessing', 'v'), ('code', 'n'), ('executed', 'v'), ('least', 'a'), ('one', None), ('data', 'n'), ('preprocessing', 'n'), ('code', 'n'), ('comprising', 'v'), ('invariant', 'a'), ('feature', 'n'), ('identification', 'n'), ('algorithm', 'r'), ('configured', 'v'), ('obtain', 'v'), ('digital', 'a'), ('representation', 'n'), ('scene', 'n'), ('scene', 'n'), ('comprising', 'v'), ('one', None), ('textual', 'a'), ('media', 'n'), ('generate', 'n'), ('set', 'v'), ('invariant', 'a'), ('features', 'n'), ('applying', 'v'), ('invariant', 'a'), ('feature', 'n'), ('identification', 'n'), ('algorithm', None), ('digital', 'a'), ('representation', 'n'), ('cluster', 'n'), ('set', 'v'), ('invariant', 'a'), ('features', 'n'), ('regions', 'n'), ('interest', 'n'), ('digital', 'a'), ('representation', 'n'), ('scene', 'n'), ('region', 'n'), ('interest', 'n'), ('region', 'n'), ('feature', 'n'), ('density', 'n'), ('classify', 'v'), ('region', 'n'), ('classifier', 'a'), ('code', 'n'), ('least', 'a'), ('one', None), ('regions', 'n'), ('interest', 'n'), ('according', 'v'), ('object', 'a'), ('type', 'n'), ('function', 'n'), ('attributes', 'v'), ('derived', 'a'), ('region', 'n'), ('feature', 'n'), ('density', 'n'), ('digital', 'a'), ('representation', 'n'), ('wherein', 'v'), ('least', 'a'), ('one', None), ('classified', 'a'), ('regions', 'n'), ('interest', 'n'), ('corresponds', 'n'), ('text', 'a'), ('use', 'n'), ('classification', 'n'), ('result', 'n'), ('corresponding', 'v'), ('least', 'a'), ('one', None), ('regions', 'n'), ('interest', 'n'), ('classify', 'n'), ('another', None), ('regions', 'n'), ('interest', 'n'), ('according', 'v'), ('object', 'a'), ('type', 'n'), ('wherein', 'n'), ('another', None), ('regions', 'n'), ('interest', 'n'), ('corresponds', 'v'), ('region', 'n'), ('interest', 'n'), ('assign', 'n'), ('region', 'n'), ('interest', 'n'), ('least', 'a'), ('one', None), ('algorithm', 'n'), ('least', 'a'), ('one', None), ('implementation', 'n'), ('diverse', 'n'), ('algorithms', 'n'), ('function', 'n'), ('region', 'n'), ('feature', 'n'), ('density', 'n'), ('region', 'n'), ('interest', 'n'), ('feature', 'n'), ('density', 'n'), ('selection', 'n'), ('criteria', 'n'), ('least', 'v'), ('one', None), ('implementation', 'n'), ('diverse', 'n'), ('algorithms', 'a'), ('configure', 'n'), ('assigned', 'v'), ('algorithms', 'a'), ('process', 'n'), ('respective', 'a'), ('regions', 'n'), ('interest', 'n'), ('wherein', 'n'), ('preprocessing', 'v'), ('code', 'n'), ('based', 'v'), ('feature', 'n'), ('density', 'n'), ('selection', 'n'), ('criteria', 'n'), ('determines', 'v'), ('ocr', 'a'), ('algorithm', 'n'), ('applicable', 'a'), ('text', 'n'), ('algorithms', 'n'), ('applicable', 'a'), ('aspects', 'n'), ('photographs', 'v'), ('logos', 'a'), ('device', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('configured', 'v'), ('execute', 'n'), ('least', 'a'), ('one', None), ('implementation', 'n'), ('algorithms', 'n'), ('stored', 'v'), ('least', 'a'), ('one', None), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('algorithm', 'a'), ('feature', 'n'), ('density', 'n'), ('selection', 'n'), ('criteria', 'n'), ('data', 'n'), ('preprocessing', 'v'), ('code', 'n'), ('executed', 'v'), ('least', 'a'), ('one', None), ('data', 'n'), ('preprocessing', 'n'), ('code', 'n'), ('comprising', 'v'), ('invariant', 'a'), ('feature', 'n'), ('identification', 'n'), ('algorithm', 'r'), ('configured', 'v'), ('obtain', 'v'), ('digital', 'a'), ('representation', 'n'), ('scene', 'n'), ('scene', 'n'), ('comprising', 'v'), ('one', None), ('textual', 'a'), ('media', 'n'), ('generate', 'n'), ('set', 'v'), ('invariant', 'a'), ('features', 'n'), ('applying', 'v'), ('invariant', 'a'), ('feature', 'n'), ('identification', 'n'), ('algorithm', None), ('digital', 'a'), ('representation', 'n'), ('cluster', 'n'), ('set', 'v'), ('invariant', 'a'), ('features', 'n'), ('regions', 'n'), ('interest', 'n'), ('digital', 'a'), ('representation', 'n'), ('scene', 'n'), ('region', 'n'), ('interest', 'n'), ('region', 'n'), ('feature', 'n'), ('density', 'n'), ('classify', 'v'), ('region', 'n'), ('classifier', 'a'), ('code', 'n'), ('least', 'a'), ('one', None), ('regions', 'n'), ('interest', 'n'), ('according', 'v'), ('object', 'a'), ('type', 'n'), ('function', 'n'), ('attributes', 'v'), ('derived', 'a'), ('region', 'n'), ('feature', 'n'), ('density', 'n'), ('digital', 'a'), ('representation', 'n'), ('wherein', 'v'), ('least', 'a'), ('one', None), ('classified', 'a'), ('regions', 'n'), ('interest', 'n'), ('corresponds', 'n'), ('text', 'a'), ('use', 'n'), ('classification', 'n'), ('result', 'n'), ('corresponding', 'v'), ('least', 'a'), ('one', None), ('regions', 'n'), ('interest', 'n'), ('classify', 'n'), ('another', None), ('regions', 'n'), ('interest', 'n'), ('according', 'v'), ('object', 'a'), ('type', 'n'), ('wherein', 'n'), ('another', None), ('regions', 'n'), ('interest', 'n'), ('corresponds', 'v'), ('region', 'n'), ('interest', 'n'), ('mobile', None), ('terminal', 'a'), ('comprising', 'v'), ('front', 'a'), ('camera', 'n'), ('configured', 'v'), ('obtain', 'v'), ('two-dimensional', 'a'), ('face', 'n'), ('glance', 'n'), ('sensor', 'n'), ('tilted', 'v'), ('certain', 'a'), ('angle', 'n'), ('disposed', 'v'), ('adjacent', 'a'), ('front', 'a'), ('camera', 'n'), ('obtain', 'v'), ('metadata', 'a'), ('face', 'n'), ('controller', 'n'), ('obtaining', 'v'), ('distance', 'n'), ('glance', 'n'), ('sensor', 'n'), ('front', 'n'), ('camera', 'n'), ('distance', 'n'), ('enabling', 'v'), ('area', 'n'), ('overlap', None), ('region', 'n'), ('first', 'a'), ('region', 'n'), ('representing', 'v'), ('range', 'n'), ('photographable', 'a'), ('front', 'n'), ('camera', 'n'), ('overlaps', 'v'), ('second', 'a'), ('region', 'n'), ('representing', 'v'), ('range', 'n'), ('photographable', 'a'), ('glance', 'n'), ('sensor', 'n'), ('maximum', 'n'), ('mobile', 'a'), ('terminal', 'a'), ('wherein', 'n'), ('controller', 'n'), ('configured', 'v'), ('obtain', 'v'), ('distance', 'n'), ('enabling', 'v'), ('area', 'n'), ('overlap', 'a'), ('region', 'n'), ('maximum', 'a'), ('glance', 'n'), ('sensor', 'n'), ('front', 'n'), ('camera', 'n'), ('varying', 'v'), ('tilting', 'v'), ('angle', 'a'), ('glance', 'n'), ('sensor', 'n'), ('mobile', 'a'), ('terminal', 'a'), ('wherein', 'n'), ('controller', 'n'), ('configured', 'v'), ('set', 'v'), ('distance', 'n'), ('enabling', 'v'), ('area', 'n'), ('overlap', 'a'), ('region', 'n'), ('maximum', 'a'), ('glance', 'n'), ('sensor', 'n'), ('front', 'n'), ('camera', 'n'), ('tilting', 'v'), ('angle', 'a'), ('glance', 'n'), ('sensor', 'n'), ('optimal', 'a'), ('disposition', 'n'), ('location', 'n'), ('glance', 'n'), ('sensor', 'n'), ('mobile', 'a'), ('terminal', 'a'), ('wherein', 'n'), ('controller', 'n'), ('configured', 'v'), ('set', 'v'), ('disposition', 'n'), ('location', 'n'), ('front', 'n'), ('camera', 'n'), ('original', 'a'), ('point', 'n'), ('calculates', 'n'), ('coordinates', 'n'), ('first', 'r'), ('triangle', 'v'), ('representing', 'v'), ('first', 'a'), ('region', 'n'), ('based', 'v'), ('field', 'n'), ('view', 'n'), ('front', 'a'), ('camera', 'n'), ('maximum', 'a'), ('photographing', 'v'), ('distance', 'n'), ('front', 'n'), ('camera', 'n'), ('mobile', 'a'), ('terminal', 'a'), ('wherein', 'n'), ('controller', 'n'), ('configured', 'v'), ('calculate', 'a'), ('coordinates', 'n'), ('second', 'a'), ('triangle', 'a'), ('representing', 'v'), ('second', 'a'), ('region', 'n'), ('based', 'v'), ('field', 'n'), ('view', 'n'), ('glance', 'n'), ('sensor', 'n'), ('maximum', 'n'), ('photographing', 'v'), ('distance', 'n'), ('glance', 'n'), ('sensor', 'n'), ('distance', 'n'), ('front', 'a'), ('camera', 'n'), ('glance', 'n'), ('sensor', 'n'), ('tilting', 'v'), ('angle', 'a'), ('glance', 'n'), ('sensor', 'n'), ('mobile', 'a'), ('terminal', 'a'), ('wherein', 'n'), ('glance', 'n'), ('sensor', 'n'), ('tilted', 'v'), ('controller', 'n'), ('configured', 'a'), ('calculate', 'n'), ('coordinates', 'n'), ('third', 'a'), ('triangle', 'a'), ('representing', 'v'), ('third', 'a'), ('region', 'n'), ('photographable', 'a'), ('glance', 'n'), ('sensor', 'n'), ('controller', 'n'), ('configured', 'v'), ('rotation-convert', 'a'), ('coordinates', 'n'), ('third', 'a'), ('triangle', 'n'), ('based', 'v'), ('tilting', 'v'), ('angle', 'a'), ('glance', 'n'), ('sensor', 'n'), ('calculate', 'n'), ('coordinates', 'n'), ('second', 'a'), ('triangle', 'v'), ('mobile', 'a'), ('terminal', 'a'), ('wherein', 'n'), ('controller', 'n'), ('configured', 'v'), ('calculate', 'n'), ('coordinates', 'n'), ('overlap', 'v'), ('region', 'n'), ('based', 'v'), ('coordinates', 'n'), ('first', 'a'), ('triangle', 'a'), ('coordinates', 'n'), ('second', 'a'), ('triangle', 'n'), ('calculates', 'n'), ('area', 'n'), ('overlap', 'v'), ('region', 'n'), ('based', 'v'), ('coordinates', 'n'), ('overlap', 'a'), ('region', 'n'), ('mobile', None), ('terminal', 'a'), ('wherein', 'n'), ('controller', 'n'), ('configured', 'v'), ('generate', 'a'), ('three-dimensional', 'a'), ('face', 'n'), ('information', 'n'), ('based', 'v'), ('face', 'n'), ('obtained', 'v'), ('front', 'a'), ('camera', 'n'), ('metadata', 'n'), ('obtained', 'v'), ('glance', 'n'), ('sensor', 'n'), ('mobile', 'a'), ('terminal', 'a'), ('wherein', 'n'), ('metadata', 'n'), ('comprises', 'v'), ('one', None), ('angle', 'n'), ('face', 'n'), ('size', 'n'), ('face', 'n'), ('location', 'n'), ('face', 'n'), ('mobile', 'a'), ('terminal', 'a'), ('wherein', 'n'), ('angle', 'a'), ('face', 'n'), ('comprises', 'n'), ('angle', 'v'), ('face', 'n'), ('rotated', 'v'), ('one', None), ('pitch', 'n'), ('axis', 'n'), ('roll', 'n'), ('axis', 'n'), ('yaw', 'n'), ('axis', 'v'), ('mobile', 'a'), ('terminal', 'a'), ('comprising', 'v'), ('memory', 'n'), ('storing', 'v'), ('generated', 'v'), ('face', 'n'), ('information', 'n'), ('wherein', None), ('controller', 'n'), ('configured', 'v'), ('performs', 'n'), ('authentication', 'n'), ('process', 'n'), ('comparing', 'v'), ('stored', 'v'), ('face', 'n'), ('information', 'n'), ('face', 'n'), ('information', 'n'), ('obtained', 'v'), ('authentication', 'n'), ('mobile', 'a'), ('terminal', 'a'), ('wherein', 'n'), ('glance', 'n'), ('sensor', 'n'), ('controlled', 'v'), ('permanently', 'r'), ('activated', 'v'), ('low', 'a'), ('power', 'n'), ('obtain', 'v'), ('front', 'a'), ('metadata', 'n'), ('front', 'v'), ('mobile', 'a'), ('terminal', 'a'), ('wherein', 'n'), ('front', 'a'), ('camera', 'n'), ('glance', 'n'), ('sensor', 'n'), ('disposed', 'v'), ('line', 'n'), ('upper', 'a'), ('end', 'n'), ('mobile', 'a'), ('terminal', 'n'), ('mobile', 'a'), ('terminal', 'a'), ('wherein', 'n'), ('glance', 'n'), ('sensor', 'n'), ('tilted', 'v'), ('one', None), ('direction', 'n'), ('direction', 'n'), ('direction', 'n'), ('left', 'v'), ('direction', 'n'), ('right', 'a'), ('direction', 'n'), ('mobile', None), ('terminal', 'a'), ('wherein', 'n'), ('metadata', 'n'), ('data', 'n'), ('changed', 'v'), ('mobile', 'a'), ('terminal', 'n'), ('tilted', 'v'), ('external', 'a'), ('physical', 'a'), ('force', 'n'), ('comprising', 'v'), ('receiving', 'v'), ('smart', 'a'), ('television', 'n'), ('tv', 'n'), ('indication', 'n'), ('upcoming', 'v'), ('media', 'n'), ('programming', 'v'), ('wherein', 'n'), ('upcoming', 'a'), ('media', 'n'), ('programming', 'v'), ('based', 'v'), ('profile', None), ('identifying', 'v'), ('one', None), ('devices', 'n'), ('communication', 'n'), ('smart', 'a'), ('tv', 'n'), ('one', None), ('devices', 'n'), ('including', 'v'), ('least', 'a'), ('one', None), ('microphone', 'n'), ('camera', 'n'), ('instructing', 'v'), ('least', 'a'), ('one', None), ('identified', 'a'), ('device', 'n'), ('detect', 'n'), ('audio', 'n'), ('signals', 'n'), ('using', 'v'), ('respective', 'a'), ('microphone', 'n'), ('detect', 'a'), ('visual', 'a'), ('signals', 'n'), ('using', 'v'), ('respective', 'a'), ('camera', 'n'), ('selecting', 'v'), ('least', 'a'), ('one', None), ('device', 'n'), ('one', None), ('devices', 'n'), ('based', 'v'), ('detected', 'a'), ('audio', 'a'), ('signal', 'n'), ('detected', 'v'), ('visual', 'a'), ('signal', 'n'), ('providing', 'v'), ('instructions', 'n'), ('selected', 'v'), ('device', 'n'), ('output', 'n'), ('notification', 'n'), ('related', 'v'), ('upcoming', 'a'), ('media', 'n'), ('programming', 'v'), ('wherein', 'n'), ('upcoming', 'a'), ('media', 'n'), ('programming', 'v'), ('one', None), ('live', 'a'), ('television', 'n'), ('program', 'n'), ('recorded', 'v'), ('television', 'n'), ('program', 'n'), ('broadcast', 'n'), ('television', 'n'), ('program', 'n'), ('application-provided', 'a'), ('program', 'n'), ('wherein', 'n'), ('selecting', 'v'), ('first', 'a'), ('device', 'n'), ('based', 'v'), ('detected', 'a'), ('audio', 'a'), ('signal', 'n'), ('includes', 'v'), ('voice', 'n'), ('comprising', 'v'), ('determining', 'v'), ('distance', 'n'), ('recognized', 'v'), ('voice', 'n'), ('wherein', 'n'), ('selecting', 'v'), ('first', 'a'), ('device', 'n'), ('based', 'v'), ('determined', 'a'), ('distance', 'n'), ('wherein', 'n'), ('selecting', 'v'), ('first', 'a'), ('device', 'n'), ('based', 'v'), ('detected', 'v'), ('visual', 'a'), ('signals', 'n'), ('includes', 'v'), ('face', 'n'), ('wherein', 'n'), ('face', 'n'), ('includes', 'v'), ('face', 'n'), ('technique', 'n'), ('comprising', 'v'), ('presenting', 'v'), ('smart', 'a'), ('tv', 'n'), ('upcoming', 'v'), ('media', 'n'), ('programming', 'v'), ('favorite', 'a'), ('channel', 'n'), ('list', 'n'), ('comprising', 'v'), ('obtaining', 'v'), ('media', 'n'), ('programming', 'v'), ('viewing', 'v'), ('data', 'n'), ('wherein', 'r'), ('media', 'n'), ('programming', 'v'), ('viewing', 'v'), ('data', 'n'), ('includes', 'v'), ('least', 'a'), ('one', None), ('historical', 'a'), ('time', 'n'), ('historical', 'a'), ('date', 'n'), ('one', None), ('media', 'n'), ('programs', 'n'), ('viewed', 'v'), ('obtaining', 'v'), ('least', 'a'), ('one', None), ('current', 'a'), ('time', 'n'), ('current', 'a'), ('date', 'n'), ('processing', 'n'), ('media', 'n'), ('programming', 'v'), ('viewing', 'v'), ('data', 'n'), ('determine', 'n'), ('probability', 'n'), ('one', None), ('media', 'n'), ('programs', 'n'), ('viewed', 'v'), ('based', 'v'), ('least', 'a'), ('one', None), ('current', 'a'), ('time', 'n'), ('current', 'a'), ('date', 'n'), ('presenting', 'n'), ('favorite', 'a'), ('channel', 'n'), ('list', 'n'), ('based', 'v'), ('determined', 'a'), ('probability', 'n'), ('one', None), ('media', 'n'), ('programs', 'n'), ('viewed', 'v'), ('wherein', 'a'), ('processing', 'n'), ('media', 'n'), ('programming', 'v'), ('viewing', 'v'), ('data', 'n'), ('includes', 'v'), ('employing', 'v'), ('neural', 'a'), ('network', 'n'), ('model', 'n'), ('wherein', 'n'), ('employing', 'v'), ('neural', 'a'), ('network', 'n'), ('model', 'n'), ('comprises', 'v'), ('determining', 'v'), ('duration', 'n'), ('one', None), ('media', 'n'), ('programs', 'n'), ('viewed', 'v'), ('least', 'a'), ('one', None), ('historical', 'a'), ('time', 'n'), ('historical', 'a'), ('date', 'n'), ('setting', 'v'), ('threshold', 'a'), ('time', 'n'), ('duration', 'n'), ('comparing', 'v'), ('determined', 'v'), ('duration', 'n'), ('threshold', 'a'), ('time', 'n'), ('duration', 'n'), ('filtering', 'v'), ('one', None), ('media', 'n'), ('programs', 'n'), ('viewed', 'v'), ('threshold', 'a'), ('time', 'n'), ('duration', 'n'), ('smart', 'a'), ('television', 'n'), ('tv', 'n'), ('comprising', 'v'), ('network', 'n'), ('interface', 'a'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('medium', 'n'), ('communication', 'n'), ('network', 'n'), ('interface', 'a'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('medium', 'n'), ('capable', 'a'), ('executing', 'v'), ('-executable', 'a'), ('program', 'n'), ('code', 'n'), ('stored', 'v'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('medium', 'n'), ('cause', 'n'), ('smart', 'a'), ('tv', 'n'), ('receive', 'n'), ('indication', 'n'), ('upcoming', 'v'), ('media', 'n'), ('programming', 'v'), ('wherein', 'n'), ('upcoming', 'a'), ('media', 'n'), ('programming', 'v'), ('based', 'v'), ('profile', 'a'), ('identify', 'v'), ('one', None), ('devices', 'n'), ('communication', 'n'), ('smart', 'a'), ('tv', 'n'), ('one', None), ('devices', 'n'), ('including', 'v'), ('least', 'a'), ('one', None), ('microphone', 'n'), ('camera', 'n'), ('instruct', 'n'), ('least', 'a'), ('one', None), ('identified', 'a'), ('device', 'n'), ('detect', 'n'), ('audio', 'n'), ('signals', 'n'), ('using', 'v'), ('respective', 'a'), ('microphone', 'n'), ('detect', 'a'), ('visual', 'a'), ('signals', 'n'), ('using', 'v'), ('respective', 'a'), ('camera', 'n'), ('select', 'n'), ('least', 'a'), ('one', None), ('device', 'n'), ('one', None), ('devices', 'n'), ('based', 'v'), ('detected', 'a'), ('audio', 'a'), ('signal', 'n'), ('detected', 'v'), ('visual', 'a'), ('signal', 'n'), ('provide', 'n'), ('instructions', 'n'), ('selected', 'v'), ('device', 'n'), ('output', 'n'), ('notification', 'n'), ('related', 'v'), ('upcoming', 'a'), ('media', 'n'), ('programming', 'v'), ('smart', 'a'), ('tv', 'n'), ('wherein', 'n'), ('selecting', 'v'), ('first', 'a'), ('device', 'n'), ('based', 'v'), ('detected', 'a'), ('audio', 'a'), ('signal', 'n'), ('includes', 'v'), ('voice', 'n'), ('smart', 'a'), ('tv', 'n'), ('wherein', 'n'), ('capable', 'a'), ('executing', 'v'), ('-executable', 'a'), ('program', 'n'), ('code', 'n'), ('determine', 'n'), ('distance', 'n'), ('recognized', 'v'), ('voice', 'n'), ('wherein', 'n'), ('selecting', 'v'), ('first', 'a'), ('device', 'n'), ('based', 'v'), ('determined', 'a'), ('distance', 'n'), ('smart', 'a'), ('tv', 'n'), ('wherein', 'n'), ('selecting', 'v'), ('first', 'a'), ('device', 'n'), ('based', 'v'), ('detected', 'v'), ('visual', 'a'), ('signals', 'n'), ('includes', 'v'), ('detecting', 'v'), ('presence', 'n'), ('smart', 'a'), ('tv', 'n'), ('wherein', 'n'), ('detecting', 'v'), ('presence', 'n'), ('includes', 'v'), ('employing', 'v'), ('one', None), ('camera', 'n'), ('microphone', 'n'), ('fingerprint', 'n'), ('sensor', 'n'), ('associated', 'v'), ('least', 'a'), ('one', None), ('smart', 'a'), ('tv', 'n'), ('mobile', 'n'), ('device', 'n'), ('smartphone', 'n'), ('laptop', 'a'), ('computer', 'n'), ('tablet', 'n'), ('device', 'n'), ('wearable', 'a'), ('device', 'n'), ('internet', 'n'), ('things', 'n'), ('iot', 'a'), ('device', 'a'), ('internet', 'n'), ('everything', 'n'), ('ioe', 'n'), ('device', 'n'), ('iot', 'n'), ('hub', 'n'), ('ioe', 'n'), ('hub', 'n'), ('smart', 'a'), ('television', 'n'), ('tv', 'n'), ('comprising', 'n'), ('means', 'v'), ('receiving', 'v'), ('indication', 'n'), ('upcoming', 'v'), ('media', 'n'), ('programming', 'v'), ('wherein', 'n'), ('upcoming', 'a'), ('media', 'n'), ('programming', 'v'), ('based', 'v'), ('profile', 'n'), ('means', 'v'), ('identifying', 'v'), ('one', None), ('devices', 'n'), ('communication', 'n'), ('smart', 'a'), ('tv', 'n'), ('one', None), ('devices', 'n'), ('including', 'v'), ('least', 'a'), ('one', None), ('microphone', 'n'), ('camera', 'n'), ('means', 'v'), ('instructing', 'v'), ('least', 'a'), ('one', None), ('identified', 'a'), ('device', 'n'), ('detect', 'n'), ('audio', 'n'), ('signals', 'n'), ('using', 'v'), ('respective', 'a'), ('microphone', 'n'), ('detect', 'a'), ('visual', 'a'), ('signals', 'n'), ('using', 'v'), ('respective', 'a'), ('camera', 'n'), ('means', 'v'), ('selecting', 'v'), ('least', 'a'), ('one', None), ('device', 'n'), ('one', None), ('devices', 'n'), ('based', 'v'), ('detected', 'a'), ('audio', 'a'), ('signal', 'n'), ('detected', 'v'), ('visual', 'a'), ('signal', 'n'), ('means', 'n'), ('providing', 'v'), ('instructions', 'n'), ('selected', 'v'), ('device', 'n'), ('output', 'n'), ('notification', 'n'), ('related', 'v'), ('upcoming', 'a'), ('media', 'n'), ('programming', 'v'), ('smart', 'a'), ('tv', 'n'), ('wherein', 'v'), ('one', None), ('devices', 'n'), ('includes', 'v'), ('least', 'a'), ('one', None), ('mobile', 'a'), ('device', 'n'), ('smartphone', 'n'), ('laptop', 'a'), ('computer', 'n'), ('tablet', 'n'), ('device', 'n'), ('wearable', 'a'), ('device', 'n'), ('internet', 'n'), ('things', 'n'), ('iot', 'a'), ('device', 'a'), ('internet', 'n'), ('everything', 'n'), ('ioe', 'n'), ('device', 'n'), ('iot', 'n'), ('hub', 'n'), ('ioe', 'n'), ('hub', 'n'), ('another', None), ('smart', 'a'), ('tv', 'n'), ('smart', 'a'), ('tv', 'n'), ('wherein', 'n'), ('upcoming', 'a'), ('media', 'n'), ('programming', 'v'), ('one', None), ('live', 'a'), ('television', 'n'), ('program', 'n'), ('recorded', 'v'), ('television', 'n'), ('program', 'n'), ('broadcast', 'n'), ('television', 'n'), ('program', 'n'), ('application-provided', 'a'), ('program', 'n'), ('smart', 'a'), ('tv', 'n'), ('wherein', 'n'), ('notification', 'n'), ('includes', 'v'), ('least', 'a'), ('one', None), ('push', 'n'), ('message', 'n'), ('sms', 'a'), ('message', 'n'), ('waysms', 'a'), ('message', 'n'), ('audio', 'n'), ('alert', 'n'), ('audio', 'a'), ('message', 'n'), ('email', 'a'), ('message', 'n'), ('smart', 'a'), ('tv', 'n'), ('comprising', 'v'), ('presenting', 'v'), ('upcoming', 'a'), ('media', 'n'), ('programming', 'v'), ('favorite', 'a'), ('channel', 'n'), ('list', 'n'), ('smart', 'a'), ('tv', 'n'), ('comprising', 'n'), ('means', 'v'), ('obtaining', 'v'), ('media', 'n'), ('programming', 'v'), ('viewing', 'v'), ('data', 'n'), ('wherein', 'r'), ('media', 'n'), ('programming', 'v'), ('viewing', 'v'), ('data', 'n'), ('includes', 'v'), ('least', 'a'), ('one', None), ('historical', 'a'), ('time', 'n'), ('historical', 'a'), ('date', 'n'), ('one', None), ('media', 'n'), ('programs', 'n'), ('viewed', 'v'), ('smart', 'a'), ('tv', 'n'), ('means', 'n'), ('obtaining', 'v'), ('least', 'a'), ('one', None), ('current', 'a'), ('time', 'n'), ('current', 'a'), ('date', 'n'), ('means', 'v'), ('processing', 'v'), ('media', 'n'), ('programming', 'v'), ('viewing', 'v'), ('data', 'n'), ('determine', 'n'), ('probability', 'n'), ('one', None), ('media', 'n'), ('programs', 'n'), ('viewed', 'v'), ('smart', 'a'), ('tv', 'n'), ('based', 'v'), ('least', 'a'), ('one', None), ('current', 'a'), ('time', 'n'), ('current', 'a'), ('date', 'n'), ('means', 'n'), ('presenting', 'v'), ('favorite', 'a'), ('channel', 'n'), ('list', 'n'), ('based', 'v'), ('determined', 'a'), ('probability', 'n'), ('one', None), ('media', 'n'), ('programs', 'n'), ('viewed', 'v'), ('smart', 'a'), ('tv', 'n'), ('wherein', 'n'), ('means', 'v'), ('processing', 'v'), ('media', 'n'), ('programming', 'v'), ('viewing', 'v'), ('data', 'n'), ('includes', 'v'), ('employing', 'v'), ('neural', 'a'), ('network', 'n'), ('model', 'n'), ('smart', 'a'), ('tv', 'n'), ('wherein', 'n'), ('employing', 'v'), ('neural', 'a'), ('network', 'n'), ('model', 'n'), ('comprises', 'v'), ('determining', 'v'), ('duration', 'n'), ('one', None), ('media', 'n'), ('programs', 'n'), ('viewed', 'v'), ('smart', 'a'), ('tv', 'n'), ('least', 'a'), ('one', None), ('historical', 'a'), ('time', 'n'), ('historical', 'a'), ('date', 'n'), ('setting', 'v'), ('threshold', 'a'), ('time', 'n'), ('duration', 'n'), ('comparing', 'v'), ('determined', 'v'), ('duration', 'n'), ('threshold', 'a'), ('time', 'n'), ('duration', 'n'), ('filtering', 'v'), ('one', None), ('media', 'n'), ('programs', 'n'), ('viewed', 'v'), ('threshold', 'a'), ('time', 'n'), ('duration', 'n'), ('smart', 'a'), ('tv', 'n'), ('comprising', 'n'), ('means', 'v'), ('adjusting', 'v'), ('least', 'a'), ('one', None), ('volume', 'n'), ('brightness', 'n'), ('smart', 'a'), ('tv', 'n'), ('wherein', 'n'), ('adjusting', 'v'), ('based', 'v'), ('least', 'a'), ('one', None), ('historical', 'a'), ('time', 'n'), ('historical', 'a'), ('date', 'n'), ('smart', 'a'), ('tv', 'n'), ('comprising', 'n'), ('means', 'v'), ('restricting', 'v'), ('access', 'n'), ('one', None), ('media', 'n'), ('programs', 'n'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('medium', 'n'), ('comprising', 'v'), ('-executable', 'a'), ('program', 'n'), ('code', 'n'), ('configured', 'v'), ('cause', 'n'), ('smart', 'a'), ('television', 'n'), ('tv', 'n'), ('receive', 'v'), ('indication', 'n'), ('upcoming', 'a'), ('media', 'n'), ('programming', 'v'), ('wherein', 'n'), ('upcoming', 'a'), ('media', 'n'), ('programming', 'v'), ('based', 'v'), ('profile', 'a'), ('identify', 'v'), ('one', None), ('devices', 'n'), ('communication', 'n'), ('smart', 'a'), ('tv', 'n'), ('one', None), ('devices', 'n'), ('including', 'v'), ('least', 'a'), ('one', None), ('microphone', 'n'), ('camera', 'n'), ('instruct', 'n'), ('least', 'a'), ('one', None), ('identified', 'a'), ('device', 'n'), ('detect', 'n'), ('audio', 'n'), ('signals', 'n'), ('using', 'v'), ('respective', 'a'), ('microphone', 'n'), ('detect', 'a'), ('visual', 'a'), ('signals', 'n'), ('using', 'v'), ('respective', 'a'), ('camera', 'n'), ('select', 'n'), ('least', 'a'), ('one', None), ('device', 'n'), ('one', None), ('devices', 'n'), ('based', 'v'), ('detected', 'a'), ('audio', 'a'), ('signal', 'n'), ('detected', 'v'), ('visual', 'a'), ('signal', 'n'), ('provide', 'n'), ('instructions', 'n'), ('selected', 'v'), ('device', 'n'), ('output', 'n'), ('notification', 'n'), ('related', 'v'), ('upcoming', 'a'), ('media', 'n'), ('programming', 'v'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('medium', 'n'), ('wherein', 'n'), ('selecting', 'v'), ('first', 'a'), ('device', 'n'), ('based', 'v'), ('detected', 'a'), ('audio', 'a'), ('signal', 'n'), ('includes', 'v'), ('voice', 'a'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('medium', 'n'), ('wherein', 'n'), ('capable', 'a'), ('executing', 'v'), ('-executable', 'a'), ('program', 'n'), ('code', 'n'), ('determine', 'n'), ('distance', 'n'), ('recognized', 'v'), ('voice', 'n'), ('wherein', 'n'), ('selecting', 'v'), ('first', 'a'), ('device', 'n'), ('based', 'v'), ('determined', 'a'), ('distance', 'n'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('medium', 'n'), ('wherein', 'n'), ('selecting', 'v'), ('first', 'a'), ('device', 'n'), ('based', 'v'), ('detected', 'v'), ('visual', 'a'), ('signals', 'n'), ('includes', 'v'), ('face', 'v'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('medium', 'n'), ('wherein', 'n'), ('face', 'n'), ('includes', 'v'), ('face', 'n'), ('technique', 'n'), ('camera', 'n'), ('comprising', 'v'), ('sensor', 'a'), ('array', 'n'), ('including', 'v'), ('sensors', 'n'), ('infrared', 'v'), ('ir', 'a'), ('illuminator', 'n'), ('configured', 'v'), ('emit', 'r'), ('active', 'a'), ('ir', 'n'), ('light', 'a'), ('ir', 'a'), ('light', 'a'), ('sub-band', 'a'), ('spectral', 'a'), ('illuminators', 'n'), ('spectral', 'a'), ('illuminator', 'n'), ('configured', 'v'), ('emit', 'r'), ('active', 'a'), ('spectral', 'a'), ('light', 'n'), ('different', 'a'), ('spectral', 'a'), ('light', 'n'), ('sub-band', 'n'), ('depth', 'n'), ('controller', 'n'), ('machine', 'n'), ('configured', 'v'), ('determine', 'a'), ('depth', 'n'), ('value', 'n'), ('sensors', 'n'), ('based', 'v'), ('active', 'a'), ('ir', 'n'), ('light', 'a'), ('spectral', 'a'), ('controller', 'n'), ('machine', 'n'), ('configured', 'v'), ('sensors', 'n'), ('determine', 'v'), ('spectral', 'a'), ('value', 'n'), ('spectral', 'a'), ('light', 'a'), ('sub-band', 'a'), ('spectral', 'a'), ('illuminators', 'n'), ('output', 'n'), ('machine', 'n'), ('configured', 'v'), ('output', 'n'), ('test', 'r'), ('depth+multi-spectral', 'a'), ('including', 'v'), ('pixels', 'n'), ('pixel', 'r'), ('corresponding', 'v'), ('one', None), ('sensors', 'n'), ('sensor', 'v'), ('array', 'a'), ('including', 'v'), ('least', 'a'), ('depth', 'a'), ('value', 'n'), ('spectral', 'a'), ('value', 'n'), ('spectral', 'a'), ('light', 'a'), ('sub-band', 'a'), ('spectral', 'a'), ('illuminators', 'n'), ('face', 'v'), ('machine', 'n'), ('previously', 'r'), ('trained', 'v'), ('set', 'v'), ('labeled', 'a'), ('training', 'n'), ('depth+multi-spectral', 'a'), ('structure', 'n'), ('test', 'n'), ('depth+multi-spectral', 'a'), ('face', 'n'), ('machine', 'n'), ('configured', 'v'), ('output', 'n'), ('confidence', 'n'), ('value', 'n'), ('indicating', 'v'), ('likelihood', 'a'), ('test', 'n'), ('depth+multi-spectral', 'a'), ('includes', 'v'), ('face', 'n'), ('camera', 'n'), ('wherein', 'v'), ('spectral', 'a'), ('value', 'n'), ('calculated', 'v'), ('based', 'v'), ('depth', 'n'), ('value', 'n'), ('determined', 'v'), ('sensor', 'a'), ('corresponds', 'n'), ('pixel', 'v'), ('camera', 'n'), ('wherein', 'n'), ('face', 'n'), ('machine', 'n'), ('configured', 'v'), ('use', None), ('convolutional', 'a'), ('neural', 'a'), ('network', 'n'), ('determine', 'n'), ('confidence', 'n'), ('value', 'n'), ('camera', 'n'), ('wherein', 'a'), ('face', 'n'), ('machine', 'n'), ('includes', 'v'), ('input', 'n'), ('nodes', 'n'), ('wherein', 'v'), ('input', 'a'), ('node', 'r'), ('configured', 'v'), ('receive', 'a'), ('pixel', 'n'), ('value', 'n'), ('array', 'n'), ('corresponding', 'v'), ('different', 'a'), ('pixel', 'n'), ('pixels', 'n'), ('test', 'v'), ('depth+multi-spectral', 'a'), ('wherein', 'n'), ('pixel', 'v'), ('value', 'n'), ('array', 'n'), ('includes', 'v'), ('depth', 'a'), ('value', 'n'), ('multi-spectral', 'a'), ('values', 'n'), ('pixel', 'v'), ('camera', 'n'), ('wherein', 'n'), ('multi-spectral', 'a'), ('values', 'n'), ('pixel', 'v'), ('include', 'v'), ('three', None), ('spectral', 'a'), ('values', 'n'), ('camera', 'v'), ('wherein', 'n'), ('output', 'n'), ('machine', 'n'), ('configured', 'v'), ('output', 'n'), ('surface', 'n'), ('normal', 'a'), ('pixel', 'a'), ('test', 'n'), ('depth+multi-spectral', 'a'), ('wherein', 'n'), ('pixel', 'v'), ('value', 'n'), ('array', 'n'), ('includes', 'v'), ('surface', 'n'), ('normal', 'a'), ('camera', 'n'), ('wherein', 'n'), ('output', 'n'), ('machine', 'n'), ('configured', 'v'), ('output', 'n'), ('curvature', 'n'), ('pixel', 'a'), ('test', 'n'), ('depth+multi-spectral', 'a'), ('wherein', 'n'), ('pixel', 'v'), ('value', 'n'), ('array', 'n'), ('includes', 'v'), ('curvature', 'n'), ('camera', 'n'), ('wherein', 'n'), ('face', 'n'), ('machine', 'n'), ('configured', 'v'), ('use', 'n'), ('models', 'n'), ('determine', 'v'), ('confidence', 'n'), ('value', 'n'), ('wherein', 'n'), ('models', 'n'), ('includes', 'v'), ('channel-specific', 'a'), ('models', 'n'), ('wherein', None), ('channel-specific', 'a'), ('model', 'n'), ('configured', 'v'), ('process', 'a'), ('different', 'a'), ('pixel', 'n'), ('parameter', 'n'), ('pixels', 'n'), ('test', 'v'), ('depth+multi-spectral', 'a'), ('wherein', 'n'), ('channel-specific', 'a'), ('model', 'n'), ('includes', 'v'), ('input', 'a'), ('nodes', 'n'), ('wherein', 'v'), ('channel-specific', 'a'), ('model', 'n'), ('input', 'n'), ('node', 'r'), ('configured', 'v'), ('receive', 'a'), ('pixel', 'n'), ('parameter', 'n'), ('value', 'n'), ('different', 'a'), ('pixel', 'n'), ('pixels', 'n'), ('test', 'v'), ('depth+multi-spectral', 'a'), ('camera', 'n'), ('wherein', 'n'), ('face', 'n'), ('machine', 'n'), ('configured', 'v'), ('use', 'r'), ('statistical', 'a'), ('model', 'n'), ('determine', 'n'), ('confidence', 'n'), ('value', 'n'), ('camera', 'n'), ('wherein', 'v'), ('statistical', 'a'), ('model', 'n'), ('includes', 'v'), ('nearest', 'a'), ('neighbor', 'n'), ('algorithm', 'n'), ('camera', 'n'), ('wherein', 'v'), ('statistical', 'a'), ('model', 'n'), ('includes', 'v'), ('support', 'n'), ('vector', 'n'), ('machine', 'n'), ('camera', 'n'), ('wherein', 'a'), ('face', 'n'), ('machine', 'n'), ('configured', 'v'), ('output', 'n'), ('location', 'n'), ('test', None), ('depth+multi-spectral', 'a'), ('bounding', 'n'), ('box', 'n'), ('around', None), ('recognized', 'v'), ('face', 'n'), ('camera', 'n'), ('wherein', 'a'), ('face', 'n'), ('machine', 'n'), ('configured', 'v'), ('output', 'n'), ('location', 'n'), ('test', None), ('depth+multi-spectral', 'a'), ('identified', 'v'), ('two-dimensional', 'a'), ('facial', 'a'), ('feature', 'n'), ('recognized', 'v'), ('face', 'n'), ('camera', 'n'), ('wherein', 'a'), ('face', 'n'), ('machine', 'n'), ('configured', 'v'), ('output', 'n'), ('location', 'n'), ('test', None), ('depth+multi-spectral', 'a'), ('identified', 'v'), ('three-dimensional', 'a'), ('facial', 'a'), ('feature', 'n'), ('recognized', 'v'), ('face', 'n'), ('camera', 'n'), ('wherein', 'a'), ('face', 'n'), ('machine', 'n'), ('configured', 'v'), ('output', 'n'), ('location', 'n'), ('test', None), ('depth+multi-spectral', 'a'), ('identified', 'a'), ('spectral', 'a'), ('feature', 'n'), ('recognized', 'v'), ('face', 'n'), ('camera', 'n'), ('wherein', 'a'), ('face', 'n'), ('machine', 'n'), ('configured', 'v'), ('output', 'n'), ('pixel', 'a'), ('test', 'n'), ('depth+multi-spectral', 'a'), ('confidence', 'n'), ('value', 'n'), ('indicating', 'v'), ('likelihood', 'n'), ('pixel', 'n'), ('included', 'v'), ('face', 'n'), ('camera', 'n'), ('wherein', 'a'), ('face', 'n'), ('machine', 'n'), ('configured', 'v'), ('output', 'n'), ('identity', 'n'), ('face', 'n'), ('recognized', 'v'), ('test', None), ('depth+multi-spectral', 'a'), ('camera', 'n'), ('wherein', 'n'), ('sensors', 'n'), ('sensor', 'v'), ('array', 'a'), ('differential', 'a'), ('sensors', 'n'), ('wherein', 'v'), ('spectral', 'a'), ('value', 'n'), ('determined', 'v'), ('based', 'v'), ('depth', 'n'), ('value', 'n'), ('differential', 'a'), ('measurement', 'a'), ('differential', 'n'), ('sensor', 'n'), ('camera', 'n'), ('comprising', 'v'), ('sensor', 'a'), ('array', 'n'), ('including', 'v'), ('sensors', 'n'), ('infrared', 'v'), ('ir', 'a'), ('illuminator', 'n'), ('configured', 'v'), ('emit', 'r'), ('active', 'a'), ('ir', 'n'), ('light', 'a'), ('ir', 'a'), ('light', 'a'), ('sub-band', 'a'), ('spectral', 'a'), ('illuminators', 'n'), ('spectral', 'a'), ('illuminator', 'n'), ('configured', 'v'), ('emit', 'r'), ('active', 'a'), ('spectral', 'a'), ('light', 'n'), ('different', 'a'), ('spectral', 'a'), ('light', 'n'), ('sub-band', 'n'), ('depth', 'n'), ('controller', 'n'), ('machine', 'n'), ('configured', 'v'), ('determine', 'a'), ('depth', 'n'), ('value', 'n'), ('sensors', 'n'), ('based', 'v'), ('active', 'a'), ('ir', 'n'), ('light', 'a'), ('spectral', 'a'), ('controller', 'n'), ('machine', 'n'), ('configured', 'v'), ('sensors', 'n'), ('determine', 'v'), ('spectral', 'a'), ('value', 'n'), ('spectral', 'a'), ('light', 'a'), ('sub-band', 'a'), ('spectral', 'a'), ('illuminators', 'n'), ('wherein', 'v'), ('spectral', 'a'), ('value', 'n'), ('calculated', 'v'), ('based', 'v'), ('depth', 'n'), ('value', 'n'), ('determined', 'v'), ('sensor', 'a'), ('corresponds', 'n'), ('pixel', 'a'), ('output', 'n'), ('machine', 'n'), ('configured', 'v'), ('output', 'n'), ('test', 'r'), ('depth+multi-spectral', 'a'), ('including', 'v'), ('pixels', 'n'), ('pixel', 'r'), ('corresponding', 'v'), ('one', None), ('sensors', 'n'), ('sensor', 'v'), ('array', 'a'), ('including', 'v'), ('least', 'a'), ('depth', 'a'), ('value', 'n'), ('spectral', 'a'), ('value', 'n'), ('spectral', 'a'), ('light', 'a'), ('sub-band', 'a'), ('spectral', 'a'), ('illuminators', 'n'), ('face', 'v'), ('machine', 'n'), ('including', 'v'), ('convolutional', 'a'), ('neural', 'a'), ('network', 'n'), ('previously', 'r'), ('trained', 'v'), ('set', 'v'), ('labeled', 'a'), ('training', 'n'), ('depth+multi-spectral', 'a'), ('structure', 'n'), ('test', 'n'), ('depth+multi-spectral', 'a'), ('face', 'n'), ('machine', 'n'), ('configured', 'v'), ('output', 'n'), ('confidence', 'n'), ('value', 'n'), ('indicating', 'v'), ('likelihood', 'a'), ('test', 'n'), ('depth+multi-spectral', 'a'), ('includes', 'v'), ('face', 'n'), ('processing', 'v'), ('comprising', 'v'), ('acquiring', 'v'), ('photo', 'n'), ('album', 'n'), ('obtained', 'v'), ('face', 'n'), ('clustering', 'v'), ('collecting', 'v'), ('face', 'n'), ('information', 'n'), ('respective', 'a'), ('photo', 'n'), ('album', 'n'), ('acquiring', 'v'), ('face', 'n'), ('parameter', 'n'), ('according', 'v'), ('face', 'n'), ('information', 'n'), ('selecting', 'v'), ('cover', 'n'), ('according', 'v'), ('face', 'n'), ('parameter', 'n'), ('taking', 'v'), ('face-region', 'a'), ('cover', 'n'), ('setting', 'v'), ('face-region', 'a'), ('cover', 'n'), ('photo', 'n'), ('album', 'a'), ('wherein', 'n'), ('selecting', 'v'), ('cover', 'n'), ('according', 'v'), ('face', 'n'), ('parameter', 'n'), ('comprises', 'v'), ('performing', 'v'), ('calculation', 'n'), ('face', 'n'), ('parameter', 'n'), ('preset', 'v'), ('way', 'n'), ('obtain', 'v'), ('cover', 'a'), ('score', 'n'), ('selecting', 'v'), ('highest', 'a'), ('cover', 'n'), ('score', 'n'), ('cover', 'n'), ('wherein', 'n'), ('selecting', 'v'), ('highest', 'a'), ('cover', 'n'), ('score', 'n'), ('cover', 'a'), ('comprises', 'v'), ('acquiring', 'v'), ('source', 'n'), ('selecting', 'v'), ('highest', 'a'), ('cover', 'n'), ('score', 'n'), ('coming', 'v'), ('preset', 'v'), ('source', 'n'), ('cover', 'n'), ('according', 'v'), ('wherein', 'a'), ('selecting', 'v'), ('highest', 'a'), ('cover', 'n'), ('score', 'n'), ('cover', 'a'), ('comprises', 'v'), ('acquiring', 'v'), ('number', 'n'), ('contained', 'v'), ('determining', 'v'), ('single-person', 'a'), ('according', 'v'), ('number', 'n'), ('selecting', 'v'), ('single-person', 'a'), ('highest', 'a'), ('cover', 'n'), ('score', 'n'), ('cover', 'n'), ('according', 'v'), ('wherein', 'a'), ('selecting', 'v'), ('highest', 'a'), ('cover', 'n'), ('score', 'n'), ('cover', 'a'), ('comprises', 'v'), ('single-person', 'a'), ('photo', 'n'), ('album', 'n'), ('determining', 'v'), ('including', 'v'), ('two', None), ('photo', 'n'), ('album', None), ('selecting', 'v'), ('highest', 'a'), ('cover', 'n'), ('score', 'n'), ('including', 'v'), ('two', None), ('cover', 'n'), ('according', 'v'), ('wherein', 'a'), ('face', 'n'), ('information', 'n'), ('comprises', 'v'), ('face', 'n'), ('feature', 'n'), ('points', 'n'), ('face', 'v'), ('parameter', 'n'), ('comprises', 'n'), ('face', 'v'), ('turning', 'v'), ('angle', 'r'), ('acquiring', 'v'), ('face', 'n'), ('parameter', 'n'), ('according', 'v'), ('face', 'n'), ('information', 'n'), ('comprises', 'v'), ('acquiring', 'v'), ('coordinate', 'n'), ('values', 'n'), ('face', 'v'), ('feature', 'n'), ('points', 'n'), ('determining', 'v'), ('distances', 'n'), ('angles', 'n'), ('face', 'v'), ('feature', 'n'), ('points', 'n'), ('determining', 'v'), ('face', 'n'), ('turning', 'v'), ('angle', 'r'), ('according', 'v'), ('distances', 'n'), ('angles', 'n'), ('according', 'v'), ('wherein', 'a'), ('face', 'n'), ('parameter', 'n'), ('comprises', 'v'), ('face', 'v'), ('ratio', 'n'), ('acquiring', 'v'), ('face', 'n'), ('parameter', 'n'), ('according', 'v'), ('face', 'n'), ('information', 'n'), ('comprises', 'v'), ('determining', 'v'), ('face', 'n'), ('region', 'n'), ('according', 'v'), ('face', 'n'), ('information', 'n'), ('calculating', 'v'), ('ratio', 'n'), ('area', 'n'), ('face', 'n'), ('region', 'n'), ('area', 'n'), ('obtain', 'v'), ('face', 'n'), ('ratio', 'n'), ('according', 'v'), ('wherein', 'n'), ('calculating', 'v'), ('face', 'n'), ('ratio', 'n'), ('comprises', 'v'), ('one', None), ('face', 'n'), ('subtracting', 'v'), ('area', 'n'), ('occupied', 'v'), ('face', 'n'), ('corresponding', 'v'), ('photo', 'n'), ('album', 'n'), ('face', 'n'), ('region', 'n'), ('obtain', 'v'), ('remaining', 'v'), ('area', 'n'), ('calculating', 'v'), ('ratio', 'n'), ('remaining', 'v'), ('area', 'n'), ('area', 'n'), ('obtain', 'v'), ('face', 'n'), ('ratio', 'n'), ('according', 'v'), ('wherein', 'n'), ('collecting', 'v'), ('face', 'n'), ('information', 'n'), ('respective', 'a'), ('photo', 'n'), ('album', 'n'), ('comprises', 'v'), ('acquiring', 'v'), ('identifications', 'n'), ('photo', 'v'), ('album', None), ('extracting', 'v'), ('face', 'n'), ('information', 'n'), ('corresponding', 'v'), ('identifications', 'n'), ('face', 'v'), ('database', 'a'), ('face', 'n'), ('database', 'n'), ('stored', 'v'), ('face', 'n'), ('results', 'n'), ('face', 'v'), ('results', 'n'), ('including', 'v'), ('face', 'n'), ('information', 'n'), ('processing', 'n'), ('apparatus', 'n'), ('comprising', 'v'), ('memory', 'n'), ('configured', 'v'), ('store', 'n'), ('instructions', 'n'), ('executable', 'a'), ('wherein', 'n'), ('configured', 'v'), ('run', 'v'), ('program', 'n'), ('corresponding', 'n'), ('instructions', 'n'), ('reading', 'v'), ('instructions', 'n'), ('stored', 'v'), ('memory', 'n'), ('perform', 'n'), ('acquiring', 'v'), ('photo', 'n'), ('album', 'n'), ('obtained', 'v'), ('face', 'n'), ('clustering', 'v'), ('collecting', 'v'), ('face', 'n'), ('information', 'n'), ('photo', 'n'), ('album', 'n'), ('acquiring', 'v'), ('face', 'n'), ('parameter', 'n'), ('according', 'v'), ('face', 'n'), ('information', 'n'), ('selecting', 'v'), ('cover', 'n'), ('according', 'v'), ('face', 'n'), ('parameter', 'n'), ('taking', 'v'), ('face-region', 'a'), ('cover', 'n'), ('setting', 'v'), ('face-region', 'a'), ('cover', 'n'), ('photo', 'n'), ('album', 'a'), ('wherein', 'n'), ('configured', 'v'), ('perform', 'a'), ('calculation', 'n'), ('face', 'n'), ('parameter', 'n'), ('preset', 'v'), ('way', 'n'), ('obtain', 'v'), ('cover', 'a'), ('score', 'n'), ('select', 'a'), ('highest', 'a'), ('cover', 'n'), ('score', 'n'), ('cover', 'n'), ('wherein', 'n'), ('configured', 'v'), ('acquire', 'v'), ('source', 'n'), ('select', 'n'), ('highest', 'a'), ('cover', 'n'), ('score', 'n'), ('coming', 'v'), ('preset', 'v'), ('source', 'n'), ('cover', 'n'), ('apparatus', 'n'), ('according', 'v'), ('wherein', 'n'), ('configured', 'v'), ('acquire', 'v'), ('number', 'n'), ('contained', 'v'), ('determine', 'a'), ('single-person', 'a'), ('according', 'v'), ('number', 'n'), ('select', 'a'), ('single-person', 'a'), ('highest', 'a'), ('cover', 'n'), ('score', 'n'), ('cover', 'n'), ('apparatus', 'n'), ('according', 'v'), ('wherein', 'n'), ('configured', 'v'), ('single-person', 'a'), ('photo', 'n'), ('album', 'n'), ('determine', 'n'), ('including', 'v'), ('two', None), ('photo', 'n'), ('album', 'n'), ('select', 'a'), ('highest', 'a'), ('cover', 'n'), ('score', 'n'), ('including', 'v'), ('two', None), ('cover', 'n'), ('apparatus', 'n'), ('according', 'v'), ('wherein', 'a'), ('face', 'n'), ('information', 'n'), ('comprises', 'v'), ('face', 'n'), ('feature', 'n'), ('points', 'n'), ('face', 'v'), ('parameter', 'n'), ('comprises', 'n'), ('face', 'v'), ('turning', 'v'), ('angle', 'n'), ('configured', 'v'), ('acquire', 'v'), ('coordinate', 'n'), ('values', 'n'), ('face', 'v'), ('feature', 'n'), ('points', 'n'), ('determine', 'a'), ('distances', 'n'), ('angles', 'n'), ('face', 'v'), ('feature', 'n'), ('points', 'n'), ('determine', 'a'), ('face', 'n'), ('turning', 'v'), ('angle', 'r'), ('according', 'v'), ('distances', 'n'), ('angles', 'n'), ('apparatus', 'v'), ('according', 'v'), ('wherein', 'n'), ('face', 'n'), ('parameter', 'n'), ('comprises', 'v'), ('face', 'v'), ('ratio', 'n'), ('configured', 'v'), ('determine', 'a'), ('face', 'n'), ('region', 'n'), ('according', 'v'), ('face', 'n'), ('information', 'n'), ('calculate', 'n'), ('ratio', 'n'), ('area', 'n'), ('face', 'n'), ('region', 'n'), ('area', 'n'), ('obtain', 'v'), ('face', 'n'), ('ratio', 'n'), ('apparatus', None), ('according', 'v'), ('wherein', 'n'), ('configured', 'v'), ('one', None), ('face', 'n'), ('subtract', 'a'), ('area', 'n'), ('occupied', 'v'), ('face', 'n'), ('corresponding', 'v'), ('photo', 'n'), ('album', 'n'), ('face', 'n'), ('region', 'n'), ('obtain', 'v'), ('remaining', 'v'), ('area', 'n'), ('calculate', 'n'), ('ratio', 'n'), ('remaining', 'v'), ('area', 'n'), ('area', 'n'), ('obtain', 'v'), ('face', 'n'), ('ratio', 'n'), ('apparatus', None), ('according', 'v'), ('wherein', 'n'), ('configured', 'v'), ('acquire', 'v'), ('identifications', 'n'), ('photo', 'v'), ('album', 'a'), ('extract', 'a'), ('face', 'n'), ('information', 'n'), ('corresponding', 'v'), ('identifications', 'n'), ('face', 'v'), ('database', 'a'), ('face', 'n'), ('database', 'n'), ('stored', 'v'), ('face', 'n'), ('results', 'n'), ('face', 'v'), ('results', 'n'), ('including', 'v'), ('face', 'n'), ('information', 'n'), ('comprising', 'v'), ('memory', 'n'), ('display', 'n'), ('screen', 'n'), ('input', 'n'), ('device', 'n'), ('connected', 'v'), ('via', None), ('system', 'n'), ('bus', 'a'), ('wherein', 'a'), ('memory', 'n'), ('stored', 'v'), ('computer', 'n'), ('programs', 'n'), ('executed', 'v'), ('cause', 'n'), ('implement', 'n'), ('processing', 'v'), ('processing', 'v'), ('comprising', 'v'), ('acquiring', 'v'), ('photo', 'n'), ('album', 'n'), ('obtained', 'v'), ('face', 'n'), ('clustering', 'v'), ('collecting', 'v'), ('face', 'n'), ('information', 'n'), ('respective', 'a'), ('photo', 'n'), ('album', 'n'), ('acquiring', 'v'), ('face', 'n'), ('parameter', 'n'), ('according', 'v'), ('face', 'n'), ('information', 'n'), ('selecting', 'v'), ('cover', 'n'), ('according', 'v'), ('face', 'n'), ('parameter', 'n'), ('taking', 'v'), ('face-region', 'a'), ('cover', 'n'), ('setting', 'v'), ('face-region', 'a'), ('cover', 'n'), ('photo', 'n'), ('album', 'a'), ('wherein', 'n'), ('selecting', 'v'), ('cover', 'n'), ('according', 'v'), ('face', 'n'), ('parameter', 'n'), ('comprises', 'v'), ('performing', 'v'), ('calculation', 'n'), ('face', 'n'), ('parameter', 'n'), ('preset', 'v'), ('way', 'n'), ('obtain', 'v'), ('cover', 'a'), ('score', 'n'), ('selecting', 'v'), ('highest', 'a'), ('cover', 'n'), ('score', 'n'), ('cover', 'n'), ('wherein', 'n'), ('selecting', 'v'), ('highest', 'a'), ('cover', 'n'), ('score', 'n'), ('cover', 'a'), ('comprises', 'v'), ('acquiring', 'v'), ('source', 'n'), ('selecting', 'v'), ('highest', 'a'), ('cover', 'n'), ('score', 'n'), ('coming', 'v'), ('preset', 'v'), ('source', 'n'), ('cover', 'n'), ('according', 'v'), ('wherein', 'a'), ('comprises', 'n'), ('least', 'a'), ('one', None), ('mobile', 'a'), ('phone', 'n'), ('tablet', 'n'), ('computer', 'n'), ('personal', 'a'), ('digital', 'n'), ('assistant', 'n'), ('wearable', 'a'), ('device', 'n'), ('computer-implemented', 'a'), ('comprising', 'n'), ('receiving', 'v'), ('computing', 'v'), ('device', 'n'), ('meeting', 'n'), ('invitation', 'n'), ('identifying', 'v'), ('location', 'n'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('meeting', 'n'), ('invitation', 'n'), ('configured', 'v'), ('provide', None), ('least', 'a'), ('one', None), ('invitee', 'n'), ('physical', 'a'), ('access', 'n'), ('location', 'n'), ('wherein', 'n'), ('meeting', 'n'), ('invitation', 'n'), ('causes', 'v'), ('system', 'n'), ('control', 'n'), ('pathway', 'r'), ('allowing', 'v'), ('physical', 'a'), ('access', 'n'), ('location', 'n'), ('providing', 'v'), ('based', 'v'), ('meeting', 'v'), ('invitation', 'n'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('physical', 'a'), ('access', 'n'), ('location', 'n'), ('controlling', 'v'), ('pathway', 'r'), ('allowing', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('physically', 'r'), ('access', 'n'), ('location', 'n'), ('pathway', 'n'), ('response', 'n'), ('positioning', 'v'), ('data', 'n'), ('indicating', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('predetermined', 'v'), ('location', 'n'), ('near', None), ('location', 'n'), ('wherein', 'n'), ('positioning', 'v'), ('data', 'n'), ('based', 'v'), ('part', 'n'), ('face', 'n'), ('camera', 'n'), ('system', 'n'), ('identifying', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('receiving', 'v'), ('positioning', 'v'), ('data', 'n'), ('face', 'n'), ('camera', 'n'), ('system', 'n'), ('identifying', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('wherein', 'n'), ('positioning', 'v'), ('data', 'n'), ('indicates', 'v'), ('pattern', 'a'), ('movement', 'n'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('determining', 'v'), ('pattern', 'a'), ('movement', 'n'), ('indicates', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('exited', 'v'), ('location', 'n'), ('revoking', 'v'), ('physical', 'a'), ('access', 'n'), ('location', 'n'), ('identified', 'v'), ('meeting', 'v'), ('invitation', 'n'), ('controlling', 'v'), ('pathway', 'r'), ('restrict', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('identified', 'v'), ('meeting', 'v'), ('invitation', 'n'), ('physical', 'a'), ('access', 'n'), ('location', 'n'), ('pathway', 'n'), ('response', 'n'), ('determining', 'v'), ('pattern', 'a'), ('movement', 'n'), ('indicates', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('exited', 'v'), ('location', 'n'), ('computer-implemented', 'a'), ('wherein', 'n'), ('determining', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('exited', 'v'), ('location', 'n'), ('comprises', 'v'), ('determining', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('passed', 'v'), ('egress', 'r'), ('associated', 'v'), ('location', 'n'), ('predetermined', 'v'), ('direction', 'n'), ('computer-implemented', 'a'), ('wherein', 'n'), ('determining', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('exited', 'v'), ('location', 'n'), ('comprises', 'v'), ('determining', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('moved', 'v'), ('area', 'n'), ('predetermined', 'v'), ('direction', 'n'), ('computer-implemented', 'a'), ('wherein', 'n'), ('positioning', 'v'), ('data', 'n'), ('indicates', 'v'), ('second', 'a'), ('pattern', 'a'), ('movement', 'n'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('wherein', 'n'), ('access', 'n'), ('secured', 'v'), ('data', 'n'), ('associated', 'v'), ('location', 'n'), ('provided', 'v'), ('response', 'n'), ('detecting', 'v'), ('second', 'a'), ('pattern', 'a'), ('movement', 'n'), ('computer-implemented', 'a'), ('comprising', 'v'), ('collating', 'n'), ('secured', 'v'), ('data', 'n'), ('public', 'a'), ('data', 'n'), ('generate', 'v'), ('resource', 'n'), ('data', 'n'), ('communicating', 'v'), ('resource', 'n'), ('data', 'n'), ('client', 'n'), ('computing', 'v'), ('device', 'n'), ('associated', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('access', 'n'), ('location', 'n'), ('provided', 'v'), ('computer-implemented', 'a'), ('wherein', 'n'), ('positioning', 'v'), ('data', 'n'), ('indicates', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('predetermined', 'v'), ('location', 'n'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('passes', 'n'), ('predetermined', 'v'), ('location', 'n'), ('computer-implemented', 'a'), ('wherein', 'n'), ('positioning', 'v'), ('data', 'n'), ('indicates', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('predetermined', 'v'), ('location', 'n'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('passes', 'n'), ('predetermined', 'v'), ('location', 'n'), ('near', None), ('location', 'n'), ('predetermined', 'v'), ('direction', 'n'), ('system', 'n'), ('comprising', 'v'), ('memory', 'n'), ('communication', 'n'), ('memory', 'n'), ('computer-readable', 'a'), ('instructions', 'n'), ('stored', 'v'), ('thereupon', 'r'), ('executed', 'v'), ('cause', 'n'), ('receive', 'a'), ('meeting', 'n'), ('invitation', 'n'), ('indicating', 'v'), ('location', 'n'), ('identity', 'n'), ('meeting', 'n'), ('invitation', 'n'), ('configured', 'v'), ('provide', None), ('least', 'a'), ('one', None), ('invitee', 'n'), ('physical', 'a'), ('access', 'n'), ('location', 'n'), ('wherein', 'n'), ('meeting', 'n'), ('invitation', 'n'), ('causes', 'v'), ('system', 'n'), ('control', 'n'), ('pathway', 'r'), ('allowing', 'v'), ('physical', 'a'), ('access', 'n'), ('location', 'n'), ('provide', None), ('least', 'a'), ('one', None), ('invitee', 'n'), ('associated', 'v'), ('identity', 'n'), ('access', 'n'), ('location', 'n'), ('controlling', 'v'), ('pathway', 'r'), ('allowing', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('physically', 'r'), ('access', 'n'), ('location', 'n'), ('pathway', 'n'), ('response', 'n'), ('positioning', 'v'), ('data', 'n'), ('indicating', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('predetermined', 'v'), ('location', 'n'), ('near', None), ('location', 'n'), ('wherein', 'n'), ('positioning', 'v'), ('data', 'n'), ('based', 'v'), ('part', 'n'), ('face', 'n'), ('camera', 'n'), ('system', 'n'), ('identifying', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('receive', 'v'), ('positioning', 'v'), ('data', 'n'), ('face', 'n'), ('camera', 'n'), ('system', 'n'), ('identifying', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('wherein', 'n'), ('positioning', 'v'), ('data', 'n'), ('indicates', 'v'), ('pattern', 'a'), ('movement', 'n'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('determine', 'n'), ('pattern', 'a'), ('movement', 'n'), ('indicates', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('exited', 'v'), ('location', 'n'), ('revoke', 'v'), ('physical', 'a'), ('access', 'n'), ('location', 'n'), ('identified', 'v'), ('meeting', 'v'), ('invitation', 'n'), ('controlling', 'v'), ('pathway', 'r'), ('restrict', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('identified', 'v'), ('meeting', 'v'), ('invitation', 'n'), ('physical', 'a'), ('access', 'n'), ('location', 'n'), ('pathway', 'n'), ('response', 'n'), ('determining', 'v'), ('pattern', 'a'), ('movement', 'n'), ('indicates', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('exited', 'v'), ('location', 'n'), ('system', 'n'), ('wherein', 'v'), ('determining', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('exited', 'v'), ('location', 'n'), ('comprises', 'v'), ('determining', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('passed', 'v'), ('egress', 'r'), ('associated', 'v'), ('location', 'n'), ('system', 'n'), ('wherein', 'v'), ('determining', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('exited', 'v'), ('location', 'n'), ('comprises', 'v'), ('determining', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('moved', 'v'), ('area', 'n'), ('predetermined', 'v'), ('direction', 'n'), ('system', 'n'), ('wherein', 'v'), ('positioning', 'v'), ('data', 'n'), ('indicates', 'v'), ('second', 'a'), ('pattern', 'a'), ('movement', 'n'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('wherein', 'n'), ('access', 'n'), ('secured', 'v'), ('data', 'n'), ('associated', 'v'), ('location', 'n'), ('provided', 'v'), ('response', 'n'), ('detecting', 'v'), ('second', 'a'), ('pattern', 'a'), ('movement', 'n'), ('system', 'n'), ('wherein', 'a'), ('instructions', 'n'), ('cause', 'v'), ('collate', 'n'), ('secured', 'v'), ('data', 'n'), ('public', 'a'), ('data', 'n'), ('generate', 'v'), ('resource', 'n'), ('data', 'n'), ('communicate', 'v'), ('resource', 'n'), ('data', 'n'), ('client', 'n'), ('computing', 'v'), ('device', 'n'), ('associated', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('access', 'n'), ('location', 'n'), ('provided', 'v'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('computer-executable', 'a'), ('instructions', 'n'), ('stored', 'v'), ('thereupon', 'r'), ('executed', 'v'), ('one', None), ('computing', 'v'), ('device', 'n'), ('cause', 'n'), ('one', None), ('computing', 'n'), ('device', 'n'), ('receive', 'v'), ('meeting', 'n'), ('invitation', 'n'), ('indicating', 'v'), ('location', 'n'), ('identity', 'n'), ('meeting', 'n'), ('invitation', 'n'), ('configured', 'v'), ('provide', None), ('least', 'a'), ('one', None), ('invitee', 'n'), ('physical', 'a'), ('access', 'n'), ('location', 'n'), ('wherein', 'n'), ('meeting', 'n'), ('invitation', 'n'), ('causes', 'v'), ('system', 'n'), ('control', 'n'), ('pathway', 'r'), ('allowing', 'v'), ('physical', 'a'), ('access', 'n'), ('location', 'n'), ('provide', None), ('least', 'a'), ('one', None), ('invitee', 'n'), ('associated', 'v'), ('identity', 'n'), ('access', 'n'), ('location', 'n'), ('controlling', 'v'), ('pathway', 'r'), ('allowing', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('physically', 'r'), ('access', 'n'), ('location', 'n'), ('pathway', 'n'), ('response', 'n'), ('positioning', 'v'), ('data', 'n'), ('indicating', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('predetermined', 'v'), ('location', 'n'), ('near', None), ('location', 'n'), ('wherein', 'n'), ('positioning', 'v'), ('data', 'n'), ('based', 'v'), ('part', 'n'), ('face', 'n'), ('camera', 'n'), ('system', 'n'), ('identifying', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('receive', 'v'), ('positioning', 'v'), ('data', 'n'), ('face', 'n'), ('camera', 'n'), ('system', 'n'), ('identifying', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('wherein', 'n'), ('positioning', 'v'), ('data', 'n'), ('indicates', 'v'), ('pattern', 'a'), ('movement', 'n'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('determine', 'n'), ('pattern', 'a'), ('movement', 'n'), ('indicates', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('exited', 'v'), ('location', 'n'), ('revoke', 'v'), ('physical', 'a'), ('access', 'n'), ('location', 'n'), ('identified', 'v'), ('meeting', 'v'), ('invitation', 'n'), ('controlling', 'v'), ('pathway', 'r'), ('restrict', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('identified', 'v'), ('meeting', 'v'), ('invitation', 'n'), ('physical', 'a'), ('access', 'n'), ('location', 'n'), ('pathway', 'n'), ('response', 'n'), ('determining', 'v'), ('pattern', 'a'), ('movement', 'n'), ('indicates', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('exited', 'v'), ('location', 'n'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('wherein', 'n'), ('determining', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('exited', 'v'), ('location', 'n'), ('comprises', 'v'), ('determining', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('passed', 'v'), ('egress', 'r'), ('associated', 'v'), ('location', 'n'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('wherein', 'n'), ('positioning', 'v'), ('data', 'n'), ('indicates', 'v'), ('second', 'a'), ('pattern', 'a'), ('movement', 'n'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('wherein', 'n'), ('access', 'n'), ('secured', 'v'), ('data', 'n'), ('associated', 'v'), ('location', 'n'), ('provided', 'v'), ('response', 'n'), ('detecting', 'v'), ('second', 'a'), ('pattern', 'a'), ('movement', 'n'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('wherein', 'n'), ('instructions', 'n'), ('cause', 'v'), ('one', None), ('collate', 'n'), ('secured', 'v'), ('data', 'n'), ('public', 'a'), ('data', 'n'), ('generate', 'v'), ('resource', 'n'), ('data', 'n'), ('communicate', 'v'), ('resource', 'n'), ('data', 'n'), ('client', 'n'), ('computing', 'v'), ('device', 'n'), ('associated', 'v'), ('least', 'a'), ('one', None), ('invitee', 'n'), ('access', 'n'), ('location', 'n'), ('provided', 'v'), ('comprising', 'v'), ('receiving', 'v'), ('piece', 'n'), ('content', 'a'), ('salient', 'n'), ('data', 'n'), ('piece', 'n'), ('content', 'n'), ('based', 'v'), ('salient', 'a'), ('data', 'n'), ('determining', 'v'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('piece', 'n'), ('content', 'n'), ('wherein', 'n'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('includes', 'v'), ('different', 'a'), ('salient', 'a'), ('events', 'n'), ('occurring', 'v'), ('piece', 'n'), ('content', 'n'), ('different', 'a'), ('times', 'n'), ('playback', 'v'), ('piece', 'n'), ('content', 'n'), ('providing', 'v'), ('viewport', 'n'), ('display', 'n'), ('device', 'n'), ('wherein', 'v'), ('movement', 'n'), ('viewport', 'n'), ('based', 'v'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('salient', 'n'), ('data', 'n'), ('playback', 'n'), ('detecting', 'v'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('piece', 'n'), ('content', 'n'), ('included', 'v'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('providing', 'v'), ('indication', 'a'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('viewport', 'n'), ('playback', 'n'), ('wherein', 'n'), ('salient', 'n'), ('data', 'n'), ('identifies', 'n'), ('salient', 'a'), ('event', 'n'), ('piece', 'n'), ('content', 'a'), ('salient', 'n'), ('data', 'n'), ('indicates', 'v'), ('salient', 'a'), ('event', 'n'), ('piece', 'n'), ('content', 'n'), ('corresponding', 'v'), ('point', 'n'), ('location', 'n'), ('salient', 'a'), ('event', 'n'), ('piece', 'n'), ('content', 'n'), ('corresponding', 'v'), ('time', 'n'), ('salient', 'a'), ('event', 'n'), ('occurs', 'v'), ('playback', 'r'), ('wherein', 'a'), ('salient', 'n'), ('data', 'n'), ('indicates', 'v'), ('salient', 'a'), ('event', 'n'), ('piece', 'n'), ('content', 'n'), ('corresponding', 'v'), ('type', 'a'), ('salient', 'a'), ('event', 'n'), ('corresponding', 'v'), ('strength', 'n'), ('value', 'n'), ('salient', 'a'), ('event', 'n'), ('wherein', 'n'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('controls', 'n'), ('movement', 'n'), ('viewport', 'n'), ('put', 'v'), ('different', 'a'), ('salient', 'a'), ('events', 'n'), ('view', 'v'), ('viewport', 'r'), ('different', 'a'), ('times', 'n'), ('playback', 'v'), ('comprising', 'v'), ('detecting', 'v'), ('one', None), ('salient', 'n'), ('events', 'n'), ('piece', 'v'), ('content', 'n'), ('based', 'v'), ('least', 'a'), ('one', None), ('following', 'v'), ('visual', 'a'), ('data', 'n'), ('piece', 'n'), ('content', 'n'), ('audio', 'n'), ('data', 'n'), ('piece', 'n'), ('content', 'n'), ('content', 'a'), ('consumption', 'n'), ('experience', 'n'), ('data', 'n'), ('piece', 'n'), ('content', 'n'), ('wherein', None), ('salient', 'n'), ('data', 'n'), ('indicative', 'a'), ('salient', 'a'), ('event', 'n'), ('detected', 'v'), ('comprising', 'v'), ('detecting', 'v'), ('one', None), ('salient', 'n'), ('events', 'n'), ('piece', 'v'), ('content', 'n'), ('based', 'v'), ('least', 'a'), ('one', None), ('following', 'v'), ('face', 'n'), ('facial', 'a'), ('emotion', 'n'), ('object', 'a'), ('motion', 'n'), ('metadata', 'n'), ('piece', 'n'), ('content', 'n'), ('wherein', None), ('salient', 'n'), ('data', 'n'), ('indicative', 'a'), ('salient', 'a'), ('event', 'n'), ('detected', 'v'), ('comprising', 'v'), ('detecting', 'v'), ('interaction', 'n'), ('indication', 'n'), ('wherein', None), ('indication', 'n'), ('comprises', 'v'), ('interactive', 'a'), ('hint', 'n'), ('response', 'n'), ('detecting', 'v'), ('interaction', 'n'), ('adapting', 'v'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('second', 'a'), ('path', 'n'), ('viewport', 'n'), ('based', 'v'), ('interaction', 'n'), ('wherein', None), ('second', 'a'), ('path', 'n'), ('viewport', 'n'), ('includes', 'v'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('providing', 'v'), ('updated', 'a'), ('viewport', 'n'), ('piece', 'n'), ('content', 'n'), ('display', 'n'), ('device', 'n'), ('wherein', 'v'), ('movement', 'n'), ('updated', 'v'), ('viewport', 'n'), ('based', 'v'), ('second', 'a'), ('path', 'n'), ('viewport', 'n'), ('salient', 'n'), ('data', 'n'), ('playback', 'v'), ('second', 'a'), ('path', 'n'), ('viewport', 'n'), ('controls', 'n'), ('movement', 'n'), ('updated', 'v'), ('viewport', 'n'), ('put', 'v'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('view', 'n'), ('updated', 'v'), ('viewport', 'n'), ('comprising', 'v'), ('changing', 'v'), ('weight', 'n'), ('assigned', 'v'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('one', None), ('salient', 'n'), ('events', 'n'), ('piece', 'v'), ('content', 'a'), ('type', 'a'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('wherein', 'a'), ('second', 'a'), ('path', 'n'), ('viewport', 'n'), ('includes', 'v'), ('one', None), ('salient', 'n'), ('events', 'n'), ('piece', 'v'), ('content', 'a'), ('type', 'a'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('system', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('non-transitory', 'a'), ('-readable', 'a'), ('memory', 'n'), ('device', 'n'), ('storing', 'v'), ('instructions', 'n'), ('executed', 'v'), ('least', 'a'), ('one', None), ('causes', 'v'), ('least', 'a'), ('one', None), ('perform', 'n'), ('operations', 'n'), ('including', 'v'), ('receiving', 'v'), ('piece', 'n'), ('content', 'a'), ('salient', 'n'), ('data', 'n'), ('piece', 'n'), ('content', 'n'), ('based', 'v'), ('salient', 'a'), ('data', 'n'), ('determining', 'v'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('piece', 'n'), ('content', 'n'), ('wherein', 'n'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('includes', 'v'), ('different', 'a'), ('salient', 'a'), ('events', 'n'), ('occurring', 'v'), ('piece', 'n'), ('content', 'n'), ('different', 'a'), ('times', 'n'), ('playback', 'v'), ('piece', 'n'), ('content', 'n'), ('providing', 'v'), ('viewport', 'n'), ('display', 'n'), ('device', 'n'), ('wherein', 'v'), ('movement', 'n'), ('viewport', 'n'), ('based', 'v'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('salient', 'n'), ('data', 'n'), ('playback', 'n'), ('detecting', 'v'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('piece', 'n'), ('content', 'n'), ('included', 'v'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('providing', 'v'), ('indication', 'a'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('viewport', 'n'), ('playback', 'n'), ('system', 'n'), ('wherein', 'a'), ('salient', 'n'), ('data', 'n'), ('identifies', 'n'), ('salient', 'a'), ('event', 'n'), ('piece', 'n'), ('content', 'a'), ('salient', 'n'), ('data', 'n'), ('indicates', 'v'), ('salient', 'a'), ('event', 'n'), ('piece', 'n'), ('content', 'n'), ('corresponding', 'v'), ('point', 'n'), ('location', 'n'), ('salient', 'a'), ('event', 'n'), ('piece', 'n'), ('content', 'n'), ('corresponding', 'v'), ('time', 'n'), ('salient', 'a'), ('event', 'n'), ('occurs', 'v'), ('playback', 'n'), ('system', 'n'), ('wherein', 'a'), ('salient', 'n'), ('data', 'n'), ('indicates', 'v'), ('salient', 'a'), ('event', 'n'), ('piece', 'n'), ('content', 'n'), ('corresponding', 'v'), ('type', 'a'), ('salient', 'a'), ('event', 'n'), ('corresponding', 'v'), ('strength', 'n'), ('value', 'n'), ('salient', 'n'), ('event', 'n'), ('system', 'n'), ('wherein', 'a'), ('salient', 'n'), ('data', 'n'), ('generated', 'v'), ('offline', 'a'), ('server', 'n'), ('system', 'n'), ('operations', 'n'), ('comprising', 'v'), ('detecting', 'v'), ('one', None), ('salient', 'n'), ('events', 'n'), ('piece', 'v'), ('content', 'n'), ('based', 'v'), ('least', 'a'), ('one', None), ('following', 'v'), ('visual', 'a'), ('data', 'n'), ('piece', 'n'), ('content', 'n'), ('audio', 'n'), ('data', 'n'), ('piece', 'n'), ('content', 'n'), ('content', 'a'), ('consumption', 'n'), ('experience', 'n'), ('data', 'n'), ('piece', 'n'), ('content', 'n'), ('wherein', None), ('salient', 'n'), ('data', 'n'), ('indicative', 'a'), ('salient', 'a'), ('event', 'n'), ('detected', 'v'), ('system', 'n'), ('operations', 'n'), ('comprising', 'v'), ('detecting', 'v'), ('one', None), ('salient', 'n'), ('events', 'n'), ('piece', 'v'), ('content', 'n'), ('based', 'v'), ('least', 'a'), ('one', None), ('following', 'v'), ('face', 'n'), ('facial', 'a'), ('emotion', 'n'), ('object', 'a'), ('motion', 'n'), ('metadata', 'n'), ('piece', 'n'), ('content', 'n'), ('wherein', None), ('salient', 'n'), ('data', 'n'), ('indicative', 'a'), ('salient', 'a'), ('event', 'n'), ('detected', 'v'), ('system', 'n'), ('operations', 'n'), ('comprising', 'v'), ('detecting', 'v'), ('interaction', 'n'), ('indication', 'n'), ('wherein', None), ('indication', 'n'), ('comprises', 'v'), ('interactive', 'a'), ('hint', 'n'), ('response', 'n'), ('detecting', 'v'), ('interaction', 'n'), ('adapting', 'v'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('second', 'a'), ('path', 'n'), ('viewport', 'n'), ('based', 'v'), ('interaction', 'n'), ('wherein', None), ('second', 'a'), ('path', 'n'), ('viewport', 'n'), ('includes', 'v'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('providing', 'v'), ('updated', 'a'), ('viewport', 'n'), ('piece', 'n'), ('content', 'n'), ('display', 'n'), ('device', 'n'), ('wherein', 'v'), ('movement', 'n'), ('updated', 'v'), ('viewport', 'n'), ('based', 'v'), ('second', 'a'), ('path', 'n'), ('viewport', 'n'), ('salient', 'n'), ('data', 'n'), ('playback', 'v'), ('second', 'a'), ('path', 'n'), ('viewport', 'n'), ('controls', 'n'), ('movement', 'n'), ('updated', 'v'), ('viewport', 'n'), ('put', 'v'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('view', 'n'), ('updated', 'v'), ('viewport', 'n'), ('system', 'n'), ('operations', 'n'), ('comprising', 'v'), ('changing', 'v'), ('weight', 'n'), ('assigned', 'v'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('one', None), ('salient', 'n'), ('events', 'n'), ('piece', 'v'), ('content', 'a'), ('type', 'a'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('system', 'n'), ('wherein', 'a'), ('second', 'a'), ('path', 'n'), ('viewport', 'n'), ('includes', 'v'), ('one', None), ('salient', 'n'), ('events', 'n'), ('piece', 'v'), ('content', 'a'), ('type', 'a'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('including', 'v'), ('instructions', 'n'), ('perform', 'v'), ('comprising', 'v'), ('receiving', 'v'), ('piece', 'n'), ('content', 'a'), ('salient', 'n'), ('data', 'n'), ('piece', 'n'), ('content', 'n'), ('based', 'v'), ('salient', 'a'), ('data', 'n'), ('determining', 'v'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('piece', 'n'), ('content', 'n'), ('wherein', 'n'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('includes', 'v'), ('different', 'a'), ('salient', 'a'), ('events', 'n'), ('occurring', 'v'), ('piece', 'n'), ('content', 'n'), ('different', 'a'), ('times', 'n'), ('playback', 'v'), ('piece', 'n'), ('content', 'n'), ('providing', 'v'), ('viewport', 'n'), ('display', 'n'), ('device', 'n'), ('wherein', 'v'), ('movement', 'n'), ('viewport', 'n'), ('based', 'v'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('salient', 'n'), ('data', 'n'), ('playback', 'n'), ('detecting', 'v'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('piece', 'n'), ('content', 'n'), ('included', 'v'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('providing', 'v'), ('indication', 'a'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('viewport', 'n'), ('playback', 'n'), ('computer', 'n'), ('readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('comprising', 'v'), ('detecting', 'v'), ('interaction', 'n'), ('indication', 'n'), ('wherein', None), ('indication', 'n'), ('comprises', 'v'), ('interactive', 'a'), ('hint', 'n'), ('response', 'n'), ('detecting', 'v'), ('interaction', 'n'), ('adapting', 'v'), ('first', 'a'), ('path', 'n'), ('viewport', 'n'), ('second', 'a'), ('path', 'n'), ('viewport', 'n'), ('based', 'v'), ('interaction', 'n'), ('wherein', None), ('second', 'a'), ('path', 'n'), ('viewport', 'n'), ('includes', 'v'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('providing', 'v'), ('updated', 'a'), ('viewport', 'n'), ('piece', 'n'), ('content', 'n'), ('display', 'n'), ('device', 'n'), ('wherein', 'v'), ('movement', 'n'), ('updated', 'v'), ('viewport', 'n'), ('based', 'v'), ('second', 'a'), ('path', 'n'), ('viewport', 'n'), ('salient', 'n'), ('data', 'n'), ('playback', 'v'), ('second', 'a'), ('path', 'n'), ('viewport', 'n'), ('controls', 'n'), ('movement', 'n'), ('updated', 'v'), ('viewport', 'n'), ('put', 'v'), ('additional', 'a'), ('salient', 'n'), ('event', 'n'), ('view', 'n'), ('updated', 'v'), ('viewport', 'n'), ('mobile', 'n'), ('device', 'n'), ('facial', 'a'), ('mobile', 'a'), ('device', 'n'), ('comprising', 'v'), ('one', None), ('cameras', 'a'), ('device', 'n'), ('memory', 'n'), ('coupled', 'v'), ('device', 'n'), ('processing', 'n'), ('system', 'n'), ('programmed', 'v'), ('receive', 'a'), ('one', None), ('cameras', 'n'), ('extract', 'a'), ('feature', 'n'), ('extractor', 'n'), ('utilizing', 'a'), ('convolutional', 'a'), ('neural', 'a'), ('network', 'n'), ('cnn', 'n'), ('enlarged', 'v'), ('intra-class', 'a'), ('variance', 'n'), ('long-tail', 'a'), ('classes', 'n'), ('feature', 'n'), ('vectors', 'n'), ('generate', 'v'), ('feature', 'n'), ('generator', 'n'), ('discriminative', 'a'), ('feature', 'n'), ('vectors', 'n'), ('feature', 'v'), ('vectors', 'n'), ('classify', 'v'), ('fully', 'r'), ('connected', 'v'), ('classifier', 'a'), ('identity', 'n'), ('discriminative', 'a'), ('feature', 'n'), ('vectors', 'n'), ('control', 'v'), ('operation', 'n'), ('mobile', 'a'), ('device', 'n'), ('react', 'n'), ('accordance', 'n'), ('identity', 'n'), ('mobile', 'a'), ('device', 'n'), ('recited', 'v'), ('includes', 'v'), ('communication', 'n'), ('system', 'n'), ('mobile', 'a'), ('device', 'n'), ('recited', 'v'), ('wherein', 'a'), ('operation', 'n'), ('tags', 'n'), ('video', 'v'), ('identity', 'n'), ('uploads', 'n'), ('video', 'v'), ('social', 'a'), ('media', 'n'), ('mobile', 'a'), ('device', 'n'), ('recited', 'v'), ('wherein', 'a'), ('operation', 'n'), ('tags', 'n'), ('video', 'v'), ('identity', 'n'), ('sends', 'n'), ('video', 'v'), ('mobile', 'a'), ('device', 'n'), ('recited', 'v'), ('wherein', 'r'), ('mobile', 'a'), ('device', 'n'), ('smart', 'v'), ('phone', 'n'), ('mobile', 'a'), ('device', 'n'), ('recited', 'v'), ('wherein', 'r'), ('mobile', 'a'), ('device', 'n'), ('body', 'n'), ('cam', 'v'), ('mobile', 'a'), ('device', 'n'), ('recited', 'v'), ('programmed', 'a'), ('train', 'n'), ('feature', 'n'), ('extractor', 'n'), ('feature', 'n'), ('generator', 'n'), ('fully', 'r'), ('connected', 'v'), ('classifier', 'a'), ('alternative', 'a'), ('bi-stage', 'n'), ('strategy', 'n'), ('mobile', 'a'), ('device', 'n'), ('recited', 'v'), ('wherein', 'a'), ('feature', 'n'), ('extractor', 'n'), ('shares', 'n'), ('covariance', 'n'), ('matrices', 'n'), ('across', None), ('classes', 'n'), ('transfer', 'v'), ('intra-class', 'a'), ('variance', 'n'), ('regular', 'a'), ('classes', 'n'), ('long-tail', 'a'), ('classes', 'n'), ('mobile', 'a'), ('device', 'n'), ('recited', 'v'), ('wherein', 'a'), ('feature', 'n'), ('generator', 'n'), ('optimizes', 'v'), ('softmax', 'a'), ('loss', 'n'), ('joint', 'n'), ('regularization', 'n'), ('weights', 'n'), ('features', 'n'), ('magnitude', 'v'), ('inner', 'a'), ('product', 'n'), ('weights', 'n'), ('features', 'n'), ('mobile', 'a'), ('device', 'n'), ('recited', 'v'), ('wherein', 'a'), ('feature', 'n'), ('extractor', 'n'), ('averages', 'n'), ('feature', 'v'), ('vector', 'n'), ('flipped', 'v'), ('feature', 'n'), ('vector', 'n'), ('flipped', 'v'), ('feature', 'n'), ('vector', 'n'), ('generated', 'v'), ('horizontally', 'r'), ('flipped', 'v'), ('frame', 'v'), ('one', None), ('mobile', 'a'), ('device', 'n'), ('recited', 'v'), ('wherein', 'r'), ('selected', 'v'), ('group', 'n'), ('consisting', 'v'), ('video', 'n'), ('frame', 'n'), ('video', 'n'), ('mobile', 'a'), ('device', 'n'), ('recited', 'v'), ('wherein', 'a'), ('communication', 'n'), ('system', 'n'), ('connects', 'v'), ('remote', 'a'), ('server', 'n'), ('includes', 'v'), ('facial', 'a'), ('network', 'n'), ('mobile', 'a'), ('device', 'n'), ('recited', 'v'), ('wherein', None), ('one', None), ('stage', 'n'), ('alternative', 'a'), ('bi-stage', 'n'), ('strategy', 'n'), ('fixes', 'n'), ('feature', 'v'), ('extractor', 'n'), ('applies', 'n'), ('feature', 'v'), ('generator', 'n'), ('generate', 'v'), ('new', 'a'), ('transferred', 'v'), ('features', 'n'), ('diverse', 'a'), ('violate', 'a'), ('decision', 'n'), ('boundary', 'n'), ('mobile', 'a'), ('device', 'n'), ('recited', 'v'), ('wherein', None), ('one', None), ('stage', 'n'), ('alternative', 'a'), ('bi-stage', 'n'), ('strategy', 'n'), ('fixes', 'n'), ('fully', 'r'), ('connected', 'v'), ('classifier', 'a'), ('updates', 'a'), ('feature', 'n'), ('extractor', 'n'), ('feature', 'n'), ('generator', 'n'), ('computer', 'n'), ('program', 'n'), ('product', 'n'), ('mobile', 'a'), ('device', 'n'), ('facial', 'a'), ('computer', 'n'), ('program', 'n'), ('product', 'n'), ('comprising', 'v'), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('program', 'n'), ('instructions', 'n'), ('embodied', 'v'), ('therewith', 'a'), ('program', 'n'), ('instructions', 'n'), ('executable', 'a'), ('computer', 'n'), ('cause', 'n'), ('computer', 'n'), ('perform', 'n'), ('comprising', 'v'), ('receiving', 'v'), ('device', 'n'), ('extracting', 'v'), ('device', 'n'), ('feature', 'n'), ('extractor', 'n'), ('utilizing', 'a'), ('convolutional', 'a'), ('neural', 'a'), ('network', 'n'), ('cnn', 'n'), ('enlarged', 'v'), ('intra-class', 'a'), ('variance', 'n'), ('long-tail', 'a'), ('classes', 'n'), ('feature', 'v'), ('vectors', 'n'), ('generating', 'v'), ('device', 'n'), ('feature', 'n'), ('generator', 'n'), ('discriminative', 'a'), ('feature', 'n'), ('vectors', 'n'), ('feature', 'v'), ('vectors', 'n'), ('classifying', 'v'), ('device', 'n'), ('utilizing', 'v'), ('fully', 'r'), ('connected', 'v'), ('classifier', 'a'), ('identity', 'n'), ('discriminative', 'a'), ('feature', 'n'), ('vector', 'n'), ('controlling', 'v'), ('operation', 'n'), ('mobile', 'a'), ('device', 'n'), ('react', 'n'), ('accordance', 'n'), ('identity', 'n'), ('computer-implemented', 'a'), ('facial', 'a'), ('mobile', 'a'), ('device', 'n'), ('comprising', 'v'), ('receiving', 'v'), ('device', 'n'), ('extracting', 'v'), ('device', 'n'), ('feature', 'n'), ('extractor', 'n'), ('utilizing', 'a'), ('convolutional', 'a'), ('neural', 'a'), ('network', 'n'), ('cnn', 'n'), ('enlarged', 'v'), ('intra-class', 'a'), ('variance', 'n'), ('long-tail', 'a'), ('classes', 'n'), ('feature', 'v'), ('vectors', 'n'), ('generating', 'v'), ('device', 'n'), ('feature', 'n'), ('generator', 'n'), ('discriminative', 'a'), ('feature', 'n'), ('vectors', 'n'), ('feature', 'v'), ('vectors', 'n'), ('classifying', 'v'), ('device', 'n'), ('utilizing', 'v'), ('fully', 'r'), ('connected', 'v'), ('classifier', 'a'), ('identity', 'n'), ('discriminative', 'a'), ('feature', 'n'), ('vector', 'n'), ('controlling', 'v'), ('operation', 'n'), ('mobile', 'a'), ('device', 'n'), ('react', 'n'), ('accordance', 'n'), ('identity', 'n'), ('computer-implemented', 'a'), ('recited', 'v'), ('wherein', 'a'), ('controlling', 'v'), ('includes', 'v'), ('tagging', 'v'), ('video', 'n'), ('identity', 'n'), ('uploading', 'v'), ('video', 'a'), ('social', 'a'), ('media', 'n'), ('computer-implemented', 'a'), ('recited', 'v'), ('wherein', 'a'), ('controlling', 'v'), ('includes', 'v'), ('tagging', 'v'), ('video', 'n'), ('identity', 'n'), ('sending', 'v'), ('video', 'a'), ('computer-implemented', 'a'), ('recited', 'v'), ('wherein', 'n'), ('extracting', 'v'), ('includes', 'v'), ('sharing', 'v'), ('covariance', 'n'), ('matrices', 'n'), ('across', None), ('classes', 'n'), ('transfer', 'v'), ('intra-class', 'a'), ('variance', 'n'), ('regular', 'a'), ('classes', 'n'), ('long-tail', 'a'), ('classes', 'n'), ('computing', 'v'), ('device', 'n'), ('comprising', 'v'), ('non-transitory', 'a'), ('machine', 'n'), ('readable', 'a'), ('medium', 'n'), ('storing', 'v'), ('machine', 'n'), ('trained', 'v'), ('mt', 'a'), ('network', 'n'), ('comprising', 'v'), ('layers', 'n'), ('processing', 'v'), ('nodes', 'n'), ('processing', 'v'), ('node', 'r'), ('configured', 'v'), ('compute', 'n'), ('first', 'r'), ('output', 'n'), ('value', 'n'), ('combining', 'v'), ('set', 'v'), ('output', 'n'), ('values', 'n'), ('set', 'v'), ('processing', 'v'), ('nodes', 'n'), ('use', 'v'), ('piecewise', 'a'), ('linear', 'a'), ('cup', 'n'), ('function', 'n'), ('compute', 'a'), ('second', 'a'), ('output', 'n'), ('value', 'n'), ('first', 'r'), ('output', 'n'), ('value', 'n'), ('processing', 'v'), ('node', 'a'), ('wherein', 'n'), ('piecewise', 'n'), ('linear', 'a'), ('cup', 'n'), ('function', 'n'), ('prior', 'r'), ('training', 'v'), ('mt', 'n'), ('network', 'n'), ('comprises', 'n'), ('least', 'v'), ('first', 'a'), ('linear', 'a'), ('section', 'n'), ('first', 'r'), ('slope', 'n'), ('followed', 'v'), ('ii', 'a'), ('second', 'a'), ('linear', 'a'), ('section', 'n'), ('negative', 'a'), ('second', 'a'), ('slope', 'n'), ('followed', 'v'), ('iii', 'a'), ('third', 'a'), ('linear', 'a'), ('section', 'n'), ('negative', 'a'), ('third', 'a'), ('slope', 'n'), ('different', 'a'), ('second', 'a'), ('slope', 'n'), ('followed', 'v'), ('iv', 'a'), ('fourth', 'a'), ('linear', 'a'), ('section', 'n'), ('positive', 'a'), ('fourth', 'a'), ('slope', 'n'), ('followed', 'v'), ('v', 'a'), ('fifth', 'a'), ('linear', 'a'), ('section', 'n'), ('positive', 'a'), ('fifth', 'a'), ('slope', 'n'), ('different', 'a'), ('fourth', 'a'), ('slope', 'n'), ('followed', 'v'), ('vi', 'a'), ('sixth', 'a'), ('linear', 'a'), ('section', 'n'), ('sixth', 'v'), ('slope', 'n'), ('wherein', 'n'), ('piecewise', 'n'), ('linear', 'a'), ('cup', 'n'), ('function', 'n'), ('symmetric', 'a'), ('vertical', 'a'), ('axis', 'n'), ('third', 'a'), ('fourth', 'a'), ('linear', 'a'), ('sections', 'n'), ('prior', 'r'), ('training', 'v'), ('mt', 'n'), ('network', 'n'), ('content', 'a'), ('capturing', 'n'), ('circuit', 'n'), ('capturing', 'v'), ('content', 'a'), ('processing', 'n'), ('mt', 'n'), ('network', 'n'), ('set', 'v'), ('processing', 'v'), ('units', 'n'), ('executing', 'v'), ('processing', 'v'), ('nodes', 'n'), ('process', 'n'), ('content', 'n'), ('captured', 'v'), ('content', 'a'), ('capturing', 'v'), ('circuit', 'n'), ('wherein', 'n'), ('training', 'n'), ('set', 'v'), ('parameters', 'n'), ('define', 'v'), ('piecewise', 'n'), ('linear', 'a'), ('cup', 'n'), ('function', 'n'), ('node', None), ('first', 'a'), ('second', 'a'), ('pluralities', 'n'), ('processing', 'v'), ('nodes', 'n'), ('processing', 'v'), ('node', 'n'), ('first', 'r'), ('processing', 'v'), ('nodes', 'n'), ('configured', 'a'), ('emulate', 'a'), ('boolean', 'n'), ('operator', 'n'), ('output', 'n'), ('value', 'n'), ('processing', 'n'), ('node', 'a'), ('range', 'n'), ('associated', 'v'), ('``', None), (\"''\", None), ('value', 'n'), ('set', 'v'), ('inputs', 'n'), ('processing', 'v'), ('node', 'n'), ('set', 'v'), ('values', 'n'), ('range', 'v'), ('associated', 'v'), ('``', None), (\"''\", None), ('ii', 'n'), ('processing', 'n'), ('node', 'a'), ('second', 'a'), ('processing', 'n'), ('nodes', 'n'), ('configured', 'v'), ('emulate', 'a'), ('boolean', 'a'), ('xnor', 'n'), ('operator', 'n'), ('output', 'n'), ('value', 'n'), ('processing', 'n'), ('node', 'a'), ('range', 'n'), ('associated', 'v'), ('``', None), (\"''\", None), ('set', 'n'), ('inputs', 'n'), ('node', 'a'), ('set', 'n'), ('values', 'n'), ('range', 'v'), ('associated', 'v'), ('``', None), (\"''\", None), ('b', 'n'), ('set', 'v'), ('inputs', 'n'), ('node', 'a'), ('set', 'n'), ('values', 'n'), ('range', 'v'), ('associated', 'v'), ('``', None), (\"''\", None), ('value', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'n'), ('third', 'a'), ('linear', 'a'), ('section', 'n'), ('piecewise', 'n'), ('linear', 'a'), ('cup', 'n'), ('function', 'n'), ('first', 'r'), ('processing', 'v'), ('node', 'n'), ('mt', 'n'), ('network', 'n'), ('different', 'a'), ('slope', 'n'), ('third', 'a'), ('linear', 'a'), ('section', 'n'), ('second', 'a'), ('processing', 'n'), ('node', 'n'), ('mt', 'n'), ('network', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'n'), ('length', 'a'), ('third', 'a'), ('section', 'n'), ('piecewise', 'n'), ('linear', 'a'), ('cup', 'n'), ('function', 'n'), ('first', 'r'), ('processing', 'v'), ('node', 'n'), ('mt', 'n'), ('network', 'n'), ('different', 'a'), ('length', 'n'), ('third', 'a'), ('section', 'n'), ('piecewise', 'n'), ('linear', 'a'), ('cup', 'n'), ('function', 'n'), ('second', 'a'), ('processing', 'n'), ('node', 'n'), ('mt', 'n'), ('network', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'n'), ('sets', 'v'), ('parameters', 'n'), ('trained', 'v'), ('part', 'n'), ('back', 'r'), ('propagating', 'v'), ('module', 'n'), ('back', 'r'), ('propagating', 'a'), ('errors', 'n'), ('output', 'n'), ('values', 'n'), ('later', 'r'), ('layers', 'n'), ('processing', 'v'), ('nodes', 'n'), ('earlier', 'r'), ('layers', 'n'), ('processing', 'v'), ('nodes', 'n'), ('adjusting', 'v'), ('set', 'n'), ('parameters', 'n'), ('define', 'v'), ('piecewise', 'n'), ('linear', 'a'), ('cup', 'n'), ('functions', 'n'), ('earlier', 'r'), ('layers', 'n'), ('processing', 'v'), ('nodes', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'n'), ('processing', 'n'), ('node', 'a'), ('uses', 'n'), ('linear', 'a'), ('function', 'n'), ('defined', 'v'), ('set', 'v'), ('parameters', 'n'), ('compute', 'v'), ('first', 'a'), ('output', 'n'), ('value', 'n'), ('processing', 'v'), ('node', None), ('wherein', 'a'), ('back', 'r'), ('propagating', 'v'), ('module', 'n'), ('back', 'r'), ('propagates', 'v'), ('errors', 'n'), ('output', 'n'), ('values', 'n'), ('later', 'r'), ('layers', 'n'), ('processing', 'v'), ('nodes', 'n'), ('earlier', 'r'), ('layers', 'n'), ('processing', 'v'), ('nodes', 'n'), ('adjusting', 'v'), ('set', 'n'), ('parameters', 'n'), ('define', 'v'), ('linear', 'a'), ('functions', 'n'), ('earlier', 'r'), ('layers', 'n'), ('processing', 'v'), ('nodes', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', None), ('first', 'a'), ('processing', 'n'), ('nodes', 'n'), ('emulate', 'v'), ('boolean', 'a'), ('operator', 'n'), ('second', 'a'), ('processing', 'n'), ('nodes', 'n'), ('emulate', 'v'), ('boolean', 'a'), ('operator', 'n'), ('enable', 'a'), ('mt', 'n'), ('network', 'n'), ('implement', 'a'), ('mathematical', 'a'), ('problems', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'n'), ('processing', 'v'), ('node', 'a'), ('layers', 'n'), ('processing', 'v'), ('nodes', 'n'), ('receive', 'a'), ('input', 'n'), ('values', 'n'), ('output', 'n'), ('values', 'n'), ('processing', 'v'), ('nodes', 'n'), ('set', 'v'), ('prior', 'a'), ('layers', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'n'), ('processing', 'n'), ('node', 'a'), ('uses', 'n'), ('linear', 'a'), ('function', 'n'), ('compute', 'n'), ('first', 'r'), ('output', 'n'), ('value', 'n'), ('processing', 'v'), ('node', 'a'), ('wherein', 'n'), ('processing', 'v'), ('node', 'n'), (\"'s\", None), ('piecewise', 'n'), ('linear', 'a'), ('cup', 'n'), ('function', 'n'), ('defined', 'v'), ('along', None), ('first', 'a'), ('second', 'a'), ('axes', 'n'), ('first', 'r'), ('axis', 'v'), ('defining', 'v'), ('range', 'n'), ('output', 'n'), ('values', 'n'), ('processing', 'v'), ('node', 'n'), (\"'s\", None), ('linear', 'a'), ('function', 'n'), ('second', 'a'), ('axis', 'n'), ('defining', 'v'), ('range', 'n'), ('output', 'n'), ('values', 'n'), ('produced', 'v'), ('piecewise', 'a'), ('linear', 'a'), ('cup', 'n'), ('function', 'n'), ('range', 'n'), ('output', 'n'), ('values', 'n'), ('processing', 'v'), ('node', 'n'), (\"'s\", None), ('linear', 'a'), ('function', 'n'), ('computing', 'v'), ('device', 'n'), ('comprising', 'v'), ('content', 'a'), ('output', 'n'), ('circuit', 'n'), ('presenting', 'v'), ('output', 'n'), ('based', 'v'), ('processing', 'n'), ('content', 'n'), ('mt', 'n'), ('network', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'n'), ('captured', 'v'), ('content', 'a'), ('one', None), ('audio', 'n'), ('segment', 'n'), ('wherein', 'n'), ('presented', 'v'), ('output', 'n'), ('output', 'n'), ('display', 'v'), ('display', 'n'), ('screen', 'n'), ('computing', 'v'), ('device', 'n'), ('audio', 'n'), ('presentation', 'n'), ('output', 'n'), ('speaker', 'n'), ('computing', 'v'), ('device', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'n'), ('computing', 'v'), ('device', 'n'), ('mobile', 'a'), ('device', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'n'), ('mt', 'n'), ('network', 'n'), ('mt', 'n'), ('neural', 'a'), ('network', 'n'), ('processing', 'v'), ('nodes', 'n'), ('mt', 'a'), ('neurons', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'n'), ('set', 'v'), ('parameters', 'n'), ('configured', 'v'), ('training', 'v'), ('processing', 'v'), ('nodes', 'n'), ('comprise', 'n'), ('least', 'v'), ('one', None), ('negative', 'a'), ('second', 'a'), ('third', 'a'), ('slopes', 'n'), ('second', 'a'), ('third', 'a'), ('linear', 'a'), ('sections', 'n'), ('positive', 'a'), ('fourth', 'a'), ('fifth', 'a'), ('slopes', 'n'), ('fourth', 'a'), ('fifth', 'a'), ('linear', 'n'), ('sections', 'n'), ('first', 'r'), ('intercept', 'a'), ('second', 'a'), ('linear', 'a'), ('section', 'n'), ('second', 'a'), ('intercept', 'n'), ('fifth', 'a'), ('linear', 'a'), ('section', 'n'), ('set', 'v'), ('lengths', 'n'), ('least', 'a'), ('second', 'a'), ('third', 'a'), ('fourth', 'a'), ('fifth', 'a'), ('sections', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'n'), ('trained', 'v'), ('set', 'v'), ('parameters', 'n'), ('define', 'v'), ('piecewise', 'n'), ('linear', 'a'), ('cup', 'n'), ('function', 'n'), ('node', 'n'), ('comprise', 'n'), ('output', 'n'), ('values', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'n'), ('first', 'r'), ('sixth', 'a'), ('slopes', 'n'), ('zerowe', 'v'), ('system', 'n'), ('comprising', 'v'), ('memory', 'n'), ('device', 'n'), ('store', 'n'), ('input', 'n'), ('including', 'v'), ('input', 'a'), ('interface', 'n'), ('receive', 'a'), ('input', 'n'), ('pre-', 'a'), ('model', 'n'), ('input', 'a'), ('yield', 'n'), ('multi-channel', 'a'), ('feature', 'n'), ('extractor', 'n'), ('extract', 'n'), ('set', 'v'), ('features', 'n'), ('based', 'v'), ('multi-channel', 'a'), ('feature', 'n'), ('selector', 'n'), ('select', 'v'), ('one', None), ('features', 'v'), ('set', 'v'), ('features', 'n'), ('multi-channel', 'a'), ('wherein', 'v'), ('one', None), ('features', 'n'), ('selected', 'v'), ('based', 'v'), ('ability', 'n'), ('differentiate', 'n'), ('features', 'n'), ('feature', 'v'), ('matcher', 'a'), ('match', 'n'), ('one', None), ('features', 'n'), ('learned', 'v'), ('feature', 'n'), ('set', 'v'), ('similarity', 'n'), ('detector', 'n'), ('determine', 'n'), ('whether', None), ('one', None), ('features', 'v'), ('meet', 'r'), ('pre-defined', 'a'), ('similarity', 'n'), ('threshold', 'n'), ('system', 'n'), ('wherein', 'v'), ('pre-', 'a'), ('activate', 'n'), ('one', None), ('channels', 'v'), ('multi-channel', 'a'), ('yield', 'n'), ('one', None), ('activated', 'v'), ('channels', 'n'), ('system', 'n'), ('wherein', 'v'), ('one', None), ('activated', 'v'), ('channels', 'n'), ('determined', 'v'), ('based', 'v'), ('ability', 'n'), ('differentiate', 'n'), ('features', 'n'), ('system', 'n'), ('wherein', 'v'), ('pre-', 'a'), ('activate', 'n'), ('one', None), ('local', 'a'), ('patches', 'v'), ('one', None), ('activated', 'v'), ('channels', 'n'), ('system', 'n'), ('wherein', 'v'), ('one', None), ('local', 'a'), ('patches', 'n'), ('determined', 'v'), ('based', 'v'), ('ability', 'n'), ('differentiate', 'n'), ('features', 'n'), ('system', 'n'), ('wherein', 'a'), ('feature', 'n'), ('matcher', 'n'), ('utilize', 'a'), ('large-scale', 'a'), ('data', 'n'), ('learning', 'v'), ('process', 'n'), ('perform', 'n'), ('feature', 'n'), ('matching', 'v'), ('apparatus', 'n'), ('comprising', 'v'), ('input', 'a'), ('interface', 'n'), ('receive', 'a'), ('input', 'n'), ('pre-', 'a'), ('model', 'n'), ('input', 'a'), ('yield', 'n'), ('multi-channel', 'a'), ('feature', 'n'), ('extractor', 'n'), ('extract', 'n'), ('set', 'v'), ('features', 'n'), ('based', 'v'), ('multi-channel', 'a'), ('feature', 'n'), ('selector', 'n'), ('select', 'v'), ('one', None), ('features', 'v'), ('set', 'v'), ('features', 'n'), ('multi-channel', 'a'), ('wherein', 'v'), ('one', None), ('features', 'n'), ('selected', 'v'), ('based', 'v'), ('ability', 'n'), ('differentiate', 'n'), ('features', 'n'), ('feature', 'v'), ('matcher', 'a'), ('match', 'n'), ('one', None), ('features', 'n'), ('learned', 'v'), ('feature', 'n'), ('set', 'v'), ('similarity', 'n'), ('detector', 'n'), ('determine', 'n'), ('whether', None), ('one', None), ('features', 'v'), ('meet', 'r'), ('pre-defined', 'a'), ('similarity', 'n'), ('threshold', 'n'), ('apparatus', 'n'), ('wherein', 'v'), ('pre-', 'a'), ('activate', 'n'), ('one', None), ('channels', 'v'), ('multi-channel', 'a'), ('yield', 'n'), ('one', None), ('activated', 'v'), ('channels', 'n'), ('apparatus', 'v'), ('wherein', None), ('one', None), ('activated', 'a'), ('channels', 'n'), ('determined', 'v'), ('based', 'v'), ('ability', 'n'), ('differentiate', 'n'), ('features', 'n'), ('apparatus', 'v'), ('wherein', 'a'), ('pre-', 'a'), ('activate', 'n'), ('one', None), ('local', 'a'), ('patches', 'v'), ('one', None), ('activated', 'a'), ('channels', 'n'), ('apparatus', 'v'), ('wherein', None), ('one', None), ('local', 'a'), ('patches', 'n'), ('determined', 'v'), ('based', 'v'), ('ability', 'n'), ('differentiate', 'n'), ('features', 'n'), ('apparatus', 'v'), ('wherein', 'a'), ('feature', 'n'), ('matcher', 'n'), ('utilize', 'a'), ('large-scale', 'a'), ('data', 'n'), ('learning', 'v'), ('process', 'n'), ('perform', 'n'), ('feature', 'n'), ('matching', 'v'), ('comprising', 'v'), ('modeling', 'v'), ('input', 'a'), ('yield', 'n'), ('multi-channel', 'r'), ('extracting', 'v'), ('set', 'v'), ('features', 'n'), ('based', 'v'), ('multi-channel', 'n'), ('selecting', 'v'), ('one', None), ('features', 'v'), ('set', 'v'), ('features', 'n'), ('multi-channel', 'a'), ('wherein', 'v'), ('one', None), ('features', 'n'), ('selected', 'v'), ('based', 'v'), ('ability', 'n'), ('differentiate', 'n'), ('features', 'n'), ('matching', 'v'), ('one', None), ('features', 'n'), ('learned', 'v'), ('feature', 'n'), ('set', 'v'), ('determining', 'v'), ('whether', None), ('one', None), ('features', 'v'), ('meet', 'r'), ('pre-defined', 'a'), ('similarity', 'n'), ('threshold', 'n'), ('wherein', 'n'), ('modeling', 'v'), ('input', 'n'), ('include', 'v'), ('activating', 'v'), ('one', None), ('channels', 'n'), ('multi-channel', 'a'), ('yield', 'n'), ('one', None), ('activated', 'v'), ('channels', 'n'), ('wherein', 'v'), ('one', None), ('activated', 'v'), ('channels', 'n'), ('determined', 'v'), ('based', 'v'), ('ability', 'n'), ('differentiate', 'n'), ('features', 'n'), ('wherein', 'v'), ('extracting', 'v'), ('features', 'n'), ('input', 'v'), ('include', 'v'), ('activating', 'v'), ('one', None), ('local', 'a'), ('patches', 'v'), ('one', None), ('activated', 'a'), ('channels', 'n'), ('wherein', 'v'), ('one', None), ('local', 'a'), ('patches', 'n'), ('determined', 'v'), ('based', 'v'), ('ability', 'n'), ('differentiate', 'n'), ('features', 'n'), ('wherein', 'v'), ('feature', 'n'), ('matcher', 'n'), ('utilizes', 'a'), ('large-scale', 'a'), ('data', 'n'), ('learning', 'v'), ('process', 'n'), ('perform', 'n'), ('feature', 'n'), ('matching', 'v'), ('least', 'a'), ('one', None), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('comprising', 'v'), ('set', 'v'), ('instructions', 'n'), ('executed', 'v'), ('computing', 'v'), ('device', 'n'), ('cause', 'n'), ('computing', 'v'), ('device', 'n'), ('model', 'n'), ('input', 'a'), ('yield', 'n'), ('multi-channel', 'a'), ('extract', 'n'), ('set', 'v'), ('features', 'n'), ('based', 'v'), ('multi-channel', 'n'), ('select', 'v'), ('one', None), ('features', 'v'), ('set', 'v'), ('features', 'n'), ('multi-channel', 'a'), ('wherein', 'n'), ('features', 'n'), ('selected', 'v'), ('based', 'v'), ('ability', 'n'), ('differentiate', 'n'), ('features', 'n'), ('match', 'v'), ('one', None), ('features', 'n'), ('learned', 'v'), ('feature', 'n'), ('set', 'v'), ('determine', 'n'), ('whether', None), ('one', None), ('features', 'v'), ('meet', 'r'), ('pre-defined', 'a'), ('similarity', 'n'), ('threshold', 'v'), ('least', 'a'), ('one', None), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('wherein', 'n'), ('instructions', 'n'), ('executed', 'v'), ('cause', 'n'), ('computing', 'v'), ('device', 'n'), ('activate', 'v'), ('one', None), ('channels', 'n'), ('multi-channel', 'a'), ('yield', 'n'), ('one', None), ('activated', 'v'), ('channels', 'n'), ('least', 'a'), ('one', None), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('wherein', 'n'), ('instructions', 'n'), ('executed', 'v'), ('cause', 'n'), ('computing', 'v'), ('device', 'n'), ('determine', 'n'), ('one', None), ('activated', 'v'), ('channels', 'n'), ('based', 'v'), ('ability', 'n'), ('differentiate', 'n'), ('features', 'n'), ('least', 'v'), ('one', None), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('wherein', 'n'), ('extracting', 'v'), ('features', 'n'), ('input', 'v'), ('include', 'v'), ('activating', 'v'), ('one', None), ('local', 'a'), ('patches', 'v'), ('one', None), ('activated', 'a'), ('channels', 'n'), ('least', 'a'), ('one', None), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('wherein', 'v'), ('one', None), ('local', 'a'), ('patches', 'n'), ('determined', 'v'), ('based', 'v'), ('ability', 'n'), ('differentiate', 'n'), ('features', 'n'), ('least', 'v'), ('one', None), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('wherein', 'a'), ('feature', 'n'), ('matcher', 'n'), ('utilize', 'a'), ('large-scale', 'a'), ('data', 'n'), ('learning', 'v'), ('process', 'n'), ('perform', 'n'), ('feature', 'n'), ('matching', 'v'), ('apparatus', 'n'), ('comprising', 'v'), ('means', 'n'), ('modeling', 'v'), ('input', 'a'), ('yield', 'n'), ('multi-channel', 'n'), ('means', 'n'), ('extracting', 'v'), ('set', 'n'), ('features', 'n'), ('based', 'v'), ('multi-channel', 'n'), ('means', 'v'), ('selecting', 'v'), ('one', None), ('features', 'v'), ('set', 'v'), ('features', 'n'), ('multi-channel', 'a'), ('wherein', 'v'), ('one', None), ('features', 'n'), ('selected', 'v'), ('based', 'v'), ('ability', 'n'), ('differentiate', 'n'), ('features', 'n'), ('means', 'v'), ('matching', 'v'), ('one', None), ('features', 'n'), ('learned', 'v'), ('feature', 'n'), ('set', 'v'), ('means', 'v'), ('determining', 'v'), ('whether', None), ('one', None), ('features', 'v'), ('meet', 'r'), ('pre-defined', 'a'), ('similarity', 'n'), ('threshold', 'n'), ('controlling', 'v'), ('terminal', 'a'), ('terminal', 'a'), ('comprising', 'v'), ('capturing', 'v'), ('apparatus', 'n'), ('least', 'a'), ('one', None), ('comprising', 'v'), ('acquiring', 'v'), ('capturing', 'v'), ('apparatus', 'a'), ('obtaining', 'v'), ('least', 'a'), ('one', None), ('motion', 'n'), ('parameter', 'n'), ('terminal', 'a'), ('motion', 'n'), ('parameter', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('motion', 'n'), ('frequency', 'n'), ('motion', 'n'), ('time', 'n'), ('two', None), ('parameters', 'n'), ('among', None), ('acceleration', 'n'), ('angular', 'a'), ('velocity', 'n'), ('motion', 'n'), ('amplitude', 'n'), ('motion', 'n'), ('frequency', 'n'), ('motion', 'n'), ('time', 'n'), ('transmitting', 'v'), ('least', 'a'), ('one', None), ('parameter', 'n'), ('threshold', 'n'), ('obtaining', 'v'), ('request', 'n'), ('data', 'n'), ('management', 'n'), ('server', 'r'), ('parameter', 'r'), ('threshold', 'a'), ('obtaining', 'v'), ('request', 'n'), ('comprising', 'v'), ('configuration', 'n'), ('information', 'n'), ('terminal', 'n'), ('receiving', 'n'), ('corresponding', 'v'), ('preset', 'n'), ('thresholds', 'n'), ('correspond', 'n'), ('configuration', 'n'), ('information', 'n'), ('response', 'n'), ('parameter', 'n'), ('threshold', 'v'), ('obtaining', 'v'), ('request', 'n'), ('comparing', 'v'), ('two', None), ('parameters', 'n'), ('corresponding', 'v'), ('preset', 'n'), ('thresholds', 'n'), ('controlling', 'v'), ('least', 'a'), ('one', None), ('perform', 'n'), ('processing', 'n'), ('acquired', 'v'), ('based', 'v'), ('least', 'a'), ('one', None), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'n'), ('threshold', 'n'), ('based', 'v'), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'n'), ('respectively', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'n'), ('thresholds', 'n'), ('wherein', 'v'), ('acquiring', 'v'), ('comprises', 'n'), ('acquiring', 'v'), ('real', 'a'), ('time', 'n'), ('obtaining', 'v'), ('comprises', 'n'), ('obtaining', 'v'), ('motion', 'n'), ('parameter', 'n'), ('terminal', 'a'), ('real', 'a'), ('time', 'n'), ('comprising', 'v'), ('response', 'n'), ('least', 'a'), ('one', None), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'n'), ('threshold', 'v'), ('obtaining', 'v'), ('motion', 'n'), ('parameter', None), ('terminal', 'a'), ('response', 'n'), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'n'), ('obtained', 'v'), ('latest', 'a'), ('time', 'n'), ('less', None), ('equal', 'a'), ('corresponding', 'n'), ('preset', 'n'), ('thresholds', 'n'), ('performing', 'v'), ('processing', 'n'), ('acquired', 'v'), ('latest', 'a'), ('time', 'n'), ('according', 'v'), ('wherein', 'n'), ('acquiring', 'v'), ('comprises', 'n'), ('controlling', 'v'), ('least', 'a'), ('one', None), ('turn', 'n'), ('capturing', 'v'), ('apparatus', 'n'), ('based', 'v'), ('face', 'n'), ('instruction', 'n'), ('acquiring', 'v'), ('capturing', 'v'), ('apparatus', 'a'), ('face', 'n'), ('capturing', 'v'), ('apparatus', 'n'), ('turned', 'v'), ('according', 'v'), ('wherein', 'n'), ('controlling', 'v'), ('perform', 'n'), ('processing', 'n'), ('comprises', 'v'), ('skipping', 'v'), ('performing', 'v'), ('face', 'n'), ('acquired', 'v'), ('face', 'n'), ('based', 'v'), ('least', 'a'), ('one', None), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'n'), ('threshold', 'n'), ('based', 'v'), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'n'), ('respectively', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'n'), ('thresholds', 'n'), ('according', 'v'), ('wherein', 'n'), ('obtaining', 'v'), ('comprises', 'n'), ('least', 'a'), ('one', None), ('obtaining', 'v'), ('acceleration', 'n'), ('terminal', 'n'), ('using', 'v'), ('acceleration', 'n'), ('sensor', 'n'), ('obtaining', 'v'), ('angular', 'a'), ('velocity', 'n'), ('terminal', 'n'), ('using', 'v'), ('gyro', 'n'), ('sensor', 'n'), ('according', 'v'), ('wherein', 'a'), ('transmitting', 'n'), ('comprises', 'n'), ('transmitting', 'v'), ('parameter', 'n'), ('threshold', 'v'), ('obtaining', 'v'), ('request', 'n'), ('data', 'n'), ('management', 'n'), ('server', 'n'), ('according', 'v'), ('preset', 'a'), ('time', 'n'), ('period', 'n'), ('according', 'v'), ('comprising', 'v'), ('generating', 'v'), ('prompt', 'a'), ('information', 'n'), ('based', 'v'), ('least', 'a'), ('one', None), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'n'), ('threshold', 'v'), ('prompt', 'a'), ('information', 'n'), ('used', 'v'), ('prompting', 'v'), ('terminal', 'a'), ('stop', 'n'), ('moving', 'v'), ('according', 'v'), ('wherein', 'a'), ('motion', 'n'), ('parameter', 'n'), ('comprises', 'v'), ('motion', 'n'), ('frequency', 'n'), ('motion', 'n'), ('time', 'n'), ('terminal', 'a'), ('comprising', 'v'), ('capturing', 'v'), ('apparatus', 'n'), ('least', 'a'), ('one', None), ('memory', 'n'), ('configured', 'v'), ('store', 'n'), ('program', 'n'), ('code', 'n'), ('least', 'a'), ('one', None), ('configured', 'a'), ('access', 'n'), ('least', 'a'), ('one', None), ('memory', 'n'), ('operate', 'n'), ('according', 'v'), ('program', 'n'), ('code', 'n'), ('program', 'n'), ('code', 'n'), ('comprising', 'v'), ('motion', 'n'), ('parameter', 'n'), ('obtaining', 'v'), ('code', 'n'), ('configured', 'v'), ('cause', 'n'), ('least', 'a'), ('one', None), ('acquire', 'v'), ('using', 'v'), ('capturing', 'v'), ('apparatus', 'n'), ('obtain', 'v'), ('motion', 'n'), ('parameter', 'n'), ('terminal', 'a'), ('motion', 'n'), ('parameter', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('motion', 'n'), ('frequency', 'n'), ('motion', 'n'), ('time', 'n'), ('two', None), ('parameters', 'n'), ('among', None), ('acceleration', 'n'), ('angular', 'a'), ('velocity', 'n'), ('motion', 'n'), ('amplitude', 'n'), ('motion', 'n'), ('frequency', 'n'), ('motion', 'n'), ('time', 'n'), ('request', 'n'), ('transmitting', 'v'), ('code', 'n'), ('configured', 'v'), ('cause', 'n'), ('least', 'a'), ('one', None), ('transmit', 'n'), ('parameter', 'n'), ('threshold', 'v'), ('obtaining', 'v'), ('request', 'n'), ('data', 'n'), ('management', 'n'), ('server', 'r'), ('parameter', 'r'), ('threshold', 'a'), ('obtaining', 'v'), ('request', 'n'), ('comprising', 'v'), ('configuration', 'n'), ('information', 'n'), ('terminal', 'n'), ('parameter', 'n'), ('threshold', 'v'), ('receiving', 'v'), ('code', 'n'), ('configured', 'v'), ('cause', 'n'), ('least', 'a'), ('one', None), ('receive', 'n'), ('corresponding', 'n'), ('preset', 'n'), ('thresholds', 'n'), ('correspond', 'n'), ('configuration', 'n'), ('information', 'n'), ('response', 'n'), ('parameter', 'n'), ('threshold', 'v'), ('obtaining', 'v'), ('request', 'n'), ('comparing', 'v'), ('code', 'n'), ('configured', 'v'), ('cause', 'n'), ('least', 'a'), ('one', None), ('compare', 'n'), ('two', None), ('parameters', 'n'), ('corresponding', 'v'), ('preset', 'n'), ('thresholds', 'n'), ('control', 'n'), ('code', 'n'), ('configured', 'v'), ('cause', 'n'), ('least', 'a'), ('one', None), ('perform', 'n'), ('processing', 'n'), ('acquired', 'v'), ('based', 'v'), ('least', 'a'), ('one', None), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'n'), ('threshold', 'n'), ('based', 'v'), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'n'), ('respectively', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'n'), ('thresholds', 'n'), ('wherein', 'v'), ('motion', 'n'), ('parameter', 'n'), ('obtaining', 'v'), ('code', 'n'), ('causes', 'n'), ('least', 'v'), ('one', None), ('acquire', 'v'), ('real', 'a'), ('time', 'n'), ('obtain', 'v'), ('motion', 'n'), ('parameter', None), ('terminal', 'a'), ('real', 'a'), ('time', 'n'), ('response', 'n'), ('least', 'a'), ('one', None), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'v'), ('threshold', 'a'), ('obtain', 'v'), ('motion', 'n'), ('parameter', 'n'), ('terminal', 'a'), ('wherein', 'n'), ('control', 'n'), ('code', 'n'), ('causes', 'v'), ('least', 'a'), ('one', None), ('response', 'n'), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'n'), ('obtained', 'v'), ('latest', 'a'), ('time', 'n'), ('less', None), ('equal', 'a'), ('corresponding', 'n'), ('preset', 'n'), ('thresholds', 'n'), ('perform', 'v'), ('processing', 'v'), ('acquired', 'v'), ('latest', 'a'), ('time', 'n'), ('terminal', 'a'), ('according', 'v'), ('wherein', 'a'), ('program', 'n'), ('code', 'n'), ('comprises', 'v'), ('face', 'v'), ('instruction', 'n'), ('receiving', 'v'), ('code', 'n'), ('configured', 'v'), ('cause', 'n'), ('least', 'a'), ('one', None), ('receive', 'a'), ('face', 'n'), ('instruction', 'n'), ('wherein', None), ('motion', 'n'), ('parameter', 'n'), ('obtaining', 'v'), ('code', 'n'), ('causes', 'n'), ('least', 'v'), ('one', None), ('control', 'n'), ('according', 'v'), ('face', 'n'), ('instruction', 'n'), ('capturing', 'v'), ('apparatus', 'n'), ('turn', 'n'), ('acquire', 'v'), ('face', 'n'), ('using', 'v'), ('capturing', 'v'), ('apparatus', 'n'), ('capturing', 'v'), ('apparatus', 'n'), ('turned', 'v'), ('wherein', 'a'), ('control', 'n'), ('code', 'n'), ('causes', 'v'), ('least', 'a'), ('one', None), ('skip', 'n'), ('performing', 'v'), ('face', 'n'), ('acquired', 'v'), ('face', 'n'), ('based', 'v'), ('least', 'a'), ('one', None), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'n'), ('threshold', 'n'), ('based', 'v'), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'n'), ('respectively', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'n'), ('thresholds', 'n'), ('terminal', 'a'), ('according', 'v'), ('wherein', 'a'), ('request', 'n'), ('transmitting', 'v'), ('code', 'n'), ('causes', 'n'), ('least', 'v'), ('one', None), ('transmit', 'n'), ('parameter', 'n'), ('threshold', 'v'), ('obtaining', 'v'), ('request', 'n'), ('data', 'n'), ('management', 'n'), ('server', 'n'), ('according', 'v'), ('preset', 'a'), ('time', 'n'), ('period', 'n'), ('terminal', 'a'), ('according', 'v'), ('wherein', 'a'), ('program', 'n'), ('code', 'n'), ('comprises', 'v'), ('prompt', 'a'), ('information', 'n'), ('generation', 'n'), ('code', 'n'), ('configured', 'v'), ('cause', 'n'), ('least', 'a'), ('one', None), ('generate', 'n'), ('prompt', 'n'), ('information', 'n'), ('based', 'v'), ('least', 'a'), ('one', None), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'n'), ('threshold', 'v'), ('prompt', 'a'), ('information', 'n'), ('used', 'v'), ('prompting', 'v'), ('terminal', 'a'), ('stop', 'n'), ('moving', 'v'), ('terminal', 'a'), ('according', 'v'), ('wherein', 'a'), ('motion', 'n'), ('parameter', 'n'), ('comprises', 'v'), ('motion', 'n'), ('frequency', 'n'), ('motion', 'n'), ('time', 'n'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('storing', 'v'), ('machine', 'n'), ('instruction', 'n'), ('executed', 'v'), ('one', None), ('causes', 'v'), ('one', None), ('perform', 'n'), ('obtaining', 'v'), ('acquired', 'v'), ('capturing', 'v'), ('apparatus', 'n'), ('obtaining', 'v'), ('motion', 'n'), ('parameter', 'n'), ('terminal', 'a'), ('terminal', 'n'), ('comprising', 'v'), ('capturing', 'v'), ('apparatus', 'a'), ('motion', 'n'), ('parameter', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('motion', 'n'), ('frequency', 'n'), ('motion', 'n'), ('time', 'n'), ('two', None), ('parameters', 'n'), ('among', None), ('acceleration', 'n'), ('angular', 'a'), ('velocity', 'n'), ('motion', 'n'), ('amplitude', 'n'), ('motion', 'n'), ('frequency', 'n'), ('motion', 'n'), ('time', 'n'), ('transmitting', 'v'), ('parameter', 'n'), ('threshold', 'v'), ('obtaining', 'v'), ('request', 'n'), ('data', 'n'), ('management', 'n'), ('server', 'r'), ('parameter', 'r'), ('threshold', 'a'), ('obtaining', 'v'), ('request', 'n'), ('comprising', 'v'), ('configuration', 'n'), ('information', 'n'), ('terminal', 'n'), ('receiving', 'n'), ('corresponding', 'v'), ('preset', 'n'), ('thresholds', 'n'), ('correspond', 'n'), ('configuration', 'n'), ('information', 'n'), ('response', 'n'), ('parameter', 'n'), ('threshold', 'v'), ('obtaining', 'v'), ('request', 'n'), ('comparing', 'v'), ('two', None), ('parameters', 'n'), ('corresponding', 'v'), ('preset', 'n'), ('thresholds', 'n'), ('controlling', 'v'), ('perform', 'n'), ('processing', 'n'), ('acquired', 'v'), ('based', 'v'), ('least', 'a'), ('one', None), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'n'), ('threshold', 'n'), ('based', 'v'), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'n'), ('respectively', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'n'), ('thresholds', 'n'), ('wherein', 'v'), ('acquiring', 'v'), ('comprises', 'n'), ('acquiring', 'v'), ('real', 'a'), ('time', 'n'), ('obtaining', 'v'), ('comprises', 'n'), ('obtaining', 'v'), ('motion', 'n'), ('parameter', 'n'), ('terminal', 'a'), ('real', 'a'), ('time', 'n'), ('comprising', 'v'), ('response', 'n'), ('least', 'a'), ('one', None), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'r'), ('greater', 'a'), ('corresponding', 'v'), ('preset', 'n'), ('threshold', 'v'), ('obtaining', 'v'), ('motion', 'n'), ('parameter', None), ('terminal', 'a'), ('response', 'n'), ('two', None), ('parameters', 'n'), ('motion', 'v'), ('parameter', 'n'), ('obtained', 'v'), ('latest', 'a'), ('time', 'n'), ('less', None), ('equal', 'a'), ('corresponding', 'n'), ('preset', 'n'), ('thresholds', 'n'), ('performing', 'v'), ('processing', 'n'), ('acquired', 'v'), ('latest', 'a'), ('time', 'n'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('according', 'v'), ('wherein', 'n'), ('acquired', 'v'), ('face', 'n'), ('processing', 'n'), ('comprises', 'v'), ('performing', 'v'), ('face', 'n'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('according', 'v'), ('wherein', 'a'), ('obtaining', 'v'), ('motion', 'n'), ('parameter', 'n'), ('comprises', 'v'), ('least', 'a'), ('one', None), ('obtaining', 'v'), ('acceleration', 'n'), ('terminal', 'n'), ('using', 'v'), ('acceleration', 'n'), ('sensor', 'n'), ('obtaining', 'v'), ('angular', 'a'), ('velocity', 'n'), ('terminal', 'n'), ('using', 'v'), ('gyro', 'a'), ('sensor', 'a'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('according', 'v'), ('wherein', 'a'), ('motion', 'n'), ('parameter', 'n'), ('comprises', 'v'), ('motion', 'n'), ('frequency', 'n'), ('motion', 'n'), ('time', 'n'), ('processing', 'v'), ('drive-through', 'a'), ('order', 'n'), ('comprising', 'v'), ('receiving', 'v'), ('customer', 'n'), ('information', 'n'), ('detected', 'v'), ('vision', 'n'), ('providing', 'v'), ('product', 'n'), ('information', 'n'), ('customer', 'n'), ('based', 'v'), ('customer', 'n'), ('information', 'n'), ('processing', 'v'), ('product', 'n'), ('order', 'n'), ('customer', 'n'), ('according', 'v'), ('wherein', 'n'), ('receiving', 'v'), ('customer', 'n'), ('information', 'n'), ('comprises', 'v'), ('least', 'a'), ('one', None), ('receiving', 'v'), ('customer', 'n'), ('information', 'n'), ('associated', 'v'), ('vehicle', 'n'), ('information', 'n'), ('detected', 'v'), ('vehicle', 'n'), ('receiving', 'v'), ('customer', 'n'), ('information', 'n'), ('associated', 'v'), ('identification', 'n'), ('information', 'n'), ('detected', 'v'), ('face', 'n'), ('according', 'v'), ('comprising', 'v'), ('determining', 'v'), ('whether', None), ('customer', 'n'), ('pre-order', 'n'), ('customer', 'n'), ('based', 'v'), ('customer', 'n'), ('information', 'n'), ('wherein', 'n'), ('customer', 'n'), ('determined', 'v'), ('pre-order', 'a'), ('customer', 'n'), ('providing', 'v'), ('product', 'n'), ('information', 'n'), ('based', 'v'), ('customer', 'n'), ('information', 'n'), ('comprises', 'v'), ('providing', 'v'), ('pre-order', 'a'), ('information', 'n'), ('using', 'v'), ('least', 'a'), ('one', None), ('audio', 'a'), ('video', 'n'), ('processing', 'v'), ('product', 'n'), ('order', 'n'), ('customer', 'n'), ('comprises', 'v'), ('providing', 'v'), ('information', 'n'), ('promptly', 'r'), ('guiding', 'v'), ('vehicle', 'n'), ('pickup', 'n'), ('stand', 'v'), ('using', 'v'), ('least', 'a'), ('one', None), ('audio', 'n'), ('video', 'n'), ('providing', 'v'), ('information', 'n'), ('additional', 'a'), ('order', 'n'), ('available', 'a'), ('according', 'v'), ('wherein', 'a'), ('product', 'n'), ('information', 'n'), ('based', 'v'), ('customer', 'n'), ('information', 'n'), ('comprises', 'v'), ('recently', 'r'), ('ordered', 'v'), ('product', 'n'), ('component', 'n'), ('frequently', 'r'), ('ordered', 'v'), ('product', 'n'), ('component', 'n'), ('order', 'n'), ('history', 'n'), ('customer', 'n'), ('information', 'n'), ('according', 'v'), ('wherein', 'n'), ('receiving', 'v'), ('customer', 'n'), ('information', 'n'), ('comprises', 'v'), ('receiving', 'v'), ('information', 'n'), ('age', 'n'), ('gender', 'n'), ('passenger', 'n'), ('detected', 'v'), ('face', 'n'), ('providing', 'v'), ('product', 'n'), ('information', 'n'), ('customer', 'n'), ('based', 'v'), ('customer', 'n'), ('information', 'n'), ('comprises', 'v'), ('providing', 'v'), ('recommended', 'v'), ('menu', 'a'), ('information', 'n'), ('differentiated', 'v'), ('according', 'v'), ('age', 'n'), ('gender', 'n'), ('according', 'v'), ('wherein', 'a'), ('processing', 'n'), ('product', 'n'), ('order', 'n'), ('customer', 'n'), ('comprises', 'v'), ('determining', 'v'), ('product', 'n'), ('component', 'n'), ('past', 'a'), ('order', 'n'), ('history', 'n'), ('component', 'n'), ('modified', 'v'), ('product', 'n'), ('component', 'n'), ('product', 'n'), ('order', 'n'), ('according', 'v'), ('wherein', 'n'), ('processing', 'n'), ('product', 'n'), ('order', 'n'), ('customer', 'n'), ('comprises', 'v'), ('paying', 'v'), ('product', 'n'), ('price', 'n'), ('according', 'v'), ('biometrics-based', 'a'), ('authentication', 'n'), ('communication', 'n'), ('system', 'n'), ('vehicle', 'n'), ('mobile', None), ('terminal', 'a'), ('according', 'v'), ('wherein', 'a'), ('processing', 'n'), ('product', 'n'), ('order', 'n'), ('customer', 'n'), ('comprises', 'v'), ('issuing', 'v'), ('payment', 'n'), ('number', 'n'), ('divided', 'v'), ('payment', 'n'), ('performing', 'v'), ('divided', 'v'), ('payments', 'n'), ('according', 'v'), ('payment', 'n'), ('requests', 'n'), ('mobile', 'v'), ('terminals', 'n'), ('payment', 'n'), ('numbers', 'n'), ('inputted', 'v'), ('according', 'v'), ('wherein', 'n'), ('processing', 'n'), ('product', 'n'), ('order', 'n'), ('customer', 'n'), ('comprises', 'v'), ('accumulating', 'v'), ('mileage', 'n'), ('account', 'n'), ('corresponding', 'v'), ('mobile', 'a'), ('terminal', 'a'), ('undergoing', 'a'), ('payment', 'n'), ('according', 'v'), ('wherein', 'n'), ('processing', 'n'), ('product', 'n'), ('order', 'n'), ('customer', 'n'), ('comprises', 'v'), ('suggesting', 'v'), ('takeout', 'r'), ('packaging', 'v'), ('according', 'v'), ('temperature', 'n'), ('product', 'n'), ('atmospheric', 'a'), ('temperature', 'n'), ('weather', 'n'), ('vehicle', 'n'), ('type', 'n'), ('apparatus', 'n'), ('configured', 'v'), ('process', 'a'), ('drive-through', 'a'), ('order', 'n'), ('apparatus', 'n'), ('comprising', 'v'), ('transceiver', 'r'), ('configured', 'v'), ('receive', 'a'), ('customer', 'n'), ('information', 'n'), ('detected', 'v'), ('vision', 'n'), ('digital', 'a'), ('signage', 'n'), ('configured', 'v'), ('provide', 'a'), ('product', 'n'), ('information', 'n'), ('customer', 'n'), ('based', 'v'), ('customer', 'n'), ('information', 'n'), ('configured', 'v'), ('process', 'n'), ('product', 'n'), ('order', 'n'), ('customer', 'n'), ('apparatus', 'n'), ('according', 'v'), ('wherein', 'n'), ('transceiver', 'n'), ('receives', 'v'), ('least', 'a'), ('one', None), ('customer', 'n'), ('information', 'n'), ('associated', 'v'), ('vehicle', 'n'), ('information', 'n'), ('detected', 'v'), ('vehicle', 'n'), ('customer', 'n'), ('information', 'n'), ('associated', 'v'), ('identification', 'n'), ('information', 'n'), ('detected', 'v'), ('face', 'n'), ('apparatus', 'n'), ('according', 'v'), ('wherein', 'n'), ('configured', 'v'), ('determine', 'n'), ('whether', None), ('customer', 'n'), ('pre-order', 'n'), ('customer', 'n'), ('based', 'v'), ('customer', 'n'), ('information', 'n'), ('customer', 'n'), ('determined', 'v'), ('pre-order', 'a'), ('customer', 'n'), ('perform', 'n'), ('control', 'n'), ('operation', 'n'), ('provide', None), ('pre-order', 'a'), ('information', 'n'), ('control', 'n'), ('digital', 'a'), ('signage', 'n'), ('output', 'n'), ('information', 'n'), ('promptly', 'r'), ('guiding', 'v'), ('vehicle', 'n'), ('pickup', 'n'), ('stand', 'v'), ('provide', 'a'), ('information', 'n'), ('additional', 'a'), ('order', 'n'), ('available', 'a'), ('apparatus', 'n'), ('according', 'v'), ('wherein', 'a'), ('product', 'n'), ('information', 'n'), ('based', 'v'), ('customer', 'n'), ('information', 'n'), ('comprises', 'v'), ('recently', 'r'), ('ordered', 'v'), ('product', 'n'), ('component', 'n'), ('frequently', 'r'), ('ordered', 'v'), ('product', 'n'), ('component', 'n'), ('order', 'n'), ('history', 'n'), ('customer', 'n'), ('information', 'n'), ('apparatus', 'n'), ('according', 'v'), ('wherein', 'n'), ('transceiver', 'n'), ('configured', 'v'), ('receive', 'a'), ('information', 'n'), ('age', 'n'), ('gender', 'n'), ('passenger', 'n'), ('detected', 'v'), ('face', 'n'), ('configured', 'v'), ('control', 'n'), ('digital', 'a'), ('signage', 'n'), ('provide', 'n'), ('recommended', 'v'), ('menu', 'a'), ('information', 'n'), ('differentiated', 'v'), ('according', 'v'), ('age', 'n'), ('gender', 'n'), ('apparatus', 'n'), ('according', 'v'), ('wherein', 'n'), ('configured', 'v'), ('determine', 'a'), ('product', 'n'), ('component', 'n'), ('past', 'a'), ('order', 'n'), ('history', 'n'), ('component', 'n'), ('modified', 'v'), ('product', 'n'), ('component', 'n'), ('product', 'n'), ('order', 'n'), ('apparatus', 'n'), ('according', 'v'), ('wherein', 'n'), ('configured', 'a'), ('pay', 'n'), ('product', 'n'), ('price', 'n'), ('according', 'v'), ('biometrics-based', 'a'), ('authentication', 'n'), ('communication', 'n'), ('system', 'n'), ('vehicle', 'n'), ('mobile', 'a'), ('terminal', 'n'), ('apparatus', 'n'), ('according', 'v'), ('wherein', 'n'), ('configured', 'v'), ('issue', 'n'), ('payment', 'n'), ('number', 'n'), ('divided', 'v'), ('payment', 'n'), ('perform', 'n'), ('divided', 'v'), ('payments', 'n'), ('according', 'v'), ('requests', 'n'), ('mobile', 'a'), ('terminals', 'n'), ('payment', 'n'), ('numbers', 'n'), ('inputted', 'v'), ('apparatus', 'r'), ('according', 'v'), ('wherein', 'n'), ('configured', 'v'), ('accumulate', 'a'), ('mileage', 'n'), ('account', 'n'), ('corresponding', 'v'), ('mobile', 'a'), ('terminal', 'a'), ('undergoing', 'a'), ('payment', 'n'), ('apparatus', 'n'), ('according', 'v'), ('wherein', 'n'), ('configured', 'v'), ('control', 'n'), ('digital', 'a'), ('signage', 'n'), ('suggest', 'v'), ('takeout', None), ('packaging', 'v'), ('according', 'v'), ('temperature', 'n'), ('product', 'n'), ('atmospheric', 'a'), ('temperature', 'n'), ('weather', 'n'), ('vehicle', 'n'), ('type', 'n'), ('information', 'n'), ('processing', 'n'), ('performed', 'v'), ('computing', 'v'), ('device', 'n'), ('one', None), ('memory', 'n'), ('storing', 'v'), ('programs', 'n'), ('executed', 'v'), ('one', None), ('comprising', 'v'), ('identifying', 'v'), ('using', 'v'), ('face', 'n'), ('one', None), ('face', 'n'), ('corresponding', 'v'), ('respective', 'a'), ('person', 'n'), ('captured', 'v'), ('first', 'r'), ('identified', 'v'), ('face', 'n'), ('extracting', 'v'), ('set', 'v'), ('profile', 'a'), ('parameters', 'n'), ('corresponding', 'v'), ('person', 'n'), ('first', 'r'), ('selecting', 'v'), ('tiles', 'n'), ('first', 'r'), ('tile', 'a'), ('matches', 'n'), ('face', 'v'), ('corresponding', 'v'), ('person', 'n'), ('first', 'a'), ('accordance', 'n'), ('predefined', 'v'), ('correspondence', 'n'), ('set', 'v'), ('profile', 'a'), ('parameters', 'n'), ('corresponding', 'v'), ('person', 'n'), ('set', 'v'), ('pre-stored', 'a'), ('description', 'n'), ('parameters', 'n'), ('first', 'r'), ('tile', None), ('generating', 'v'), ('second', 'a'), ('covering', 'v'), ('respective', 'a'), ('persons', 'n'), ('first', 'r'), ('corresponding', 'v'), ('first', 'a'), ('tiles', 'n'), ('sharing', 'v'), ('first', 'a'), ('second', 'a'), ('predefined', 'v'), ('order', 'n'), ('via', None), ('group', 'n'), ('chat', None), ('session', 'n'), ('wherein', 'v'), ('first', 'a'), ('second', 'a'), ('displayed', 'v'), ('group', 'n'), ('chat', None), ('session', 'n'), ('one', None), ('time', 'n'), ('one', None), ('two', None), ('replaced', 'v'), ('two', None), ('periodically', 'r'), ('wherein', 'v'), ('extracting', 'v'), ('set', 'v'), ('profile', 'a'), ('parameters', 'n'), ('corresponding', 'v'), ('person', 'n'), ('first', 'r'), ('includes', 'v'), ('determining', 'v'), ('one', None), ('descriptive', 'a'), ('labels', 'n'), ('corresponding', 'v'), ('identified', 'a'), ('face', 'n'), ('corresponding', 'v'), ('person', 'n'), ('using', 'v'), ('first', 'a'), ('machine', 'n'), ('learning', 'v'), ('model', 'n'), ('wherein', 'n'), ('first', 'a'), ('machine', 'n'), ('learning', 'v'), ('model', 'n'), ('trained', 'v'), ('facial', 'a'), ('corresponding', 'v'), ('descriptive', 'a'), ('labels', 'n'), ('wherein', 'v'), ('extracting', 'v'), ('set', 'v'), ('profile', 'a'), ('parameters', 'n'), ('corresponding', 'v'), ('person', 'n'), ('first', 'r'), ('includes', 'v'), ('determining', 'v'), ('identity', 'n'), ('corresponding', 'v'), ('person', 'n'), ('based', 'v'), ('identified', 'a'), ('face', 'n'), ('corresponding', 'v'), ('person', 'n'), ('locating', 'v'), ('respective', 'a'), ('profile', 'n'), ('information', 'n'), ('first', 'r'), ('person', 'n'), ('based', 'v'), ('determined', 'v'), ('identity', 'n'), ('corresponding', 'v'), ('person', 'n'), ('using', 'v'), ('one', None), ('characteristics', 'n'), ('respective', 'a'), ('profile', 'a'), ('information', 'n'), ('first', 'r'), ('person', 'n'), ('set', 'v'), ('profile', 'n'), ('parameters', 'n'), ('corresponding', 'v'), ('identified', 'a'), ('face', 'n'), ('corresponding', 'v'), ('person', 'n'), ('wherein', 'v'), ('least', 'a'), ('first', 'a'), ('one', None), ('first', 'a'), ('tiles', 'v'), ('dynamic', 'a'), ('tile', 'a'), ('least', 'a'), ('second', 'a'), ('one', None), ('first', 'a'), ('tiles', 'v'), ('static', 'a'), ('tile', 'n'), ('including', 'v'), ('receiving', 'v'), ('comments', 'n'), ('different', 'a'), ('group', 'n'), ('chat', None), ('session', 'n'), ('comment', 'n'), ('including', 'v'), ('descriptive', 'a'), ('term', 'n'), ('respective', 'a'), ('person', 'n'), ('identified', 'v'), ('first', 'a'), ('choosing', 'v'), ('descriptive', 'a'), ('label', 'n'), ('respective', 'a'), ('person', 'n'), ('according', 'v'), ('comments', 'n'), ('updating', 'v'), ('second', 'a'), ('adding', 'v'), ('descriptive', 'a'), ('label', 'n'), ('adjacent', 'n'), ('first', 'r'), ('tile', 'r'), ('respective', 'a'), ('person', 'n'), ('computing', 'v'), ('device', 'n'), ('information', 'n'), ('processing', 'v'), ('comprising', 'v'), ('one', None), ('memory', 'n'), ('storing', 'v'), ('instructions', 'n'), ('executed', 'v'), ('one', None), ('cause', 'n'), ('perform', 'n'), ('operations', 'n'), ('comprising', 'v'), ('identifying', 'v'), ('using', 'v'), ('face', 'n'), ('one', None), ('face', 'n'), ('corresponding', 'v'), ('respective', 'a'), ('person', 'n'), ('captured', 'v'), ('first', 'r'), ('identified', 'v'), ('face', 'n'), ('extracting', 'v'), ('set', 'v'), ('profile', 'a'), ('parameters', 'n'), ('corresponding', 'v'), ('person', 'n'), ('first', 'r'), ('selecting', 'v'), ('tiles', 'n'), ('first', 'r'), ('tile', 'a'), ('matches', 'n'), ('face', 'v'), ('corresponding', 'v'), ('person', 'n'), ('first', 'a'), ('accordance', 'n'), ('predefined', 'v'), ('correspondence', 'n'), ('set', 'v'), ('profile', 'a'), ('parameters', 'n'), ('corresponding', 'v'), ('person', 'n'), ('set', 'v'), ('pre-stored', 'a'), ('description', 'n'), ('parameters', 'n'), ('first', 'r'), ('tile', None), ('generating', 'v'), ('second', 'a'), ('covering', 'v'), ('respective', 'a'), ('persons', 'n'), ('first', 'r'), ('corresponding', 'v'), ('first', 'a'), ('tiles', 'n'), ('sharing', 'v'), ('first', 'a'), ('second', 'a'), ('predefined', 'v'), ('order', 'n'), ('via', None), ('group', 'n'), ('chat', None), ('session', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'v'), ('first', 'a'), ('second', 'a'), ('displayed', 'v'), ('group', 'n'), ('chat', None), ('session', 'n'), ('one', None), ('time', 'n'), ('one', None), ('two', None), ('replaced', 'v'), ('two', None), ('periodically', 'r'), ('computing', 'v'), ('device', 'n'), ('wherein', 'r'), ('extracting', 'v'), ('set', 'v'), ('profile', 'a'), ('parameters', 'n'), ('corresponding', 'v'), ('person', 'n'), ('first', 'r'), ('includes', 'v'), ('determining', 'v'), ('one', None), ('descriptive', 'a'), ('labels', 'n'), ('corresponding', 'v'), ('identified', 'a'), ('face', 'n'), ('corresponding', 'v'), ('person', 'n'), ('using', 'v'), ('first', 'a'), ('machine', 'n'), ('learning', 'v'), ('model', 'n'), ('wherein', 'n'), ('first', 'a'), ('machine', 'n'), ('learning', 'v'), ('model', 'n'), ('trained', 'v'), ('facial', 'a'), ('corresponding', 'v'), ('descriptive', 'a'), ('labels', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'r'), ('extracting', 'v'), ('set', 'v'), ('profile', 'a'), ('parameters', 'n'), ('corresponding', 'v'), ('person', 'n'), ('first', 'r'), ('includes', 'v'), ('determining', 'v'), ('identity', 'n'), ('corresponding', 'v'), ('person', 'n'), ('based', 'v'), ('identified', 'a'), ('face', 'n'), ('corresponding', 'v'), ('person', 'n'), ('locating', 'v'), ('respective', 'a'), ('profile', 'n'), ('information', 'n'), ('first', 'r'), ('person', 'n'), ('based', 'v'), ('determined', 'v'), ('identity', 'n'), ('corresponding', 'v'), ('person', 'n'), ('using', 'v'), ('one', None), ('characteristics', 'n'), ('respective', 'a'), ('profile', 'a'), ('information', 'n'), ('first', 'r'), ('person', 'n'), ('set', 'v'), ('profile', 'n'), ('parameters', 'n'), ('corresponding', 'v'), ('identified', 'a'), ('face', 'n'), ('corresponding', 'v'), ('person', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'n'), ('least', 'v'), ('first', 'a'), ('one', None), ('first', 'a'), ('tiles', 'v'), ('dynamic', 'a'), ('tile', 'a'), ('least', 'a'), ('second', 'a'), ('one', None), ('first', 'a'), ('tiles', 'v'), ('static', 'a'), ('tile', 'n'), ('computing', 'v'), ('device', 'n'), ('wherein', 'n'), ('operations', 'n'), ('include', 'v'), ('receiving', 'v'), ('comments', 'n'), ('different', 'a'), ('group', 'n'), ('chat', None), ('session', 'n'), ('comment', 'n'), ('including', 'v'), ('descriptive', 'a'), ('term', 'n'), ('respective', 'a'), ('person', 'n'), ('identified', 'v'), ('first', 'a'), ('choosing', 'v'), ('descriptive', 'a'), ('label', 'n'), ('respective', 'a'), ('person', 'n'), ('according', 'v'), ('comments', 'n'), ('updating', 'v'), ('second', 'a'), ('adding', 'v'), ('descriptive', 'a'), ('label', 'n'), ('adjacent', 'n'), ('first', 'r'), ('tile', 'r'), ('respective', 'a'), ('person', 'n'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('storing', 'v'), ('instructions', 'n'), ('executed', 'v'), ('computing', 'v'), ('device', 'n'), ('one', None), ('cause', 'n'), ('computing', 'v'), ('device', 'n'), ('perform', 'n'), ('operations', 'n'), ('comprising', 'v'), ('identifying', 'v'), ('using', 'v'), ('face', 'n'), ('one', None), ('face', 'n'), ('corresponding', 'v'), ('respective', 'a'), ('person', 'n'), ('captured', 'v'), ('first', 'r'), ('identified', 'v'), ('face', 'n'), ('extracting', 'v'), ('set', 'v'), ('profile', 'a'), ('parameters', 'n'), ('corresponding', 'v'), ('person', 'n'), ('first', 'r'), ('selecting', 'v'), ('tiles', 'n'), ('first', 'r'), ('tile', 'a'), ('matches', 'n'), ('face', 'v'), ('corresponding', 'v'), ('person', 'n'), ('first', 'a'), ('accordance', 'n'), ('predefined', 'v'), ('correspondence', 'n'), ('set', 'v'), ('profile', 'a'), ('parameters', 'n'), ('corresponding', 'v'), ('person', 'n'), ('set', 'v'), ('pre-stored', 'a'), ('description', 'n'), ('parameters', 'n'), ('first', 'r'), ('tile', None), ('generating', 'v'), ('second', 'a'), ('covering', 'v'), ('respective', 'a'), ('persons', 'n'), ('first', 'r'), ('corresponding', 'v'), ('first', 'a'), ('tiles', 'n'), ('sharing', 'v'), ('first', 'a'), ('second', 'a'), ('predefined', 'v'), ('order', 'n'), ('via', None), ('group', 'n'), ('chat', None), ('session', 'n'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('wherein', 'n'), ('first', 'a'), ('second', 'a'), ('displayed', 'v'), ('group', 'n'), ('chat', None), ('session', 'n'), ('one', None), ('time', 'n'), ('one', None), ('two', None), ('replaced', 'v'), ('two', None), ('periodically', 'r'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('wherein', 'n'), ('extracting', 'v'), ('set', 'v'), ('profile', 'a'), ('parameters', 'n'), ('corresponding', 'v'), ('person', 'n'), ('first', 'r'), ('includes', 'v'), ('determining', 'v'), ('one', None), ('descriptive', 'a'), ('labels', 'n'), ('corresponding', 'v'), ('identified', 'a'), ('face', 'n'), ('corresponding', 'v'), ('person', 'n'), ('using', 'v'), ('first', 'a'), ('machine', 'n'), ('learning', 'v'), ('model', 'n'), ('wherein', 'n'), ('first', 'a'), ('machine', 'n'), ('learning', 'v'), ('model', 'n'), ('trained', 'v'), ('facial', 'a'), ('corresponding', 'v'), ('descriptive', 'a'), ('labels', 'n'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('wherein', 'n'), ('extracting', 'v'), ('set', 'v'), ('profile', 'a'), ('parameters', 'n'), ('corresponding', 'v'), ('person', 'n'), ('first', 'r'), ('includes', 'v'), ('determining', 'v'), ('identity', 'n'), ('corresponding', 'v'), ('person', 'n'), ('based', 'v'), ('identified', 'a'), ('face', 'n'), ('corresponding', 'v'), ('person', 'n'), ('locating', 'v'), ('respective', 'a'), ('profile', 'n'), ('information', 'n'), ('first', 'r'), ('person', 'n'), ('based', 'v'), ('determined', 'v'), ('identity', 'n'), ('corresponding', 'v'), ('person', 'n'), ('using', 'v'), ('one', None), ('characteristics', 'n'), ('respective', 'a'), ('profile', 'a'), ('information', 'n'), ('first', 'r'), ('person', 'n'), ('set', 'v'), ('profile', 'n'), ('parameters', 'n'), ('corresponding', 'v'), ('identified', 'a'), ('face', 'n'), ('corresponding', 'v'), ('person', 'n'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('wherein', 'n'), ('least', 'v'), ('first', 'a'), ('one', None), ('first', 'a'), ('tiles', 'v'), ('dynamic', 'a'), ('tile', 'a'), ('least', 'a'), ('second', 'a'), ('one', None), ('first', 'a'), ('tiles', 'v'), ('static', 'a'), ('tile', None), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('wherein', 'a'), ('operations', 'n'), ('include', 'v'), ('receiving', 'v'), ('comments', 'n'), ('different', 'a'), ('group', 'n'), ('chat', None), ('session', 'n'), ('comment', 'n'), ('including', 'v'), ('descriptive', 'a'), ('term', 'n'), ('respective', 'a'), ('person', 'n'), ('identified', 'v'), ('first', 'a'), ('choosing', 'v'), ('descriptive', 'a'), ('label', 'n'), ('respective', 'a'), ('person', 'n'), ('according', 'v'), ('comments', 'n'), ('updating', 'v'), ('second', 'a'), ('adding', 'v'), ('descriptive', 'a'), ('label', 'n'), ('adjacent', 'n'), ('first', 'r'), ('tile', 'r'), ('respective', 'a'), ('person', 'n'), ('comprising', 'v'), ('computing', 'v'), ('system', 'n'), ('determining', 'v'), ('performance', 'n'), ('metric', 'a'), ('eye', 'n'), ('system', 'n'), ('first', 'a'), ('performance', 'n'), ('threshold', 'n'), ('wherein', 'n'), ('eye', 'n'), ('system', 'n'), ('associated', 'v'), ('head-mounted', 'a'), ('display', 'n'), ('worn', 'n'), ('based', 'v'), ('determination', 'n'), ('performance', 'n'), ('metric', 'a'), ('eye', 'n'), ('system', 'n'), ('first', 'a'), ('performance', 'n'), ('threshold', 'v'), ('computer', 'n'), ('system', 'n'), ('performing', 'v'), ('receiving', 'v'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('estimating', 'v'), ('region', 'n'), ('looking', 'v'), ('within', None), ('field', 'n'), ('view', 'n'), ('head-mounted', 'a'), ('display', 'n'), ('based', 'v'), ('received', 'v'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('determining', 'v'), ('vergence', 'n'), ('distance', 'n'), ('based', 'v'), ('least', 'a'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('estimated', 'v'), ('region', 'n'), ('looking', 'v'), ('locations', 'n'), ('one', None), ('objects', 'v'), ('scene', 'n'), ('displayed', 'v'), ('head-mounted', 'a'), ('display', 'n'), ('adjusting', 'v'), ('one', None), ('configurations', 'n'), ('head-mounted', 'a'), ('display', 'n'), ('based', 'v'), ('determined', 'a'), ('vergence', 'n'), ('distance', 'n'), ('wherein', 'v'), ('one', None), ('configurations', 'n'), ('head-mounted', 'a'), ('display', 'n'), ('comprise', 'n'), ('one', None), ('rendering', 'n'), ('position', 'n'), ('display', 'n'), ('screen', 'a'), ('position', 'n'), ('optics', 'n'), ('block', 'v'), ('comprising', 'v'), ('determining', 'v'), ('performance', 'n'), ('metric', 'a'), ('eye', 'n'), ('system', 'n'), ('second', 'a'), ('performance', 'n'), ('threshold', 'v'), ('receiving', 'v'), ('eye', 'n'), ('data', 'n'), ('eye', 'n'), ('system', 'n'), ('determining', 'v'), ('vergence', 'n'), ('distance', 'n'), ('based', 'v'), ('eye', 'n'), ('data', 'n'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('comprising', 'v'), ('receiving', 'v'), ('one', None), ('second', 'a'), ('inputs', 'n'), ('associated', 'v'), ('one', None), ('displaying', 'n'), ('elements', 'n'), ('scene', 'v'), ('displayed', 'v'), ('head-mounted', 'a'), ('display', 'n'), ('determining', 'v'), ('vergence', 'n'), ('distance', 'n'), ('based', 'v'), ('least', 'a'), ('eye', 'n'), ('data', 'n'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('one', None), ('second', 'a'), ('inputs', 'n'), ('associated', 'v'), ('one', None), ('displaying', 'n'), ('elements', 'n'), ('scene', 'v'), ('comprising', 'v'), ('feeding', 'v'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('fusion', 'n'), ('algorithm', None), ('wherein', 'a'), ('fusion', 'n'), ('algorithm', 'n'), ('assigns', 'n'), ('weight', 'v'), ('score', 'r'), ('input', 'a'), ('one', None), ('first', 'a'), ('inputs', 'v'), ('determining', 'v'), ('vergence', 'n'), ('distance', 'n'), ('using', 'v'), ('fusion', 'n'), ('algorithm', 'n'), ('based', 'v'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('determining', 'v'), ('z-depth', 'a'), ('display', 'n'), ('screen', 'n'), ('confidence', 'n'), ('score', 'n'), ('based', 'v'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('comprising', 'v'), ('comparing', 'v'), ('confidence', 'n'), ('score', 'n'), ('confidence', 'n'), ('level', 'n'), ('threshold', 'a'), ('response', 'n'), ('determination', 'n'), ('confidence', 'n'), ('score', 'n'), ('confidence', 'n'), ('level', 'n'), ('threshold', 'v'), ('feeding', 'v'), ('one', None), ('second', 'a'), ('inputs', 'n'), ('associated', 'v'), ('one', None), ('displaying', 'n'), ('elements', 'n'), ('scene', 'a'), ('fusion', 'n'), ('algorithm', None), ('determining', 'v'), ('z-depth', 'a'), ('display', 'n'), ('screen', 'n'), ('using', 'v'), ('fusion', 'n'), ('algorithm', 'n'), ('based', 'v'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('one', None), ('second', 'a'), ('inputs', 'n'), ('associated', 'v'), ('one', None), ('displaying', 'n'), ('elements', 'n'), ('scene', 'v'), ('comparing', 'v'), ('comparing', 'v'), ('fusion', 'n'), ('algorithm', 'n'), ('confidence', 'n'), ('scores', 'n'), ('associated', 'v'), ('combinations', 'n'), ('inputs', 'n'), ('determining', 'v'), ('fusion', 'n'), ('algorithm', None), ('z-depth', 'a'), ('display', 'n'), ('screen', 'n'), ('based', 'v'), ('combination', 'n'), ('inputs', 'n'), ('associated', 'v'), ('highest', 'a'), ('confidence', 'n'), ('score', 'n'), ('wherein', 'v'), ('z-depth', 'a'), ('confidence', 'n'), ('score', 'n'), ('determined', 'v'), ('fusion', 'n'), ('algorithm', None), ('using', 'v'), ('piecewise', 'n'), ('comparison', 'n'), ('one', None), ('first', 'a'), ('inputs', 'v'), ('one', None), ('second', 'n'), ('inputs', 'v'), ('wherein', 'a'), ('z-depth', 'a'), ('confidence', 'n'), ('score', 'n'), ('determined', 'v'), ('based', 'v'), ('correlation', 'n'), ('two', None), ('inputs', 'n'), ('one', None), ('first', 'a'), ('inputs', 'v'), ('one', None), ('second', 'n'), ('inputs', 'v'), ('wherein', 'a'), ('fusion', 'n'), ('algorithm', 'n'), ('comprises', 'v'), ('machine', 'n'), ('learning', 'v'), ('ml', 'a'), ('algorithm', 'a'), ('wherein', 'n'), ('machine', 'n'), ('learning', 'v'), ('ml', 'a'), ('algorithm', 'a'), ('determines', 'n'), ('combination', 'n'), ('first', 'r'), ('inputs', 'v'), ('fed', 'a'), ('fusion', 'n'), ('algorithm', 'v'), ('wherein', None), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('comprise', 'n'), ('one', None), ('hand', 'n'), ('position', 'n'), ('hand', 'n'), ('direction', 'n'), ('hand', 'n'), ('movement', 'n'), ('hand', 'n'), ('gesture', 'n'), ('head', 'n'), ('position', 'n'), ('head', 'n'), ('direction', 'n'), ('head', 'n'), ('movement', 'n'), ('head', 'n'), ('gesture', 'n'), ('gaze', 'n'), ('angle', 'v'), ('rea', 'n'), ('body', 'n'), ('gesture', 'n'), ('body', 'n'), ('posture', 'n'), ('body', 'n'), ('movement', 'n'), ('behavior', 'n'), ('weighted', 'v'), ('combination', 'n'), ('one', None), ('related', 'a'), ('parameters', 'n'), ('wherein', 'v'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('received', 'v'), ('one', None), ('controller', 'n'), ('sensor', 'n'), ('camera', 'n'), ('microphone', 'n'), ('accelerometer', 'n'), ('headset', 'v'), ('worn', 'v'), ('mobile', 'a'), ('device', 'n'), ('wherein', 'v'), ('one', None), ('second', 'n'), ('inputs', 'n'), ('associated', 'v'), ('one', None), ('displaying', 'n'), ('elements', 'n'), ('comprise', 'v'), ('one', None), ('z-buffer', 'n'), ('value', 'n'), ('associated', 'v'), ('displaying', 'v'), ('element', 'n'), ('displaying', 'v'), ('element', 'n'), ('marked', 'v'), ('developer', 'n'), ('analysis', 'n'), ('result', 'n'), ('shape', 'n'), ('displaying', 'v'), ('element', 'a'), ('face', 'n'), ('result', 'n'), ('object', 'v'), ('result', 'n'), ('person', 'n'), ('identified', 'v'), ('displaying', 'v'), ('content', 'n'), ('object', 'n'), ('identified', 'v'), ('displaying', 'v'), ('content', 'a'), ('correlation', 'n'), ('two', None), ('displaying', 'v'), ('elements', 'n'), ('weighted', 'v'), ('combination', 'n'), ('one', None), ('second', 'n'), ('inputs', 'v'), ('comprising', 'v'), ('determining', 'v'), ('performance', 'n'), ('metric', 'a'), ('eye', 'n'), ('system', 'n'), ('second', 'a'), ('performance', 'n'), ('threshold', 'v'), ('receiving', 'v'), ('one', None), ('second', 'a'), ('inputs', 'n'), ('associated', 'v'), ('one', None), ('displaying', 'n'), ('elements', 'n'), ('scene', 'v'), ('displayed', 'v'), ('head-mounted', 'a'), ('display', 'n'), ('determining', 'v'), ('vergence', 'n'), ('distance', 'n'), ('based', 'v'), ('least', 'a'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('one', None), ('second', 'a'), ('inputs', 'n'), ('associated', 'v'), ('one', None), ('displaying', 'n'), ('elements', 'n'), ('wherein', 'v'), ('determining', 'v'), ('performance', 'n'), ('metric', 'a'), ('eye', 'n'), ('system', 'n'), ('second', 'a'), ('performance', 'n'), ('threshold', 'n'), ('comprises', 'v'), ('determining', 'v'), ('eye', 'n'), ('system', 'n'), ('exist', 'v'), ('fails', 'n'), ('provide', 'v'), ('eye', 'n'), ('data', 'n'), ('wherein', 'v'), ('performance', 'n'), ('metric', 'a'), ('eye', 'n'), ('system', 'n'), ('comprises', 'v'), ('one', None), ('accuracy', 'n'), ('parameter', 'n'), ('eye', 'n'), ('system', 'n'), ('precision', 'n'), ('parameter', 'n'), ('eye', 'n'), ('system', 'n'), ('value', 'n'), ('parameter', 'n'), ('eye', 'n'), ('system', 'n'), ('detectability', 'n'), ('pupil', 'v'), ('metric', 'a'), ('based', 'v'), ('one', None), ('parameters', 'n'), ('associated', 'a'), ('parameter', 'n'), ('change', 'n'), ('parameter', 'n'), ('changing', 'v'), ('trend', 'n'), ('data', 'n'), ('availability', 'n'), ('weighted', 'v'), ('combination', 'n'), ('one', None), ('performance', 'n'), ('related', 'a'), ('parameters', 'n'), ('wherein', 'v'), ('one', None), ('parameters', 'n'), ('associated', 'v'), ('comprise', 'n'), ('one', None), ('eye', 'n'), ('distance', 'n'), ('pupil', 'a'), ('position', 'n'), ('pupil', 'n'), ('status', 'n'), ('correlation', 'n'), ('two', None), ('pupils', 'n'), ('head', 'v'), ('size', 'n'), ('position', 'n'), ('headset', 'v'), ('worn', 'a'), ('angle', 'n'), ('headset', 'n'), ('worn', 'a'), ('direction', 'n'), ('headset', 'n'), ('worn', 'a'), ('alignment', 'a'), ('eyes', 'n'), ('weighted', 'v'), ('combination', 'n'), ('one', None), ('related', 'a'), ('parameters', 'n'), ('associated', 'v'), ('wherein', 'a'), ('first', 'a'), ('performance', 'n'), ('threshold', 'n'), ('comprises', 'v'), ('one', None), ('pre-determined', 'a'), ('value', 'n'), ('pre-determined', 'a'), ('range', 'n'), ('state', 'n'), ('data', 'n'), ('changing', 'v'), ('speed', 'n'), ('data', 'n'), ('trend', 'n'), ('data', 'n'), ('change', 'v'), ('one', None), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('media', 'n'), ('embodying', 'v'), ('software', 'n'), ('operable', 'a'), ('executed', 'v'), ('computing', 'n'), ('system', 'n'), ('determine', 'a'), ('performance', 'n'), ('metric', 'a'), ('eye', 'n'), ('system', 'n'), ('first', 'a'), ('performance', 'n'), ('threshold', 'n'), ('wherein', 'n'), ('eye', 'n'), ('system', 'n'), ('associated', 'v'), ('head-mounted', 'a'), ('display', 'n'), ('worn', 'n'), ('based', 'v'), ('determination', 'n'), ('performance', 'n'), ('metric', 'a'), ('eye', 'n'), ('system', 'n'), ('first', 'a'), ('performance', 'n'), ('threshold', 'a'), ('media', 'n'), ('embodying', 'v'), ('software', 'n'), ('operable', 'a'), ('executed', 'v'), ('computing', 'v'), ('system', 'n'), ('receive', 'v'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('estimate', 'n'), ('region', 'n'), ('looking', 'v'), ('within', None), ('field', 'n'), ('view', 'n'), ('head-mounted', 'a'), ('display', 'n'), ('based', 'v'), ('received', 'v'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('determine', 'a'), ('vergence', 'n'), ('distance', 'n'), ('based', 'v'), ('least', 'a'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('estimated', 'v'), ('region', 'n'), ('looking', 'v'), ('locations', 'n'), ('one', None), ('objects', 'v'), ('scene', 'n'), ('displayed', 'v'), ('head-mounted', 'a'), ('display', 'n'), ('adjust', 'v'), ('one', None), ('configurations', 'n'), ('head-mounted', 'a'), ('display', 'n'), ('based', 'v'), ('determined', 'a'), ('vergence', 'n'), ('distance', 'n'), ('system', 'n'), ('comprising', 'v'), ('one', None), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('media', 'n'), ('embodying', 'v'), ('instructions', 'n'), ('one', None), ('coupled', 'v'), ('storage', 'n'), ('media', 'n'), ('operable', 'a'), ('execute', 'a'), ('instructions', 'n'), ('determine', 'v'), ('performance', 'n'), ('metric', 'a'), ('eye', 'n'), ('system', 'n'), ('first', 'a'), ('performance', 'n'), ('threshold', 'n'), ('wherein', 'n'), ('eye', 'n'), ('system', 'n'), ('associated', 'v'), ('head-mounted', 'a'), ('display', 'n'), ('worn', 'n'), ('based', 'v'), ('determination', 'n'), ('performance', 'n'), ('metric', 'a'), ('eye', 'n'), ('system', 'n'), ('first', 'a'), ('performance', 'n'), ('threshold', 'n'), ('system', 'n'), ('configured', 'v'), ('receive', 'a'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('estimate', 'n'), ('region', 'n'), ('looking', 'v'), ('within', None), ('field', 'n'), ('view', 'n'), ('head-mounted', 'a'), ('display', 'n'), ('based', 'v'), ('received', 'v'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('determine', 'a'), ('vergence', 'n'), ('distance', 'n'), ('based', 'v'), ('least', 'a'), ('one', None), ('first', 'a'), ('inputs', 'n'), ('associated', 'v'), ('body', 'n'), ('estimated', 'v'), ('region', 'n'), ('looking', 'v'), ('locations', 'n'), ('one', None), ('objects', 'v'), ('scene', 'n'), ('displayed', 'v'), ('head-mounted', 'a'), ('display', 'n'), ('adjust', 'v'), ('one', None), ('configurations', 'n'), ('head-mounted', 'a'), ('display', 'n'), ('based', 'v'), ('determined', 'a'), ('vergence', 'n'), ('distance', 'n'), ('computer-implemented', 'a'), ('-based', 'v'), ('self-guided', 'a'), ('object', 'a'), ('detection', 'n'), ('comprising', 'v'), ('receiving', 'v'), ('device', 'n'), ('set', 'v'), ('respective', 'a'), ('grid', 'a'), ('thereon', 'n'), ('labeled', 'v'), ('regarding', 'v'), ('respective', 'a'), ('object', 'n'), ('detected', 'v'), ('using', 'v'), ('grid', 'a'), ('level', 'n'), ('label', 'n'), ('data', 'n'), ('training', 'n'), ('device', 'n'), ('grid-based', 'a'), ('object', 'n'), ('detector', 'n'), ('using', 'v'), ('grid', 'a'), ('level', 'n'), ('label', 'n'), ('data', 'n'), ('determining', 'v'), ('device', 'n'), ('respective', 'a'), ('bounding', 'n'), ('box', 'n'), ('respective', 'a'), ('object', 'a'), ('applying', 'v'), ('local', 'a'), ('segmentation', 'n'), ('training', 'n'), ('device', 'n'), ('region-based', 'a'), ('convolutional', 'a'), ('neural', 'a'), ('network', 'n'), ('rcnn', 'n'), ('joint', 'n'), ('object', 'a'), ('localization', 'n'), ('object', 'n'), ('classification', 'n'), ('using', 'v'), ('respective', 'a'), ('bounding', 'n'), ('box', 'n'), ('respective', 'a'), ('object', 'a'), ('input', 'n'), ('rcnn', 'v'), ('computer-implemented', 'a'), ('comprising', 'n'), ('performing', 'v'), ('action', 'n'), ('responsive', 'a'), ('object', 'n'), ('localization', 'n'), ('object', 'a'), ('classification', 'n'), ('respective', 'a'), ('new', 'a'), ('object', 'a'), ('new', 'a'), ('rcnn', 'n'), ('applied', 'v'), ('computer-implemented', 'a'), ('wherein', 'a'), ('action', 'n'), ('comprises', 'v'), ('autonomously', 'r'), ('controlling', 'v'), ('motor', 'n'), ('vehicle', 'n'), ('avoid', 'v'), ('collision', 'n'), ('new', 'a'), ('object', 'a'), ('responsive', 'a'), ('object', 'n'), ('localization', 'n'), ('object', 'a'), ('classification', 'n'), ('respective', 'a'), ('new', 'a'), ('object', 'a'), ('computer-implemented', 'a'), ('wherein', 'n'), ('local', 'a'), ('segmentation', 'n'), ('performed', 'v'), ('using', 'v'), ('self-similarity', 'a'), ('search', 'n'), ('template', 'n'), ('matching', 'v'), ('provide', 'r'), ('respective', 'a'), ('bounding', 'v'), ('box', 'n'), ('around', None), ('respective', 'a'), ('object', 'a'), ('set', 'v'), ('computer-implemented', 'a'), ('wherein', 'a'), ('local', 'a'), ('segmentation', 'n'), ('applied', 'v'), ('segment', 'n'), ('respective', 'a'), ('target', 'n'), ('region', 'n'), ('therein', None), ('computer-implemented', 'a'), ('wherein', 'a'), ('region-based', 'a'), ('convolutional', 'a'), ('neural', 'a'), ('network', 'n'), ('rcnn', 'n'), ('forms', 'n'), ('model', 'v'), ('object', 'a'), ('training', 'n'), ('stage', 'n'), ('detect', 'a'), ('objects', 'v'), ('new', 'a'), ('inference', 'n'), ('stage', 'n'), ('computer-implemented', 'a'), ('wherein', 'n'), ('performed', 'v'), ('system', 'n'), ('selected', 'v'), ('group', 'n'), ('consisting', 'v'), ('surveillance', 'n'), ('system', 'n'), ('face', 'n'), ('detection', 'n'), ('system', 'n'), ('face', 'n'), ('system', 'n'), ('cancer', 'n'), ('detection', 'n'), ('system', 'n'), ('object', 'a'), ('system', 'n'), ('advanced', 'v'), ('driver-assistance', 'n'), ('system', 'n'), ('computer', 'n'), ('program', 'n'), ('product', 'n'), ('-based', 'v'), ('self-guided', 'a'), ('object', 'a'), ('detection', 'n'), ('computer', 'n'), ('program', 'n'), ('product', 'n'), ('comprising', 'v'), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('program', 'n'), ('instructions', 'n'), ('embodied', 'v'), ('therewith', 'a'), ('program', 'n'), ('instructions', 'n'), ('executable', 'a'), ('computer', 'n'), ('cause', 'n'), ('computer', 'n'), ('perform', 'n'), ('comprising', 'v'), ('receiving', 'v'), ('device', 'n'), ('set', 'v'), ('respective', 'a'), ('grid', 'a'), ('thereon', 'n'), ('labeled', 'v'), ('regarding', 'v'), ('respective', 'a'), ('object', 'n'), ('detected', 'v'), ('using', 'v'), ('grid', 'a'), ('level', 'n'), ('label', 'n'), ('data', 'n'), ('training', 'n'), ('device', 'n'), ('grid-based', 'a'), ('object', 'n'), ('detector', 'n'), ('using', 'v'), ('grid', 'a'), ('level', 'n'), ('label', 'n'), ('data', 'n'), ('determining', 'v'), ('device', 'n'), ('respective', 'a'), ('bounding', 'n'), ('box', 'n'), ('respective', 'a'), ('object', 'a'), ('applying', 'v'), ('local', 'a'), ('segmentation', 'n'), ('training', 'n'), ('device', 'n'), ('region-based', 'a'), ('convolutional', 'a'), ('neural', 'a'), ('network', 'n'), ('rcnn', 'n'), ('joint', 'n'), ('object', 'a'), ('localization', 'n'), ('object', 'n'), ('classification', 'n'), ('using', 'v'), ('respective', 'a'), ('bounding', 'n'), ('box', 'n'), ('respective', 'a'), ('object', 'a'), ('input', 'n'), ('rcnn', 'v'), ('computer', 'n'), ('program', 'n'), ('product', 'n'), ('wherein', 'n'), ('comprises', 'v'), ('performing', 'v'), ('action', 'n'), ('responsive', 'a'), ('object', 'n'), ('localization', 'n'), ('object', 'a'), ('classification', 'n'), ('respective', 'a'), ('new', 'a'), ('object', 'a'), ('new', 'a'), ('rcnn', 'n'), ('applied', 'v'), ('computer', 'n'), ('program', 'n'), ('product', 'n'), ('wherein', 'v'), ('action', 'n'), ('comprises', 'v'), ('autonomously', 'r'), ('controlling', 'v'), ('motor', 'n'), ('vehicle', 'n'), ('avoid', 'v'), ('collision', 'n'), ('new', 'a'), ('object', 'a'), ('responsive', 'a'), ('object', 'n'), ('localization', 'n'), ('object', 'a'), ('classification', 'n'), ('respective', 'a'), ('new', 'a'), ('object', 'a'), ('computer', 'n'), ('program', 'n'), ('product', 'n'), ('wherein', 'v'), ('local', 'a'), ('segmentation', 'n'), ('performed', 'v'), ('using', 'v'), ('self-similarity', 'a'), ('search', 'n'), ('template', 'n'), ('matching', 'v'), ('provide', 'r'), ('respective', 'a'), ('bounding', 'v'), ('box', 'n'), ('around', None), ('respective', 'a'), ('object', 'n'), ('set', 'v'), ('computer', 'n'), ('program', 'n'), ('product', 'n'), ('wherein', 'v'), ('local', 'a'), ('segmentation', 'n'), ('applied', 'v'), ('segment', 'n'), ('respective', 'a'), ('target', 'n'), ('region', 'n'), ('therein', 'a'), ('computer', 'n'), ('program', 'n'), ('product', 'n'), ('wherein', 'v'), ('region-based', 'a'), ('convolutional', 'a'), ('neural', 'a'), ('network', 'n'), ('rcnn', 'n'), ('forms', 'n'), ('model', 'v'), ('object', 'a'), ('training', 'n'), ('stage', 'n'), ('detect', 'a'), ('objects', 'v'), ('new', 'a'), ('inference', 'n'), ('stage', 'n'), ('computer', 'n'), ('program', 'n'), ('product', 'n'), ('wherein', 'n'), ('performed', 'v'), ('system', 'n'), ('selected', 'v'), ('group', 'n'), ('consisting', 'v'), ('surveillance', 'n'), ('system', 'n'), ('face', 'n'), ('detection', 'n'), ('system', 'n'), ('face', 'n'), ('system', 'n'), ('cancer', 'n'), ('detection', 'n'), ('system', 'n'), ('object', 'a'), ('system', 'n'), ('advanced', 'v'), ('driver-assistance', 'n'), ('system', 'n'), ('computer', 'n'), ('processing', 'v'), ('system', 'n'), ('-based', 'v'), ('self-guided', 'a'), ('object', 'a'), ('detection', 'n'), ('comprising', 'v'), ('memory', 'n'), ('device', 'n'), ('storing', 'v'), ('program', 'n'), ('code', 'n'), ('device', 'n'), ('running', 'v'), ('program', 'n'), ('code', 'n'), ('receive', 'v'), ('set', 'v'), ('respective', 'a'), ('grid', 'a'), ('thereon', 'n'), ('labeled', 'v'), ('regarding', 'v'), ('respective', 'a'), ('object', 'n'), ('detected', 'v'), ('using', 'v'), ('grid', 'a'), ('level', 'n'), ('label', 'n'), ('data', 'n'), ('train', 'v'), ('grid-based', 'a'), ('object', 'n'), ('detector', 'n'), ('using', 'v'), ('grid', 'a'), ('level', 'n'), ('label', 'n'), ('data', 'n'), ('determine', 'v'), ('respective', 'a'), ('bounding', 'n'), ('box', 'n'), ('respective', 'a'), ('object', 'a'), ('applying', 'v'), ('local', 'a'), ('segmentation', 'n'), ('train', 'n'), ('region-based', 'a'), ('convolutional', 'a'), ('neural', 'a'), ('network', 'n'), ('rcnn', 'n'), ('joint', 'n'), ('object', 'a'), ('localization', 'n'), ('object', 'n'), ('classification', 'n'), ('using', 'v'), ('respective', 'a'), ('bounding', 'n'), ('box', 'n'), ('respective', 'a'), ('object', 'a'), ('input', 'n'), ('rcnn', 'v'), ('computer', 'n'), ('processing', 'n'), ('system', 'n'), ('wherein', 'a'), ('device', 'n'), ('runs', 'v'), ('program', 'n'), ('code', 'n'), ('perform', 'v'), ('action', 'n'), ('responsive', 'a'), ('object', 'n'), ('localization', 'n'), ('object', 'a'), ('classification', 'n'), ('respective', 'a'), ('new', 'a'), ('object', 'a'), ('new', 'a'), ('rcnn', 'n'), ('applied', 'v'), ('computer', 'n'), ('processing', 'v'), ('system', 'n'), ('wherein', 'a'), ('action', 'n'), ('comprises', 'v'), ('autonomously', 'r'), ('controlling', 'v'), ('motor', 'n'), ('vehicle', 'n'), ('avoid', 'v'), ('collision', 'n'), ('new', 'a'), ('object', 'a'), ('responsive', 'a'), ('object', 'n'), ('localization', 'n'), ('object', 'a'), ('classification', 'n'), ('respective', 'a'), ('new', 'a'), ('object', 'a'), ('computer', 'n'), ('processing', 'n'), ('system', 'n'), ('wherein', 'v'), ('local', 'a'), ('segmentation', 'n'), ('performed', 'v'), ('using', 'v'), ('self-similarity', 'a'), ('search', 'n'), ('template', 'n'), ('matching', 'v'), ('provide', 'r'), ('respective', 'a'), ('bounding', 'v'), ('box', 'n'), ('around', None), ('respective', 'a'), ('object', 'n'), ('set', 'v'), ('computer', 'n'), ('processing', 'v'), ('system', 'n'), ('wherein', 'a'), ('region-based', 'a'), ('convolutional', 'a'), ('neural', 'a'), ('network', 'n'), ('rcnn', 'n'), ('forms', 'n'), ('model', 'v'), ('object', 'a'), ('training', 'n'), ('stage', 'n'), ('detect', 'a'), ('objects', 'v'), ('new', 'a'), ('inference', 'n'), ('stage', 'n'), ('computer', 'n'), ('processing', 'n'), ('system', 'n'), ('wherein', 'v'), ('computer', 'n'), ('processing', 'n'), ('system', 'n'), ('comprised', 'v'), ('system', 'n'), ('selected', 'v'), ('group', 'n'), ('consisting', 'v'), ('surveillance', 'n'), ('system', 'n'), ('face', 'n'), ('detection', 'n'), ('system', 'n'), ('face', 'n'), ('system', 'n'), ('cancer', 'n'), ('detection', 'n'), ('system', 'n'), ('object', 'a'), ('system', 'n'), ('advanced', 'v'), ('driver-assistance', 'n'), ('system', 'n'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('utilizing', 'v'), ('database', 'n'), ('normalized', 'v'), ('stored', 'v'), ('comprising', 'v'), ('capturing', 'v'), ('using', 'v'), ('camera', 'n'), ('detecting', 'v'), ('face', 'n'), ('captured', 'v'), ('normalizing', 'a'), ('detected', 'a'), ('facial', 'a'), ('match', 'n'), ('normalized', 'v'), ('stored', 'v'), ('identifying', 'v'), ('facial', 'a'), ('features', 'n'), ('normalized', 'v'), ('detected', 'a'), ('facial', 'a'), ('generating', 'v'), ('facial', 'a'), ('metrics', 'n'), ('facial', 'a'), ('features', 'n'), ('calculating', 'v'), ('euclidean', 'a'), ('distances', 'n'), ('facial', 'a'), ('metrics', 'n'), ('normalized', 'v'), ('detected', 'a'), ('facial', 'a'), ('corresponding', 'v'), ('facial', 'a'), ('metrics', 'n'), ('stored', 'v'), ('comparing', 'v'), ('euclidean', 'a'), ('distance', 'n'), ('predetermined', 'v'), ('threshold', 'a'), ('responsive', 'a'), ('euclidean', 'a'), ('distance', 'n'), ('comparison', 'n'), ('producing', 'v'), ('reduced', 'a'), ('candidate', 'a'), ('list', 'n'), ('best', 'a'), ('possible', 'a'), ('matches', 'n'), ('normalized', 'v'), ('stored', 'v'), ('comparing', 'v'), ('parallel', 'r'), ('normalized', 'v'), ('detected', 'v'), ('facial', 'a'), ('normalized', 'v'), ('stored', 'v'), ('reduced', 'a'), ('candidate', 'n'), ('list', 'n'), ('utilizing', 'v'), ('face', 'n'), ('algorithms', 'n'), ('parallel', 'r'), ('processing', 'v'), ('system', 'n'), ('uses', 'v'), ('different', 'a'), ('face', 'n'), ('algorithm', 'v'), ('responsive', 'a'), ('comparison', 'n'), ('producing', 'v'), ('best', 'a'), ('match', 'n'), ('results', 'n'), ('parallel', 'a'), ('subset', 'n'), ('reduced', 'v'), ('candidate', 'a'), ('list', 'n'), ('selecting', 'v'), ('final', 'a'), ('match', 'n'), ('best', 'a'), ('match', 'n'), ('results', 'n'), ('using', 'v'), ('deep', 'a'), ('learning', 'v'), ('neural', 'a'), ('network', 'n'), ('face', 'n'), ('algorithm', 'n'), ('trained', 'v'), ('outputs', 'n'), ('individual', 'a'), ('face', 'n'), ('algorithms', 'n'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('wherein', 'n'), ('detecting', 'v'), ('face', 'n'), ('captured', 'v'), ('comprises', 'n'), ('utilizing', 'v'), ('opencv', 'n'), ('detect', 'a'), ('face', 'n'), ('captured', 'v'), ('extracting', 'v'), ('location', 'n'), ('eyes', 'n'), ('tip', 'v'), ('nose', 'a'), ('face', 'n'), ('determining', 'v'), ('distance', 'n'), ('eyes', 'n'), ('cropping', 'v'), ('face', 'n'), ('captured', 'v'), ('width', 'a'), ('height', 'n'), ('cropped', 'v'), ('face', 'n'), ('function', 'n'), ('distance', 'n'), ('eyes', 'n'), ('rotating', 'v'), ('face', 'n'), ('angle', 'n'), ('rotation', 'n'), ('function', 'n'), ('distance', 'n'), ('eyes', 'n'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('wherein', 'n'), ('width', 'n'), ('cropped', 'v'), ('face', 'n'), ('times', 'n'), ('distance', 'a'), ('eyes', 'n'), ('height', 'v'), ('cropped', 'v'), ('face', 'n'), ('times', 'n'), ('distance', 'a'), ('eyes', 'n'), ('angle', 'v'), ('rotation', 'n'), ('angle', 'n'), ('formed', 'v'), ('straight', 'a'), ('line', 'n'), ('joining', 'v'), ('eyes', 'n'), ('x-axis', 'a'), ('face', 'n'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('wherein', 'n'), ('rotating', 'v'), ('face', 'n'), ('comprises', 'n'), ('rotating', 'v'), ('face', 'n'), ('provide', 'v'), ('frontal', 'a'), ('face', 'n'), ('pattern', 'n'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('comprising', 'v'), ('step', 'n'), ('proportionally', 'r'), ('rescaling', 'v'), ('cropped', 'v'), ('rotated', 'v'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('proportional', 'a'), ('rescaling', 'n'), ('yields', 'n'), ('cropped', 'v'), ('rotated', 'a'), ('size', 'n'), ('=', 'n'), ('pixels', 'n'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('wherein', 'v'), ('facial', 'a'), ('features', 'n'), ('identified', 'v'), ('normalized', 'a'), ('detected', 'v'), ('facial', 'a'), ('comprise', 'n'), ('pair', 'n'), ('eyes', 'n'), ('tip', 'v'), ('nose', 'a'), ('mouth', 'n'), ('center', 'n'), ('mouth', 'n'), ('chin', 'a'), ('area', 'n'), ('comprising', 'v'), ('bottom', 'a'), ('top', 'a'), ('left', 'v'), ('landmark', 'n'), ('top', 'a'), ('right', 'n'), ('landmark', 'n'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('wherein', 'n'), ('generating', 'v'), ('facial', 'a'), ('metrics', 'n'), ('comprises', 'n'), ('calculating', 'v'), ('distance', 'n'), ('pair', 'n'), ('eyes', 'n'), ('distance', 'v'), ('eyes', 'n'), ('tip', 'v'), ('nose', 'a'), ('distance', 'n'), ('equal', 'a'), ('width', 'n'), ('mouth', 'n'), ('distance', 'n'), ('tip', 'n'), ('nose', 'r'), ('center', 'a'), ('mouth', 'n'), ('distance', 'n'), ('bottom', 'n'), ('chin', 'n'), ('center', 'n'), ('mouth', 'n'), ('distance', 'n'), ('top', 'n'), ('left', 'v'), ('landmark', 'n'), ('chin', 'n'), ('tip', 'n'), ('nose', 'a'), ('distance', 'n'), ('top', 'a'), ('right', 'n'), ('landmark', 'n'), ('chin', 'a'), ('tip', 'n'), ('nose', 'r'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('wherein', 'n'), ('performing', 'v'), ('euclidean', 'a'), ('distance', 'n'), ('match', 'n'), ('comprises', 'v'), ('partitioning', 'v'), ('normalized', 'v'), ('stored', 'v'), ('substantially', 'r'), ('equal', 'a'), ('subsets', 'n'), ('performing', 'v'), ('euclidean', 'a'), ('distance', 'n'), ('match', 'n'), ('facial', 'a'), ('metrics', 'n'), ('normalized', 'v'), ('detected', 'a'), ('facial', 'a'), ('corresponding', 'v'), ('facial', 'a'), ('metrics', 'n'), ('stored', 'v'), ('subsets', 'n'), ('normalized', 'v'), ('stored', 'a'), ('separate', 'a'), ('parallel', 'n'), ('processing', 'n'), ('system', 'n'), ('generate', 'a'), ('euclidean', 'a'), ('distance', 'n'), ('stored', 'v'), ('subset', 'a'), ('comparing', 'v'), ('euclidean', 'a'), ('distance', 'n'), ('predetermined', 'v'), ('threshold', 'a'), ('separate', 'a'), ('responsive', 'a'), ('euclidean', 'a'), ('distance', 'n'), ('comparison', 'n'), ('producing', 'v'), ('reduced', 'a'), ('candidate', 'a'), ('list', 'n'), ('best', 'a'), ('possible', 'a'), ('matches', 'n'), ('normalized', 'v'), ('stored', 'a'), ('subset', 'n'), ('combining', 'n'), ('reduced', 'a'), ('candidate', 'n'), ('lists', 'n'), ('subset', 'v'), ('produce', 'v'), ('single', 'a'), ('reduced', 'v'), ('candidate', 'a'), ('list', 'n'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('wherein', 'n'), ('face', 'n'), ('algorithms', 'v'), ('utilized', 'a'), ('comparing', 'n'), ('parallel', 'r'), ('normalized', 'v'), ('detected', 'v'), ('facial', 'a'), ('normalized', 'v'), ('stored', 'v'), ('reduced', 'a'), ('candidate', 'n'), ('list', 'n'), ('consists', 'v'), ('face', 'v'), ('algorithms', 'r'), ('selected', 'v'), ('group', 'n'), ('consisting', 'v'), ('principle', 'a'), ('component', 'a'), ('analysis', 'n'), ('pca-based', 'a'), ('algorithms', 'n'), ('linear', 'a'), ('discriminant', 'a'), ('analysis', 'n'), ('lda', 'n'), ('algorithms', 'a'), ('independent', 'a'), ('component', 'n'), ('analysis', 'n'), ('ica', 'n'), ('algorithms', None), ('kernel-based', 'a'), ('algorithms', 'a'), ('feature-based', 'a'), ('techniques', 'n'), ('algorithms', 'v'), ('based', 'v'), ('neural', 'a'), ('networks', 'n'), ('algorithms', 'v'), ('based', 'v'), ('transforms', 'n'), ('model-based', 'a'), ('face', 'n'), ('algorithms', 'n'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('wherein', None), ('pca-based', 'a'), ('algorithms', 'n'), ('include', 'v'), ('eigen', 'a'), ('face', 'n'), ('detection', 'n'), ('lda', 'v'), ('algorithms', 'a'), ('include', 'v'), ('fisher', 'a'), ('face', 'n'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('wherein', 'n'), ('comparing', 'v'), ('parallel', 'r'), ('captured', 'v'), ('normalized', 'a'), ('stored', 'v'), ('reduced', 'a'), ('candidate', 'n'), ('list', 'n'), ('comprises', 'v'), ('partitioning', 'v'), ('reduced', 'v'), ('candidate', 'a'), ('list', 'n'), ('substantially', 'r'), ('equal', 'a'), ('subsets', 'n'), ('processing', 'v'), ('subset', 'n'), ('different', 'a'), ('parallel', 'r'), ('processing', 'v'), ('system', 'n'), ('uses', 'v'), ('unique', 'a'), ('face', 'n'), ('algorithm', 'n'), ('produce', 'v'), ('best', 'a'), ('match', 'n'), ('results', 'n'), ('using', 'v'), ('reduce', 'v'), ('function', 'n'), ('mapreduce', 'n'), ('program', 'n'), ('combine', 'n'), ('best', 'r'), ('match', 'n'), ('results', 'n'), ('subsets', 'n'), ('produce', 'v'), ('single', 'a'), ('set', 'n'), ('best', 'a'), ('match', 'n'), ('results', 'n'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('wherein', 'n'), ('partitioning', 'v'), ('reduced', 'v'), ('candidate', 'a'), ('list', 'n'), ('comprises', 'n'), ('selecting', 'v'), ('comprising', 'v'), ('subset', 'n'), ('optimizing', 'v'), ('variance', 'n'), ('according', 'v'), ('following', 'v'), ('equation', 'n'), ('n', None), ('number', 'n'), ('rows', 'n'), ('columns', 'v'), ('face', 'n'), ('vector', 'n'), ('n', None), ('number', 'n'), ('groups', 'n'), ('σij', 'v'), ('standard', 'a'), ('deviation', 'n'), ('dimension', 'n'), ('group', 'n'), ('j', 'v'), ('face', 'n'), ('vector', 'n'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('wherein', 'n'), ('selecting', 'v'), ('comprising', 'v'), ('subset', 'n'), ('optimizing', 'v'), ('variance', 'n'), ('according', 'v'), ('following', 'v'), ('equation', 'n'), ('dμi', None), ('μj', 'a'), ('euclidean', 'a'), ('distance', 'n'), ('mean', 'n'), ('group', 'n'), ('mean', 'v'), ('group', 'n'), ('j', 'n'), ('face', 'n'), ('vector', 'n'), ('l', 'a'), ('number', 'n'), ('group', 'n'), ('levels', 'n'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('selecting', 'v'), ('final', 'a'), ('match', 'n'), ('best', 'a'), ('match', 'n'), ('results', 'n'), ('utilizing', 'a'), ('deep', 'a'), ('learning', 'n'), ('neural', 'a'), ('network', 'n'), ('face', 'n'), ('algorithm', 'n'), ('comprises', 'v'), ('utilizing', 'v'), ('either', None), ('adaboost', 'a'), ('machine-learning', 'a'), ('algorithm', 'a'), ('neural', 'a'), ('networks', 'n'), ('machine-learning', 'a'), ('model', 'n'), ('scalable', 'a'), ('parallel', 'a'), ('cloud-based', 'a'), ('face', 'n'), ('normalizing', 'v'), ('detected', 'v'), ('facial', 'a'), ('match', 'n'), ('normalized', 'v'), ('stored', 'v'), ('includes', 'v'), ('normalizing', 'n'), ('detected', 'v'), ('facial', 'a'), ('size', 'n'), ('illumination', 'n'), ('normalized', 'v'), ('stored', 'a'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('medium', 'n'), ('containing', 'v'), ('executable', 'a'), ('program', 'n'), ('instructions', 'n'), ('causing', 'v'), ('computer', 'n'), ('perform', 'n'), ('face', 'n'), ('comprising', 'v'), ('detecting', 'v'), ('face', 'n'), ('captured', 'v'), ('camera', 'n'), ('normalizing', 'n'), ('detected', 'v'), ('facial', 'a'), ('match', 'n'), ('normalized', 'v'), ('stored', 'v'), ('identifying', 'v'), ('facial', 'a'), ('features', 'n'), ('normalized', 'v'), ('detected', 'a'), ('facial', 'a'), ('generating', 'v'), ('facial', 'a'), ('metrics', 'n'), ('facial', 'a'), ('features', 'n'), ('calculating', 'v'), ('euclidean', 'a'), ('distances', 'n'), ('facial', 'a'), ('metrics', 'n'), ('normalized', 'v'), ('detected', 'a'), ('facial', 'a'), ('corresponding', 'v'), ('facial', 'a'), ('metrics', 'n'), ('stored', 'v'), ('comparing', 'v'), ('euclidean', 'a'), ('distance', 'n'), ('predetermined', 'v'), ('threshold', 'a'), ('responsive', 'a'), ('euclidean', 'a'), ('distance', 'n'), ('comparison', 'n'), ('producing', 'v'), ('reduced', 'a'), ('candidate', 'a'), ('list', 'n'), ('best', 'a'), ('possible', 'a'), ('matches', 'n'), ('normalized', 'v'), ('stored', 'v'), ('comparing', 'v'), ('parallel', 'r'), ('captured', 'v'), ('normalized', 'a'), ('stored', 'v'), ('reduced', 'a'), ('candidate', 'n'), ('list', 'n'), ('utilizing', 'v'), ('face', 'n'), ('algorithms', 'n'), ('parallel', 'r'), ('processing', 'v'), ('system', 'n'), ('uses', 'v'), ('different', 'a'), ('face', 'n'), ('algorithm', 'v'), ('responsive', 'a'), ('comparison', 'n'), ('producing', 'v'), ('best', 'a'), ('match', 'n'), ('results', 'n'), ('parallel', 'a'), ('subset', 'n'), ('reduced', 'v'), ('candidate', 'a'), ('list', 'n'), ('selecting', 'v'), ('final', 'a'), ('match', 'n'), ('best', 'a'), ('match', 'n'), ('results', 'n'), ('using', 'v'), ('deep', 'a'), ('learning', 'v'), ('neural', 'a'), ('network', 'n'), ('face', 'n'), ('algorithm', 'n'), ('trained', 'v'), ('outputs', 'n'), ('individual', 'a'), ('face', 'n'), ('algorithms', None), ('non-transitory', 'a'), ('computer-readable', 'a'), ('medium', 'n'), ('containing', 'v'), ('executable', 'a'), ('program', 'n'), ('instructions', 'n'), ('wherein', 'v'), ('face', 'n'), ('algorithms', 'n'), ('utilized', 'a'), ('comparing', 'n'), ('parallel', 'r'), ('normalized', 'v'), ('detected', 'v'), ('facial', 'a'), ('normalized', 'v'), ('stored', 'v'), ('reduced', 'a'), ('candidate', 'n'), ('list', 'n'), ('consists', 'v'), ('face', 'v'), ('algorithms', 'r'), ('selected', 'v'), ('group', 'n'), ('consisting', 'v'), ('principle', 'a'), ('component', 'a'), ('analysis', 'n'), ('pca-based', 'a'), ('algorithms', 'n'), ('linear', 'a'), ('discriminant', 'a'), ('analysis', 'n'), ('lda', 'n'), ('algorithms', 'a'), ('independent', 'a'), ('component', 'n'), ('analysis', 'n'), ('ica', 'n'), ('algorithms', None), ('kernel-based', 'a'), ('algorithms', 'a'), ('feature-based', 'a'), ('techniques', 'n'), ('algorithms', 'v'), ('based', 'v'), ('neural', 'a'), ('networks', 'n'), ('algorithms', 'v'), ('based', 'v'), ('transforms', 'n'), ('model-based', 'a'), ('face', 'n'), ('algorithms', None), ('non-transitory', 'a'), ('computer-readable', 'a'), ('medium', 'n'), ('containing', 'v'), ('executable', 'a'), ('program', 'n'), ('instructions', 'n'), ('wherein', 'v'), ('pca-based', 'a'), ('algorithms', 'n'), ('include', 'v'), ('eigen', 'a'), ('face', 'n'), ('detection', 'n'), ('lda', 'v'), ('algorithms', 'a'), ('include', 'v'), ('fisher', 'a'), ('face', 'n'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('medium', 'n'), ('containing', 'v'), ('executable', 'a'), ('program', 'n'), ('instructions', 'n'), ('selecting', 'v'), ('final', 'a'), ('match', 'n'), ('best', 'a'), ('match', 'n'), ('results', 'n'), ('utilizing', 'a'), ('deep', 'a'), ('learning', 'n'), ('neural', 'a'), ('network', 'n'), ('face', 'n'), ('algorithm', 'n'), ('comprises', 'v'), ('utilizing', 'v'), ('either', None), ('adaboost', 'a'), ('machine-learning', 'a'), ('algorithm', 'a'), ('neural', 'a'), ('networks', 'n'), ('machine-learning', 'a'), ('model', 'n'), ('imaging', 'v'), ('device', 'n'), ('comprising', 'v'), ('condensing', 'v'), ('lens', 'n'), ('sensor', 'n'), ('configured', 'v'), ('detect', 'a'), ('light', 'a'), ('passing', 'v'), ('condensing', 'v'), ('lens', 'n'), ('comprising', 'v'), ('pixel', 'n'), ('matrix', 'n'), ('wherein', 'n'), ('pixel', 'n'), ('matrix', 'n'), ('comprises', 'v'), ('phase', 'a'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('regular', 'a'), ('pixels', 'n'), ('configured', 'v'), ('turn', 'v'), ('phase', 'a'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('autofocusing', 'v'), ('output', 'n'), ('autofocused', 'v'), ('pixel', 'a'), ('data', 'n'), ('completing', 'v'), ('autofocusing', 'v'), ('divide', 'n'), ('autofocused', 'v'), ('pixel', 'n'), ('data', 'n'), ('first', 'r'), ('subframe', 'a'), ('second', 'a'), ('subframe', 'n'), ('calculate', 'n'), ('features', 'n'), ('least', 'v'), ('one', None), ('first', 'a'), ('subframe', 'a'), ('second', 'a'), ('subframe', 'n'), ('wherein', 'n'), ('features', 'v'), ('comprise', 'v'), ('module', 'a'), ('widths', 'n'), ('finder', 'v'), ('pattern', 'a'), ('finder', 'n'), ('pattern', 'n'), ('predetermined', 'v'), ('ratio', 'a'), ('harr-like', 'a'), ('feature', 'n'), ('gabor', 'n'), ('feature', 'n'), ('determine', 'n'), ('operating', 'v'), ('resolution', 'n'), ('regular', 'a'), ('pixels', 'n'), ('according', 'v'), ('features', 'n'), ('calculated', 'v'), ('least', 'a'), ('one', None), ('first', 'a'), ('subframe', 'a'), ('second', 'a'), ('subframe', 'n'), ('divided', 'v'), ('autofocused', 'a'), ('pixel', 'n'), ('data', 'n'), ('imaging', 'v'), ('device', 'n'), ('ed', None), ('wherein', 'a'), ('phase', 'n'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('comprises', 'v'), ('first', 'a'), ('pixel', 'a'), ('second', 'a'), ('pixel', 'n'), ('cover', 'n'), ('layer', 'n'), ('covering', 'v'), ('upon', None), ('first', 'a'), ('region', 'n'), ('first', 'r'), ('pixel', 'v'), ('upon', None), ('second', 'a'), ('region', 'n'), ('second', 'a'), ('pixel', 'n'), ('wherein', 'n'), ('first', 'a'), ('region', 'n'), ('second', 'a'), ('region', 'n'), ('mirror', 'n'), ('symmetrical', 'a'), ('microlens', 'n'), ('aligned', 'v'), ('least', 'a'), ('one', None), ('first', 'a'), ('pixel', 'a'), ('second', 'a'), ('pixel', 'n'), ('imaging', 'v'), ('device', 'n'), ('ed', 'n'), ('wherein', 'n'), ('first', 'a'), ('region', 'n'), ('second', 'a'), ('region', 'n'), ('%', 'n'), ('%', 'n'), ('area', 'n'), ('single', 'a'), ('pixel', 'n'), ('imaging', 'v'), ('device', 'n'), ('ed', None), ('wherein', 'n'), ('configured', 'v'), ('perform', 'n'), ('autofocusing', 'v'), ('using', 'v'), ('dual', 'a'), ('pixel', 'n'), ('autofocus', 'n'), ('technique', 'n'), ('according', 'v'), ('pixel', 'n'), ('data', 'n'), ('phase', 'n'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('completing', 'v'), ('autofocusing', 'v'), ('imaging', 'a'), ('device', 'n'), ('ed', 'n'), ('wherein', 'n'), ('configured', 'v'), ('divide', 'a'), ('pixel', 'n'), ('data', 'n'), ('phase', 'n'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('third', 'a'), ('subframe', 'a'), ('fourth', 'a'), ('subframe', 'n'), ('completing', 'v'), ('autofocusing', 'v'), ('perform', 'n'), ('autofocusing', 'v'), ('according', 'v'), ('third', 'a'), ('subframe', 'a'), ('fourth', 'a'), ('subframe', 'n'), ('imaging', 'v'), ('device', 'n'), ('ed', None), ('wherein', 'n'), ('configured', 'v'), ('calibrate', 'a'), ('brightness', 'a'), ('third', 'a'), ('subframe', 'n'), ('fourth', 'a'), ('subframe', 'a'), ('identical', 'a'), ('using', 'v'), ('shading', 'v'), ('algorithm', 'a'), ('imaging', 'v'), ('device', 'n'), ('ed', None), ('wherein', 'n'), ('operating', 'v'), ('resolution', 'n'), ('selected', 'v'), ('first', 'a'), ('resolution', 'n'), ('smaller', 'a'), ('number', 'n'), ('regular', 'a'), ('pixels', 'n'), ('second', 'a'), ('resolution', 'n'), ('larger', 'a'), ('first', 'a'), ('resolution', 'n'), ('imaging', 'v'), ('device', 'n'), ('ed', 'n'), ('wherein', None), ('regular', 'a'), ('pixels', 'n'), ('turned', 'v'), ('autofocusing', 'v'), ('imaging', 'v'), ('device', 'n'), ('ed', None), ('wherein', 'a'), ('number', 'n'), ('phase', 'n'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'v'), ('smaller', 'a'), ('regular', 'a'), ('pixels', 'n'), ('imaging', 'v'), ('device', 'n'), ('comprising', 'v'), ('condensing', 'v'), ('lens', 'n'), ('sensor', 'n'), ('configured', 'v'), ('detect', 'a'), ('light', 'a'), ('passing', 'v'), ('condensing', 'v'), ('lens', 'n'), ('comprising', 'v'), ('pixel', 'n'), ('matrix', 'n'), ('wherein', 'n'), ('pixel', 'n'), ('matrix', 'n'), ('comprises', 'v'), ('phase', 'a'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('regular', 'a'), ('pixels', 'n'), ('configured', 'v'), ('turn', 'v'), ('phase', 'a'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('autofocusing', 'v'), ('output', 'n'), ('autofocused', 'v'), ('pixel', 'a'), ('data', 'n'), ('completing', 'v'), ('autofocusing', 'v'), ('divide', 'n'), ('autofocused', 'v'), ('pixel', 'n'), ('data', 'n'), ('first', 'r'), ('subframe', 'a'), ('second', 'a'), ('subframe', 'n'), ('calculate', 'n'), ('features', 'n'), ('least', 'v'), ('one', None), ('first', 'a'), ('subframe', 'a'), ('second', 'a'), ('subframe', 'n'), ('wherein', 'n'), ('features', 'v'), ('comprise', 'v'), ('module', 'a'), ('widths', 'n'), ('finder', 'v'), ('pattern', 'a'), ('finder', 'n'), ('pattern', 'n'), ('predetermined', 'v'), ('ratio', 'a'), ('harr-like', 'a'), ('feature', 'n'), ('gabor', 'n'), ('feature', 'n'), ('select', 'v'), ('decoding', 'v'), ('using', 'v'), ('pixel', 'a'), ('data', 'n'), ('regular', 'a'), ('pixels', 'n'), ('according', 'v'), ('features', 'n'), ('calculated', 'v'), ('least', 'a'), ('one', None), ('first', 'a'), ('subframe', 'a'), ('second', 'a'), ('subframe', 'n'), ('divided', 'v'), ('autofocused', 'a'), ('pixel', 'n'), ('data', 'n'), ('imaging', 'v'), ('device', 'n'), ('ed', None), ('wherein', 'a'), ('phase', 'n'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('comprises', 'v'), ('first', 'a'), ('pixel', 'a'), ('second', 'a'), ('pixel', 'n'), ('cover', 'n'), ('layer', 'n'), ('covering', 'v'), ('upon', None), ('first', 'a'), ('region', 'n'), ('first', 'r'), ('pixel', 'v'), ('upon', None), ('second', 'a'), ('region', 'n'), ('second', 'a'), ('pixel', 'n'), ('wherein', 'n'), ('first', 'a'), ('region', 'n'), ('second', 'a'), ('region', 'n'), ('mirror', 'n'), ('symmetrical', 'a'), ('microlens', 'n'), ('aligned', 'v'), ('least', 'a'), ('one', None), ('first', 'a'), ('pixel', 'a'), ('second', 'a'), ('pixel', 'n'), ('imaging', 'v'), ('device', 'n'), ('ed', None), ('wherein', 'n'), ('configured', 'v'), ('perform', 'n'), ('autofocusing', 'v'), ('using', 'v'), ('dual', 'a'), ('pixel', 'n'), ('autofocus', 'n'), ('technique', 'n'), ('according', 'v'), ('pixel', 'n'), ('data', 'n'), ('phase', 'n'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('completing', 'v'), ('autofocusing', 'v'), ('imaging', 'a'), ('device', 'n'), ('ed', 'n'), ('wherein', 'n'), ('configured', 'v'), ('divide', 'a'), ('pixel', 'n'), ('data', 'n'), ('phase', 'n'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('third', 'a'), ('subframe', 'a'), ('fourth', 'a'), ('subframe', 'n'), ('completing', 'v'), ('autofocusing', 'v'), ('calibrate', 'a'), ('brightness', 'a'), ('third', 'a'), ('subframe', 'n'), ('fourth', 'a'), ('subframe', 'a'), ('identical', 'a'), ('using', 'v'), ('shading', 'v'), ('algorithm', 'a'), ('perform', 'n'), ('autofocusing', 'v'), ('according', 'v'), ('third', 'a'), ('subframe', 'a'), ('fourth', 'a'), ('subframe', 'n'), ('imaging', 'v'), ('device', 'n'), ('ed', None), ('wherein', 'n'), ('configured', 'v'), ('calculate', 'n'), ('features', 'n'), ('using', 'v'), ('least', 'a'), ('one', None), ('rule', 'n'), ('based', 'v'), ('algorithm', 'r'), ('machine', 'n'), ('learning', 'v'), ('algorithm', 'a'), ('imaging', 'v'), ('device', 'n'), ('ed', None), ('wherein', 'n'), ('decoding', 'v'), ('decoding', 'v'), ('qr', 'n'), ('codes', 'n'), ('face', 'v'), ('operating', 'v'), ('imaging', 'v'), ('device', 'n'), ('imaging', 'v'), ('device', 'n'), ('comprising', 'v'), ('phase', 'n'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('regular', 'a'), ('pixels', 'n'), ('operating', 'v'), ('comprising', 'v'), ('turning', 'v'), ('phase', 'n'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('autofocusing', 'v'), ('outputting', 'v'), ('autofocused', 'a'), ('frame', 'n'), ('completing', 'v'), ('autofocusing', 'v'), ('dividing', 'v'), ('autofocused', 'a'), ('frame', 'n'), ('acquired', 'v'), ('phase', 'a'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('first', 'r'), ('subframe', 'a'), ('second', 'a'), ('subframe', 'n'), ('calculating', 'v'), ('features', 'n'), ('least', 'a'), ('one', None), ('first', 'a'), ('subframe', 'a'), ('second', 'a'), ('subframe', 'n'), ('wherein', 'a'), ('feature', 'n'), ('comprise', 'n'), ('module', 'n'), ('widths', 'n'), ('finder', 'v'), ('pattern', 'a'), ('finder', 'n'), ('pattern', 'n'), ('predetermined', 'v'), ('ratio', 'a'), ('harr-like', 'a'), ('feature', 'n'), ('gabor', 'n'), ('feature', 'n'), ('selectively', 'r'), ('activating', 'v'), ('least', 'a'), ('part', 'n'), ('regular', 'a'), ('pixels', 'n'), ('according', 'v'), ('features', 'n'), ('calculated', 'v'), ('least', 'a'), ('one', None), ('first', 'a'), ('subframe', 'a'), ('second', 'a'), ('subframe', 'n'), ('divided', 'v'), ('autofocused', 'a'), ('frame', 'n'), ('operating', 'v'), ('ed', 'n'), ('wherein', 'n'), ('selectively', 'r'), ('activating', 'v'), ('comprises', 'n'), ('activating', 'v'), ('first', 'a'), ('part', 'n'), ('regular', 'a'), ('pixels', 'n'), ('perform', 'v'), ('decoding', 'v'), ('according', 'v'), ('pixel', 'n'), ('data', 'n'), ('first', 'a'), ('part', 'n'), ('regular', 'a'), ('pixels', 'n'), ('activating', 'v'), ('regular', 'a'), ('pixels', 'n'), ('perform', 'v'), ('according', 'v'), ('pixel', 'n'), ('data', 'n'), ('regular', 'a'), ('pixels', 'n'), ('operating', 'v'), ('ed', 'n'), ('wherein', 'n'), ('pixel', 'n'), ('data', 'n'), ('phase', 'n'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('captured', 'v'), ('frame', 'a'), ('pixel', 'n'), ('data', 'n'), ('regular', 'a'), ('pixels', 'n'), ('also', 'r'), ('used', 'v'), ('performing', 'v'), ('decoding', 'v'), ('operating', 'v'), ('ed', 'n'), ('wherein', 'n'), ('decoding', 'v'), ('decoding', 'v'), ('qr', 'n'), ('codes', 'n'), ('face', 'v'), ('operating', 'v'), ('ed', 'n'), ('wherein', 'n'), ('phase', 'n'), ('detection', 'n'), ('pixel', 'n'), ('pairs', 'n'), ('partially', 'r'), ('covered', 'v'), ('pixels', 'n'), ('structure', 'n'), ('dual', 'a'), ('pixel', 'n'), ('apparatus', 'n'), ('comprising', 'v'), ('first', 'a'), ('camera', 'n'), ('module', 'n'), ('configured', 'v'), ('obtain', 'v'), ('first', 'a'), ('object', 'a'), ('first', 'a'), ('field', 'n'), ('view', 'n'), ('second', 'a'), ('camera', 'n'), ('module', 'n'), ('configured', 'v'), ('obtain', 'v'), ('second', 'a'), ('object', 'a'), ('second', 'a'), ('field', 'n'), ('view', 'n'), ('different', 'a'), ('first', 'a'), ('field', 'n'), ('view', 'n'), ('first', 'r'), ('depth', 'v'), ('map', 'n'), ('generator', 'n'), ('configured', 'v'), ('generate', 'n'), ('first', 'r'), ('depth', 'v'), ('map', 'n'), ('first', 'r'), ('based', 'v'), ('first', 'a'), ('second', 'a'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('generator', 'n'), ('configured', 'v'), ('generate', 'a'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('second', 'n'), ('based', 'v'), ('first', 'r'), ('second', 'a'), ('first', 'a'), ('depth', 'n'), ('map', 'n'), ('apparatus', 'n'), ('wherein', 'v'), ('first', 'a'), ('field', 'n'), ('view', 'n'), ('narrow', 'a'), ('angle', 'a'), ('second', 'a'), ('field', 'n'), ('view', 'n'), ('wider', 'v'), ('angle', 'n'), ('apparatus', 'n'), ('wherein', 'a'), ('second', 'n'), ('divided', 'v'), ('primary', 'a'), ('region', 'n'), ('residual', 'a'), ('region', 'n'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('generator', 'n'), ('comprises', 'v'), ('relationship', 'n'), ('estimating', 'v'), ('module', 'n'), ('configured', 'v'), ('estimate', 'n'), ('relationship', 'n'), ('primary', 'a'), ('region', 'n'), ('residual', 'a'), ('region', 'n'), ('based', 'v'), ('first', 'a'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('estimating', 'v'), ('module', 'n'), ('configured', 'v'), ('estimate', 'n'), ('depth', 'n'), ('map', 'v'), ('residual', 'a'), ('region', 'n'), ('based', 'v'), ('estimated', 'v'), ('relationship', 'n'), ('first', 'a'), ('depth', 'n'), ('map', 'n'), ('apparatus', 'n'), ('wherein', 'v'), ('least', 'a'), ('one', None), ('relationship', 'n'), ('estimating', 'v'), ('module', 'n'), ('depth', 'n'), ('map', 'n'), ('estimating', 'v'), ('module', 'n'), ('performs', 'n'), ('estimating', 'v'), ('operation', 'n'), ('based', 'v'), ('neural', 'a'), ('network', 'n'), ('module', 'n'), ('apparatus', 'n'), ('comprising', 'v'), ('depth', 'a'), ('map', 'a'), ('fusion', 'n'), ('unit', 'n'), ('configured', 'v'), ('generate', 'a'), ('third', 'a'), ('depth', 'n'), ('map', 'a'), ('second', 'a'), ('performing', 'v'), ('fusion', 'n'), ('operation', 'n'), ('based', 'v'), ('first', 'r'), ('depth', 'a'), ('map', 'a'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('apparatus', 'n'), ('wherein', 'n'), ('depth', 'n'), ('map', 'a'), ('fusion', 'n'), ('unit', 'n'), ('comprises', 'v'), ('tone', None), ('mapping', 'n'), ('module', 'n'), ('configured', 'v'), ('generate', 'a'), ('tone-mapped', 'a'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('correspond', 'n'), ('first', 'r'), ('depth', 'v'), ('map', 'n'), ('performing', 'v'), ('bias', 'a'), ('removing', 'v'), ('operation', 'n'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('fusion', 'n'), ('module', 'n'), ('configured', 'v'), ('generate', 'a'), ('third', 'a'), ('depth', 'n'), ('map', 'n'), ('fusing', 'v'), ('tone-mapped', 'a'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('first', 'r'), ('depth', 'v'), ('map', 'n'), ('apparatus', 'n'), ('wherein', 'n'), ('depth', 'n'), ('map', 'a'), ('fusion', 'n'), ('unit', 'n'), ('comprises', 'v'), ('propagating', 'v'), ('module', 'n'), ('configured', 'v'), ('generate', 'n'), ('propagated', 'v'), ('first', 'a'), ('depth', 'n'), ('map', 'n'), ('second', 'a'), ('iterated', 'v'), ('propagating', 'v'), ('first', 'a'), ('depth', 'n'), ('map', 'n'), ('based', 'v'), ('first', 'r'), ('depth', 'a'), ('map', 'a'), ('second', 'a'), ('fusion', 'n'), ('module', 'n'), ('generates', 'v'), ('third', 'a'), ('depth', 'n'), ('map', 'n'), ('fusing', 'v'), ('tone-mapped', 'a'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('propagated', 'v'), ('first', 'a'), ('depth', 'n'), ('map', 'n'), ('apparatus', 'n'), ('wherein', 'n'), ('depth', 'n'), ('map', 'a'), ('fusion', 'n'), ('unit', 'n'), ('comprises', 'v'), ('post-processing', 'a'), ('module', 'n'), ('configured', 'v'), ('perform', 'a'), ('post-processing', 'a'), ('operation', 'n'), ('third', 'a'), ('depth', 'n'), ('map', 'n'), ('generated', 'v'), ('fusion', 'n'), ('module', 'n'), ('provide', None), ('post-processed', 'a'), ('third', 'a'), ('depth', 'n'), ('map', 'n'), ('apparatus', 'n'), ('wherein', 'v'), ('post-processing', 'a'), ('module', 'n'), ('performs', 'n'), ('post-processing', 'a'), ('operation', 'n'), ('filtering', 'v'), ('interface', 'n'), ('generated', 'v'), ('third', 'a'), ('depth', 'a'), ('map', 'n'), ('accordance', 'n'), ('fusion', 'n'), ('fusion', 'n'), ('module', 'n'), ('apparatus', None), ('wherein', 'a'), ('post-processing', 'a'), ('module', 'n'), ('removes', 'v'), ('artifacts', 'n'), ('generated', 'v'), ('third', 'a'), ('depth', 'a'), ('map', 'n'), ('accordance', 'n'), ('fusion', 'n'), ('fusion', 'n'), ('module', 'n'), ('apparatus', 'n'), ('wherein', 'n'), ('first', 'r'), ('depth', 'v'), ('map', 'n'), ('generator', 'n'), ('analyses', 'v'), ('distance', 'n'), ('relationship', 'n'), ('first', 'a'), ('second', 'n'), ('generates', 'n'), ('first', 'r'), ('depth', 'v'), ('map', 'n'), ('first', 'r'), ('based', 'v'), ('distance', 'n'), ('relationship', 'n'), ('processing', 'v'), ('electronic', 'a'), ('apparatus', 'n'), ('comprising', 'v'), ('obtaining', 'v'), ('first', 'a'), ('object', 'n'), ('using', 'v'), ('first', 'a'), ('camera', 'n'), ('module', 'n'), ('obtaining', 'v'), ('second', 'a'), ('object', 'a'), ('using', 'v'), ('second', 'a'), ('camera', 'n'), ('module', 'n'), ('generating', 'v'), ('first', 'a'), ('depth', 'n'), ('map', 'n'), ('first', 'r'), ('based', 'v'), ('first', 'a'), ('second', 'a'), ('estimating', 'v'), ('relationship', 'n'), ('primary', 'a'), ('region', 'n'), ('second', 'a'), ('residual', 'a'), ('region', 'n'), ('second', 'n'), ('based', 'v'), ('first', 'a'), ('second', 'a'), ('generating', 'v'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('second', 'a'), ('based', 'v'), ('estimated', 'v'), ('relationship', 'n'), ('primary', 'a'), ('region', 'n'), ('residual', 'a'), ('region', 'n'), ('first', 'r'), ('depth', 'v'), ('map', 'n'), ('wherein', 'n'), ('electronic', 'a'), ('apparatus', 'n'), ('comprises', 'n'), ('first', 'r'), ('camera', 'v'), ('module', 'n'), ('including', 'v'), ('first', 'a'), ('lens', 'n'), ('first', 'a'), ('field', 'n'), ('view', 'n'), ('second', 'a'), ('camera', 'n'), ('module', 'n'), ('including', 'v'), ('second', 'a'), ('lens', 'a'), ('second', 'a'), ('field', 'n'), ('view', 'n'), ('wider', 'v'), ('first', 'a'), ('field', 'n'), ('view', 'n'), ('wherein', 'v'), ('generating', 'v'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('comprises', 'v'), ('estimating', 'v'), ('depth', 'n'), ('map', None), ('residual', 'a'), ('region', 'n'), ('based', 'v'), ('estimated', 'v'), ('relationship', 'n'), ('primary', 'a'), ('region', 'n'), ('residual', 'a'), ('region', 'n'), ('first', 'r'), ('depth', 'v'), ('map', 'a'), ('generating', 'v'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('based', 'v'), ('depth', 'a'), ('map', 'a'), ('residual', 'a'), ('region', 'n'), ('first', 'r'), ('depth', 'v'), ('map', 'n'), ('wherein', 'n'), ('estimating', 'v'), ('relationship', 'n'), ('primary', 'a'), ('region', 'n'), ('second', 'n'), ('performed', 'v'), ('using', 'v'), ('neural', 'a'), ('network', 'n'), ('model', 'n'), ('comprising', 'v'), ('performing', 'v'), ('pre-processing', 'a'), ('operation', 'n'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('generating', 'v'), ('third', 'a'), ('depth', 'n'), ('map', 'n'), ('residual', 'a'), ('fusing', 'v'), ('second', 'a'), ('depth', 'a'), ('map', 'n'), ('pre-processing', 'a'), ('operation', 'n'), ('performed', 'v'), ('first', 'a'), ('depth', 'n'), ('map', 'n'), ('wherein', None), ('performing', 'v'), ('pre-processing', 'a'), ('operation', 'n'), ('comprises', 'n'), ('performing', 'v'), ('tone', 'n'), ('mapping', 'n'), ('operation', 'n'), ('depth', 'n'), ('map', 'n'), ('primary', 'a'), ('region', 'n'), ('depth', 'n'), ('map', 'v'), ('residual', 'a'), ('region', 'n'), ('based', 'v'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('operating', 'v'), ('electronic', 'a'), ('apparatus', 'n'), ('electronic', 'a'), ('apparatus', 'n'), ('including', 'v'), ('first', 'a'), ('camera', 'n'), ('module', 'n'), ('providing', 'v'), ('first', 'a'), ('object', 'n'), ('using', 'v'), ('first', 'a'), ('field', 'n'), ('view', 'n'), ('second', 'a'), ('camera', 'n'), ('module', 'n'), ('providing', 'v'), ('second', 'a'), ('object', 'a'), ('using', 'v'), ('second', 'a'), ('field', 'n'), ('view', 'n'), ('wider', 'v'), ('first', 'a'), ('field', 'n'), ('view', 'n'), ('generating', 'v'), ('depth', 'n'), ('map', None), ('second', 'a'), ('based', 'v'), ('primary', 'a'), ('region', 'n'), ('second', 'a'), ('residual', 'a'), ('region', 'n'), ('second', 'a'), ('operating', 'n'), ('comprising', 'v'), ('generating', 'v'), ('first', 'a'), ('depth', 'n'), ('map', 'n'), ('primary', 'a'), ('region', 'n'), ('estimating', 'v'), ('relationship', 'n'), ('first', 'a'), ('second', 'a'), ('estimating', 'v'), ('relationship', 'n'), ('primary', 'a'), ('region', 'n'), ('residual', 'a'), ('region', 'n'), ('based', 'v'), ('first', 'a'), ('second', 'a'), ('generating', 'v'), ('second', 'a'), ('depth', 'n'), ('map', 'a'), ('second', 'a'), ('estimating', 'v'), ('depth', 'n'), ('map', None), ('second', 'a'), ('region', 'n'), ('based', 'v'), ('estimated', 'v'), ('relationship', 'n'), ('primary', 'a'), ('region', 'n'), ('residual', 'a'), ('region', 'n'), ('generating', 'v'), ('depth', 'a'), ('map', 'a'), ('second', 'n'), ('fusing', 'v'), ('first', 'a'), ('depth', 'n'), ('map', 'n'), ('second', 'a'), ('depth', 'n'), ('map', 'n'), ('operation', 'n'), ('comprising', 'v'), ('executing', 'v'), ('application', 'n'), ('applies', 'n'), ('effect', 'n'), ('second', 'n'), ('based', 'v'), ('depth', 'n'), ('map', 'a'), ('residual', 'a'), ('operation', 'n'), ('wherein', None), ('application', 'n'), ('applies', 'v'), ('least', 'a'), ('one', None), ('effect', 'n'), ('auto-focusing', 'a'), ('out-focusing', 'a'), ('forebackground', 'n'), ('separation', 'n'), ('face', 'n'), ('object', 'a'), ('detection', 'n'), ('within', None), ('frame', 'n'), ('augmented', 'a'), ('reality', 'n'), ('second', 'n'), ('based', 'v'), ('depth', 'a'), ('map', 'a'), ('second', 'a'), ('payment', 'n'), ('based', 'v'), ('face', 'n'), ('comprising', 'v'), ('acquiring', 'v'), ('first', 'a'), ('face', 'n'), ('information', 'n'), ('target', 'n'), ('extracting', 'v'), ('first', 'a'), ('characteristic', 'a'), ('information', 'n'), ('first', 'r'), ('face', 'n'), ('information', 'n'), ('wherein', None), ('first', 'a'), ('characteristic', 'a'), ('information', 'n'), ('includes', 'v'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('target', 'n'), ('gaze', 'n'), ('information', 'n'), ('target', 'n'), ('determining', 'v'), ('whether', None), ('target', 'n'), ('willingness', 'n'), ('pay', 'n'), ('according', 'v'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('target', 'n'), ('gaze', 'n'), ('information', 'n'), ('target', 'n'), ('including', 'v'), ('determining', 'v'), ('whether', None), ('angle', 'a'), ('rotation', 'n'), ('preset', 'v'), ('direction', 'n'), ('less', 'r'), ('angle', 'a'), ('threshold', 'n'), ('wherein', 'n'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('includes', 'v'), ('angle', 'a'), ('rotation', 'n'), ('preset', 'v'), ('direction', 'n'), ('determining', 'v'), ('whether', None), ('probability', 'n'), ('value', 'n'), ('gazes', 'v'), ('payment', 'n'), ('screen', 'n'), ('greater', 'a'), ('probability', 'n'), ('threshold', 'n'), ('wherein', 'n'), ('gaze', 'a'), ('information', 'n'), ('includes', 'v'), ('probability', 'n'), ('value', 'n'), ('gazes', 'v'), ('payment', 'n'), ('screen', 'n'), ('response', 'n'), ('determining', 'v'), ('angle', 'a'), ('rotation', 'n'), ('preset', 'v'), ('direction', 'n'), ('less', 'r'), ('angle', 'a'), ('threshold', 'n'), ('probability', 'n'), ('value', 'n'), ('gazes', 'v'), ('payment', 'n'), ('screen', 'n'), ('greater', 'a'), ('probability', 'n'), ('threshold', 'v'), ('determining', 'v'), ('target', 'n'), ('willingness', 'a'), ('pay', 'n'), ('response', 'n'), ('determining', 'v'), ('target', 'n'), ('willingness', 'a'), ('pay', 'n'), ('completing', 'v'), ('payment', 'n'), ('operation', 'n'), ('based', 'v'), ('face', 'n'), ('ed', 'a'), ('wherein', 'n'), ('completing', 'v'), ('payment', 'n'), ('operation', 'n'), ('based', 'v'), ('face', 'n'), ('comprises', 'v'), ('triggering', 'v'), ('performing', 'v'), ('payment', 'n'), ('initiating', 'n'), ('operation', 'n'), ('acquire', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('based', 'v'), ('face', 'n'), ('determining', 'v'), ('whether', None), ('second', 'a'), ('characteristic', 'a'), ('information', 'n'), ('extracted', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('indicates', 'v'), ('willingness', 'a'), ('pay', 'n'), ('response', 'n'), ('determining', 'v'), ('second', 'a'), ('characteristic', 'a'), ('information', 'n'), ('indicates', 'v'), ('willingness', 'a'), ('pay', 'n'), ('triggering', 'v'), ('performing', 'v'), ('payment', 'n'), ('confirmation', 'n'), ('operation', 'n'), ('complete', 'a'), ('payment', 'n'), ('operation', 'n'), ('based', 'v'), ('payment', 'n'), ('account', 'n'), ('information', 'n'), ('corresponding', 'v'), ('target', 'n'), ('ed', 'a'), ('wherein', 'n'), ('determining', 'v'), ('whether', None), ('second', 'a'), ('characteristic', 'a'), ('information', 'n'), ('extracted', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('indicates', 'v'), ('willingness', 'a'), ('pay', 'n'), ('comprises', 'n'), ('determining', 'v'), ('whether', None), ('current', 'a'), ('corresponding', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('consistent', 'a'), ('target', 'n'), ('response', 'n'), ('determining', 'v'), ('current', 'a'), ('consistent', 'a'), ('target', 'n'), ('determining', 'v'), ('whether', None), ('target', 'n'), ('willingness', 'n'), ('pay', 'n'), ('according', 'v'), ('second', 'a'), ('characteristic', 'a'), ('information', 'n'), ('extracted', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('ed', None), ('wherein', 'n'), ('extracting', 'v'), ('first', 'a'), ('characteristic', 'a'), ('information', 'n'), ('first', 'r'), ('face', 'n'), ('information', 'n'), ('comprises', 'v'), ('determining', 'v'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('target', 'n'), ('using', 'v'), ('head', 'a'), ('posture', 'n'), ('model', 'n'), ('based', 'v'), ('first', 'a'), ('face', 'n'), ('information', 'n'), ('determining', 'v'), ('gaze', 'n'), ('information', 'n'), ('target', 'n'), ('using', 'v'), ('gaze', 'a'), ('information', 'n'), ('model', 'n'), ('based', 'v'), ('characteristics', 'n'), ('eye', 'n'), ('region', 'n'), ('first', 'r'), ('face', 'n'), ('information', 'n'), ('ed', None), ('wherein', 'n'), ('head', 'n'), ('posture', 'n'), ('model', 'n'), ('obtained', 'v'), ('training', 'v'), ('acquiring', 'v'), ('first', 'a'), ('sample', 'n'), ('data', 'n'), ('set', 'v'), ('wherein', 'n'), ('first', 'a'), ('sample', 'n'), ('data', 'n'), ('set', 'v'), ('includes', 'v'), ('pieces', 'n'), ('first', 'r'), ('sample', 'a'), ('data', 'n'), ('pieces', 'n'), ('first', 'r'), ('sample', 'a'), ('data', 'n'), ('includes', 'v'), ('correspondence', 'n'), ('sample', 'n'), ('face', 'v'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('determining', 'v'), ('mean', 'n'), ('data', 'n'), ('variance', 'n'), ('data', 'n'), ('sample', 'n'), ('face', 'n'), ('pieces', 'n'), ('first', 'r'), ('sample', 'a'), ('data', 'n'), ('preprocessing', 'v'), ('sample', 'a'), ('face', 'n'), ('contained', 'v'), ('pieces', 'n'), ('first', 'a'), ('sample', 'n'), ('data', 'n'), ('based', 'v'), ('mean', 'a'), ('data', 'n'), ('variance', 'n'), ('data', 'n'), ('obtain', 'v'), ('preprocessed', 'a'), ('sample', 'a'), ('face', 'n'), ('setting', 'v'), ('preprocessed', 'a'), ('sample', 'a'), ('face', 'n'), ('corresponding', 'v'), ('head', 'a'), ('posture', 'n'), ('information', 'n'), ('first', 'r'), ('model', 'v'), ('training', 'v'), ('sample', 'a'), ('performing', 'v'), ('training', 'n'), ('using', 'v'), ('machine', 'n'), ('learning', 'n'), ('based', 'v'), ('first', 'a'), ('model', 'n'), ('training', 'n'), ('samples', 'n'), ('obtain', 'v'), ('head', 'a'), ('posture', 'n'), ('model', 'n'), ('ed', 'a'), ('wherein', 'n'), ('gaze', 'n'), ('information', 'n'), ('model', 'n'), ('obtained', 'v'), ('training', 'v'), ('acquiring', 'v'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('set', 'v'), ('wherein', 'a'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('set', 'v'), ('includes', 'v'), ('pieces', 'n'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('pieces', 'n'), ('second', 'a'), ('sample', 'a'), ('data', 'n'), ('includes', 'v'), ('correspondence', 'n'), ('sample', 'n'), ('eye', 'n'), ('gaze', 'n'), ('information', 'n'), ('determining', 'v'), ('mean', 'n'), ('data', 'n'), ('variance', 'n'), ('data', 'n'), ('sample', 'n'), ('eye', 'n'), ('pieces', 'n'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('preprocessing', 'v'), ('sample', 'n'), ('eye', 'n'), ('contained', 'v'), ('pieces', 'n'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('based', 'v'), ('mean', 'a'), ('data', 'n'), ('variance', 'n'), ('data', 'n'), ('obtain', 'v'), ('preprocessed', 'a'), ('sample', 'n'), ('eye', 'n'), ('setting', 'v'), ('preprocessed', 'a'), ('sample', 'n'), ('eye', 'n'), ('corresponding', 'v'), ('gaze', 'a'), ('information', 'n'), ('second', 'a'), ('model', 'n'), ('training', 'v'), ('sample', 'a'), ('performing', 'v'), ('training', 'n'), ('using', 'v'), ('machine', 'n'), ('learning', 'v'), ('based', 'v'), ('second', 'a'), ('model', 'n'), ('training', 'n'), ('samples', 'n'), ('obtain', 'v'), ('gaze', 'a'), ('information', 'n'), ('model', 'n'), ('ed', 'n'), ('wherein', 'v'), ('angle', 'a'), ('rotation', 'n'), ('preset', 'v'), ('direction', 'n'), ('comprises', 'v'), ('pitch', 'v'), ('angle', 'n'), ('yaw', 'n'), ('angle', 'n'), ('roll', 'n'), ('angle', 'n'), ('wherein', 'n'), ('pitch', 'n'), ('angle', 'n'), ('refers', 'n'), ('angle', 'v'), ('rotation', 'n'), ('around', None), ('x-axis', 'a'), ('yaw', 'n'), ('angle', 'n'), ('refers', 'n'), ('angle', 'v'), ('rotation', 'n'), ('around', None), ('y-axis', 'a'), ('roll', 'n'), ('angle', 'n'), ('refers', 'n'), ('angle', 'v'), ('rotation', 'n'), ('around', None), ('z-axis', 'a'), ('payment', 'n'), ('device', 'n'), ('based', 'v'), ('face', 'n'), ('comprising', 'v'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('storing', 'v'), ('instructions', 'n'), ('executable', 'a'), ('cause', 'n'), ('device', 'n'), ('perform', 'n'), ('operations', 'n'), ('comprising', 'v'), ('acquiring', 'v'), ('first', 'a'), ('face', 'n'), ('information', 'n'), ('target', 'n'), ('extracting', 'v'), ('first', 'a'), ('characteristic', 'a'), ('information', 'n'), ('first', 'r'), ('face', 'n'), ('information', 'n'), ('wherein', None), ('first', 'a'), ('characteristic', 'a'), ('information', 'n'), ('includes', 'v'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('target', 'n'), ('gaze', 'n'), ('information', 'n'), ('target', 'n'), ('determining', 'v'), ('whether', None), ('target', 'n'), ('willingness', 'n'), ('pay', 'n'), ('according', 'v'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('target', 'n'), ('gaze', 'n'), ('information', 'n'), ('target', 'n'), ('including', 'v'), ('determining', 'v'), ('whether', None), ('angle', 'a'), ('rotation', 'n'), ('preset', 'v'), ('direction', 'n'), ('less', 'r'), ('angle', 'a'), ('threshold', 'n'), ('wherein', 'n'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('includes', 'v'), ('angle', 'a'), ('rotation', 'n'), ('preset', 'v'), ('direction', 'n'), ('determining', 'v'), ('whether', None), ('probability', 'n'), ('value', 'n'), ('gazes', 'v'), ('payment', 'n'), ('screen', 'n'), ('greater', 'a'), ('probability', 'n'), ('threshold', 'n'), ('wherein', 'n'), ('gaze', 'a'), ('information', 'n'), ('includes', 'v'), ('probability', 'n'), ('value', 'n'), ('gazes', 'v'), ('payment', 'n'), ('screen', 'n'), ('response', 'n'), ('determining', 'v'), ('angle', 'a'), ('rotation', 'n'), ('preset', 'v'), ('direction', 'n'), ('less', 'r'), ('angle', 'a'), ('threshold', 'n'), ('probability', 'n'), ('value', 'n'), ('gazes', 'v'), ('payment', 'n'), ('screen', 'n'), ('greater', 'a'), ('probability', 'n'), ('threshold', 'v'), ('determining', 'v'), ('target', 'n'), ('willingness', 'a'), ('pay', 'n'), ('response', 'n'), ('determining', 'v'), ('target', 'n'), ('willingness', 'a'), ('pay', 'n'), ('completing', 'v'), ('payment', 'n'), ('operation', 'n'), ('based', 'v'), ('face', 'n'), ('device', 'n'), ('ed', None), ('wherein', 'n'), ('completing', 'v'), ('payment', 'n'), ('operation', 'n'), ('based', 'v'), ('face', 'n'), ('comprises', 'v'), ('triggering', 'v'), ('performing', 'v'), ('payment', 'n'), ('initiating', 'n'), ('operation', 'n'), ('acquire', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('based', 'v'), ('face', 'n'), ('determining', 'v'), ('whether', None), ('second', 'a'), ('characteristic', 'a'), ('information', 'n'), ('extracted', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('indicates', 'v'), ('willingness', 'a'), ('pay', 'n'), ('response', 'n'), ('determining', 'v'), ('second', 'a'), ('characteristic', 'a'), ('information', 'n'), ('indicates', 'v'), ('willingness', 'a'), ('pay', 'n'), ('triggering', 'v'), ('performing', 'v'), ('payment', 'n'), ('confirmation', 'n'), ('operation', 'n'), ('complete', 'a'), ('payment', 'n'), ('operation', 'n'), ('based', 'v'), ('payment', 'n'), ('account', 'n'), ('information', 'n'), ('corresponding', 'v'), ('target', 'n'), ('device', 'n'), ('ed', 'n'), ('wherein', None), ('determining', 'v'), ('whether', None), ('second', 'a'), ('characteristic', 'a'), ('information', 'n'), ('extracted', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('indicates', 'v'), ('willingness', 'a'), ('pay', 'n'), ('comprises', 'n'), ('determining', 'v'), ('whether', None), ('current', 'a'), ('corresponding', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('consistent', 'a'), ('target', 'n'), ('response', 'n'), ('determining', 'v'), ('current', 'a'), ('consistent', 'a'), ('target', 'n'), ('determining', 'v'), ('whether', None), ('target', 'n'), ('willingness', 'n'), ('pay', 'n'), ('according', 'v'), ('second', 'a'), ('characteristic', 'a'), ('information', 'n'), ('extracted', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('device', 'n'), ('ed', 'n'), ('wherein', 'n'), ('extracting', 'v'), ('first', 'a'), ('characteristic', 'a'), ('information', 'n'), ('first', 'r'), ('face', 'n'), ('information', 'n'), ('comprises', 'v'), ('determining', 'v'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('target', 'n'), ('using', 'v'), ('head', 'a'), ('posture', 'n'), ('model', 'n'), ('based', 'v'), ('first', 'a'), ('face', 'n'), ('information', 'n'), ('determining', 'v'), ('gaze', 'n'), ('information', 'n'), ('target', 'n'), ('using', 'v'), ('gaze', 'a'), ('information', 'n'), ('model', 'n'), ('based', 'v'), ('characteristics', 'n'), ('eye', 'n'), ('region', 'n'), ('first', 'r'), ('face', 'n'), ('information', 'n'), ('device', 'n'), ('ed', 'n'), ('wherein', None), ('head', 'n'), ('posture', 'n'), ('model', 'n'), ('obtained', 'v'), ('training', 'v'), ('acquiring', 'v'), ('first', 'a'), ('sample', 'n'), ('data', 'n'), ('set', 'v'), ('wherein', 'n'), ('first', 'a'), ('sample', 'n'), ('data', 'n'), ('set', 'v'), ('includes', 'v'), ('pieces', 'n'), ('first', 'r'), ('sample', 'a'), ('data', 'n'), ('pieces', 'n'), ('first', 'r'), ('sample', 'a'), ('data', 'n'), ('includes', 'v'), ('correspondence', 'n'), ('sample', 'n'), ('face', 'v'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('determining', 'v'), ('mean', 'n'), ('data', 'n'), ('variance', 'n'), ('data', 'n'), ('sample', 'n'), ('face', 'n'), ('pieces', 'n'), ('first', 'r'), ('sample', 'a'), ('data', 'n'), ('preprocessing', 'v'), ('sample', 'a'), ('face', 'n'), ('contained', 'v'), ('pieces', 'n'), ('first', 'a'), ('sample', 'n'), ('data', 'n'), ('based', 'v'), ('mean', 'a'), ('data', 'n'), ('variance', 'n'), ('data', 'n'), ('obtain', 'v'), ('preprocessed', 'a'), ('sample', 'a'), ('face', 'n'), ('setting', 'v'), ('preprocessed', 'a'), ('sample', 'a'), ('face', 'n'), ('corresponding', 'v'), ('head', 'a'), ('posture', 'n'), ('information', 'n'), ('first', 'r'), ('model', 'v'), ('training', 'v'), ('sample', 'a'), ('performing', 'v'), ('training', 'n'), ('using', 'v'), ('machine', 'n'), ('learning', 'n'), ('based', 'v'), ('first', 'a'), ('model', 'n'), ('training', 'n'), ('samples', 'n'), ('obtain', 'v'), ('head', 'a'), ('posture', 'n'), ('model', 'n'), ('device', 'n'), ('ed', None), ('wherein', 'n'), ('gaze', 'n'), ('information', 'n'), ('model', 'n'), ('obtained', 'v'), ('training', 'v'), ('acquiring', 'v'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('set', 'v'), ('wherein', 'a'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('set', 'v'), ('includes', 'v'), ('pieces', 'n'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('pieces', 'n'), ('second', 'a'), ('sample', 'a'), ('data', 'n'), ('includes', 'v'), ('correspondence', 'n'), ('sample', 'n'), ('eye', 'n'), ('gaze', 'n'), ('information', 'n'), ('determining', 'v'), ('mean', 'n'), ('data', 'n'), ('variance', 'n'), ('data', 'n'), ('sample', 'n'), ('eye', 'n'), ('pieces', 'n'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('preprocessing', 'v'), ('sample', 'n'), ('eye', 'n'), ('contained', 'v'), ('pieces', 'n'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('based', 'v'), ('mean', 'a'), ('data', 'n'), ('variance', 'n'), ('data', 'n'), ('obtain', 'v'), ('preprocessed', 'a'), ('sample', 'n'), ('eye', 'n'), ('setting', 'v'), ('preprocessed', 'a'), ('sample', 'n'), ('eye', 'n'), ('corresponding', 'v'), ('gaze', 'a'), ('information', 'n'), ('second', 'a'), ('model', 'n'), ('training', 'v'), ('sample', 'a'), ('performing', 'v'), ('training', 'n'), ('using', 'v'), ('machine', 'n'), ('learning', 'v'), ('second', 'a'), ('model', 'n'), ('training', 'n'), ('samples', 'n'), ('obtain', 'v'), ('gaze', 'a'), ('information', 'n'), ('model', 'n'), ('device', 'n'), ('ed', None), ('wherein', 'n'), ('angle', 'n'), ('rotation', 'n'), ('preset', 'v'), ('direction', 'n'), ('comprises', 'v'), ('pitch', 'v'), ('angle', 'n'), ('yaw', 'n'), ('angle', 'n'), ('roll', 'n'), ('angle', 'n'), ('wherein', 'n'), ('pitch', 'n'), ('angle', 'n'), ('refers', 'n'), ('angle', 'v'), ('rotation', 'n'), ('around', None), ('x-axis', 'a'), ('yaw', 'n'), ('angle', 'n'), ('refers', 'n'), ('angle', 'v'), ('rotation', 'n'), ('around', None), ('y-axis', 'a'), ('roll', 'n'), ('angle', 'n'), ('refers', 'n'), ('angle', 'v'), ('rotation', 'n'), ('around', None), ('z-axis', 'a'), ('non-transitory', 'a'), ('computer-readable', 'a'), ('storage', 'n'), ('medium', 'n'), ('payment', 'n'), ('based', 'v'), ('face', 'n'), ('configured', 'v'), ('instructions', 'n'), ('executable', 'a'), ('one', None), ('cause', 'n'), ('one', None), ('perform', 'n'), ('operations', 'n'), ('comprising', 'v'), ('acquiring', 'v'), ('first', 'a'), ('face', 'n'), ('information', 'n'), ('target', 'n'), ('extracting', 'v'), ('first', 'a'), ('characteristic', 'a'), ('information', 'n'), ('first', 'r'), ('face', 'n'), ('information', 'n'), ('wherein', None), ('first', 'a'), ('characteristic', 'a'), ('information', 'n'), ('includes', 'v'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('target', 'n'), ('gaze', 'n'), ('information', 'n'), ('target', 'n'), ('determining', 'v'), ('whether', None), ('target', 'n'), ('willingness', 'n'), ('pay', 'n'), ('according', 'v'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('target', 'n'), ('gaze', 'n'), ('information', 'n'), ('target', 'n'), ('including', 'v'), ('determining', 'v'), ('whether', None), ('angle', 'a'), ('rotation', 'n'), ('preset', 'v'), ('direction', 'n'), ('less', 'r'), ('angle', 'a'), ('threshold', 'n'), ('wherein', 'n'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('includes', 'v'), ('angle', 'a'), ('rotation', 'n'), ('preset', 'v'), ('direction', 'n'), ('determining', 'v'), ('whether', None), ('probability', 'n'), ('value', 'n'), ('gazes', 'v'), ('payment', 'n'), ('screen', 'n'), ('greater', 'a'), ('probability', 'n'), ('threshold', 'n'), ('wherein', 'n'), ('gaze', 'a'), ('information', 'n'), ('includes', 'v'), ('probability', 'n'), ('value', 'n'), ('gazes', 'v'), ('payment', 'n'), ('screen', 'n'), ('response', 'n'), ('determining', 'v'), ('angle', 'a'), ('rotation', 'n'), ('preset', 'v'), ('direction', 'n'), ('less', 'r'), ('angle', 'a'), ('threshold', 'n'), ('probability', 'n'), ('value', 'n'), ('gazes', 'v'), ('payment', 'n'), ('screen', 'n'), ('greater', 'a'), ('probability', 'n'), ('threshold', 'v'), ('determining', 'v'), ('target', 'n'), ('willingness', 'a'), ('pay', 'n'), ('response', 'n'), ('determining', 'v'), ('target', 'n'), ('willingness', 'a'), ('pay', 'n'), ('completing', 'v'), ('payment', 'n'), ('operation', 'n'), ('based', 'v'), ('face', 'n'), ('storage', 'n'), ('medium', 'n'), ('ed', 'v'), ('wherein', 'a'), ('completing', 'v'), ('payment', 'n'), ('operation', 'n'), ('based', 'v'), ('face', 'n'), ('comprises', 'v'), ('triggering', 'v'), ('performing', 'v'), ('payment', 'n'), ('initiating', 'n'), ('operation', 'n'), ('acquire', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('based', 'v'), ('face', 'n'), ('determining', 'v'), ('whether', None), ('second', 'a'), ('characteristic', 'a'), ('information', 'n'), ('extracted', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('indicates', 'v'), ('willingness', 'a'), ('pay', 'n'), ('response', 'n'), ('determining', 'v'), ('second', 'a'), ('characteristic', 'a'), ('information', 'n'), ('indicates', 'v'), ('willingness', 'a'), ('pay', 'n'), ('triggering', 'v'), ('performing', 'v'), ('payment', 'n'), ('confirmation', 'n'), ('operation', 'n'), ('complete', 'a'), ('payment', 'n'), ('operation', 'n'), ('based', 'v'), ('payment', 'n'), ('account', 'n'), ('information', 'n'), ('corresponding', 'v'), ('target', 'n'), ('storage', 'n'), ('medium', 'n'), ('ed', 'n'), ('wherein', None), ('determining', 'v'), ('whether', None), ('second', 'a'), ('characteristic', 'a'), ('information', 'n'), ('extracted', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('indicates', 'v'), ('willingness', 'a'), ('pay', 'n'), ('comprises', 'n'), ('determining', 'v'), ('whether', None), ('current', 'a'), ('corresponding', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('consistent', 'a'), ('target', 'n'), ('response', 'n'), ('determining', 'v'), ('current', 'a'), ('consistent', 'a'), ('target', 'n'), ('determining', 'v'), ('whether', None), ('target', 'n'), ('willingness', 'n'), ('pay', 'n'), ('according', 'v'), ('second', 'a'), ('characteristic', 'a'), ('information', 'n'), ('extracted', 'v'), ('second', 'a'), ('face', 'n'), ('information', 'n'), ('storage', 'n'), ('medium', 'n'), ('ed', 'n'), ('wherein', 'n'), ('extracting', 'v'), ('first', 'a'), ('characteristic', 'a'), ('information', 'n'), ('first', 'r'), ('face', 'n'), ('information', 'n'), ('comprises', 'v'), ('determining', 'v'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('target', 'n'), ('using', 'v'), ('head', 'a'), ('posture', 'n'), ('model', 'n'), ('based', 'v'), ('first', 'a'), ('face', 'n'), ('information', 'n'), ('determining', 'v'), ('gaze', 'n'), ('information', 'n'), ('target', 'n'), ('using', 'v'), ('gaze', 'a'), ('information', 'n'), ('model', 'n'), ('based', 'v'), ('characteristics', 'n'), ('eye', 'n'), ('region', 'n'), ('first', 'r'), ('face', 'n'), ('information', 'n'), ('storage', 'n'), ('medium', 'n'), ('ed', 'n'), ('wherein', None), ('head', 'n'), ('posture', 'n'), ('model', 'n'), ('obtained', 'v'), ('training', 'v'), ('acquiring', 'v'), ('first', 'a'), ('sample', 'n'), ('data', 'n'), ('set', 'v'), ('wherein', 'n'), ('first', 'a'), ('sample', 'n'), ('data', 'n'), ('set', 'v'), ('includes', 'v'), ('pieces', 'n'), ('first', 'r'), ('sample', 'a'), ('data', 'n'), ('pieces', 'n'), ('first', 'r'), ('sample', 'a'), ('data', 'n'), ('includes', 'v'), ('correspondence', 'n'), ('sample', 'n'), ('face', 'v'), ('head', 'n'), ('posture', 'n'), ('information', 'n'), ('determining', 'v'), ('mean', 'n'), ('data', 'n'), ('variance', 'n'), ('data', 'n'), ('sample', 'n'), ('face', 'n'), ('pieces', 'n'), ('first', 'r'), ('sample', 'a'), ('data', 'n'), ('preprocessing', 'v'), ('sample', 'a'), ('face', 'n'), ('contained', 'v'), ('pieces', 'n'), ('first', 'a'), ('sample', 'n'), ('data', 'n'), ('based', 'v'), ('mean', 'a'), ('data', 'n'), ('variance', 'n'), ('data', 'n'), ('obtain', 'v'), ('preprocessed', 'a'), ('sample', 'a'), ('face', 'n'), ('setting', 'v'), ('preprocessed', 'a'), ('sample', 'a'), ('face', 'n'), ('corresponding', 'v'), ('head', 'a'), ('posture', 'n'), ('information', 'n'), ('first', 'r'), ('model', 'v'), ('training', 'v'), ('sample', 'a'), ('performing', 'v'), ('training', 'n'), ('using', 'v'), ('machine', 'n'), ('learning', 'n'), ('based', 'v'), ('first', 'a'), ('model', 'n'), ('training', 'n'), ('samples', 'n'), ('obtain', 'v'), ('head', 'a'), ('posture', 'n'), ('model', 'n'), ('wherein', 'n'), ('gaze', 'a'), ('information', 'n'), ('model', 'n'), ('obtained', 'v'), ('training', 'v'), ('acquiring', 'v'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('set', 'v'), ('wherein', 'a'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('set', 'v'), ('includes', 'v'), ('pieces', 'n'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('pieces', 'n'), ('second', 'a'), ('sample', 'a'), ('data', 'n'), ('includes', 'v'), ('correspondence', 'n'), ('sample', 'n'), ('eye', 'n'), ('gaze', 'n'), ('information', 'n'), ('determining', 'v'), ('mean', 'n'), ('data', 'n'), ('variance', 'n'), ('data', 'n'), ('sample', 'n'), ('eye', 'n'), ('pieces', 'n'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('preprocessing', 'v'), ('sample', 'n'), ('eye', 'n'), ('contained', 'v'), ('pieces', 'n'), ('second', 'a'), ('sample', 'n'), ('data', 'n'), ('based', 'v'), ('mean', 'a'), ('data', 'n'), ('variance', 'n'), ('data', 'n'), ('obtain', 'v'), ('preprocessed', 'a'), ('sample', 'n'), ('eye', 'n'), ('setting', 'v'), ('preprocessed', 'a'), ('sample', 'n'), ('eye', 'n'), ('corresponding', 'v'), ('gaze', 'a'), ('information', 'n'), ('second', 'a'), ('model', 'n'), ('training', 'v'), ('sample', 'a'), ('performing', 'v'), ('training', 'n'), ('using', 'v'), ('machine', 'n'), ('learning', 'v'), ('based', 'v'), ('second', 'a'), ('model', 'n'), ('training', 'n'), ('samples', 'n'), ('obtain', 'v'), ('gaze', 'a'), ('information', 'n'), ('model', 'n'), ('storage', 'n'), ('medium', 'n'), ('ed', 'n'), ('wherein', 'n'), ('angle', 'a'), ('rotation', 'n'), ('preset', 'v'), ('direction', 'n'), ('comprises', 'v'), ('pitch', 'v'), ('angle', 'n'), ('yaw', 'n'), ('angle', 'n'), ('roll', 'n'), ('angle', 'n'), ('wherein', 'n'), ('pitch', 'n'), ('angle', 'n'), ('refers', 'n'), ('angle', 'v'), ('rotation', 'n'), ('around', None), ('x-axis', 'a'), ('yaw', 'n'), ('angle', 'n'), ('refers', 'n'), ('angle', 'v'), ('rotation', 'n'), ('around', None), ('y-axis', 'a'), ('roll', 'n'), ('angle', 'n'), ('refers', 'n'), ('angle', 'v'), ('rotation', 'n'), ('around', None), ('z-axis', 'a'), ('comprising', 'v'), ('detecting', 'v'), ('motion', 'n'), ('detection', 'n'), ('module', 'n'), ('motion', 'n'), ('subject', 'n'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('assigning', 'v'), ('unique', 'a'), ('session', 'n'), ('identification', 'n'), ('number', 'n'), ('subject', 'a'), ('detected', 'v'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('detecting', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'n'), ('detected', 'v'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('generating', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'n'), ('assessing', 'v'), ('quality', 'n'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('determining', 'v'), ('identity', 'n'), ('subject', 'n'), ('based', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('identifying', 'v'), ('intent', 'n'), ('subject', 'a'), ('authorizing', 'v'), ('access', 'n'), ('point', 'n'), ('entry', 'n'), ('based', 'v'), ('determined', 'v'), ('identity', 'n'), ('subject', 'n'), ('based', 'v'), ('intent', 'n'), ('subject', 'a'), ('comprising', 'v'), ('determining', 'v'), ('one', None), ('additional', 'a'), ('subjects', 'n'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('assigning', 'v'), ('unique', 'a'), ('session', 'n'), ('identification', 'n'), ('number', 'n'), ('one', None), ('additional', 'a'), ('subjects', 'n'), ('detected', 'v'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('wherein', None), ('assessing', 'v'), ('quality', 'n'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('comprises', 'v'), ('assessing', 'v'), ('whether', None), ('quality', 'n'), ('facial', 'a'), ('area', 'n'), ('object', 'n'), ('equates', 'n'), ('predetermined', 'v'), ('metric', 'a'), ('quality', 'n'), ('upon', None), ('determining', 'v'), ('quality', 'n'), ('facial', 'a'), ('area', 'n'), ('object', 'v'), ('inferior', 'a'), ('predetermined', 'v'), ('metric', 'a'), ('quality', 'n'), ('discarding', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('generating', 'v'), ('second', 'a'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('comprising', 'v'), ('detecting', 'v'), ('whether', None), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('photographic', 'a'), ('upon', None), ('detecting', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('photographic', 'a'), ('generating', 'v'), ('warning', 'v'), ('restrict', 'a'), ('access', 'n'), ('point', 'n'), ('entry', 'n'), ('comprising', 'v'), ('conducing', 'v'), ('incremental', 'a'), ('training', 'n'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('wherein', 'n'), ('conducing', 'v'), ('incremental', 'a'), ('training', 'n'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('comprises', 'n'), ('capturing', 'v'), ('first', 'a'), ('facial', 'a'), ('area', 'n'), ('facial', 'a'), ('landmarks', 'n'), ('converting', 'v'), ('first', 'a'), ('facial', 'a'), ('area', 'n'), ('first', 'r'), ('numeric', 'a'), ('vector', 'n'), ('capturing', 'v'), ('second', 'a'), ('facial', 'a'), ('area', 'n'), ('facial', 'a'), ('landmarks', 'n'), ('converting', 'v'), ('second', 'a'), ('facial', 'a'), ('area', 'n'), ('second', 'a'), ('numeric', 'a'), ('vector', 'n'), ('calculating', 'v'), ('weighted', 'v'), ('mean', 'a'), ('first', 'a'), ('numeric', 'a'), ('vector', 'n'), ('second', 'a'), ('numeric', 'a'), ('vector', 'n'), ('wherein', 'n'), ('weighted', 'v'), ('mean', 'a'), ('represents', 'v'), ('change', 'v'), ('facial', 'a'), ('area', 'n'), ('storing', 'v'), ('weighted', 'a'), ('mean', 'a'), ('database', 'n'), ('wherein', 'n'), ('determining', 'v'), ('identity', 'n'), ('subject', 'n'), ('based', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('comprises', 'v'), ('comparing', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'n'), ('stored', 'v'), ('database', 'n'), ('authenticating', 'v'), ('subject', 'a'), ('wherein', 'n'), ('identifying', 'v'), ('intent', 'n'), ('subject', 'n'), ('comprises', 'v'), ('upon', None), ('detecting', 'v'), ('facial', 'a'), ('area', 'n'), ('bounding', 'v'), ('box', 'n'), ('commencing', 'v'), ('authentication', 'n'), ('subject', 'a'), ('calculating', 'v'), ('directional', 'a'), ('vector', 'n'), ('face', 'n'), ('subject', 'a'), ('determine', 'a'), ('intent', 'n'), ('subject', 'a'), ('gain', 'n'), ('access', 'n'), ('point', 'n'), ('entry', 'n'), ('based', 'v'), ('directional', 'a'), ('vector', 'n'), ('face', 'n'), ('subject', 'a'), ('granting', 'v'), ('access', 'n'), ('point', 'n'), ('entry', 'n'), ('based', 'v'), ('authentication', 'n'), ('subject', 'n'), ('based', 'v'), ('determining', 'v'), ('intent', 'n'), ('subject', 'a'), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('medium', 'n'), ('program', 'n'), ('instructions', 'n'), ('stored', 'v'), ('thereon', 'a'), ('response', 'n'), ('execution', 'n'), ('computing', 'v'), ('device', 'n'), ('cause', 'n'), ('computing', 'v'), ('device', 'n'), ('perform', 'n'), ('operations', 'n'), ('comprising', 'v'), ('detecting', 'v'), ('motion', 'n'), ('subject', 'n'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('assigning', 'v'), ('unique', 'a'), ('session', 'n'), ('identification', 'n'), ('number', 'n'), ('subject', 'a'), ('detected', 'v'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('detecting', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'n'), ('detected', 'v'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('generating', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'n'), ('assessing', 'v'), ('quality', 'n'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('determining', 'v'), ('identity', 'n'), ('subject', 'n'), ('based', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('identifying', 'v'), ('intent', 'n'), ('subject', 'a'), ('authorizing', 'v'), ('access', 'n'), ('point', 'n'), ('entry', 'n'), ('based', 'v'), ('determined', 'v'), ('identity', 'n'), ('subject', 'n'), ('based', 'v'), ('intent', 'n'), ('subject', 'a'), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('medium', 'n'), ('comprising', 'v'), ('determining', 'v'), ('one', None), ('additional', 'a'), ('subjects', 'n'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('assigning', 'v'), ('unique', 'a'), ('session', 'n'), ('identification', 'n'), ('number', 'n'), ('one', None), ('additional', 'a'), ('subjects', 'n'), ('detected', 'v'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('medium', 'n'), ('wherein', 'n'), ('assessing', 'v'), ('quality', 'n'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('comprises', 'v'), ('assessing', 'v'), ('whether', None), ('quality', 'n'), ('facial', 'a'), ('area', 'n'), ('object', 'n'), ('equates', 'n'), ('predetermined', 'v'), ('metric', 'a'), ('quality', 'n'), ('upon', None), ('determining', 'v'), ('quality', 'n'), ('facial', 'a'), ('area', 'n'), ('object', 'v'), ('inferior', 'a'), ('predetermined', 'v'), ('metric', 'a'), ('quality', 'n'), ('discarding', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('generating', 'v'), ('second', 'a'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('medium', 'n'), ('comprising', 'v'), ('detecting', 'v'), ('whether', None), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('photographic', 'a'), ('upon', None), ('detecting', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('photographic', 'a'), ('generating', 'v'), ('warning', 'v'), ('restrict', 'a'), ('access', 'n'), ('access', 'n'), ('point', 'n'), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('medium', 'n'), ('comprising', 'v'), ('conducing', 'v'), ('incremental', 'a'), ('training', 'n'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('non-transitory', 'a'), ('computer', 'n'), ('readable', 'a'), ('medium', 'n'), ('wherein', 'n'), ('conducing', 'v'), ('incremental', 'a'), ('training', 'n'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('comprises', 'n'), ('capturing', 'v'), ('first', 'a'), ('facial', 'a'), ('area', 'n'), ('facial', 'a'), ('landmarks', 'n'), ('converting', 'v'), ('first', 'a'), ('facial', 'a'), ('area', 'n'), ('first', 'r'), ('numeric', 'a'), ('vector', 'n'), ('capturing', 'v'), ('second', 'a'), ('facial', 'a'), ('area', 'n'), ('facial', 'a'), ('landmarks', 'n'), ('converting', 'v'), ('second', 'a'), ('facial', 'a'), ('area', 'n'), ('second', 'a'), ('numeric', 'a'), ('vector', 'n'), ('calculating', 'v'), ('weighted', 'v'), ('mean', 'a'), ('first', 'a'), ('numeric', 'a'), ('vector', 'n'), ('second', 'a'), ('numeric', 'a'), ('vector', 'n'), ('wherein', 'n'), ('weighted', 'v'), ('mean', 'a'), ('represents', 'v'), ('change', 'v'), ('facial', 'a'), ('area', 'n'), ('storing', 'v'), ('weighted', 'a'), ('mean', 'a'), ('database', 'n'), ('apparatus', 'n'), ('face', 'n'), ('comprising', 'v'), ('memory', 'n'), ('store', 'n'), ('computer', 'n'), ('program', 'n'), ('instructions', 'n'), ('computer', 'n'), ('program', 'n'), ('instructions', 'n'), ('executed', 'v'), ('cause', 'n'), ('perform', 'n'), ('operations', 'n'), ('comprising', 'v'), ('detecting', 'v'), ('motion', 'n'), ('subject', 'n'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('assigning', 'v'), ('unique', 'a'), ('session', 'n'), ('identification', 'n'), ('number', 'n'), ('subject', 'a'), ('detected', 'v'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('detecting', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'n'), ('detected', 'v'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('generating', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'n'), ('assessing', 'v'), ('quality', 'n'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('determining', 'v'), ('identity', 'n'), ('subject', 'n'), ('based', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('identifying', 'v'), ('intent', 'n'), ('subject', 'a'), ('authorizing', 'v'), ('access', 'n'), ('point', 'n'), ('entry', 'n'), ('based', 'v'), ('determined', 'v'), ('identity', 'n'), ('subject', 'n'), ('based', 'v'), ('intent', 'n'), ('subject', 'a'), ('apparatus', 'n'), ('comprising', 'v'), ('determining', 'v'), ('one', None), ('additional', 'a'), ('subjects', 'n'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('assigning', 'v'), ('unique', 'a'), ('session', 'n'), ('identification', 'n'), ('number', 'n'), ('one', None), ('additional', 'a'), ('subjects', 'n'), ('detected', 'v'), ('within', None), ('predetermined', 'a'), ('area', 'n'), ('view', 'n'), ('apparatus', 'v'), ('wherein', 'n'), ('assessing', 'v'), ('quality', 'n'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('comprises', 'v'), ('assessing', 'v'), ('whether', None), ('quality', 'n'), ('facial', 'a'), ('area', 'n'), ('object', 'n'), ('equates', 'n'), ('predetermined', 'v'), ('metric', 'a'), ('quality', 'n'), ('upon', None), ('determining', 'v'), ('quality', 'n'), ('facial', 'a'), ('area', 'n'), ('object', 'v'), ('inferior', 'a'), ('predetermined', 'v'), ('metric', 'a'), ('quality', 'n'), ('discarding', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('generating', 'v'), ('second', 'a'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('apparatus', 'n'), ('comprising', 'v'), ('detecting', 'v'), ('whether', None), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('photographic', 'a'), ('upon', None), ('detecting', 'v'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('photographic', 'a'), ('generating', 'v'), ('warning', 'v'), ('restrict', 'a'), ('access', 'n'), ('access', 'n'), ('point', 'n'), ('apparatus', 'n'), ('comprising', 'v'), ('conducing', 'v'), ('incremental', 'a'), ('training', 'n'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('apparatus', 'n'), ('wherein', 'n'), ('conducing', 'v'), ('incremental', 'a'), ('training', 'n'), ('facial', 'a'), ('area', 'n'), ('subject', 'a'), ('comprises', 'n'), ('capturing', 'v'), ('first', 'a'), ('facial', 'a'), ('area', 'n'), ('facial', 'a'), ('landmarks', 'n'), ('converting', 'v'), ('first', 'a'), ('facial', 'a'), ('area', 'n'), ('first', 'r'), ('numeric', 'a'), ('vector', 'n'), ('capturing', 'v'), ('second', 'a'), ('facial', 'a'), ('area', 'n'), ('facial', 'a'), ('landmarks', 'n'), ('converting', 'v'), ('second', 'a'), ('facial', 'a'), ('area', 'n'), ('second', 'a'), ('numeric', 'a'), ('vector', 'n'), ('calculating', 'v'), ('weighted', 'v'), ('mean', 'a'), ('first', 'a'), ('numeric', 'a'), ('vector', 'n'), ('second', 'a'), ('numeric', 'a'), ('vector', 'n'), ('wherein', 'n'), ('weighted', 'v'), ('mean', 'a'), ('represents', 'v'), ('change', 'v'), ('facial', 'a'), ('area', 'n'), ('storing', 'v'), ('weighted', 'a'), ('mean', 'a'), ('database', 'n'), ('robot', 'n'), ('comprising', 'v'), ('body', 'n'), ('configured', 'v'), ('rotate', 'a'), ('tilt', 'n'), ('camera', 'n'), ('coupled', 'v'), ('body', 'n'), ('configured', 'a'), ('rotate', 'n'), ('tilt', 'n'), ('according', 'v'), ('rotate', 'n'), ('tilt', 'n'), ('body', 'n'), ('wherein', 'a'), ('camera', 'n'), ('configured', 'v'), ('acquire', 'v'), ('video', 'n'), ('space', 'n'), ('face', 'n'), ('unit', 'n'), ('configured', 'v'), ('recognize', 'a'), ('respective', 'a'), ('one', None), ('persons', 'n'), ('video', 'v'), ('unit', 'n'), ('configured', 'v'), ('track', 'a'), ('motion', 'n'), ('recognized', 'v'), ('one', None), ('persons', 'n'), ('controller', 'n'), ('configured', 'v'), ('calculate', 'a'), ('respective', 'a'), ('size', 'n'), ('one', None), ('persons', 'n'), ('select', 'v'), ('first', 'a'), ('person', 'n'), ('among', None), ('one', None), ('persons', 'n'), ('based', 'v'), ('calculated', 'a'), ('sizes', 'n'), ('control', 'n'), ('least', 'v'), ('one', None), ('direction', 'n'), ('rotation', 'n'), ('camera', 'n'), ('angle', 'n'), ('tilt', 'n'), ('camera', 'n'), ('focal', 'a'), ('distance', 'n'), ('camera', 'n'), ('based', 'v'), ('tracked', 'a'), ('motion', 'n'), ('recognized', 'v'), ('face', 'n'), ('first', 'r'), ('person', 'n'), ('robot', 'a'), ('wherein', 'n'), ('controller', 'n'), ('configured', 'v'), ('control', 'a'), ('direction', 'n'), ('rotation', 'n'), ('camera', 'n'), ('angle', 'n'), ('tilt', 'n'), ('camera', 'n'), ('achieve', 'v'), ('particular', 'a'), ('camera', 'n'), ('relative', 'a'), ('face', 'n'), ('first', 'r'), ('person', 'n'), ('control', 'n'), ('focal', 'a'), ('distance', 'n'), ('camera', 'n'), ('comparing', 'v'), ('respective', 'a'), ('sizes', 'n'), ('face', 'v'), ('first', 'a'), ('person', 'n'), ('motion', 'n'), ('first', 'r'), ('person', 'n'), ('robot', 'a'), ('wherein', 'n'), ('particular', 'a'), ('occurs', 'n'), ('camera', 'v'), ('general', 'a'), ('direction', 'n'), ('face', 'n'), ('first', 'r'), ('person', 'n'), ('robot', 'a'), ('wherein', 'n'), ('controller', 'n'), ('configured', 'v'), ('normalize', 'a'), ('sizes', 'n'), ('one', None), ('persons', 'n'), ('based', 'v'), ('interocular', 'a'), ('distance', 'n'), ('select', 'n'), ('first', 'r'), ('person', 'n'), ('based', 'v'), ('normalized', 'a'), ('sizes', 'n'), ('one', None), ('persons', 'n'), ('robot', 'v'), ('wherein', 'a'), ('controller', 'n'), ('configured', 'v'), ('select', 'a'), ('person', 'n'), ('largest', 'a'), ('face', 'n'), ('size', 'n'), ('among', None), ('one', None), ('persons', 'n'), ('first', 'a'), ('person', 'n'), ('robot', 'n'), ('comprising', 'v'), ('microphone', 'n'), ('configured', 'v'), ('receive', 'a'), ('spoken', 'n'), ('audio', 'n'), ('present', 'a'), ('space', 'n'), ('wherein', 'n'), ('controller', 'n'), ('configured', 'v'), ('select', 'a'), ('first', 'a'), ('person', 'n'), ('based', 'v'), ('received', 'v'), ('spoken', 'a'), ('audio', 'a'), ('robot', 'n'), ('wherein', 'n'), ('controller', 'n'), ('configured', 'v'), ('control', 'n'), ('gain', 'n'), ('microphone', 'n'), ('comparing', 'v'), ('respective', 'a'), ('sizes', 'n'), ('face', 'v'), ('first', 'a'), ('person', 'n'), ('motion', 'n'), ('first', 'r'), ('person', 'n'), ('robot', 'a'), ('wherein', 'n'), ('controller', 'n'), ('configured', 'v'), ('calculate', 'a'), ('position', 'n'), ('spoken', 'v'), ('audio', 'n'), ('provided', 'v'), ('select', 'a'), ('first', 'a'), ('person', 'n'), ('based', 'v'), ('whether', None), ('one', None), ('persons', 'n'), ('position', 'n'), ('voice', 'n'), ('signal', 'n'), ('provided', 'v'), ('robot', 'a'), ('wherein', 'a'), ('controller', 'n'), ('configured', 'v'), ('select', 'a'), ('second', 'a'), ('person', 'n'), ('first', 'a'), ('person', 'n'), ('among', None), ('one', None), ('persons', 'n'), ('second', 'a'), ('person', 'n'), ('located', 'v'), ('position', 'n'), ('spoken', 'v'), ('audio', 'n'), ('provided', 'v'), ('robot', 'a'), ('wherein', 'a'), ('controller', 'n'), ('configured', 'v'), ('select', 'a'), ('second', 'a'), ('person', 'n'), ('largest', 'a'), ('face', 'n'), ('size', 'n'), ('first', 'a'), ('person', 'n'), ('among', None), ('one', None), ('persons', 'n'), ('none', 'n'), ('one', None), ('persons', 'n'), ('located', 'v'), ('position', 'n'), ('spoken', 'v'), ('audio', 'n'), ('provided', 'v'), ('robot', 'a'), ('wherein', 'a'), ('controller', 'n'), ('configured', 'v'), ('select', 'a'), ('second', 'a'), ('person', 'n'), ('largest', 'a'), ('face', 'n'), ('size', 'n'), ('first', 'a'), ('person', 'n'), ('among', None), ('one', None), ('persons', 'n'), ('persons', 'n'), ('among', None), ('one', None), ('persons', 'n'), ('located', 'v'), ('position', 'n'), ('spoken', 'v'), ('audio', 'n'), ('provided', 'v'), ('robot', 'n'), ('comprising', 'v'), ('speaker', 'n'), ('wherein', 'n'), ('controller', 'n'), ('configured', 'v'), ('control', 'n'), ('volume', 'n'), ('speaker', 'n'), ('comparing', 'v'), ('respective', 'a'), ('sizes', 'n'), ('face', 'v'), ('first', 'a'), ('person', 'n'), ('motion', 'n'), ('first', 'r'), ('person', 'n'), ('robot', 'a'), ('wherein', 'n'), ('body', 'n'), ('configured', 'v'), ('rotate', 'a'), ('lateral', 'a'), ('direction', 'n'), ('tilt', 'v'), ('vertical', 'a'), ('direction', 'n'), ('comprising', 'v'), ('camera', 'n'), ('coupled', 'v'), ('body', 'n'), ('configured', 'a'), ('rotate', 'n'), ('tilt', 'n'), ('wherein', 'n'), ('camera', 'n'), ('configured', 'v'), ('acquire', 'v'), ('video', 'n'), ('space', 'n'), ('within', None), ('one', None), ('persons', 'n'), ('positioned', 'v'), ('configured', 'a'), ('recognize', 'n'), ('respective', 'a'), ('one', None), ('persons', 'n'), ('video', 'v'), ('track', 'a'), ('motion', 'n'), ('recognized', 'v'), ('one', None), ('persons', 'n'), ('calculate', 'v'), ('respective', 'a'), ('size', 'n'), ('one', None), ('persons', 'n'), ('select', 'v'), ('first', 'a'), ('person', 'n'), ('among', None), ('one', None), ('persons', 'n'), ('based', 'v'), ('calculated', 'a'), ('sizes', 'n'), ('control', 'n'), ('least', 'v'), ('one', None), ('direction', 'n'), ('rotation', 'n'), ('camera', 'n'), ('angle', 'n'), ('tilt', 'n'), ('camera', 'n'), ('focal', 'a'), ('distance', 'n'), ('camera', 'n'), ('based', 'v'), ('tracked', 'a'), ('motion', 'n'), ('recognized', 'v'), ('face', 'n'), ('first', 'r'), ('person', 'n'), ('comprising', 'v'), ('acquiring', 'v'), ('camera', 'n'), ('video', 'n'), ('space', 'n'), ('within', None), ('one', None), ('persons', 'n'), ('positioned', 'v'), ('respective', 'a'), ('one', None), ('persons', 'n'), ('video', 'a'), ('motion', 'n'), ('recognized', 'v'), ('one', None), ('persons', 'n'), ('calculating', 'v'), ('respective', 'a'), ('size', 'n'), ('one', None), ('persons', 'n'), ('selecting', 'v'), ('first', 'a'), ('person', 'n'), ('among', None), ('one', None), ('persons', 'n'), ('based', 'v'), ('calculated', 'a'), ('sizes', 'n'), ('controlling', 'v'), ('least', 'a'), ('one', None), ('direction', 'n'), ('rotation', 'n'), ('camera', 'n'), ('angle', 'n'), ('tilt', 'n'), ('camera', 'n'), ('focal', 'a'), ('distance', 'n'), ('camera', 'n'), ('based', 'v'), ('tracked', 'a'), ('motion', 'n'), ('recognized', 'v'), ('face', 'n'), ('first', 'r'), ('person', 'n'), ('inferring', 'v'), ('topics', 'n'), ('multimodal', 'v'), ('file', 'n'), ('comprising', 'v'), ('receiving', 'v'), ('multimodal', 'n'), ('file', 'n'), ('extracting', 'v'), ('set', 'v'), ('entities', 'n'), ('multimodal', 'v'), ('file', 'n'), ('linking', 'v'), ('set', 'v'), ('entities', 'n'), ('produce', 'v'), ('set', 'v'), ('linked', 'v'), ('entities', 'n'), ('obtaining', 'v'), ('reference', 'n'), ('information', 'n'), ('set', 'v'), ('entities', 'n'), ('based', 'v'), ('least', 'a'), ('reference', 'n'), ('information', 'n'), ('generating', 'v'), ('graph', 'n'), ('set', 'v'), ('linked', 'v'), ('entities', 'n'), ('graph', 'v'), ('comprising', 'v'), ('nodes', 'n'), ('edges', 'n'), ('based', 'v'), ('least', 'a'), ('nodes', 'a'), ('edges', 'n'), ('graph', 'v'), ('determining', 'v'), ('clusters', 'n'), ('graph', 'v'), ('based', 'v'), ('least', 'a'), ('clusters', 'n'), ('graph', 'v'), ('identifying', 'v'), ('topic', 'n'), ('candidates', 'n'), ('extracting', 'v'), ('features', 'n'), ('clusters', 'n'), ('graph', 'v'), ('based', 'v'), ('least', 'a'), ('extracted', 'a'), ('features', 'n'), ('selecting', 'v'), ('least', 'a'), ('one', None), ('topicid', 'n'), ('among', None), ('topic', 'a'), ('candidates', 'n'), ('represent', 'v'), ('least', 'a'), ('one', None), ('cluster', 'n'), ('indexing', 'v'), ('multimodal', 'n'), ('file', 'r'), ('least', 'a'), ('one', None), ('topicid', 'n'), ('wherein', 'n'), ('multimodal', 'n'), ('file', 'n'), ('comprises', 'v'), ('video', 'a'), ('portion', 'n'), ('audio', 'n'), ('portion', 'n'), ('wherein', None), ('extracting', 'v'), ('set', 'n'), ('entities', 'n'), ('multimodal', 'v'), ('file', 'n'), ('comprises', 'n'), ('detecting', 'v'), ('objects', 'n'), ('video', 'a'), ('portion', 'n'), ('multimodal', 'n'), ('file', 'n'), ('detecting', 'v'), ('text', 'a'), ('audio', 'a'), ('portion', 'n'), ('multimodal', 'n'), ('file', 'n'), ('wherein', 'n'), ('detecting', 'v'), ('objects', 'n'), ('comprises', 'v'), ('performing', 'v'), ('face', 'n'), ('wherein', 'n'), ('detecting', 'v'), ('text', 'a'), ('comprises', 'n'), ('performing', 'v'), ('speech', 'n'), ('text', 'n'), ('process', 'n'), ('comprising', 'v'), ('identifying', 'v'), ('language', 'n'), ('used', 'v'), ('audio', 'a'), ('portion', 'n'), ('multimodal', 'n'), ('file', 'n'), ('wherein', 'n'), ('performing', 'v'), ('speech', 'a'), ('text', 'n'), ('process', 'n'), ('comprises', 'v'), ('performing', 'v'), ('speech', 'n'), ('text', 'n'), ('process', 'n'), ('identified', 'v'), ('language', 'n'), ('comprising', 'v'), ('translating', 'v'), ('detected', 'v'), ('text', 'a'), ('comprising', 'v'), ('determining', 'v'), ('significant', 'a'), ('clusters', 'n'), ('insignificant', 'a'), ('clusters', 'n'), ('determined', 'v'), ('clusters', 'n'), ('wherein', 'v'), ('extracting', 'v'), ('features', 'n'), ('clusters', 'n'), ('graph', 'v'), ('comprises', 'n'), ('extracting', 'v'), ('features', 'n'), ('significant', 'a'), ('clusters', 'n'), ('graph', 'v'), ('wherein', 'r'), ('extracting', 'v'), ('features', 'n'), ('clusters', 'n'), ('graph', 'v'), ('comprises', 'n'), ('least', 'v'), ('one', None), ('process', 'n'), ('selected', 'v'), ('list', 'n'), ('consisting', 'v'), ('determining', 'v'), ('graph', 'a'), ('diameter', 'n'), ('determining', 'v'), ('jaccard', 'a'), ('coefficient', 'n'), ('wherein', 'n'), ('selecting', 'v'), ('least', 'a'), ('one', None), ('topicid', 'a'), ('represent', 'n'), ('least', 'a'), ('one', None), ('cluster', 'n'), ('comprises', 'n'), ('based', 'v'), ('least', 'r'), ('extracted', 'a'), ('features', 'n'), ('mapping', 'v'), ('topic', 'n'), ('candidates', 'n'), ('probability', 'n'), ('interval', 'v'), ('based', 'v'), ('least', 'a'), ('mapping', 'v'), ('ranking', 'v'), ('topic', 'n'), ('candidates', 'n'), ('within', None), ('least', 'a'), ('one', None), ('cluster', 'n'), ('selecting', 'v'), ('least', 'a'), ('one', None), ('topicid', 'n'), ('based', 'v'), ('least', 'a'), ('ranking', 'a'), ('comprising', 'v'), ('translating', 'v'), ('least', 'a'), ('one', None), ('topicid', 'n'), ('wherein', 'n'), ('indexing', 'v'), ('multimodal', 'n'), ('file', 'r'), ('least', 'a'), ('one', None), ('topicid', 'n'), ('comprises', 'v'), ('indexing', 'v'), ('multimodal', 'n'), ('file', 'r'), ('least', 'a'), ('one', None), ('translated', 'v'), ('topicid', 'n'), ('system', 'n'), ('inferring', 'v'), ('topics', 'n'), ('multimodal', 'v'), ('file', 'n'), ('system', 'n'), ('comprising', 'v'), ('entity', 'n'), ('extraction', 'n'), ('component', 'n'), ('comprising', 'v'), ('object', 'a'), ('detection', 'n'), ('component', 'n'), ('speech', 'n'), ('text', 'a'), ('component', 'n'), ('operative', 'a'), ('extract', 'n'), ('set', 'v'), ('entities', 'n'), ('multimodal', 'v'), ('file', 'n'), ('comprising', 'v'), ('video', 'a'), ('portion', 'n'), ('audio', 'n'), ('portion', 'n'), ('entity', 'n'), ('linking', 'v'), ('component', 'a'), ('operative', 'a'), ('link', 'n'), ('extracted', 'v'), ('set', 'a'), ('entities', 'n'), ('produce', 'v'), ('set', 'v'), ('linked', 'v'), ('entities', 'n'), ('information', 'n'), ('retrieval', 'n'), ('component', 'n'), ('operative', 'a'), ('obtain', 'v'), ('reference', 'n'), ('information', 'n'), ('extracted', 'v'), ('set', 'n'), ('entities', 'n'), ('graphing', 'v'), ('analysis', 'n'), ('component', 'n'), ('operative', 'a'), ('generate', 'n'), ('graph', 'n'), ('set', 'v'), ('linked', 'v'), ('entities', 'n'), ('graph', 'v'), ('comprising', 'v'), ('nodes', 'n'), ('edges', 'n'), ('based', 'v'), ('least', 'a'), ('nodes', 'a'), ('edges', 'n'), ('graph', 'v'), ('determine', 'a'), ('clusters', 'n'), ('graph', 'v'), ('based', 'v'), ('least', 'a'), ('clusters', 'n'), ('graph', 'v'), ('identify', 'v'), ('topic', 'n'), ('candidates', 'n'), ('extract', 'a'), ('features', 'n'), ('clusters', 'n'), ('graph', 'v'), ('topicid', 'a'), ('selection', 'n'), ('component', 'n'), ('operative', 'a'), ('rank', 'n'), ('topic', 'n'), ('candidates', 'n'), ('within', None), ('least', 'a'), ('one', None), ('cluster', 'n'), ('based', 'v'), ('least', 'a'), ('ranking', 'a'), ('select', 'a'), ('least', 'a'), ('one', None), ('topicid', 'n'), ('among', None), ('topic', 'a'), ('candidates', 'n'), ('represent', 'v'), ('least', 'a'), ('one', None), ('cluster', 'n'), ('video', 'n'), ('indexer', 'v'), ('operative', 'a'), ('index', 'n'), ('multimodal', 'n'), ('file', None), ('least', 'a'), ('one', None), ('topicid', 'n'), ('system', 'n'), ('wherein', 'a'), ('object', 'a'), ('detection', 'n'), ('component', 'n'), ('operative', 'a'), ('perform', 'n'), ('face', 'n'), ('system', 'n'), ('wherein', 'a'), ('speech', 'n'), ('text', 'n'), ('component', 'n'), ('operative', 'a'), ('extract', 'a'), ('entity', 'n'), ('information', 'n'), ('least', 'a'), ('two', None), ('different', 'a'), ('languages', 'n'), ('one', None), ('computer', 'n'), ('storage', 'n'), ('devices', 'n'), ('computer-executable', 'a'), ('instructions', 'n'), ('stored', 'v'), ('thereon', 'n'), ('inferring', 'v'), ('topics', 'n'), ('multimodal', 'a'), ('file', 'n'), ('execution', 'n'), ('computer', 'n'), ('cause', 'v'), ('computer', 'n'), ('perform', 'n'), ('operations', 'n'), ('comprising', 'v'), ('receiving', 'v'), ('multimodal', 'n'), ('file', 'n'), ('comprising', 'v'), ('video', 'a'), ('portion', 'n'), ('audio', 'n'), ('portion', 'n'), ('extracting', 'v'), ('set', 'n'), ('entities', 'n'), ('multimodal', 'v'), ('file', 'n'), ('wherein', 'n'), ('extracting', 'v'), ('set', 'v'), ('entities', 'n'), ('multimodal', 'v'), ('file', 'n'), ('comprises', 'n'), ('detecting', 'v'), ('objects', 'n'), ('video', 'a'), ('portion', 'n'), ('multimodal', 'n'), ('file', 'n'), ('face', 'n'), ('detecting', 'v'), ('text', 'a'), ('audio', 'a'), ('portion', 'n'), ('multimodal', 'n'), ('file', 'n'), ('speech', 'n'), ('text', 'n'), ('process', 'n'), ('disambiguating', 'v'), ('among', None), ('set', 'v'), ('detected', 'v'), ('entity', 'n'), ('names', 'n'), ('linking', 'v'), ('set', 'n'), ('entities', 'n'), ('produce', 'v'), ('set', 'v'), ('linked', 'v'), ('entities', 'n'), ('obtaining', 'v'), ('reference', 'n'), ('information', 'n'), ('set', 'v'), ('entities', 'n'), ('based', 'v'), ('least', 'a'), ('reference', 'n'), ('information', 'n'), ('generating', 'v'), ('graph', 'n'), ('set', 'v'), ('linked', 'v'), ('entities', 'n'), ('graph', 'v'), ('comprising', 'v'), ('nodes', 'n'), ('edges', 'n'), ('based', 'v'), ('least', 'a'), ('nodes', 'a'), ('edges', 'n'), ('graph', 'v'), ('determining', 'v'), ('clusters', 'n'), ('graph', 'v'), ('determining', 'v'), ('significant', 'a'), ('clusters', 'n'), ('insignificant', 'a'), ('clusters', 'n'), ('determined', 'v'), ('clusters', 'n'), ('based', 'v'), ('least', 'a'), ('significant', 'a'), ('clusters', 'n'), ('graph', 'v'), ('identifying', 'v'), ('topic', 'n'), ('candidates', 'n'), ('extracting', 'v'), ('features', 'n'), ('significant', 'a'), ('clusters', 'n'), ('graph', 'v'), ('based', 'v'), ('least', 'a'), ('extracted', 'a'), ('features', 'n'), ('mapping', 'v'), ('topic', 'n'), ('candidates', 'n'), ('probability', 'n'), ('interval', 'v'), ('based', 'v'), ('least', 'a'), ('mapping', 'v'), ('ranking', 'v'), ('topic', 'n'), ('candidates', 'n'), ('within', None), ('least', 'a'), ('one', None), ('significant', 'a'), ('cluster', 'n'), ('based', 'v'), ('ranking', 'v'), ('selecting', 'v'), ('least', 'a'), ('one', None), ('topicid', 'n'), ('among', None), ('topic', 'a'), ('candidates', 'n'), ('represent', 'v'), ('least', 'a'), ('one', None), ('significant', 'a'), ('cluster', 'n'), ('indexing', 'v'), ('multimodal', 'n'), ('file', 'r'), ('least', 'a'), ('one', None), ('topicid', 'n'), ('one', None), ('computer', 'n'), ('storage', 'n'), ('devices', 'n'), ('wherein', 'v'), ('operations', 'n'), ('comprise', 'v'), ('identifying', 'v'), ('language', 'n'), ('used', 'v'), ('audio', 'a'), ('portion', 'n'), ('multimodal', 'n'), ('file', 'n'), ('detecting', 'v'), ('text', 'a'), ('audio', 'a'), ('portion', 'n'), ('multimodal', 'n'), ('file', 'n'), ('speech', 'n'), ('text', 'n'), ('process', 'n'), ('comprises', 'v'), ('performing', 'v'), ('speech', 'n'), ('text', 'n'), ('process', 'n'), ('identified', 'v'), ('language权利要求', 'a'), ('system', 'n'), ('alerting', 'v'), ('vision', 'n'), ('impairment', 'n'), ('said', 'v'), ('system', 'n'), ('comprising', 'v'), ('processing', 'v'), ('unit', 'n'), ('configured', 'v'), ('operable', 'a'), ('receiving', 'v'), ('scene', 'n'), ('data', 'n'), ('indicative', 'a'), ('scene', 'n'), ('least', 'a'), ('one', None), ('consumer', 'n'), ('environment', 'n'), ('identifying', 'v'), ('scene', 'n'), ('data', 'n'), ('certain', 'a'), ('consumer', 'n'), ('identifying', 'v'), ('event', 'n'), ('indicative', 'a'), ('behavioral', 'a'), ('compensation', 'n'), ('vision', 'n'), ('impairment', 'n'), ('upon', None), ('identification', 'n'), ('event', 'n'), ('sending', 'v'), ('notification', 'n'), ('relating', 'v'), ('vision', 'n'), ('impairment', 'n'), ('system', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('sensing', 'v'), ('unit', 'n'), ('configured', 'v'), ('operable', 'a'), ('detecting', 'v'), ('scene', 'n'), ('data', 'n'), ('system', 'n'), ('wherein', 'n'), ('said', 'v'), ('least', 'a'), ('one', None), ('sensing', 'v'), ('unit', 'n'), ('comprises', 'v'), ('least', 'a'), ('one', None), ('least', 'a'), ('one', None), ('imaging', 'v'), ('unit', 'n'), ('configured', 'v'), ('operable', 'a'), ('capturing', 'v'), ('least', 'a'), ('one', None), ('least', 'a'), ('portion', 'n'), ('consumer', 'n'), (\"'s\", None), ('body', 'n'), ('least', 'v'), ('one', None), ('motion', 'n'), ('detector', 'n'), ('configured', 'v'), ('operable', 'a'), ('detecting', 'v'), ('consumer', 'n'), ('data', 'n'), ('indicative', 'a'), ('motion', 'n'), ('consumer', 'n'), ('least', 'a'), ('one', None), ('eye', 'n'), ('tracker', 'n'), ('configured', 'v'), ('operable', 'a'), ('eye', 'n'), ('motion', 'n'), ('consumer', 'n'), ('system', 'n'), ('wherein', 'v'), ('least', 'a'), ('one', None), ('imaging', 'v'), ('unit', 'n'), ('comprises', 'v'), ('cameras', 'n'), ('placed', 'v'), ('different', 'a'), ('heights', 'n'), ('system', 'n'), ('one', None), ('wherein', 'n'), ('said', 'v'), ('sensing', 'v'), ('unit', 'n'), ('accommodated', 'v'), ('optical', 'a'), ('digital', 'a'), ('eyewear', 'n'), ('frame', 'n'), ('display', 'n'), ('system', 'n'), ('one', None), ('wherein', 'n'), ('said', 'v'), ('processing', 'n'), ('unit', 'n'), ('configured', 'v'), ('operable', 'a'), ('identifying', 'v'), ('consumer', 'n'), (\"'s\", None), ('condition', 'n'), ('said', 'v'), ('consumer', 'n'), (\"'s\", None), ('condition', 'n'), ('comprising', 'v'), ('consumer', 'n'), ('data', 'n'), ('indicative', 'a'), ('consumer', 'n'), (\"'s\", None), ('position', 'n'), ('location', 'n'), ('relative', 'a'), ('least', 'a'), ('one', None), ('object', 'a'), ('consumer', 'n'), (\"'s\", None), ('environment', 'n'), ('said', 'v'), ('consumer', 'n'), ('data', 'n'), ('comprises', 'n'), ('least', 'v'), ('one', None), ('consumer', 'n'), (\"'s\", None), ('face', 'n'), ('eyewear', 'a'), ('posture', 'n'), ('position', 'n'), ('sound', 'n'), ('motion', 'n'), ('system', 'n'), ('one', None), ('wherein', 'n'), ('said', 'v'), ('event', 'n'), ('comprises', 'n'), ('least', 'v'), ('one', None), ('position', 'n'), ('head', 'n'), ('increase', 'n'), ('decrease', 'n'), ('viewing', 'v'), ('distance', 'n'), ('consumer', 'n'), ('viewed', 'v'), ('object', 'a'), ('changing', 'v'), ('position', 'n'), ('eyeglasses', 'n'), ('worn', 'a'), ('consumer', 'n'), ('system', 'n'), ('one', None), ('wherein', 'n'), ('said', 'v'), ('event', 'n'), ('identified', 'v'), ('identifying', 'a'), ('feature', 'n'), ('indicative', 'a'), ('behavioral', 'a'), ('compensation', 'n'), ('performing', 'v'), ('bruckner', 'a'), ('test', 'n'), ('performing', 'v'), ('hirschberg', 'a'), ('test', 'n'), ('measuring', 'v'), ('blink', 'n'), ('count', 'n'), ('frequency', 'n'), ('system', 'n'), ('wherein', 'a'), ('feature', 'n'), ('indicative', 'a'), ('behavioral', 'a'), ('compensation', 'n'), ('comprises', 'n'), ('squinting', 'v'), ('head', 'n'), ('certain', 'a'), ('distances', 'n'), ('object', 'v'), ('consumer', 'n'), (\"'s\", None), ('eyes', 'n'), ('certain', 'a'), ('position', 'n'), ('eyeglasses', 'v'), ('consumer', 'n'), (\"'s\", None), ('face', 'n'), ('strabismus', 'n'), ('cataracts', 'v'), ('reflections', 'n'), ('eye', 'n'), ('system', 'n'), ('one', None), ('wherein', 'n'), ('notification', 'n'), ('includes', 'v'), ('least', 'a'), ('one', None), ('data', 'n'), ('indicative', 'n'), ('identified', 'v'), ('event', 'n'), ('data', 'n'), ('indicative', 'a'), ('identified', 'a'), ('consumer', 'n'), ('ophthalmologic', 'n'), ('recommendations', 'n'), ('based', 'v'), ('identified', 'a'), ('event', 'n'), ('lack', 'n'), ('events', 'n'), ('appointment', 'a'), ('vision', 'n'), ('test', 'n'), ('system', 'n'), ('one', None), ('wherein', 'n'), ('said', 'v'), ('processing', 'n'), ('unit', 'n'), ('comprises', 'v'), ('memory', 'n'), ('storing', 'v'), ('least', 'a'), ('one', None), ('reference', 'n'), ('data', 'n'), ('indicative', 'a'), ('behavioral', 'a'), ('compensation', 'n'), ('vision', 'n'), ('impairment', 'n'), ('data', 'n'), ('indicative', 'a'), ('notification', 'n'), ('data', 'n'), ('indicative', 'a'), ('follow-up', 'a'), ('notification', 'n'), ('system', 'n'), ('wherein', 'n'), ('said', 'v'), ('processing', 'n'), ('unit', 'n'), ('configured', 'v'), ('least', 'a'), ('one', None), ('identifying', 'v'), ('event', 'n'), ('upon', None), ('comparison', 'n'), ('detected', 'v'), ('data', 'n'), ('reference', 'n'), ('data', 'n'), ('determining', 'v'), ('probability', 'n'), ('vision', 'n'), ('impairment', 'a'), ('consumer', 'n'), ('based', 'v'), ('comparison', 'n'), ('system', 'n'), ('one', None), ('wherein', 'n'), ('said', 'v'), ('processing', 'n'), ('unit', 'n'), ('comprises', 'v'), ('communication', 'n'), ('interface', 'n'), ('configured', 'v'), ('sending', 'v'), ('notification', 'n'), ('least', 'a'), ('one', None), ('identified', 'a'), ('consumer', 'n'), ('party', 'n'), ('system', 'n'), ('one', None), ('wherein', 'n'), ('said', 'v'), ('processing', 'n'), ('unit', 'n'), ('configured', 'v'), ('providing', 'v'), ('frame', 'n'), ('recommendation', 'n'), ('system', 'n'), ('one', None), ('wherein', 'n'), ('said', 'v'), ('memory', 'n'), ('configured', 'v'), ('storing', 'a'), ('database', 'n'), ('including', 'v'), ('multiplicity', 'n'), ('data', 'n'), ('sets', 'n'), ('related', 'a'), ('spectacle', 'n'), ('frame', 'n'), ('models', 'n'), ('sizes', 'v'), ('system', 'n'), ('according', 'v'), ('wherein', 'n'), ('said', 'v'), ('processing', 'v'), ('unit', 'n'), ('configured', 'v'), ('operable', 'a'), ('correlate', 'n'), ('frames', 'n'), ('parameters', 'n'), ('ophthalmic', 'v'), ('prescriptions', 'n'), ('system', 'n'), ('according', 'v'), ('wherein', 'n'), ('said', 'v'), ('processing', 'v'), ('unit', 'n'), ('configured', 'v'), ('operable', 'a'), ('correlate', 'n'), ('frames', 'n'), ('parameters', 'n'), ('facial', 'a'), ('features', 'n'), ('system', 'n'), ('according', 'v'), ('wherein', 'n'), ('said', 'v'), ('processing', 'v'), ('unit', 'n'), ('configured', 'v'), ('operable', 'a'), ('correlate', 'n'), ('frames', 'n'), ('parameters', 'n'), ('eyewear', 'v'), ('preferences', 'n'), ('system', 'n'), ('according', 'v'), ('comprising', 'v'), ('server', 'n'), ('least', 'a'), ('one', None), ('computer', 'n'), ('entity', 'n'), ('linked', 'v'), ('server', 'r'), ('via', None), ('network', 'n'), ('wherein', 'n'), ('said', 'v'), ('network', 'n'), ('configured', 'v'), ('receive', 'a'), ('respond', 'n'), ('requests', 'n'), ('sent', 'v'), ('across', None), ('network', 'n'), ('transmitting', 'v'), ('one', None), ('modules', 'n'), ('computer', 'n'), ('executable', 'a'), ('program', 'n'), ('instructions', 'n'), ('displayable', 'a'), ('data', 'n'), ('network', 'n'), ('connected', 'v'), ('computer', 'n'), ('platform', 'n'), ('response', 'n'), ('request', 'n'), ('wherein', 'n'), ('said', 'v'), ('modules', 'n'), ('include', 'v'), ('modules', 'n'), ('configured', 'v'), ('receive', 'a'), ('transmit', 'n'), ('information', 'n'), ('transmitting', 'v'), ('frame', 'n'), ('recommendation', 'n'), ('optical', 'a'), ('lens', 'v'), ('option', 'n'), ('recommendation', 'n'), ('based', 'v'), ('received', 'v'), ('information', 'n'), ('display', 'n'), ('network', 'n'), ('connected', 'v'), ('computer', 'n'), ('platform', 'n'), ('computer', 'n'), ('program', 'n'), ('instructions', 'n'), ('stored', 'v'), ('local', 'a'), ('storage', 'n'), ('executed', 'v'), ('processing', 'v'), ('unit', 'n'), ('cause', 'n'), ('processing', 'v'), ('unit', 'n'), ('receive', 'a'), ('data', 'n'), ('indicative', 'a'), ('scene', 'n'), ('least', 'a'), ('one', None), ('consumer', 'n'), ('environment', 'n'), ('identify', 'v'), ('data', 'n'), ('certain', 'a'), ('consumer', 'n'), ('identify', 'v'), ('event', 'n'), ('indicative', 'a'), ('behavioral', 'a'), ('compensation', 'n'), ('vision', 'n'), ('impairment', 'n'), ('upon', None), ('identification', 'n'), ('event', 'n'), ('send', 'v'), ('notification', 'n'), ('relating', 'v'), ('vision', 'n'), ('impairment', 'a'), ('computer', 'n'), ('program', 'n'), ('product', 'n'), ('stored', 'v'), ('tangible', 'a'), ('computer', 'n'), ('readable', 'a'), ('medium', 'n'), ('comprising', 'v'), ('library', 'a'), ('software', 'n'), ('modules', 'n'), ('cause', 'v'), ('computer', 'n'), ('executing', 'v'), ('prompt', 'a'), ('information', 'n'), ('pertinent', 'n'), ('least', 'a'), ('one', None), ('eyeglasses', 'v'), ('recommendation', 'n'), ('optical', 'a'), ('lens', 'v'), ('option', 'n'), ('recommendation', 'n'), ('store', 'n'), ('said', 'v'), ('information', 'n'), ('display', 'n'), ('eyewear', 'v'), ('recommendations', 'n'), ('computer', 'n'), ('program', 'n'), ('product', 'n'), ('wherein', 'n'), ('said', 'v'), ('library', 'a'), ('comprises', 'n'), ('module', 'n'), ('frame', 'n'), ('selection', 'n'), ('point', 'n'), ('sales', 'n'), ('advertising', 'v'), ('computer', 'n'), ('platform', 'n'), ('facilitating', 'v'), ('eye', 'n'), ('glasses', 'n'), ('marketing', 'v'), ('selection', 'n'), ('comprising', 'v'), ('camera', 'n'), ('configured', 'v'), ('execute', 'a'), ('computer', 'n'), ('program', 'n'), ('instructions', 'n'), ('cause', 'v'), ('take', 'v'), ('consumer', 'n'), ('identify', 'n'), ('certain', 'a'), ('consumer', 'n'), ('identify', 'v'), ('event', 'n'), ('indicative', 'a'), ('behavioral', 'a'), ('compensation', 'n'), ('vision', 'n'), ('impairment', 'n'), ('upon', None), ('identification', 'n'), ('event', 'n'), ('sending', 'v'), ('notification', 'n'), ('relating', 'v'), ('vision', 'n'), ('impairment', 'a'), ('local', 'a'), ('storage', 'n'), ('executable', 'a'), ('instructions', 'n'), ('carrying', 'v'), ('storage', 'n'), ('information', 'n'), ('alerting', 'v'), ('vision', 'n'), ('impairment', 'n'), ('said', 'v'), ('comprising', 'v'), ('identifying', 'v'), ('certain', 'a'), ('individual', 'a'), ('scene', 'n'), ('data', 'n'), ('indicative', 'a'), ('scene', 'n'), ('least', 'a'), ('one', None), ('consumer', 'n'), ('environment', 'n'), ('identifying', 'v'), ('event', 'n'), ('indicative', 'a'), ('behavioral', 'a'), ('compensation', 'n'), ('vision', 'n'), ('impairment', 'n'), ('upon', None), ('identification', 'n'), ('event', 'n'), ('sending', 'v'), ('notification', 'n'), ('vision', 'n'), ('impairment', 'n'), ('comprising', 'v'), ('detecting', 'v'), ('data', 'n'), ('indicative', 'a'), ('scene', 'n'), ('least', 'a'), ('one', None), ('consumer', 'n'), ('retail', 'a'), ('environment', 'n'), ('wherein', 'n'), ('detecting', 'v'), ('data', 'n'), ('indicative', 'a'), ('least', 'a'), ('one', None), ('consumer', 'n'), ('comprises', 'v'), ('least', 'a'), ('one', None), ('capturing', 'v'), ('least', 'a'), ('one', None), ('least', 'a'), ('one', None), ('consumer', 'n'), ('detecting', 'v'), ('data', 'n'), ('indicative', 'a'), ('motion', 'n'), ('consumer', 'n'), ('eye', 'n'), ('motion', 'n'), ('consumer', 'n'), ('wherein', None), ('capturing', 'v'), ('least', 'a'), ('one', None), ('least', 'a'), ('one', None), ('consumer', 'n'), ('comprises', 'v'), ('continuously', 'r'), ('recording', 'v'), ('scene', 'n'), ('one', None), ('comprising', 'n'), ('identifying', 'v'), ('data', 'n'), ('consumer', 'n'), (\"'\", None), ('condition', 'n'), ('including', 'v'), ('data', 'n'), ('indicative', 'a'), ('consumer', 'n'), (\"'s\", None), ('position', 'n'), ('location', 'n'), ('relative', 'a'), ('consumer', 'n'), (\"'s\", None), ('environment', 'n'), ('said', 'v'), ('data', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('consumer', 'n'), (\"'s\", None), ('face', 'n'), ('posture', 'n'), ('position', 'n'), ('sound', 'n'), ('motion', 'n'), ('one', None), ('wherein', 'n'), ('said', 'v'), ('event', 'n'), ('comprises', 'n'), ('least', 'v'), ('one', None), ('position', 'n'), ('head', 'n'), ('increase', 'n'), ('decrease', 'n'), ('viewing', 'v'), ('distance', 'n'), ('consumer', 'n'), ('viewed', 'v'), ('object', 'a'), ('changing', 'v'), ('position', 'n'), ('eyeglasses', 'n'), ('worn', 'a'), ('consumer', 'n'), ('one', None), ('wherein', 'n'), ('identifying', 'v'), ('event', 'n'), ('comprises', 'n'), ('identifying', 'v'), ('feature', 'n'), ('indicative', 'a'), ('behavioral', 'a'), ('compensation', 'n'), ('performing', 'v'), ('bruckner', 'a'), ('test', 'n'), ('performing', 'v'), ('hirschberg', 'a'), ('test', 'n'), ('measuring', 'v'), ('blink', 'n'), ('countfrequency', 'n'), ('wherein', 'a'), ('feature', 'n'), ('indicative', 'a'), ('behavioral', 'a'), ('compensation', 'n'), ('comprises', 'n'), ('squinting', 'v'), ('head', 'n'), ('certain', 'a'), ('distances', 'n'), ('object', 'v'), ('consumer', 'n'), (\"'s\", None), ('eyes', 'n'), ('certain', 'a'), ('position', 'n'), ('eyeglasses', 'v'), ('consumer', 'n'), (\"'s\", None), ('face', 'n'), ('strabismus', 'n'), ('cataracts', 'v'), ('reflections', 'n'), ('eye', 'n'), ('one', None), ('wherein', 'n'), ('identifying', 'v'), ('least', 'a'), ('one', None), ('consumer', 'n'), ('retail', 'a'), ('environment', 'n'), ('comprising', 'v'), ('least', 'a'), ('one', None), ('receiving', 'v'), ('data', 'n'), ('characterizing', 'v'), ('retail', 'a'), ('environment', 'n'), ('performing', 'v'), ('face', 'n'), ('one', None), ('wherein', 'n'), ('sending', 'v'), ('notification', 'n'), ('comprising', 'v'), ('sending', 'v'), ('notification', 'n'), ('least', 'a'), ('one', None), ('identified', 'a'), ('consumer', 'n'), ('third', 'a'), ('party', 'n'), ('one', None), ('wherein', 'n'), ('notification', 'n'), ('includes', 'v'), ('least', 'a'), ('one', None), ('data', 'n'), ('indicative', 'n'), ('identified', 'v'), ('event', 'n'), ('data', 'n'), ('indicative', 'a'), ('identified', 'a'), ('consumer', 'n'), ('ophthalmologic', 'n'), ('recommendations', 'n'), ('based', 'v'), ('identified', 'a'), ('event', 'n'), ('lack', 'n'), ('events', 'n'), ('appointment', 'a'), ('vision', 'n'), ('test', 'n'), ('one', None), ('comprising', 'v'), ('storing', 'v'), ('least', 'a'), ('one', None), ('reference', 'n'), ('data', 'n'), ('indicative', 'a'), ('behavioral', 'a'), ('compensation', 'n'), ('vision', 'n'), ('impairment', 'n'), ('data', 'n'), ('indicative', 'a'), ('notification', 'n'), ('data', 'n'), ('indicative', 'a'), ('follow-up', 'a'), ('notification', 'n'), ('comprising', 'v'), ('identifying', 'v'), ('event', 'n'), ('upon', None), ('comparison', 'n'), ('detected', 'v'), ('data', 'n'), ('reference', 'n'), ('data', 'n'), ('determining', 'v'), ('probability', 'n'), ('vision', 'n'), ('impairment', 'a'), ('consumer', 'n'), ('based', 'v'), ('comparison', 'a'), ('computer', 'n'), ('program', 'n'), ('intended', 'v'), ('stored', 'a'), ('memory', 'n'), ('unit', 'n'), ('computer', 'n'), ('system', 'n'), ('removable', 'a'), ('memory', 'n'), ('medium', 'n'), ('adapted', 'v'), ('cooperate', 'a'), ('reader', 'n'), ('unit', 'n'), ('comprising', 'v'), ('instructions', 'n'), ('implementing', 'v'), ('according', 'v')]\n"
     ]
    }
   ],
   "source": [
    "def simpler_pos_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return \"a\"\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return \"v\"\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return \"n\"\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return \"r\"\n",
    "    else:         \n",
    "        return None\n",
    "    \n",
    "simpler_POS_text_a = []\n",
    "\n",
    "# for each tuple of the list, we create a new tuple: the first element is the token, the second is\n",
    "# the simplified pos tag, obtained calling the function simpler_pos_tag()\n",
    "# then we append the new created tuple to a new list, which will be the output\n",
    "for tuple in cleaned_POS_text_a:\n",
    "    if tuple[1] == 'NNP':   #this is because there is some text in japanese categorized as 'NNP';\n",
    "                            #no other relevant words are categorized in such a way\n",
    "        continue;\n",
    "    POS_tuple = (tuple[0], simpler_pos_tag(tuple[1]))\n",
    "    simpler_POS_text_a.append(POS_tuple)\n",
    "    \n",
    "print(simpler_POS_text_a)\n",
    "\n",
    "simpler_POS_text_c = []\n",
    "\n",
    "for tuple in cleaned_POS_text_c:\n",
    "    if tuple[1] == 'NNP':   #this is because there is some text in japanese categorized as 'NNP';\n",
    "                            #no other relevant words are categorized in such a way\n",
    "        continue;\n",
    "    POS_tuple = (tuple[0], simpler_pos_tag(tuple[1]))\n",
    "    simpler_POS_text_c.append(POS_tuple)\n",
    "    \n",
    "print(simpler_POS_text_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0c67ce",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "In this step we lemmatize the pos text, so we obtain the final two vectors with all the lemmas we need for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ed471ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c56f1de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['electronic', 'apparatus', 'include', 'capture', 'storage', 'operation', 'method', 'thereof', 'provide', 'capture', 'capture', 'storage', 'record', 'module', 'couple', 'capture', 'storage', 'configure', 'configure', 'capture', 'capture', 'head', 'perform', 'obtain', 'detect', 'facial', 'landmark', 'within', 'estimate', 'head', 'posture', 'angle', 'accord', 'facial', 'landmark', 'calculate', 'gaze', 'position', 'gaze', 'screen', 'accord', 'head', 'posture', 'angle', 'rotation', 'reference', 'angle', 'predetermine', 'calibration', 'position', 'configure', 'screen', 'display', 'correspond', 'visual', 'effect', 'accord', 'gaze', 'positionthe', 'present', 'disclosure', 'provide', 'product', 'thereof', 'adopts', 'fusion', 'method', 'perform', 'machine', 'learn', 'computation', 'technical', 'effect', 'present', 'disclosure', 'include', 'few', 'computation', 'less', 'power', 'consumptiona', 'method', 'detect', 'body', 'information', 'passenger', 'vehicle', 'base', 'human', \"'\", 'status', 'provide', 'method', 'include', 'step', 'passenger', 'body', 'information-detecting', 'inputting', 'interior', 'vehicle', 'face', 'network', 'detect', 'face', 'passenger', 'output', 'passenger', 'feature', 'information', 'inputting', 'interior', 'body', 'network', 'detect', 'body', 'output', 'body-part', 'length', 'information', 'b', 'retrieve', 'specific', 'height', 'mapping', 'information', 'refer', 'height', 'map', 'table', 'ratio', 'segment', 'body', 'portion', 'human', 'group', 'height', 'per', 'human', 'group', 'acquire', 'specific', 'height', 'specific', 'passenger', 'retrieve', 'specific', 'weight', 'map', 'information', 'weight', 'mapping', 'table', 'correlation', 'height', 'weight', 'per', 'human', 'group', 'acquire', 'weight', 'specific', 'passenger', 'refer', 'specific', 'heighttechniques', 'relate', 'improved', 'video', 'cod', 'base', 'face', 'detection', 'region', 'extraction', 'track', 'discuss', 'technique', 'may', 'include', 'perform', 'facial', 'search', 'video', 'frame', 'determine', 'candidate', 'video', 'frame', 'test', 'candidate', 'base', 'skin', 'tone', 'information', 'determine', 'valid', 'invalid', 'rejecting', 'invalid', 'encode', 'video', 'frame', 'base', 'valid', 'generate', 'cod', 'bitstreama', 'method', 'manage', 'smart', 'database', 'store', 'facial', 'face', 'provide', 'method', 'include', 'step', 'manage', 'count', 'specific', 'facial', 'corresponding', 'specific', 'person', 'smart', 'database', 'new', 'facial', 'continuously', 'store', 'determine', 'whether', 'first', 'count', 'value', 'represent', 'count', 'specific', 'facial', 'satisfies', 'first', 'set', 'value', 'b', 'first', 'count', 'value', 'satisfies', 'first', 'set', 'value', 'inputting', 'specific', 'facial', 'neural', 'aggregation', 'network', 'generate', 'quality', 'score', 'specific', 'facial', 'aggregation', 'specific', 'facial', 'second', 'count', 'value', 'represent', 'count', 'specific', 'quality', 'score', 'among', 'quality', 'score', 'high', 'count', 'thereof', 'satisfies', 'second', 'set', 'value', 'delete', 'part', 'specific', 'facial', 'corresponding', 'uncounted', 'quality', 'score', 'smart', 'databasea', 'capable', 'determine', 'algorithm', 'apply', 'region', 'interest', 'within', 'digital', 'representation', 'present', 'preprocessing', 'module', 'utilizes', 'one', 'feature', 'identification', 'algorithm', 'determine', 'region', 'interest', 'base', 'feature', 'density', 'preprocessing', 'module', 'leverage', 'feature', 'density', 'signature', 'region', 'determine', 'diverse', 'module', 'operate', 'region', 'interest', 'specific', 'embodiment', 'focus', 'structure', 'document', 'also', 'present', 'disclosed', 'approach', 'enhance', 'addition', 'object', 'classifier', 'classifies', 'type', 'object', 'find', 'region', 'interestdisclosed', 'mobile', 'terminal', 'mobile', 'terminal', 'may', 'include', 'front', 'camera', 'obtain', 'face', 'glance', 'sensor', 'tilt', 'certain', 'angle', 'dispose', 'adjacent', 'front', 'camera', 'obtain', 'metadata', 'face', 'controller', 'obtain', 'distance', 'glance', 'sensor', 'front', 'camera', 'distance', 'enable', 'area', 'overlap', 'region', 'first', 'region', 'represent', 'range', 'photographable', 'front', 'camera', 'overlap', 'second', 'region', 'represent', 'range', 'photographable', 'glance', 'sensor', 'maximumthis', 'disclosure', 'provide', 'method', 'apparatus', 'include', 'computer', 'program', 'encode', 'computer', 'storage', 'medium', 'intelligent', 'rout', 'notification', 'related', 'medium', 'program', 'one', 'aspect', 'smart', 'television', 'tv', 'implement', 'track', \"'s\", 'tv', 'watch', 'behavior', 'anticipate', 'programming', 'base', 'behavior', 'aspect', 'smart', 'tv', 'implement', 'detect', \"'s\", 'presence', 'base', 'detection', 'automatically', 'change', 'tv', 'channel', 'medium', 'program', 'analyze', 'desirable', 'aspect', 'smart', 'tv', 'implement', 'transmit', 'notification', 'instruction', 'electronic', 'within', 'network', 'attempt', 'alert', 'upcoming', 'medium', 'program', 'additionally', 'smart', 'tv', 'implement', 'transmit', 'detection', 'instruction', 'electronic', 'within', 'network', 'whereby', 'electronic', 'attempt', 'detect', \"'s\", 'presence', 'voice', 'configure', 'output', 'test', 'depth+multi-spectral', 'include', 'pixel', 'pixel', 'correspond', 'one', 'sensor', 'sensor', 'array', 'camera', 'include', 'least', 'depth', 'value', 'spectral', 'value', 'spectral', 'light', 'sub-band', 'spectral', 'illuminators', 'camera', 'face', 'machine', 'previously', 'train', 'set', 'labeled', 'training', 'depth+multi-spectral', 'structure', 'test', 'depth+multi-spectral', 'face', 'machine', 'configure', 'output', 'confidence', 'value', 'indicate', 'likelihood', 'test', 'depth+multi-spectral', 'include', 'faceembodiments', 'present', 'disclosure', 'relate', 'processing', 'method', 'apparatus', 'electronic', 'method', 'include', 'acquire', 'photo', 'album', 'obtain', 'face', 'cluster', 'collect', 'face', 'information', 'respective', 'photo', 'album', 'acquire', 'face', 'parameter', 'accord', 'face', 'information', 'select', 'cover', 'accord', 'face', 'parameter', 'take', 'face-region', 'cover', 'set', 'face-region', 'cover', 'photo', 'albumtechniques', 'describe', 'herein', 'provide', 'location-based', 'access', 'control', 'secure', 'resource', 'generally', 'describe', 'configuration', 'disclose', 'herein', 'enable', 'dynamically', 'modify', 'access', 'secure', 'resource', 'base', 'one', 'location-related', 'action', 'example', 'technique', 'disclose', 'herein', 'enable', 'compute', 'control', 'access', 'resource', 'compute', 'display', 'secure', 'location', 'secure', 'data', 'configuration', 'technique', 'disclose', 'herein', 'enable', 'controlled', 'access', 'secure', 'resource', 'base', 'least', 'part', 'invitation', 'associate', 'location', 'position', 'data', 'indicate', 'location', 'one', 'embodiment', 'provide', 'method', 'comprise', 'receive', 'piece', 'content', 'salient', 'moment', 'data', 'piece', 'content', 'method', 'comprises', 'base', 'salient', 'moment', 'data', 'determine', 'first', 'path', 'viewport', 'piece', 'content', 'method', 'comprise', 'display', 'viewport', 'display', 'movement', 'viewport', 'base', 'first', 'path', 'playback', 'piece', 'content', 'method', 'comprise', 'generate', 'augmentation', 'salient', 'moment', 'occur', 'piece', 'content', 'present', 'augmentation', 'viewport', 'portion', 'playback', 'augmentation', 'comprise', 'interactive', 'hint', 'guide', 'viewport', 'salient', 'momenta', 'computer-implemented', 'method', 'computer', 'program', 'product', 'provide', 'facial', 'method', 'include', 'receive', 'method', 'also', 'include', 'extracting', 'feature', 'extractor', 'utilizing', 'convolutional', 'neural', 'network', 'cnn', 'enlarge', 'intra-class', 'variance', 'long-tail', 'class', 'feature', 'vector', 'method', 'additionally', 'include', 'generating', 'feature', 'generator', 'discriminative', 'feature', 'vector', 'feature', 'vector', 'method', 'include', 'classify', 'utilize', 'fully', 'connect', 'classifier', 'identity', 'discriminative', 'feature', 'vector', 'method', 'also', 'include', 'control', 'operation', '-based', 'machine', 'react', 'accordance', 'identitysome', 'embodiment', 'invention', 'provide', 'efficient', 'expressive', 'machine-trained', 'network', 'perform', 'machine', 'learn', 'machine-trained', 'mt', 'network', 'embodiment', 'use', 'novel', 'processing', 'node', 'novel', 'activation', 'function', 'allow', 'mt', 'network', 'efficiently', 'define', 'few', 'processing', 'node', 'layer', 'complex', 'mathematical', 'expression', 'solve', 'particular', 'problem', 'eg', 'face', 'speech', 'etc', 'embodiment', 'activation', 'function', 'eg', 'cup', 'function', 'use', 'numerous', 'process', 'node', 'mt', 'network', 'machine', 'learn', 'activation', 'function', 'configure', 'differently', 'different', 'process', 'node', 'different', 'node', 'emulate', 'implement', 'two', 'different', 'function', 'eg', 'two', 'boolean', 'logical', 'operator', 'xor', 'activation', 'function', 'embodiment', 'periodic', 'function', 'configure', 'implement', 'different', 'function', 'eg', 'different', 'sinusoidal', 'functionsmethods', 'may', 'provide', 'facial', 'least', 'one', 'input', 'utilize', 'hierarchical', 'feature', 'learn', 'pair-wise', 'receptive', 'field', 'theory', 'may', 'use', 'input', 'generate', 'pre-processed', 'multi-channel', 'channel', 'pre-processed', 'may', 'activate', 'base', 'amount', 'feature', 'rich', 'detail', 'within', 'channel', 'similarly', 'local', 'patch', 'may', 'activate', 'base', 'discriminant', 'within', 'local', 'patch', 'may', 'extract', 'local', 'patch', 'discriminant', 'may', 'select', 'order', 'perform', 'feature', 'match', 'pair', 'set', 'may', 'utilize', 'patch', 'feature', 'pool', 'pair-wise', 'matching', 'large-scale', 'training', 'order', 'quickly', 'accurately', 'perform', 'facial', 'low', 'cost', 'memory', 'computationa', 'method', 'control', 'terminal', 'provide', 'terminal', 'include', 'capture', 'apparatus', 'least', 'one', 'acquire', 'capture', 'apparatus', 'motion', 'parameter', 'terminal', 'obtain', 'processing', 'acquire', 'control', 'performed', 'base', 'motion', 'parameter', 'equal', 'less', 'preset', 'parameter', 'threshold', 'skip', 'base', 'motion', 'parameter', 'great', 'preset', 'parameter', 'thresholda', 'drive-through', 'order', 'processing', 'method', 'apparatus', 'disclose', 'drive-through', 'order', 'processing', 'method', 'include', 'receive', 'customer', 'information', 'detect', 'vision', 'provide', 'product', 'information', 'base', 'customer', 'information', 'process', 'product', 'order', 'customer', 'accord', 'present', 'disclosure', 'possible', 'rapidly', 'process', 'order', 'use', 'customer', 'information', 'base', 'customer', 'use', 'artificial', 'intelligence', 'ai', 'model', 'machine', 'learn', 'g', 'networkan', 'processing', 'method', 'perform', 'compute', 'include', 'identify', 'use', 'face', 'one', 'face', 'face', 'correspond', 'respective', 'person', 'capture', 'first', 'identify', 'face', 'extract', 'set', 'profile', 'parameter', 'correspond', 'person', 'first', 'select', 'tile', 'first', 'tile', 'match', 'face', 'correspond', 'person', 'first', 'accordance', 'predefined', 'correspondence', 'set', 'profile', 'parameter', 'correspond', 'person', 'set', 'pre-stored', 'description', 'parameter', 'first', 'tile', 'generate', 'second', 'cover', 'face', 'respective', 'person', 'first', 'correspond', 'first', 'tile', 'share', 'first', 'second', 'predefined', 'order', 'via', 'group', 'chat', 'sessionin', 'one', 'embodiment', 'artificial', 'reality', 'determines', 'performance', 'metric', 'eye', 'track', 'first', 'performance', 'threshold', 'eye', 'track', 'associate', 'head-mounted', 'display', 'wear', 'artificial', 'reality', 'receives', 'first', 'inputs', 'associate', 'body', 'determine', 'region', 'look', 'within', 'field', 'view', 'head-mounted', 'display', 'base', 'receive', 'first', 'input', 'determines', 'vergence', 'distance', 'base', 'least', 'first', 'input', 'associate', 'body', 'region', 'look', 'location', 'one', 'object', 'scene', 'display', 'head-mounted', 'display', 'adjust', 'one', 'configuration', 'head-mounted', 'display', 'base', 'determined', 'vergence', 'distance', 'computer-implemented', 'method', 'provide', '-based', 'self-guided', 'object', 'detection', 'method', 'include', 'receive', 'set', 'respective', 'grid', 'thereon', 'label', 'regard', 'respective', 'object', 'detect', 'use', 'grid', 'level', 'label', 'data', 'method', 'include', 'train', 'grid-based', 'object', 'detector', 'use', 'grid', 'level', 'label', 'data', 'method', 'also', 'include', 'determine', 'respective', 'bounding', 'box', 'respective', 'object', 'apply', 'local', 'segmentation', 'method', 'additionally', 'include', 'train', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'joint', 'object', 'localization', 'object', 'use', 'respective', 'bounding', 'box', 'respective', 'object', 'input', 'rcnna', 'method', 'face', 'comprise', 'multiple', 'phase', 'implement', 'parallel', 'architecture', 'first', 'phase', 'normalization', 'phase', 'whereby', 'capture', 'normalized', 'size', 'orientation', 'illumination', 'store', 'preexisting', 'database', 'second', 'phase', 'feature', 'extractiondistance', 'matrix', 'phase', 'distance', 'matrix', 'generate', 'captured', 'coarse', 'phase', 'generate', 'distance', 'matrix', 'compare', 'distance', 'matrix', 'database', 'use', 'euclidean', 'distance', 'match', 'create', 'candidate', 'list', 'detail', 'phase', 'multiple', 'face', 'algorithm', 'applied', 'candidate', 'list', 'produce', 'final', 'result', 'distance', 'matrix', 'normalized', 'database', 'may', 'break', 'parallel', 'list', 'parallelization', 'feature', 'extractiondistance', 'matrix', 'phase', 'candidate', 'list', 'may', 'also', 'group', 'accord', 'dissimilarity', 'algorithm', 'parallel', 'processing', 'detailed', 'phasean', 'image', 'include', 'pixel', 'matrix', 'provide', 'pixel', 'matrix', 'include', 'phase', 'detection', 'pixel', 'regular', 'pixel', 'performs', 'autofocusing', 'accord', 'pixel', 'data', 'phase', 'detection', 'pixel', 'determine', 'operate', 'resolution', 'regular', 'pixel', 'accord', 'autofocused', 'pixel', 'data', 'phase', 'detection', 'pixel', 'wherein', 'phase', 'detection', 'pixel', 'always-on', 'pixel', 'regular', 'pixel', 'selectively', 'turn', 'autofocusing', 'accomplishedan', 'apparatus', 'include', 'first', 'camera', 'module', 'provide', 'first', 'object', 'first', 'field', 'view', 'second', 'camera', 'module', 'provide', 'second', 'object', 'second', 'field', 'view', 'different', 'first', 'field', 'view', 'first', 'depth', 'map', 'generator', 'generates', 'first', 'depth', 'map', 'first', 'base', 'first', 'second', 'second', 'depth', 'map', 'generator', 'generate', 'second', 'depth', 'map', 'second', 'base', 'first', 'second', 'first', 'depth', 'mapmethods', 'apparatus', 'include', 'computer', 'program', 'encode', 'computer', 'storage', 'medium', 'payment', 'base', 'face', 'provide', 'one', 'method', 'include', 'acquire', 'first', 'face', 'information', 'target', 'extract', 'first', 'characteristic', 'information', 'first', 'face', 'information', 'wherein', 'first', 'characteristic', 'information', 'include', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'determine', 'whether', 'target', 'willingness', 'pay', 'accord', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'include', 'determine', 'whether', 'angle', 'rotation', 'preset', 'direction', 'less', 'angle', 'threshold', 'whether', 'probability', 'value', 'gaze', 'payment', 'screen', 'great', 'probability', 'threshold', 'response', 'determine', 'target', 'willingness', 'pay', 'complete', 'payment', 'operation', 'base', 'face', 'novel', 'method', 'apparatus', 'face', 'authentication', 'disclose', 'disclose', 'method', 'comprise', 'detect', 'motion', 'subject', 'within', 'predetermined', 'area', 'view', 'assign', 'unique', 'session', 'identification', 'number', 'subject', 'detect', 'within', 'predetermined', 'area', 'view', 'detect', 'facial', 'area', 'subject', 'detect', 'within', 'predetermined', 'area', 'view', 'generate', 'facial', 'area', 'subject', 'assess', 'quality', 'facial', 'area', 'subject', 'conduce', 'incremental', 'training', 'facial', 'area', 'subject', 'determine', 'identity', 'subject', 'base', 'facial', 'area', 'subject', 'identify', 'intent', 'subject', 'authorize', 'access', 'point', 'entry', 'base', 'determine', 'identity', 'subject', 'base', 'intent', 'subjectdisclosed', 'herein', 'robot', 'electronic', 'acquire', 'video', 'method', 'acquire', 'video', 'use', 'robot', 'robot', 'include', 'camera', 'configure', 'rotate', 'lateral', 'direction', 'tilt', 'vertical', 'direction', 'control', 'least', 'one', 'direction', 'rotation', 'camera', 'angle', 'tilt', 'camera', 'focal', 'distance', 'camera', 'track', 'video', 'acquire', 'camera', 'method', 'disclose', 'infer', 'topic', 'file', 'contain', 'audio', 'video', 'example', 'multimodal', 'multimedia', 'file', 'order', 'facilitate', 'video', 'index', 'set', 'entity', 'extract', 'file', 'link', 'produce', 'graph', 'reference', 'information', 'also', 'obtain', 'set', 'entity', 'entity', 'may', 'draw', 'example', 'wikipedia', 'category', 'large', 'ontological', 'data', 'source', 'analysis', 'graph', 'use', 'unsupervised', 'learn', 'permit', 'determine', 'cluster', 'graph', 'extract', 'cluster', 'possibly', 'use', 'supervise', 'learn', 'provide', 'selection', 'topic', 'identifier', 'topic', 'identifier', 'use', 'index', 'filea', 'face', 'method', 'neural', 'network', 'train', 'method', 'apparatus', 'electronic', 'method', 'comprises', 'obtain', 'first', 'face', 'mean', 'first', 'camera', 'extract', 'first', 'face', 'feature', 'first', 'face', 'compare', 'first', 'face', 'feature', 'pre-stored', 'second', 'face', 'feature', 'obtain', 'reference', 'similarity', 'second', 'face', 'feature', 'obtain', 'extracting', 'feature', 'second', 'face', 'obtain', 'second', 'camera', 'second', 'camera', 'first', 'camera', 'different', 'type', 'camera', 'determine', 'accord', 'reference', 'similarity', 'whether', 'first', 'face', 'feature', 'second', 'face', 'feature', 'correspond', 'person', 'present', 'invention', 'disclose', 'technique', 'alert', 'vision', 'impairment', 'comprise', 'process', 'unit', 'configure', 'operable', 'receive', 'scene', 'data', 'indicative', 'scene', 'least', 'one', 'consumer', 'environment', 'identify', 'scene', 'data', 'certain', 'consumer', 'identify', 'event', 'indicative', 'behavioral', 'compensation', 'vision', 'impairment', 'upon', 'identification', 'event', 'send', 'notification', 'relate', 'vision', 'impairment']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_text_a = []\n",
    "\n",
    "for tuple in simpler_POS_text_a:\n",
    "    if (tuple[1] == None):\n",
    "        lemmatized_text_a.append(lemmatizer.lemmatize(tuple[0]))\n",
    "    else:\n",
    "        lemmatized_text_a.append(lemmatizer.lemmatize(tuple[0], pos=tuple[1]))\n",
    "    \n",
    "print(lemmatized_text_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "376d05e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['configure', 'make', 'screen', 'display', 'frame', 'comprise', 'capture', 'device', 'storage', 'device', 'store', 'module', 'couple', 'capture', 'device', 'storage', 'device', 'configure', 'execute', 'module', 'storage', 'device', 'configure', 'screen', 'display', 'marker', 'object', 'predetermine', 'position', 'configure', 'capture', 'device', 'capture', 'first', 'head', 'look', 'predetermined', 'position', 'perform', 'first', 'face', 'operation', 'first', 'head', 'obtain', 'first', 'face', 'region', 'correspond', 'predetermined', 'position', 'detect', 'first', 'facial', 'landmark', 'correspond', 'first', 'face', 'region', 'calculate', 'rotation', 'reference', 'angle', 'look', 'predetermine', 'position', 'accord', 'first', 'facial', 'landmark', 'configure', 'capture', 'device', 'capture', 'second', 'head', 'perform', 'second', 'head', 'obtain', 'second', 'face', 'region', 'detect', 'second', 'facial', 'landmark', 'within', 'second', 'face', 'region', 'estimate', 'head', 'posture', 'angle', 'accord', 'second', 'facial', 'landmark', 'calculate', 'gaze', 'position', 'screen', 'accord', 'head', 'posture', 'angle', 'rotation', 'reference', 'angle', 'predetermine', 'position', 'configure', 'screen', 'display', 'correspond', 'visual', 'effect', 'accord', 'gaze', 'position', 'accord', 'wherein', 'gaze', 'position', 'comprises', 'first', 'coordinate', 'value', 'first', 'axial', 'direction', 'second', 'coordinate', 'value', 'second', 'axial', 'direction', 'accord', 'wherein', 'head', 'posture', 'angle', 'comprise', 'head', 'pitch', 'angle', 'head', 'yaw', 'angle', 'rotation', 'reference', 'angle', 'comprise', 'first', 'pitch', 'angle', 'second', 'pitch', 'angle', 'first', 'yaw', 'angle', 'second', 'yaw', 'angle', 'correspond', 'predetermine', 'position', 'accord', 'wherein', 'performs', 'interpolation', 'operation', 'extrapolation', 'operation', 'accord', 'first', 'yaw', 'angle', 'second', 'yaw', 'angle', 'first', 'position', 'correspond', 'first', 'yaw', 'angle', 'among', 'predetermined', 'position', 'second', 'position', 'correspond', 'second', 'yaw', 'angle', 'among', 'predetermined', 'position', 'head', 'yaw', 'angle', 'thereby', 'obtain', 'first', 'coordinate', 'value', 'gaze', 'position', 'performs', 'interpolation', 'operation', 'extrapolation', 'operation', 'accord', 'first', 'pitch', 'angle', 'second', 'pitch', 'angle', 'third', 'position', 'correspond', 'first', 'pitch', 'angle', 'among', 'predetermined', 'position', 'fourth', 'position', 'correspond', 'second', 'pitch', 'angle', 'among', 'predetermined', 'position', 'head', 'pitch', 'angle', 'thereby', 'obtain', 'second', 'coordinate', 'value', 'gaze', 'position', 'accord', 'wherein', 'calculates', 'first', 'view', 'distance', 'screen', 'accord', 'first', 'facial', 'landmark', 'estimate', 'second', 'view', 'distance', 'screen', 'accord', 'second', 'facial', 'landmark', 'adjust', 'rotation', 'reference', 'angle', 'gaze', 'position', 'accord', 'second', 'viewing', 'distance', 'first', 'view', 'distance', 'accord', 'wherein', 'map', 'two-dimensional', 'position', 'coordinate', 'second', 'facial', 'landmark', 'plane', 'coordinate', 'system', 'three-dimensional', 'position', 'coordinate', 'three-dimensional', 'coordinate', 'system', 'estimate', 'head', 'posture', 'angle', 'accord', 'three-dimensional', 'position', 'coordinate', 'second', 'facial', 'landmark', 'accord', 'wherein', 'second', 'head', 'comprise', 'wearable', 'device', 'second', 'facial', 'landmark', 'comprise', 'third', 'facial', 'landmark', 'cover', 'wearable', 'device', 'accord', 'wherein', 'second', 'head', 'comprise', 'wearable', 'device', 'second', 'facial', 'landmark', 'comprise', 'one', 'simulate', 'landmark', 'mark', 'wearable', 'device', 'operate', 'adapt', 'comprise', 'capture', 'device', 'make', 'screen', 'display', 'frame', 'comprise', 'configure', 'screen', 'display', 'marker', 'object', 'predetermine', 'position', 'configure', 'capture', 'device', 'capture', 'first', 'head', 'look', 'predetermined', 'position', 'perform', 'first', 'face', 'operation', 'first', 'head', 'obtain', 'first', 'face', 'region', 'correspond', 'predetermined', 'position', 'detect', 'first', 'facial', 'landmark', 'correspond', 'first', 'face', 'region', 'calculate', 'rotation', 'reference', 'angle', 'look', 'predetermine', 'position', 'accord', 'first', 'facial', 'landmark', 'configure', 'capture', 'device', 'capture', 'second', 'head', 'perform', 'second', 'head', 'obtain', 'second', 'face', 'region', 'detect', 'second', 'facial', 'landmark', 'within', 'second', 'face', 'region', 'estimate', 'head', 'posture', 'angle', 'accord', 'second', 'facial', 'landmark', 'calculate', 'gaze', 'position', 'screen', 'accord', 'head', 'posture', 'angle', 'rotation', 'reference', 'angle', 'predetermine', 'position', 'configure', 'screen', 'display', 'correspond', 'visual', 'effect', 'accord', 'gaze', 'position', 'operation', 'accord', 'wherein', 'gaze', 'position', 'comprises', 'first', 'coordinate', 'value', 'first', 'axial', 'direction', 'second', 'coordinate', 'value', 'second', 'axial', 'direction', 'operation', 'accord', 'wherein', 'head', 'posture', 'angle', 'comprise', 'head', 'pitch', 'angle', 'head', 'yaw', 'angle', 'rotation', 'reference', 'angle', 'comprise', 'first', 'pitch', 'angle', 'second', 'pitch', 'angle', 'first', 'yaw', 'angle', 'second', 'yaw', 'angle', 'correspond', 'predetermined', 'position', 'operation', 'accord', 'wherein', 'step', 'calculate', 'gaze', 'position', 'screen', 'accord', 'head', 'posture', 'angle', 'rotation', 'reference', 'angle', 'predetermine', 'position', 'comprises', 'perform', 'interpolation', 'operation', 'extrapolation', 'operation', 'accord', 'first', 'yaw', 'angle', 'second', 'yaw', 'angle', 'first', 'position', 'correspond', 'first', 'yaw', 'angle', 'among', 'predetermined', 'position', 'second', 'position', 'correspond', 'second', 'yaw', 'angle', 'among', 'predetermined', 'position', 'head', 'yaw', 'angle', 'thereby', 'obtain', 'first', 'coordinate', 'value', 'gaze', 'position', 'perform', 'interpolation', 'operation', 'extrapolation', 'operation', 'accord', 'first', 'pitch', 'angle', 'second', 'pitch', 'angle', 'third', 'position', 'correspond', 'first', 'pitch', 'angle', 'among', 'predetermined', 'position', 'fourth', 'position', 'correspond', 'second', 'pitch', 'angle', 'among', 'predetermined', 'position', 'head', 'pitch', 'angle', 'thereby', 'obtain', 'second', 'coordinate', 'value', 'gaze', 'position', 'operation', 'accord', 'wherein', 'comprises', 'calculate', 'first', 'view', 'distance', 'screen', 'accord', 'first', 'facial', 'landmark', 'estimate', 'second', 'view', 'distance', 'screen', 'accord', 'second', 'facial', 'landmark', 'adjust', 'rotation', 'reference', 'angle', 'gaze', 'position', 'accord', 'second', 'viewing', 'distance', 'first', 'view', 'distance', 'operation', 'accord', 'wherein', 'comprises', 'map', 'two-dimensional', 'position', 'coordinate', 'second', 'facial', 'landmark', 'plane', 'coordinate', 'system', 'three-dimensional', 'position', 'coordinate', 'three-dimensional', 'coordinate', 'system', 'estimate', 'head', 'posture', 'angle', 'accord', 'three-dimensional', 'position', 'coordinate', 'second', 'facial', 'landmark', 'operation', 'accord', 'wherein', 'second', 'head', 'comprise', 'wearable', 'device', 'second', 'facial', 'landmark', 'comprise', 'third', 'facial', 'landmark', 'cover', 'wearable', 'device', 'operation', 'accord', 'wherein', 'second', 'head', 'comprise', 'wearable', 'device', 'second', 'facial', 'landmark', 'comprise', 'one', 'simulate', 'landmark', 'mark', 'wearable', 'device', 'computation', 'apply', 'compute', 'system', 'wherein', 'compute', 'system', 'comprise', 'control', 'unit', 'computation', 'group', 'general', 'storage', 'unit', 'wherein', 'control', 'unit', 'comprise', 'first', 'memory', 'decode', 'logic', 'controller', 'wherein', 'computation', 'group', 'comprise', 'group', 'controller', 'compute', 'unit', 'general', 'storage', 'unit', 'configure', 'store', 'data', 'computation', 'comprise', 'receive', 'controller', 'first', 'level', 'instruction', 'sequence', 'partition', 'decode', 'logic', 'first', 'level', 'instruction', 'sequence', 'second', 'level', 'instruction', 'sequence', 'create', 'controller', 'thread', 'second', 'level', 'instruction', 'sequence', 'allocate', 'controller', 'independent', 'register', 'well', 'configure', 'independent', 'address', 'function', 'thread', 'thread', 'wherein', 'integer', 'great', 'equal', 'obtain', 'group', 'controller', 'computation', 'type', 'second', 'level', 'instruction', 'sequence', 'obtain', 'correspond', 'fusion', 'computation', 'manner', 'computation', 'type', 'accord', 'computation', 'type', 'adopt', 'compute', 'unit', 'fusion', 'computation', 'manner', 'call', 'thread', 'perform', 'computation', 'second', 'level', 'instruction', 'sequence', 'obtain', 'final', 'result', 'wherein', 'obtain', 'group', 'controller', 'computation', 'type', 'second', 'level', 'instruction', 'sequence', 'obtain', 'correspond', 'fusion', 'computation', 'manner', 'computation', 'type', 'accord', 'computation', 'type', 'adopt', 'compute', 'unit', 'fusion', 'computation', 'manner', 'call', 'thread', 'perform', 'computation', 'second', 'instruction', 'sequence', 'obtain', 'final', 'result', 'computation', 'type', 'represent', 'computation', 'operation', 'type', 'group', 'controller', 'call', 'combine', 'computation', 'manner', 'single', 'instruction', 'multiple', 'data', 'type', 'combination', 'single', 'instruction', 'multiple', 'thread', 'use', 'thread', 'perform', 'combine', 'computation', 'manner', 'obtain', 'final', 'result', 'include', 'partition', 'decode', 'logic', 'thread', 'n', 'wrap', 'allocate', 'compute', 'unit', 'convert', 'group', 'controller', 'second', 'instruction', 'sequence', 'second', 'control', 'signal', 'send', 'second', 'control', 'signal', 'compute', 'unit', 'call', 'compute', 'unit', 'wrap', 'allocate', 'compute', 'unit', 'second', 'control', 'signal', 'fetch', 'correspond', 'data', 'accord', 'independent', 'address', 'function', 'perform', 'compute', 'unit', 'computation', 'data', 'obtain', 'intermediate', 'result', 'splice', 'intermediate', 'result', 'obtain', 'final', 'result', 'wherein', 'obtain', 'group', 'controller', 'computation', 'type', 'second', 'level', 'instruction', 'sequence', 'obtain', 'correspond', 'fusion', 'computation', 'manner', 'computation', 'type', 'accord', 'computation', 'type', 'adopt', 'compute', 'unit', 'fusion', 'computation', 'manner', 'call', 'thread', 'perform', 'computation', 'second', 'instruction', 'sequence', 'obtain', 'final', 'result', 'computation', 'type', 'represent', 'computation', 'operation', 'different', 'type', 'group', 'controller', 'call', 'simultaneous', 'multi-threading', 'thread', 'perform', 'computation', 'obtain', 'final', 'result', 'include', 'partition', 'decode', 'logic', 'thread', 'n', 'wrap', 'convert', 'second', 'instruction', 'sequence', 'second', 'control', 'signal', 'obtain', 'group', 'controller', 'computation', 'type', 'support', 'compute', 'unit', 'allocate', 'controller', 'n', 'wrap', 'second', 'control', 'signal', 'correspond', 'compute', 'unit', 'support', 'computation', 'type', 'wrap', 'second', 'control', 'signal', 'call', 'compute', 'unit', 'wrap', 'allocate', 'compute', 'unit', 'second', 'control', 'signal', 'fetch', 'compute', 'unit', 'correspond', 'data', 'perform', 'compute', 'unit', 'computation', 'data', 'obtain', 'intermediate', 'result', 'splice', 'intermediate', 'result', 'obtain', 'final', 'result', 'comprise', 'wrap', 'wrap', 'block', 'add', 'wrap', 'wait', 'queue', 'data', 'wrap', 'already', 'fetch', 'add', 'wrap', 'preparation', 'queue', 'wherein', 'preparation', 'queue', 'queue', 'wrap', 'schedule', 'execute', 'locate', 'compute', 'resource', 'idle', 'wherein', 'first', 'level', 'instruction', 'sequence', 'include', 'long', 'instruction', 'second', 'level', 'instruction', 'sequence', 'include', 'instruction', 'sequence', 'wherein', 'compute', 'system', 'include', 'tree', 'module', 'wherein', 'tree', 'module', 'include', 'root', 'port', 'branch', 'port', 'wherein', 'root', 'port', 'tree', 'module', 'connect', 'group', 'controller', 'branch', 'port', 'tree', 'module', 'connect', 'compute', 'unit', 'compute', 'unit', 'respectively', 'tree', 'module', 'configure', 'forward', 'data', 'block', 'wrap', 'instruction', 'sequence', 'group', 'controller', 'compute', 'unit', 'wherein', 'tree', 'module', 'n-ary', 'tree', 'wherein', 'n', 'integer', 'great', 'equal', 'wherein', 'compute', 'system', 'include', 'branch', 'process', 'circuit', 'wherein', 'branch', 'process', 'circuit', 'connect', 'group', 'controller', 'compute', 'unit', 'branch', 'process', 'circuit', 'configure', 'forward', 'data', 'wrap', 'instruction', 'sequence', 'group', 'controller', 'compute', 'unit', 'compute', 'system', 'comprise', 'control', 'unit', 'computation', 'group', 'general', 'storage', 'unit', 'wherein', 'control', 'unit', 'include', 'first', 'memory', 'decode', 'logic', 'controller', 'computation', 'group', 'include', 'group', 'controller', 'compute', 'unit', 'general', 'storage', 'unit', 'configure', 'store', 'data', 'controller', 'configure', 'receive', 'first', 'level', 'instruction', 'sequence', 'control', 'first', 'memory', 'decode', 'logic', 'decode', 'logic', 'configure', 'partition', 'first', 'level', 'instruction', 'sequence', 'second', 'level', 'instruction', 'sequence', 'controller', 'configured', 'create', 'thread', 'second', 'level', 'instruction', 'sequence', 'allocate', 'independent', 'register', 'configure', 'independent', 'address', 'function', 'thread', 'thread', 'integer', 'great', 'equal', 'controller', 'configure', 'convert', 'second', 'instruction', 'sequence', 'control', 'signal', 'send', 'group', 'controller', 'group', 'controller', 'configure', 'receive', 'control', 'signal', 'obtain', 'computational', 'type', 'control', 'signal', 'divide', 'thread', 'n', 'wrap', 'allocate', 'n', 'wrap', 'control', 'signal', 'compute', 'unit', 'accord', 'computational', 'type', 'compute', 'unit', 'configure', 'fetch', 'data', 'general', 'storage', 'unit', 'allocate', 'wrap', 'control', 'signal', 'perform', 'computation', 'obtain', 'intermediate', 'result', 'group', 'controller', 'configure', 'splice', 'intermediate', 'result', 'obtain', 'final', 'computation', 'result', 'compute', 'system', 'wherein', 'compute', 'unit', 'include', 'addition', 'compute', 'unit', 'multiplication', 'compute', 'unit', 'activation', 'compute', 'unit', 'dedicate', 'compute', 'unit', 'compute', 'system', 'wherein', 'dedicate', 'compute', 'unit', 'include', 'face', 'compute', 'unit', 'graphic', 'compute', 'unit', 'fingerprint', 'compute', 'unit', 'neural', 'network', 'compute', 'unit', 'compute', 'system', 'wherein', 'group', 'controller', 'configure', 'computation', 'type', 'control', 'signal', 'graphic', 'computation', 'fingerprint', 'identification', 'face', 'neural', 'network', 'operation', 'allocate', 'control', 'signal', 'face', 'compute', 'unit', 'graphic', 'compute', 'unit', 'fingerprint', 'compute', 'unit', 'neural', 'network', 'compute', 'unit', 'respectively', 'compute', 'system', 'wherein', 'first', 'level', 'instruction', 'sequence', 'include', 'long', 'instruction', 'second', 'level', 'instruction', 'sequence', 'include', 'instruction', 'sequence', 'compute', 'system', 'comprise', 'tree', 'module', 'wherein', 'tree', 'module', 'include', 'root', 'port', 'branch', 'port', 'wherein', 'root', 'port', 'tree', 'module', 'connect', 'group', 'controller', 'branch', 'port', 'tree', 'module', 'connect', 'compute', 'unit', 'compute', 'unit', 'respectively', 'tree', 'module', 'configure', 'forward', 'data', 'block', 'wrap', 'instruction', 'sequence', 'group', 'controller', 'compute', 'unit', 'compute', 'system', 'wherein', 'tree', 'module', 'n-ary', 'tree', 'wherein', 'n', 'integer', 'great', 'equal', 'computing', 'system', 'wherein', 'compute', 'system', 'include', 'branch', 'processing', 'circuit', 'branch', 'process', 'circuit', 'connect', 'group', 'controller', 'compute', 'unit', 'branch', 'process', 'circuit', 'configure', 'forward', 'data', 'wrap', 'instruction', 'sequence', 'group', 'controller', 'compute', 'unit', 'computer', 'program', 'product', 'comprise', 'non-instant', 'computer', 'readable', 'storage', 'medium', 'wherein', 'computer', 'program', 'store', 'non-instant', 'computer', 'readable', 'storage', 'medium', 'computer', 'program', 'capable', 'cause', 'computer', 'perform', 'operation', 'detect', 'body', 'information', 'one', 'passenger', 'vehicle', 'base', 'human', \"'\", 'status', 'comprise', 'step', 'least', 'one', 'interior', 'interior', 'vehicle', 'acquire', 'passenger', 'body', 'information-detecting', 'device', 'perform', 'process', 'inputting', 'interior', 'face', 'network', 'thereby', 'allow', 'face', 'network', 'detect', 'passenger', 'interior', 'thus', 'output', 'multiple', 'piece', 'passenger', 'feature', 'information', 'correspond', 'detect', 'ii', 'process', 'inputting', 'interior', 'body', 'network', 'thereby', 'allow', 'body', 'network', 'detect', 'body', 'passenger', 'interior', 'thus', 'output', 'body-part', 'length', 'information', 'detect', 'body', 'b', 'passenger', 'body', 'information-detecting', 'device', 'perform', 'process', 'retrieve', 'specific', 'height', 'mapping', 'information', 'correspond', 'specific', 'passenger', 'feature', 'information', 'specific', 'passenger', 'height', 'map', 'table', 'store', 'height', 'map', 'information', 'represent', 'respective', 'one', 'predetermine', 'ratio', 'one', 'segment', 'body', 'portion', 'human', 'group', 'height', 'per', 'human', 'group', 'process', 'acquire', 'specific', 'height', 'specific', 'passenger', 'specific', 'height', 'map', 'information', 'refer', 'specific', 'body-part', 'length', 'information', 'specific', 'passenger', 'process', 'retrieve', 'specific', 'weight', 'map', 'information', 'correspond', 'specific', 'passenger', 'feature', 'information', 'weight', 'map', 'table', 'store', 'multiple', 'piece', 'weight', 'map', 'information', 'represent', 'predetermine', 'correlation', 'height', 'weight', 'per', 'human', 'group', 'process', 'acquire', 'weight', 'specific', 'passenger', 'specific', 'weight', 'map', 'information', 'refer', 'specific', 'height', 'specific', 'passenger', 'wherein', 'step', 'passenger', 'body', 'information-detecting', 'device', 'performs', 'process', 'inputting', 'interior', 'body', 'network', 'thereby', 'allow', 'body', 'network', 'output', 'one', 'one', 'channel', 'correspond', 'interior', 'via', 'feature', 'extraction', 'network', 'ii', 'generate', 'least', 'one', 'keypoint', 'heatmap', 'least', 'one', 'part', 'affinity', 'field', 'one', 'channel', 'correspond', 'via', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'iii', 'extract', 'keypoints', 'keypoint', 'heatmap', 'via', 'keypoint', 'detector', 'group', 'extract', 'keypoints', 'refer', 'part', 'affinity', 'field', 'thus', 'generate', 'body', 'part', 'per', 'passenger', 'result', 'allow', 'body', 'network', 'output', 'multiple', 'piece', 'body-part', 'length', 'information', 'passenger', 'refer', 'body', 'part', 'per', 'passenger', 'wherein', 'feature', 'extraction', 'network', 'include', 'least', 'one', 'convolutional', 'layer', 'applies', 'least', 'one', 'convolution', 'operation', 'interior', 'thereby', 'output', 'wherein', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'include', 'one', 'fully', 'convolutional', 'network', 'convolutional', 'layer', 'apply', 'fully-convolution', 'operation', 'convolution', 'operation', 'thereby', 'generate', 'keypoint', 'heatmap', 'part', 'affinity', 'field', 'wherein', 'keypoint', 'detector', 'connects', 'refer', 'part', 'affinity', 'field', 'pair', 'respectively', 'high', 'mutual', 'connection', 'probability', 'connect', 'among', 'extracted', 'keypoints', 'thereby', 'group', 'extract', 'keypoints', 'wherein', 'feature', 'extraction', 'network', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'learn', 'learning', 'device', 'perform', 'process', 'inputting', 'least', 'one', 'training', 'include', 'one', 'object', 'train', 'feature', 'extraction', 'network', 'thereby', 'allow', 'feature', 'extraction', 'network', 'generate', 'one', 'train', 'one', 'channel', 'apply', 'least', 'one', 'convolutional', 'operation', 'train', 'ii', 'process', 'inputting', 'train', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'thereby', 'allow', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'generate', 'one', 'keypoint', 'heatmaps', 'train', 'one', 'part', 'affinity', 'field', 'train', 'one', 'channel', 'train', 'iii', 'process', 'inputting', 'keypoint', 'heatmaps', 'train', 'part', 'affinity', 'field', 'train', 'keypoint', 'detector', 'thereby', 'allow', 'keypoint', 'detector', 'extract', 'keypoints', 'train', 'keypoint', 'heatmaps', 'train', 'process', 'grouping', 'extract', 'keypoints', 'train', 'refer', 'part', 'affinity', 'field', 'train', 'thereby', 'detect', 'keypoints', 'per', 'object', 'train', 'iv', 'process', 'allow', 'loss', 'layer', 'calculate', 'one', 'loss', 'refer', 'keypoints', 'per', 'object', 'train', 'correspond', 'ground', 'truth', 'thereby', 'adjust', 'one', 'parameter', 'feature', 'extraction', 'network', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'loss', 'minimize', 'backpropagation', 'use', 'loss', 'wherein', 'step', 'passenger', 'body', 'information-detecting', 'device', 'performs', 'process', 'inputting', 'interior', 'face', 'network', 'thereby', 'allow', 'face', 'network', 'detect', 'passenger', 'locate', 'interior', 'via', 'face', 'detector', 'output', 'multiple', 'piece', 'passenger', 'feature', 'information', 'facial', 'via', 'facial', 'feature', 'classifier', 'wherein', 'step', 'passenger', 'body', 'information-detecting', 'device', 'performs', 'process', 'inputting', 'interior', 'face', 'network', 'thereby', 'allow', 'face', 'network', 'apply', 'least', 'one', 'convolution', 'operation', 'interior', 'thus', 'output', 'least', 'one', 'feature', 'map', 'correspond', 'interior', 'via', 'least', 'one', 'convolutional', 'layer', 'ii', 'output', 'one', 'proposal', 'box', 'passenger', 'estimate', 'located', 'feature', 'map', 'via', 'region', 'proposal', 'network', 'iii', 'apply', 'pool', 'operation', 'one', 'region', 'correspond', 'proposal', 'box', 'feature', 'map', 'thus', 'output', 'least', 'one', 'feature', 'vector', 'via', 'pool', 'layer', 'iv', 'apply', 'fully-connected', 'operation', 'feature', 'vector', 'thus', 'output', 'multiple', 'piece', 'passenger', 'feature', 'information', 'correspond', 'passenger', 'correspond', 'proposal', 'box', 'via', 'fully', 'connect', 'layer', 'wherein', 'multiple', 'piece', 'passenger', 'feature', 'information', 'include', 'age', 'gender', 'race', 'correspond', 'passenger', 'passenger', 'body', 'information-detecting', 'device', 'detect', 'body', 'information', 'one', 'passenger', 'vehicle', 'base', 'human', \"'\", 'status', 'comprise', 'least', 'one', 'memory', 'store', 'instruction', 'least', 'one', 'configure', 'execute', 'instruction', 'perform', 'support', 'another', 'device', 'perform', 'least', 'one', 'interior', 'interior', 'vehicle', 'acquire', 'process', 'inputting', 'interior', 'face', 'network', 'thereby', 'allow', 'face', 'network', 'detect', 'passenger', 'interior', 'thus', 'output', 'multiple', 'piece', 'passenger', 'feature', 'information', 'correspond', 'detect', 'ii', 'process', 'inputting', 'interior', 'body', 'network', 'thereby', 'allow', 'body', 'network', 'detect', 'body', 'passenger', 'interior', 'thus', 'output', 'body-part', 'length', 'information', 'detect', 'body', 'ii', 'process', 'retrieve', 'specific', 'height', 'mapping', 'information', 'correspond', 'specific', 'passenger', 'feature', 'information', 'specific', 'passenger', 'height', 'map', 'table', 'store', 'height', 'map', 'information', 'represent', 'respective', 'one', 'predetermine', 'ratio', 'one', 'segment', 'body', 'portion', 'human', 'group', 'height', 'per', 'human', 'group', 'process', 'acquire', 'specific', 'height', 'specific', 'passenger', 'specific', 'height', 'map', 'information', 'refer', 'specific', 'body-part', 'length', 'information', 'specific', 'passenger', 'process', 'retrieve', 'specific', 'weight', 'map', 'information', 'correspond', 'specific', 'passenger', 'feature', 'information', 'weight', 'map', 'table', 'store', 'multiple', 'piece', 'weight', 'map', 'information', 'represent', 'predetermine', 'correlation', 'height', 'weight', 'per', 'human', 'group', 'process', 'acquire', 'weight', 'specific', 'passenger', 'specific', 'weight', 'map', 'information', 'refer', 'specific', 'height', 'specific', 'passenger', 'passenger', 'body', 'information-detecting', 'device', 'wherein', 'process', 'performs', 'process', 'inputting', 'interior', 'body', 'network', 'thereby', 'allow', 'body', 'network', 'output', 'one', 'one', 'channel', 'correspond', 'interior', 'via', 'feature', 'extraction', 'network', 'ii', 'generate', 'least', 'one', 'keypoint', 'heatmap', 'least', 'one', 'part', 'affinity', 'field', 'one', 'channel', 'correspond', 'via', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'iii', 'extract', 'keypoints', 'keypoint', 'heatmap', 'via', 'keypoint', 'detector', 'group', 'extract', 'keypoints', 'refer', 'part', 'affinity', 'field', 'thus', 'generate', 'body', 'part', 'per', 'passenger', 'result', 'allow', 'body', 'network', 'output', 'multiple', 'piece', 'body-part', 'length', 'information', 'passenger', 'refer', 'body', 'part', 'per', 'passenger', 'passenger', 'body', 'information-detecting', 'device', 'wherein', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'include', 'one', 'fully', 'convolutional', 'network', 'convolutional', 'layer', 'apply', 'fully-convolution', 'operation', 'convolution', 'operation', 'thereby', 'generate', 'keypoint', 'heatmap', 'part', 'affinity', 'field', 'passenger', 'body', 'information-detecting', 'device', 'wherein', 'keypoint', 'detector', 'connects', 'refer', 'part', 'affinity', 'field', 'pair', 'respectively', 'high', 'mutual', 'connection', 'probability', 'connect', 'among', 'extracted', 'keypoints', 'thereby', 'group', 'extract', 'keypoints', 'passenger', 'body', 'information-detecting', 'device', 'wherein', 'feature', 'extraction', 'network', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'learn', 'learning', 'device', 'perform', 'process', 'inputting', 'least', 'one', 'training', 'include', 'one', 'object', 'train', 'feature', 'extraction', 'network', 'thereby', 'allow', 'feature', 'extraction', 'network', 'generate', 'one', 'train', 'one', 'channel', 'apply', 'least', 'one', 'convolutional', 'operation', 'train', 'ii', 'process', 'inputting', 'train', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'thereby', 'allow', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'generate', 'one', 'keypoint', 'heatmaps', 'train', 'one', 'part', 'affinity', 'field', 'train', 'one', 'channel', 'train', 'iii', 'process', 'inputting', 'keypoint', 'heatmaps', 'train', 'part', 'affinity', 'field', 'train', 'keypoint', 'detector', 'thereby', 'allow', 'keypoint', 'detector', 'extract', 'keypoints', 'train', 'keypoint', 'heatmaps', 'train', 'process', 'grouping', 'extract', 'keypoints', 'train', 'refer', 'part', 'affinity', 'field', 'train', 'thereby', 'detect', 'keypoints', 'per', 'object', 'train', 'iv', 'process', 'allow', 'loss', 'layer', 'calculate', 'one', 'loss', 'refer', 'keypoints', 'per', 'object', 'train', 'correspond', 'ground', 'truth', 'thereby', 'adjust', 'one', 'parameter', 'feature', 'extraction', 'network', 'keypoint', 'heatmap', '&', 'part', 'affinity', 'field', 'extractor', 'loss', 'minimize', 'backpropagation', 'use', 'loss', 'passenger', 'body', 'information-detecting', 'device', 'wherein', 'process', 'performs', 'process', 'inputting', 'interior', 'face', 'network', 'thereby', 'allow', 'face', 'network', 'apply', 'least', 'one', 'convolution', 'operation', 'interior', 'thus', 'output', 'least', 'one', 'feature', 'map', 'correspond', 'interior', 'via', 'least', 'one', 'convolutional', 'layer', 'ii', 'output', 'one', 'proposal', 'box', 'passenger', 'estimate', 'located', 'feature', 'map', 'via', 'region', 'proposal', 'network', 'iii', 'apply', 'pool', 'operation', 'one', 'region', 'correspond', 'proposal', 'box', 'feature', 'map', 'thus', 'output', 'least', 'one', 'feature', 'vector', 'via', 'pool', 'layer', 'iv', 'apply', 'fully-connected', 'operation', 'feature', 'vector', 'thus', 'output', 'multiple', 'piece', 'passenger', 'feature', 'information', 'correspond', 'passenger', 'correspond', 'proposal', 'box', 'via', 'fully', 'connect', 'layer', 'computer', 'implement', 'perform', 'video', 'cod', 'base', 'face', 'detection', 'comprise', 'receive', 'video', 'frame', 'comprise', 'one', 'video', 'frame', 'video', 'sequence', 'determine', 'video', 'frame', 'key', 'frame', 'video', 'sequence', 'perform', 'response', 'video', 'frame', 'key', 'frame', 'video', 'sequence', 'multi-stage', 'facial', 'search', 'video', 'frame', 'base', 'predetermined', 'feature', 'template', 'predetermine', 'number', 'stage', 'determine', 'first', 'candidate', 'face', 'region', 'second', 'candidate', 'face', 'region', 'video', 'frame', 'test', 'first', 'second', 'candidate', 'face', 'region', 'base', 'skin', 'tone', 'information', 'determine', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'second', 'candidate', 'face', 'region', 'invalid', 'face', 'region', 'reject', 'second', 'candidate', 'face', 'region', 'output', 'first', 'candidate', 'face', 'region', 'encode', 'video', 'frame', 'base', 'least', 'part', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'generate', 'cod', 'bitstream', 'wherein', 'skin', 'tone', 'information', 'comprise', 'skin', 'probability', 'map', 'wherein', 'say', 'test', 'first', 'second', 'candidate', 'face', 'region', 'base', 'skin', 'tone', 'information', 'perform', 'response', 'video', 'frame', 'key', 'frame', 'video', 'sequence', 'wherein', 'first', 'candidate', 'face', 'region', 'comprise', 'rectangular', 'region', 'comprise', 'determine', 'free', 'form', 'shape', 'face', 'region', 'correspond', 'first', 'candidate', 'face', 'region', 'wherein', 'free', 'form', 'shape', 'face', 'region', 'least', 'one', 'pixel', 'accuracy', 'small', 'block', 'pixel', 'accuracy', 'wherein', 'determine', 'free', 'form', 'shape', 'face', 'region', 'comprise', 'generate', 'enhance', 'skip', 'probability', 'map', 'correspond', 'first', 'candidate', 'face', 'region', 'binarizing', 'enhance', 'skip', 'probability', 'map', 'overlay', 'binarized', 'enhanced', 'skip', 'probability', 'map', 'least', 'portion', 'video', 'frame', 'provide', 'free', 'form', 'shape', 'face', 'region', 'wherein', 'second', 'video', 'frame', 'comprise', 'non-key', 'frame', 'video', 'sequence', 'comprise', 'perform', 'face', 'detection', 'second', 'video', 'frame', 'video', 'sequence', 'base', 'free', 'form', 'shape', 'face', 'region', 'comprise', 'second', 'free', 'form', 'shape', 'face', 'region', 'second', 'video', 'frame', 'base', 'free', 'form', 'shape', 'face', 'region', 'video', 'frame', 'wherein', 'second', 'free', 'form', 'shape', 'face', 'region', 'comprise', 'determine', 'location', 'second', 'valid', 'face', 'region', 'second', 'video', 'frame', 'base', 'displacement', 'offset', 'respect', 'first', 'candidate', 'face', 'region', 'comprise', 'determine', 'displacement', 'offset', 'base', 'offset', 'centroid', 'bound', 'box', 'around', 'skin', 'enhance', 'region', 'correspond', 'first', 'candidate', 'face', 'region', 'second', 'centroid', 'second', 'bounding', 'box', 'around', 'second', 'skin', 'enhance', 'region', 'second', 'video', 'frame', 'wherein', 'encode', 'video', 'frame', 'base', 'least', 'part', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'comprise', 'least', 'one', 'reduce', 'quantization', 'parameter', 'correspond', 'first', 'candidate', 'face', 'region', 'adjust', 'lambda', 'value', 'first', 'candidate', 'face', 'region', 'disable', 'skip', 'cod', 'first', 'candidate', 'face', 'region', 'wherein', 'bitstream', 'comprise', 'least', 'one', 'hadvanced', 'video', 'cod', 'avc', 'compliant', 'bitstream', 'hhigh', 'efficiency', 'video', 'cod', 'hevc', 'compliant', 'bitstream', 'vp', 'compliant', 'bitstream', 'vp', 'compliant', 'bitstream', 'alliance', 'open', 'medium', 'aom', 'av', 'compliant', 'bitstream', 'computer', 'implement', 'perform', 'face', 'detection', 'comprise', 'receive', 'video', 'frame', 'sequence', 'video', 'frame', 'perform', 'multi-stage', 'facial', 'search', 'video', 'frame', 'base', 'predetermined', 'feature', 'template', 'predetermine', 'number', 'stage', 'determine', 'first', 'candidate', 'face', 'region', 'second', 'candidate', 'face', 'region', 'video', 'frame', 'test', 'first', 'second', 'candidate', 'face', 'region', 'base', 'skin', 'tone', 'information', 'determine', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'second', 'candidate', 'face', 'region', 'invalid', 'face', 'region', 'reject', 'second', 'candidate', 'face', 'region', 'output', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'process', 'provide', 'index', 'indicative', 'person', 'present', 'video', 'frame', 'base', 'valid', 'face', 'region', 'wherein', 'sequence', 'video', 'frame', 'comprises', 'sequence', 'surveillance', 'video', 'frame', 'comprise', 'perform', 'face', 'surveillance', 'video', 'frame', 'base', 'valid', 'face', 'region', 'wherein', 'sequence', 'video', 'frame', 'comprises', 'sequence', 'decode', 'video', 'frame', 'comprise', 'add', 'marker', 'correspond', 'receive', 'video', 'frame', 'perform', 'face', 'receive', 'video', 'frame', 'base', 'valid', 'face', 'region', 'wherein', 'sequence', 'video', 'frame', 'receive', 'device', 'login', 'attempt', 'comprise', 'perform', 'face', 'base', 'valid', 'face', 'region', 'allow', 'access', 'device', 'secure', 'face', 'recognize', 'wherein', 'sequence', 'video', 'frame', 'comprises', 'sequence', 'videoconferencing', 'frame', 'comprise', 'encode', 'video', 'frame', 'base', 'least', 'part', 'valid', 'face', 'region', 'generate', 'cod', 'bitstream', 'wherein', 'encode', 'video', 'frame', 'comprise', 'encode', 'background', 'region', 'video', 'frame', 'bitstream', 'comprise', 'encode', 'video', 'frame', 'base', 'least', 'part', 'valid', 'face', 'region', 'generate', 'cod', 'bitstream', 'wherein', 'encode', 'video', 'frame', 'comprise', 'include', 'metadata', 'correspond', 'valid', 'face', 'region', 'bitstream', 'comprise', 'decode', 'cod', 'bitstream', 'generate', 'decode', 'video', 'frame', 'determine', 'metadata', 'correspond', 'valid', 'face', 'region', 'bitstream', 'comprise', 'least', 'one', 'replace', 'valid', 'face', 'region', 'base', 'decode', 'metadata', 'crop', 'display', 'data', 'correspond', 'valid', 'face', 'region', 'base', 'decode', 'metadata', 'index', 'decode', 'video', 'frame', 'base', 'decode', 'metadata', 'system', 'perform', 'video', 'cod', 'base', 'face', 'detection', 'comprise', 'memory', 'configure', 'store', 'video', 'frame', 'comprise', 'one', 'video', 'frame', 'video', 'sequence', 'couple', 'memory', 'receive', 'video', 'frame', 'determine', 'video', 'frame', 'key', 'frame', 'video', 'sequence', 'perform', 'response', 'video', 'frame', 'key', 'frame', 'video', 'sequence', 'multi-stage', 'facial', 'search', 'video', 'frame', 'base', 'predetermined', 'feature', 'template', 'predetermine', 'number', 'stage', 'determine', 'first', 'candidate', 'face', 'region', 'second', 'candidate', 'face', 'region', 'video', 'frame', 'test', 'first', 'second', 'candidate', 'face', 'region', 'base', 'skin', 'tone', 'information', 'determine', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'second', 'candidate', 'face', 'region', 'invalid', 'face', 'region', 'reject', 'second', 'candidate', 'face', 'region', 'output', 'first', 'candidate', 'face', 'region', 'encode', 'video', 'frame', 'base', 'least', 'part', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'generate', 'cod', 'bitstream', 'system', 'wherein', 'skin', 'tone', 'information', 'comprise', 'skin', 'probability', 'map', 'system', 'wherein', 'first', 'candidate', 'face', 'region', 'comprise', 'rectangular', 'region', 'determine', 'free', 'form', 'shape', 'face', 'region', 'correspond', 'first', 'candidate', 'face', 'region', 'wherein', 'free', 'form', 'shape', 'face', 'region', 'least', 'one', 'pixel', 'accuracy', 'small', 'block', 'pixel', 'accuracy', 'system', 'wherein', 'determine', 'free', 'form', 'shape', 'face', 'region', 'comprise', 'generate', 'enhance', 'skip', 'probability', 'map', 'correspond', 'first', 'candidate', 'face', 'region', 'binarize', 'enhance', 'skip', 'probability', 'map', 'overlay', 'binarized', 'enhanced', 'skip', 'probability', 'map', 'least', 'portion', 'video', 'frame', 'provide', 'free', 'form', 'shape', 'face', 'region', 'system', 'wherein', 'second', 'video', 'frame', 'comprise', 'non-key', 'frame', 'video', 'sequence', 'perform', 'face', 'detection', 'second', 'video', 'frame', 'video', 'sequence', 'base', 'free', 'form', 'shape', 'face', 'region', 'system', 'wherein', 'track', 'second', 'free', 'form', 'shape', 'face', 'region', 'second', 'video', 'frame', 'base', 'free', 'form', 'shape', 'face', 'region', 'video', 'frame', 'system', 'wherein', 'encode', 'video', 'frame', 'base', 'least', 'part', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'comprise', 'reduce', 'quantization', 'parameter', 'correspond', 'first', 'candidate', 'face', 'region', 'adjust', 'lambda', 'value', 'first', 'candidate', 'face', 'region', 'disable', 'skip', 'cod', 'first', 'candidate', 'face', 'region', 'least', 'one', 'non-transitory', 'machine', 'readable', 'medium', 'comprise', 'instruction', 'response', 'execute', 'device', 'cause', 'device', 'perform', 'video', 'cod', 'base', 'face', 'detection', 'receive', 'video', 'frame', 'comprise', 'one', 'video', 'frame', 'video', 'sequence', 'determine', 'video', 'frame', 'key', 'frame', 'video', 'sequence', 'perform', 'response', 'video', 'frame', 'key', 'frame', 'video', 'sequence', 'multi-stage', 'facial', 'search', 'video', 'frame', 'base', 'predetermined', 'feature', 'template', 'predetermine', 'number', 'stage', 'determine', 'first', 'candidate', 'face', 'region', 'second', 'candidate', 'face', 'region', 'video', 'frame', 'test', 'first', 'second', 'candidate', 'face', 'region', 'base', 'skin', 'tone', 'information', 'determine', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'second', 'candidate', 'face', 'region', 'invalid', 'face', 'region', 'reject', 'second', 'candidate', 'face', 'region', 'output', 'first', 'candidate', 'face', 'region', 'encode', 'video', 'frame', 'base', 'least', 'part', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'generate', 'cod', 'bitstream', 'non-transitory', 'machine', 'readable', 'medium', 'wherein', 'skin', 'tone', 'information', 'comprise', 'skin', 'probability', 'map', 'non-transitory', 'machine', 'readable', 'medium', 'wherein', 'first', 'candidate', 'face', 'region', 'comprise', 'rectangular', 'region', 'machine', 'readable', 'medium', 'comprise', 'instruction', 'response', 'execute', 'device', 'cause', 'device', 'perform', 'video', 'cod', 'base', 'face', 'detection', 'determine', 'free', 'form', 'shape', 'face', 'region', 'correspond', 'first', 'candidate', 'face', 'region', 'wherein', 'free', 'form', 'shape', 'face', 'region', 'least', 'one', 'pixel', 'accuracy', 'small', 'block', 'pixel', 'accuracy', 'non-transitory', 'machine', 'readable', 'medium', 'wherein', 'determine', 'free', 'form', 'shape', 'face', 'region', 'comprise', 'generate', 'enhance', 'skip', 'probability', 'map', 'correspond', 'first', 'candidate', 'face', 'region', 'binarizing', 'enhance', 'skip', 'probability', 'map', 'overlay', 'binarized', 'enhanced', 'skip', 'probability', 'map', 'least', 'portion', 'video', 'frame', 'provide', 'free', 'form', 'shape', 'face', 'region', 'non-transitory', 'machine', 'readable', 'medium', 'wherein', 'second', 'video', 'frame', 'comprise', 'non-key', 'frame', 'video', 'sequence', 'machine', 'readable', 'medium', 'comprise', 'instruction', 'response', 'execute', 'device', 'cause', 'device', 'perform', 'video', 'cod', 'base', 'face', 'detection', 'perform', 'face', 'detection', 'second', 'video', 'frame', 'video', 'sequence', 'base', 'free', 'form', 'shape', 'face', 'region', 'non-transitory', 'machine', 'readable', 'medium', 'machine', 'readable', 'medium', 'comprise', 'instruction', 'response', 'execute', 'device', 'cause', 'device', 'perform', 'video', 'cod', 'base', 'face', 'detection', 'second', 'free', 'form', 'shape', 'face', 'region', 'second', 'video', 'frame', 'base', 'free', 'form', 'shape', 'face', 'region', 'video', 'frame', 'non-transitory', 'machine', 'readable', 'medium', 'wherein', 'encode', 'video', 'frame', 'base', 'least', 'part', 'first', 'candidate', 'face', 'region', 'valid', 'face', 'region', 'comprise', 'least', 'one', 'reduce', 'quantization', 'parameter', 'correspond', 'first', 'candidate', 'face', 'region', 'adjust', 'lambda', 'value', 'first', 'candidate', 'face', 'region', 'disable', 'skip', 'cod', 'first', 'candidate', 'face', 'region', 'manage', 'smart', 'database', 'store', 'facial', 'face', 'comprise', 'step', 'manage', 'device', 'perform', 'process', 'count', 'one', 'specific', 'facial', 'corresponding', 'least', 'one', 'specific', 'person', 'store', 'smart', 'database', 'new', 'facial', 'face', 'continuously', 'store', 'process', 'determine', 'whether', 'first', 'count', 'value', 'represent', 'count', 'specific', 'facial', 'satisfies', 'preset', 'first', 'set', 'value', 'b', 'first', 'count', 'value', 'determine', 'satisfy', 'first', 'set', 'value', 'manage', 'device', 'perform', 'process', 'inputting', 'specific', 'facial', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'neural', 'aggregation', 'network', 'generate', 'quality', 'score', 'specific', 'facial', 'aggregation', 'specific', 'facial', 'process', 'sort', 'quality', 'score', 'correspond', 'specific', 'facial', 'descending', 'order', 'quality', 'score', 'process', 'counting', 'sort', 'specific', 'facial', 'descending', 'order', 'second', 'count', 'value', 'represent', 'number', 'count', 'part', 'specific', 'facial', 'becomes', 'equal', 'preset', 'second', 'set', 'value', 'process', 'delete', 'uncounted', 'part', 'specific', 'facial', 'smart', 'database', 'comprise', 'step', 'c', 'manage', 'device', 'perform', 'process', 'generate', 'least', 'one', 'optimal', 'feature', 'weight', 'summation', 'one', 'feature', 'specific', 'facial', 'use', 'counted', 'part', 'quality', 'score', 'process', 'set', 'optimal', 'feature', 'representative', 'face', 'correspond', 'specific', 'person', 'wherein', 'step', 'b', 'manage', 'device', 'performs', 'process', 'inputting', 'specific', 'facial', 'cnn', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'cnn', 'generate', 'one', 'feature', 'correspond', 'specific', 'facial', 'process', 'inputting', 'least', 'one', 'feature', 'vector', 'feature', 'embed', 'aggregation', 'module', 'include', 'least', 'two', 'attention', 'block', 'thereby', 'allow', 'aggregation', 'module', 'generate', 'quality', 'score', 'feature', 'wherein', 'step', 'b', 'manage', 'device', 'performs', 'process', 'match', 'i-', 'one', 'feature', 'correspond', 'specific', 'facial', 'store', 'smart', 'database', 'i-', 'quality', 'score', 'ii', 'specific', 'person', 'process', 'store', 'match', 'feature', 'match', 'quality', 'score', 'smart', 'database', 'comprise', 'step', 'manage', 'device', 'perform', 'one', 'process', 'learn', 'face', 'system', 'use', 'specific', 'facial', 'corresponding', 'specific', 'person', 'store', 'smart', 'database', 'ii', 'process', 'transmit', 'specific', 'facial', 'corresponding', 'specific', 'person', 'learning', 'device', 'correspond', 'face', 'system', 'thereby', 'allow', 'learn', 'device', 'learn', 'face', 'system', 'use', 'specific', 'facial', 'wherein', 'neural', 'aggregation', 'network', 'learn', 'learning', 'device', 'repeat', 'process', 'inputting', 'multiple', 'facial', 'training', 'correspond', 'set', 'single', 'face', 'video', 'single', 'face', 'cnn', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'cnn', 'generate', 'one', 'feature', 'train', 'apply', 'least', 'one', 'convolution', 'operation', 'facial', 'training', 'ii', 'process', 'inputting', 'least', 'one', 'feature', 'vector', 'training', 'feature', 'train', 'embedded', 'aggregation', 'module', 'include', 'least', 'two', 'attention', 'block', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'aggregation', 'module', 'generate', 'quality', 'score', 'train', 'feature', 'train', 'aggregation', 'feature', 'train', 'use', 'one', 'attention', 'parameter', 'learn', 'previous', 'iteration', 'iii', 'process', 'output', 'least', 'one', 'optimal', 'feature', 'training', 'weighted', 'summation', 'feature', 'train', 'use', 'quality', 'score', 'train', 'iv', 'process', 'update', 'attention', 'parameter', 'learn', 'previous', 'iteration', 'least', 'two', 'attention', 'block', 'one', 'loss', 'minimize', 'outputted', 'loss', 'layer', 'refer', 'optimal', 'feature', 'train', 'correspond', 'ground', 'truth', 'manage', 'device', 'manage', 'smart', 'database', 'store', 'facial', 'face', 'comprise', 'least', 'one', 'memory', 'store', 'instruction', 'least', 'one', 'configure', 'execute', 'instruction', 'perform', 'support', 'another', 'device', 'perform', 'process', 'count', 'one', 'specific', 'facial', 'corresponding', 'least', 'one', 'specific', 'person', 'store', 'smart', 'database', 'new', 'facial', 'face', 'continuously', 'store', 'process', 'determine', 'whether', 'first', 'count', 'value', 'represent', 'count', 'specific', 'facial', 'satisfies', 'preset', 'first', 'set', 'value', 'ii', 'first', 'count', 'value', 'determine', 'satisfy', 'first', 'set', 'value', 'process', 'inputting', 'specific', 'facial', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'neural', 'aggregation', 'network', 'generate', 'quality', 'score', 'specific', 'facial', 'aggregation', 'specific', 'facial', 'process', 'sort', 'quality', 'score', 'correspond', 'specific', 'facial', 'descending', 'order', 'quality', 'score', 'process', 'counting', 'sort', 'specific', 'facial', 'descending', 'order', 'second', 'count', 'value', 'represent', 'number', 'count', 'part', 'specific', 'facial', 'becomes', 'equal', 'preset', 'second', 'set', 'value', 'process', 'delete', 'uncounted', 'part', 'specific', 'facial', 'smart', 'database', 'manage', 'device', 'wherein', 'perform', 'iii', 'process', 'generate', 'least', 'one', 'optimal', 'feature', 'weight', 'summation', 'one', 'feature', 'specific', 'facial', 'use', 'counted', 'part', 'quality', 'score', 'process', 'set', 'optimal', 'feature', 'representative', 'face', 'correspond', 'specific', 'person', 'manage', 'device', 'wherein', 'process', 'ii', 'performs', 'process', 'inputting', 'specific', 'facial', 'cnn', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'cnn', 'generate', 'one', 'feature', 'correspond', 'specific', 'facial', 'process', 'inputting', 'least', 'one', 'feature', 'vector', 'feature', 'embed', 'aggregation', 'module', 'include', 'least', 'two', 'attention', 'block', 'thereby', 'allow', 'aggregation', 'module', 'generate', 'quality', 'score', 'feature', 'manage', 'device', 'wherein', 'process', 'ii', 'performs', 'process', 'match', 'i-', 'one', 'feature', 'correspond', 'specific', 'facial', 'store', 'smart', 'database', 'i-', 'quality', 'score', 'ii', 'specific', 'person', 'process', 'store', 'match', 'feature', 'match', 'quality', 'score', 'smart', 'database', 'manage', 'device', 'wherein', 'perform', 'iv', 'one', 'process', 'learn', 'face', 'system', 'use', 'specific', 'facial', 'corresponding', 'specific', 'person', 'store', 'smart', 'database', 'ii', 'process', 'transmit', 'specific', 'facial', 'corresponding', 'specific', 'person', 'learning', 'device', 'correspond', 'face', 'system', 'thereby', 'allow', 'learn', 'device', 'learn', 'face', 'system', 'use', 'specific', 'facial', 'managing', 'device', 'wherein', 'neural', 'aggregation', 'network', 'learn', 'learning', 'device', 'repeat', 'process', 'inputting', 'multiple', 'facial', 'training', 'correspond', 'set', 'single', 'face', 'video', 'single', 'face', 'cnn', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'cnn', 'generate', 'one', 'feature', 'train', 'apply', 'least', 'one', 'convolution', 'operation', 'facial', 'training', 'ii', 'process', 'inputting', 'least', 'one', 'feature', 'vector', 'training', 'feature', 'train', 'embedded', 'aggregation', 'module', 'include', 'least', 'two', 'attention', 'block', 'neural', 'aggregation', 'network', 'thereby', 'allow', 'aggregation', 'module', 'generate', 'quality', 'score', 'train', 'feature', 'train', 'aggregation', 'feature', 'train', 'use', 'one', 'attention', 'parameter', 'learn', 'previous', 'iteration', 'iii', 'process', 'output', 'least', 'one', 'optimal', 'feature', 'training', 'weighted', 'summation', 'feature', 'train', 'use', 'quality', 'score', 'train', 'iv', 'process', 'update', 'attention', 'parameter', 'learn', 'previous', 'iteration', 'least', 'two', 'attention', 'block', 'one', 'loss', 'minimize', 'outputted', 'loss', 'layer', 'refer', 'optimal', 'feature', 'train', 'correspond', 'ground', 'truth', 'object', 'data', 'process', 'system', 'comprise', 'least', 'one', 'configure', 'execute', 'least', 'one', 'implementation', 'algorithm', 'store', 'least', 'one', 'non-transitory', 'computer-readable', 'storage', 'medium', 'algorithm', 'feature', 'density', 'selection', 'criterion', 'data', 'preprocessing', 'code', 'execute', 'least', 'one', 'data', 'preprocessing', 'code', 'comprise', 'invariant', 'feature', 'identification', 'algorithm', 'configure', 'obtain', 'digital', 'representation', 'scene', 'scene', 'comprise', 'one', 'textual', 'medium', 'generate', 'set', 'invariant', 'feature', 'apply', 'invariant', 'feature', 'identification', 'algorithm', 'digital', 'representation', 'cluster', 'set', 'invariant', 'feature', 'region', 'interest', 'digital', 'representation', 'scene', 'region', 'interest', 'region', 'feature', 'density', 'classify', 'region', 'classifier', 'code', 'least', 'one', 'region', 'interest', 'accord', 'object', 'type', 'function', 'attribute', 'derived', 'region', 'feature', 'density', 'digital', 'representation', 'wherein', 'least', 'one', 'classified', 'region', 'interest', 'corresponds', 'text', 'use', 'classification', 'result', 'correspond', 'least', 'one', 'region', 'interest', 'classify', 'another', 'region', 'interest', 'accord', 'object', 'type', 'wherein', 'another', 'region', 'interest', 'correspond', 'region', 'interest', 'system', 'wherein', 'preprocessing', 'code', 'base', 'feature', 'density', 'selection', 'criterion', 'determine', 'ocr', 'algorithm', 'applicable', 'text', 'algorithm', 'applicable', 'aspect', 'photograph', 'logos', 'system', 'wherein', 'create', 'profile', 'camera-equipped', 'smartphone', 'include', 'information', 'visually', 'impaired', 'cause', 'prioritized', 'execution', 'ocr', 'algorithm', 'text', 'reader', 'program', 'begin', 'read', 'text', 'quickly', 'possible', 'system', 'comprise', 'audio', 'tactile', 'feedback', 'mechanism', 'help', 'position', 'smart', 'phone', 'relative', 'text', 'system', 'comprise', '``', 'hold', 'still', \"''\", 'audio', 'feedback', 'signal', 'send', 'text', 'center', 'capture', 'scene', 'system', 'wherein', 'digital', 'representation', 'comprises', 'least', 'one', 'follow', 'type', 'digital', 'data', 'data', 'video', 'data', 'audio', 'data', 'system', 'wherein', 'invariant', 'feature', 'identification', 'algorithm', 'comprise', 'least', 'one', 'follow', 'feature', 'identification', 'algorithm', 'fast', 'sift', 'freak', 'brisk', 'harris', 'daisy', 'mser', 'system', 'wherein', 'invariant', 'feature', 'identification', 'algorithm', 'include', 'least', 'one', 'follow', 'edge', 'detection', 'algorithm', 'corner', 'detection', 'algorithm', 'saliency', 'map', 'algorithm', 'curve', 'detection', 'algorithm', 'texton', 'identification', 'algorithm', 'wavelet', 'algorithm', 'system', 'wherein', 'least', 'one', 'region', 'interest', 'represent', 'least', 'one', 'physical', 'object', 'scene', 'system', 'wherein', 'least', 'one', 'region', 'interest', 'represent', 'least', 'one', 'textual', 'medium', 'scene', 'system', 'wherein', 'region', 'interest', 'represent', 'document', 'textual', 'medium', 'system', 'wherein', 'region', 'interest', 'represent', 'financial', 'document', 'system', 'wherein', 'region', 'interest', 'represent', 'structure', 'document', 'system', 'wherein', 'least', 'one', 'implementation', 'algorithm', 'include', 'least', 'one', 'follow', 'template', 'drive', 'algorithm', 'face', 'algorithm', 'optical', 'character', 'algorithm', 'speech', 'algorithm', 'object', 'algorithm', 'system', 'wherein', 'data', 'preprocessing', 'code', 'configure', 'assign', 'region', 'interest', 'least', 'one', 'algorithm', 'function', 'scene', 'context', 'derive', 'digital', 'representation', 'system', 'wherein', 'scene', 'context', 'include', 'least', 'one', 'follow', 'type', 'data', 'location', 'position', 'time', 'identity', 'news', 'event', 'medical', 'event', 'promotion', 'system', 'comprise', 'mobile', 'device', 'comprise', 'least', 'one', 'implementation', 'algorithm', 'data', 'preprocessing', 'code', 'system', 'wherein', 'mobile', 'device', 'comprises', 'least', 'one', 'follow', 'smart', 'phone', 'tablet', 'wearable', 'glass', 'toy', 'vehicle', 'computer', 'phablet', 'system', 'comprise', 'network-accessible', 'server', 'device', 'comprise', 'least', 'one', 'implementation', 'algorithm', 'data', 'preprocessing', 'code', 'system', 'wherein', 'object', 'type', 'include', 'least', 'one', 'follow', 'face', 'animal', 'vehicle', 'document', 'plant', 'building', 'appliance', 'clothing', 'body', 'part', 'toy', 'object', 'data', 'processing', 'system', 'comprise', 'least', 'one', 'configure', 'execute', 'least', 'one', 'implementation', 'algorithm', 'store', 'least', 'one', 'non-transitory', 'computer-readable', 'storage', 'medium', 'algorithm', 'feature', 'density', 'selection', 'criterion', 'data', 'preprocessing', 'code', 'execute', 'least', 'one', 'data', 'preprocessing', 'code', 'comprise', 'invariant', 'feature', 'identification', 'algorithm', 'configure', 'obtain', 'digital', 'representation', 'scene', 'scene', 'comprise', 'one', 'textual', 'medium', 'generate', 'set', 'invariant', 'feature', 'apply', 'invariant', 'feature', 'identification', 'algorithm', 'digital', 'representation', 'cluster', 'set', 'invariant', 'feature', 'region', 'interest', 'digital', 'representation', 'scene', 'region', 'interest', 'region', 'feature', 'density', 'classify', 'region', 'classifier', 'code', 'least', 'one', 'region', 'interest', 'accord', 'object', 'type', 'function', 'attribute', 'derived', 'region', 'feature', 'density', 'digital', 'representation', 'wherein', 'least', 'one', 'classified', 'region', 'interest', 'corresponds', 'text', 'use', 'classification', 'result', 'correspond', 'least', 'one', 'region', 'interest', 'classify', 'another', 'region', 'interest', 'accord', 'object', 'type', 'wherein', 'another', 'region', 'interest', 'correspond', 'region', 'interest', 'assign', 'region', 'interest', 'least', 'one', 'algorithm', 'least', 'one', 'implementation', 'diverse', 'algorithm', 'function', 'region', 'feature', 'density', 'region', 'interest', 'feature', 'density', 'selection', 'criterion', 'least', 'one', 'implementation', 'diverse', 'algorithms', 'configure', 'assign', 'algorithms', 'process', 'respective', 'region', 'interest', 'wherein', 'preprocessing', 'code', 'base', 'feature', 'density', 'selection', 'criterion', 'determine', 'ocr', 'algorithm', 'applicable', 'text', 'algorithm', 'applicable', 'aspect', 'photograph', 'logos', 'device', 'comprise', 'least', 'one', 'configure', 'execute', 'least', 'one', 'implementation', 'algorithm', 'store', 'least', 'one', 'non-transitory', 'computer-readable', 'storage', 'medium', 'algorithm', 'feature', 'density', 'selection', 'criterion', 'data', 'preprocessing', 'code', 'execute', 'least', 'one', 'data', 'preprocessing', 'code', 'comprise', 'invariant', 'feature', 'identification', 'algorithm', 'configure', 'obtain', 'digital', 'representation', 'scene', 'scene', 'comprise', 'one', 'textual', 'medium', 'generate', 'set', 'invariant', 'feature', 'apply', 'invariant', 'feature', 'identification', 'algorithm', 'digital', 'representation', 'cluster', 'set', 'invariant', 'feature', 'region', 'interest', 'digital', 'representation', 'scene', 'region', 'interest', 'region', 'feature', 'density', 'classify', 'region', 'classifier', 'code', 'least', 'one', 'region', 'interest', 'accord', 'object', 'type', 'function', 'attribute', 'derived', 'region', 'feature', 'density', 'digital', 'representation', 'wherein', 'least', 'one', 'classified', 'region', 'interest', 'corresponds', 'text', 'use', 'classification', 'result', 'correspond', 'least', 'one', 'region', 'interest', 'classify', 'another', 'region', 'interest', 'accord', 'object', 'type', 'wherein', 'another', 'region', 'interest', 'correspond', 'region', 'interest', 'mobile', 'terminal', 'comprise', 'front', 'camera', 'configure', 'obtain', 'two-dimensional', 'face', 'glance', 'sensor', 'tilt', 'certain', 'angle', 'dispose', 'adjacent', 'front', 'camera', 'obtain', 'metadata', 'face', 'controller', 'obtain', 'distance', 'glance', 'sensor', 'front', 'camera', 'distance', 'enable', 'area', 'overlap', 'region', 'first', 'region', 'represent', 'range', 'photographable', 'front', 'camera', 'overlap', 'second', 'region', 'represent', 'range', 'photographable', 'glance', 'sensor', 'maximum', 'mobile', 'terminal', 'wherein', 'controller', 'configure', 'obtain', 'distance', 'enable', 'area', 'overlap', 'region', 'maximum', 'glance', 'sensor', 'front', 'camera', 'vary', 'tilt', 'angle', 'glance', 'sensor', 'mobile', 'terminal', 'wherein', 'controller', 'configure', 'set', 'distance', 'enable', 'area', 'overlap', 'region', 'maximum', 'glance', 'sensor', 'front', 'camera', 'tilt', 'angle', 'glance', 'sensor', 'optimal', 'disposition', 'location', 'glance', 'sensor', 'mobile', 'terminal', 'wherein', 'controller', 'configure', 'set', 'disposition', 'location', 'front', 'camera', 'original', 'point', 'calculates', 'coordinate', 'first', 'triangle', 'represent', 'first', 'region', 'base', 'field', 'view', 'front', 'camera', 'maximum', 'photograph', 'distance', 'front', 'camera', 'mobile', 'terminal', 'wherein', 'controller', 'configure', 'calculate', 'coordinate', 'second', 'triangle', 'represent', 'second', 'region', 'base', 'field', 'view', 'glance', 'sensor', 'maximum', 'photograph', 'distance', 'glance', 'sensor', 'distance', 'front', 'camera', 'glance', 'sensor', 'tilt', 'angle', 'glance', 'sensor', 'mobile', 'terminal', 'wherein', 'glance', 'sensor', 'tilt', 'controller', 'configured', 'calculate', 'coordinate', 'third', 'triangle', 'represent', 'third', 'region', 'photographable', 'glance', 'sensor', 'controller', 'configure', 'rotation-convert', 'coordinate', 'third', 'triangle', 'base', 'tilt', 'angle', 'glance', 'sensor', 'calculate', 'coordinate', 'second', 'triangle', 'mobile', 'terminal', 'wherein', 'controller', 'configure', 'calculate', 'coordinate', 'overlap', 'region', 'base', 'coordinate', 'first', 'triangle', 'coordinate', 'second', 'triangle', 'calculates', 'area', 'overlap', 'region', 'base', 'coordinate', 'overlap', 'region', 'mobile', 'terminal', 'wherein', 'controller', 'configure', 'generate', 'three-dimensional', 'face', 'information', 'base', 'face', 'obtain', 'front', 'camera', 'metadata', 'obtain', 'glance', 'sensor', 'mobile', 'terminal', 'wherein', 'metadata', 'comprise', 'one', 'angle', 'face', 'size', 'face', 'location', 'face', 'mobile', 'terminal', 'wherein', 'angle', 'face', 'comprises', 'angle', 'face', 'rotate', 'one', 'pitch', 'axis', 'roll', 'axis', 'yaw', 'axis', 'mobile', 'terminal', 'comprise', 'memory', 'store', 'generate', 'face', 'information', 'wherein', 'controller', 'configure', 'performs', 'authentication', 'process', 'compare', 'store', 'face', 'information', 'face', 'information', 'obtain', 'authentication', 'mobile', 'terminal', 'wherein', 'glance', 'sensor', 'control', 'permanently', 'activate', 'low', 'power', 'obtain', 'front', 'metadata', 'front', 'mobile', 'terminal', 'wherein', 'front', 'camera', 'glance', 'sensor', 'dispose', 'line', 'upper', 'end', 'mobile', 'terminal', 'mobile', 'terminal', 'wherein', 'glance', 'sensor', 'tilt', 'one', 'direction', 'direction', 'direction', 'leave', 'direction', 'right', 'direction', 'mobile', 'terminal', 'wherein', 'metadata', 'data', 'change', 'mobile', 'terminal', 'tilt', 'external', 'physical', 'force', 'comprise', 'receive', 'smart', 'television', 'tv', 'indication', 'upcoming', 'medium', 'program', 'wherein', 'upcoming', 'medium', 'program', 'base', 'profile', 'identify', 'one', 'device', 'communication', 'smart', 'tv', 'one', 'device', 'include', 'least', 'one', 'microphone', 'camera', 'instruct', 'least', 'one', 'identified', 'device', 'detect', 'audio', 'signal', 'use', 'respective', 'microphone', 'detect', 'visual', 'signal', 'use', 'respective', 'camera', 'select', 'least', 'one', 'device', 'one', 'device', 'base', 'detected', 'audio', 'signal', 'detect', 'visual', 'signal', 'provide', 'instruction', 'select', 'device', 'output', 'notification', 'relate', 'upcoming', 'medium', 'program', 'wherein', 'upcoming', 'medium', 'program', 'one', 'live', 'television', 'program', 'record', 'television', 'program', 'broadcast', 'television', 'program', 'application-provided', 'program', 'wherein', 'select', 'first', 'device', 'base', 'detected', 'audio', 'signal', 'include', 'voice', 'comprise', 'determine', 'distance', 'recognize', 'voice', 'wherein', 'select', 'first', 'device', 'base', 'determined', 'distance', 'wherein', 'select', 'first', 'device', 'base', 'detect', 'visual', 'signal', 'include', 'face', 'wherein', 'face', 'include', 'face', 'technique', 'comprise', 'present', 'smart', 'tv', 'upcoming', 'medium', 'program', 'favorite', 'channel', 'list', 'comprise', 'obtain', 'medium', 'program', 'view', 'data', 'wherein', 'medium', 'program', 'view', 'data', 'include', 'least', 'one', 'historical', 'time', 'historical', 'date', 'one', 'medium', 'program', 'view', 'obtain', 'least', 'one', 'current', 'time', 'current', 'date', 'processing', 'medium', 'program', 'view', 'data', 'determine', 'probability', 'one', 'medium', 'program', 'view', 'base', 'least', 'one', 'current', 'time', 'current', 'date', 'presenting', 'favorite', 'channel', 'list', 'base', 'determined', 'probability', 'one', 'medium', 'program', 'view', 'wherein', 'processing', 'medium', 'program', 'view', 'data', 'include', 'employ', 'neural', 'network', 'model', 'wherein', 'employ', 'neural', 'network', 'model', 'comprise', 'determine', 'duration', 'one', 'medium', 'program', 'view', 'least', 'one', 'historical', 'time', 'historical', 'date', 'set', 'threshold', 'time', 'duration', 'compare', 'determine', 'duration', 'threshold', 'time', 'duration', 'filter', 'one', 'medium', 'program', 'view', 'threshold', 'time', 'duration', 'smart', 'television', 'tv', 'comprise', 'network', 'interface', 'non-transitory', 'computer-readable', 'medium', 'communication', 'network', 'interface', 'non-transitory', 'computer-readable', 'medium', 'capable', 'execute', '-executable', 'program', 'code', 'store', 'non-transitory', 'computer-readable', 'medium', 'cause', 'smart', 'tv', 'receive', 'indication', 'upcoming', 'medium', 'program', 'wherein', 'upcoming', 'medium', 'program', 'base', 'profile', 'identify', 'one', 'device', 'communication', 'smart', 'tv', 'one', 'device', 'include', 'least', 'one', 'microphone', 'camera', 'instruct', 'least', 'one', 'identified', 'device', 'detect', 'audio', 'signal', 'use', 'respective', 'microphone', 'detect', 'visual', 'signal', 'use', 'respective', 'camera', 'select', 'least', 'one', 'device', 'one', 'device', 'base', 'detected', 'audio', 'signal', 'detect', 'visual', 'signal', 'provide', 'instruction', 'select', 'device', 'output', 'notification', 'relate', 'upcoming', 'medium', 'program', 'smart', 'tv', 'wherein', 'select', 'first', 'device', 'base', 'detected', 'audio', 'signal', 'include', 'voice', 'smart', 'tv', 'wherein', 'capable', 'execute', '-executable', 'program', 'code', 'determine', 'distance', 'recognize', 'voice', 'wherein', 'select', 'first', 'device', 'base', 'determined', 'distance', 'smart', 'tv', 'wherein', 'select', 'first', 'device', 'base', 'detect', 'visual', 'signal', 'include', 'detect', 'presence', 'smart', 'tv', 'wherein', 'detect', 'presence', 'include', 'employ', 'one', 'camera', 'microphone', 'fingerprint', 'sensor', 'associate', 'least', 'one', 'smart', 'tv', 'mobile', 'device', 'smartphone', 'laptop', 'computer', 'tablet', 'device', 'wearable', 'device', 'internet', 'thing', 'iot', 'device', 'internet', 'everything', 'ioe', 'device', 'iot', 'hub', 'ioe', 'hub', 'smart', 'television', 'tv', 'comprising', 'mean', 'receive', 'indication', 'upcoming', 'medium', 'program', 'wherein', 'upcoming', 'medium', 'program', 'base', 'profile', 'mean', 'identify', 'one', 'device', 'communication', 'smart', 'tv', 'one', 'device', 'include', 'least', 'one', 'microphone', 'camera', 'mean', 'instruct', 'least', 'one', 'identified', 'device', 'detect', 'audio', 'signal', 'use', 'respective', 'microphone', 'detect', 'visual', 'signal', 'use', 'respective', 'camera', 'mean', 'select', 'least', 'one', 'device', 'one', 'device', 'base', 'detected', 'audio', 'signal', 'detect', 'visual', 'signal', 'mean', 'provide', 'instruction', 'select', 'device', 'output', 'notification', 'relate', 'upcoming', 'medium', 'program', 'smart', 'tv', 'wherein', 'one', 'device', 'include', 'least', 'one', 'mobile', 'device', 'smartphone', 'laptop', 'computer', 'tablet', 'device', 'wearable', 'device', 'internet', 'thing', 'iot', 'device', 'internet', 'everything', 'ioe', 'device', 'iot', 'hub', 'ioe', 'hub', 'another', 'smart', 'tv', 'smart', 'tv', 'wherein', 'upcoming', 'medium', 'program', 'one', 'live', 'television', 'program', 'record', 'television', 'program', 'broadcast', 'television', 'program', 'application-provided', 'program', 'smart', 'tv', 'wherein', 'notification', 'include', 'least', 'one', 'push', 'message', 'sms', 'message', 'waysms', 'message', 'audio', 'alert', 'audio', 'message', 'email', 'message', 'smart', 'tv', 'comprise', 'present', 'upcoming', 'medium', 'program', 'favorite', 'channel', 'list', 'smart', 'tv', 'comprising', 'mean', 'obtain', 'medium', 'program', 'view', 'data', 'wherein', 'medium', 'program', 'view', 'data', 'include', 'least', 'one', 'historical', 'time', 'historical', 'date', 'one', 'medium', 'program', 'view', 'smart', 'tv', 'mean', 'obtain', 'least', 'one', 'current', 'time', 'current', 'date', 'mean', 'process', 'medium', 'program', 'view', 'data', 'determine', 'probability', 'one', 'medium', 'program', 'view', 'smart', 'tv', 'base', 'least', 'one', 'current', 'time', 'current', 'date', 'mean', 'present', 'favorite', 'channel', 'list', 'base', 'determined', 'probability', 'one', 'medium', 'program', 'view', 'smart', 'tv', 'wherein', 'mean', 'process', 'medium', 'program', 'view', 'data', 'include', 'employ', 'neural', 'network', 'model', 'smart', 'tv', 'wherein', 'employ', 'neural', 'network', 'model', 'comprise', 'determine', 'duration', 'one', 'medium', 'program', 'view', 'smart', 'tv', 'least', 'one', 'historical', 'time', 'historical', 'date', 'set', 'threshold', 'time', 'duration', 'compare', 'determine', 'duration', 'threshold', 'time', 'duration', 'filter', 'one', 'medium', 'program', 'view', 'threshold', 'time', 'duration', 'smart', 'tv', 'comprising', 'mean', 'adjust', 'least', 'one', 'volume', 'brightness', 'smart', 'tv', 'wherein', 'adjust', 'base', 'least', 'one', 'historical', 'time', 'historical', 'date', 'smart', 'tv', 'comprising', 'mean', 'restrict', 'access', 'one', 'medium', 'program', 'non-transitory', 'computer-readable', 'medium', 'comprise', '-executable', 'program', 'code', 'configure', 'cause', 'smart', 'television', 'tv', 'receive', 'indication', 'upcoming', 'medium', 'program', 'wherein', 'upcoming', 'medium', 'program', 'base', 'profile', 'identify', 'one', 'device', 'communication', 'smart', 'tv', 'one', 'device', 'include', 'least', 'one', 'microphone', 'camera', 'instruct', 'least', 'one', 'identified', 'device', 'detect', 'audio', 'signal', 'use', 'respective', 'microphone', 'detect', 'visual', 'signal', 'use', 'respective', 'camera', 'select', 'least', 'one', 'device', 'one', 'device', 'base', 'detected', 'audio', 'signal', 'detect', 'visual', 'signal', 'provide', 'instruction', 'select', 'device', 'output', 'notification', 'relate', 'upcoming', 'medium', 'program', 'non-transitory', 'computer-readable', 'medium', 'wherein', 'select', 'first', 'device', 'base', 'detected', 'audio', 'signal', 'include', 'voice', 'non-transitory', 'computer-readable', 'medium', 'wherein', 'capable', 'execute', '-executable', 'program', 'code', 'determine', 'distance', 'recognize', 'voice', 'wherein', 'select', 'first', 'device', 'base', 'determined', 'distance', 'non-transitory', 'computer-readable', 'medium', 'wherein', 'select', 'first', 'device', 'base', 'detect', 'visual', 'signal', 'include', 'face', 'non-transitory', 'computer-readable', 'medium', 'wherein', 'face', 'include', 'face', 'technique', 'camera', 'comprise', 'sensor', 'array', 'include', 'sensor', 'infrared', 'ir', 'illuminator', 'configure', 'emit', 'active', 'ir', 'light', 'ir', 'light', 'sub-band', 'spectral', 'illuminators', 'spectral', 'illuminator', 'configure', 'emit', 'active', 'spectral', 'light', 'different', 'spectral', 'light', 'sub-band', 'depth', 'controller', 'machine', 'configure', 'determine', 'depth', 'value', 'sensor', 'base', 'active', 'ir', 'light', 'spectral', 'controller', 'machine', 'configure', 'sensor', 'determine', 'spectral', 'value', 'spectral', 'light', 'sub-band', 'spectral', 'illuminators', 'output', 'machine', 'configure', 'output', 'test', 'depth+multi-spectral', 'include', 'pixel', 'pixel', 'correspond', 'one', 'sensor', 'sensor', 'array', 'include', 'least', 'depth', 'value', 'spectral', 'value', 'spectral', 'light', 'sub-band', 'spectral', 'illuminators', 'face', 'machine', 'previously', 'train', 'set', 'labeled', 'training', 'depth+multi-spectral', 'structure', 'test', 'depth+multi-spectral', 'face', 'machine', 'configure', 'output', 'confidence', 'value', 'indicate', 'likelihood', 'test', 'depth+multi-spectral', 'include', 'face', 'camera', 'wherein', 'spectral', 'value', 'calculate', 'base', 'depth', 'value', 'determine', 'sensor', 'corresponds', 'pixel', 'camera', 'wherein', 'face', 'machine', 'configure', 'use', 'convolutional', 'neural', 'network', 'determine', 'confidence', 'value', 'camera', 'wherein', 'face', 'machine', 'include', 'input', 'node', 'wherein', 'input', 'node', 'configure', 'receive', 'pixel', 'value', 'array', 'correspond', 'different', 'pixel', 'pixel', 'test', 'depth+multi-spectral', 'wherein', 'pixel', 'value', 'array', 'include', 'depth', 'value', 'multi-spectral', 'value', 'pixel', 'camera', 'wherein', 'multi-spectral', 'value', 'pixel', 'include', 'three', 'spectral', 'value', 'camera', 'wherein', 'output', 'machine', 'configure', 'output', 'surface', 'normal', 'pixel', 'test', 'depth+multi-spectral', 'wherein', 'pixel', 'value', 'array', 'include', 'surface', 'normal', 'camera', 'wherein', 'output', 'machine', 'configure', 'output', 'curvature', 'pixel', 'test', 'depth+multi-spectral', 'wherein', 'pixel', 'value', 'array', 'include', 'curvature', 'camera', 'wherein', 'face', 'machine', 'configure', 'use', 'model', 'determine', 'confidence', 'value', 'wherein', 'model', 'include', 'channel-specific', 'model', 'wherein', 'channel-specific', 'model', 'configure', 'process', 'different', 'pixel', 'parameter', 'pixel', 'test', 'depth+multi-spectral', 'wherein', 'channel-specific', 'model', 'include', 'input', 'node', 'wherein', 'channel-specific', 'model', 'input', 'node', 'configure', 'receive', 'pixel', 'parameter', 'value', 'different', 'pixel', 'pixel', 'test', 'depth+multi-spectral', 'camera', 'wherein', 'face', 'machine', 'configure', 'use', 'statistical', 'model', 'determine', 'confidence', 'value', 'camera', 'wherein', 'statistical', 'model', 'include', 'near', 'neighbor', 'algorithm', 'camera', 'wherein', 'statistical', 'model', 'include', 'support', 'vector', 'machine', 'camera', 'wherein', 'face', 'machine', 'configure', 'output', 'location', 'test', 'depth+multi-spectral', 'bounding', 'box', 'around', 'recognize', 'face', 'camera', 'wherein', 'face', 'machine', 'configure', 'output', 'location', 'test', 'depth+multi-spectral', 'identify', 'two-dimensional', 'facial', 'feature', 'recognize', 'face', 'camera', 'wherein', 'face', 'machine', 'configure', 'output', 'location', 'test', 'depth+multi-spectral', 'identify', 'three-dimensional', 'facial', 'feature', 'recognize', 'face', 'camera', 'wherein', 'face', 'machine', 'configure', 'output', 'location', 'test', 'depth+multi-spectral', 'identified', 'spectral', 'feature', 'recognize', 'face', 'camera', 'wherein', 'face', 'machine', 'configure', 'output', 'pixel', 'test', 'depth+multi-spectral', 'confidence', 'value', 'indicate', 'likelihood', 'pixel', 'include', 'face', 'camera', 'wherein', 'face', 'machine', 'configure', 'output', 'identity', 'face', 'recognize', 'test', 'depth+multi-spectral', 'camera', 'wherein', 'sensor', 'sensor', 'array', 'differential', 'sensor', 'wherein', 'spectral', 'value', 'determine', 'base', 'depth', 'value', 'differential', 'measurement', 'differential', 'sensor', 'camera', 'comprise', 'sensor', 'array', 'include', 'sensor', 'infrared', 'ir', 'illuminator', 'configure', 'emit', 'active', 'ir', 'light', 'ir', 'light', 'sub-band', 'spectral', 'illuminators', 'spectral', 'illuminator', 'configure', 'emit', 'active', 'spectral', 'light', 'different', 'spectral', 'light', 'sub-band', 'depth', 'controller', 'machine', 'configure', 'determine', 'depth', 'value', 'sensor', 'base', 'active', 'ir', 'light', 'spectral', 'controller', 'machine', 'configure', 'sensor', 'determine', 'spectral', 'value', 'spectral', 'light', 'sub-band', 'spectral', 'illuminators', 'wherein', 'spectral', 'value', 'calculate', 'base', 'depth', 'value', 'determine', 'sensor', 'corresponds', 'pixel', 'output', 'machine', 'configure', 'output', 'test', 'depth+multi-spectral', 'include', 'pixel', 'pixel', 'correspond', 'one', 'sensor', 'sensor', 'array', 'include', 'least', 'depth', 'value', 'spectral', 'value', 'spectral', 'light', 'sub-band', 'spectral', 'illuminators', 'face', 'machine', 'include', 'convolutional', 'neural', 'network', 'previously', 'train', 'set', 'labeled', 'training', 'depth+multi-spectral', 'structure', 'test', 'depth+multi-spectral', 'face', 'machine', 'configure', 'output', 'confidence', 'value', 'indicate', 'likelihood', 'test', 'depth+multi-spectral', 'include', 'face', 'process', 'comprise', 'acquire', 'photo', 'album', 'obtain', 'face', 'cluster', 'collect', 'face', 'information', 'respective', 'photo', 'album', 'acquire', 'face', 'parameter', 'accord', 'face', 'information', 'select', 'cover', 'accord', 'face', 'parameter', 'take', 'face-region', 'cover', 'set', 'face-region', 'cover', 'photo', 'album', 'wherein', 'select', 'cover', 'accord', 'face', 'parameter', 'comprise', 'perform', 'calculation', 'face', 'parameter', 'preset', 'way', 'obtain', 'cover', 'score', 'select', 'high', 'cover', 'score', 'cover', 'wherein', 'select', 'high', 'cover', 'score', 'cover', 'comprise', 'acquire', 'source', 'select', 'high', 'cover', 'score', 'come', 'preset', 'source', 'cover', 'accord', 'wherein', 'select', 'high', 'cover', 'score', 'cover', 'comprise', 'acquire', 'number', 'contain', 'determine', 'single-person', 'accord', 'number', 'select', 'single-person', 'high', 'cover', 'score', 'cover', 'accord', 'wherein', 'select', 'high', 'cover', 'score', 'cover', 'comprise', 'single-person', 'photo', 'album', 'determine', 'include', 'two', 'photo', 'album', 'select', 'high', 'cover', 'score', 'include', 'two', 'cover', 'accord', 'wherein', 'face', 'information', 'comprise', 'face', 'feature', 'point', 'face', 'parameter', 'comprises', 'face', 'turn', 'angle', 'acquire', 'face', 'parameter', 'accord', 'face', 'information', 'comprise', 'acquire', 'coordinate', 'value', 'face', 'feature', 'point', 'determine', 'distance', 'angle', 'face', 'feature', 'point', 'determine', 'face', 'turn', 'angle', 'accord', 'distance', 'angle', 'accord', 'wherein', 'face', 'parameter', 'comprise', 'face', 'ratio', 'acquire', 'face', 'parameter', 'accord', 'face', 'information', 'comprise', 'determine', 'face', 'region', 'accord', 'face', 'information', 'calculate', 'ratio', 'area', 'face', 'region', 'area', 'obtain', 'face', 'ratio', 'accord', 'wherein', 'calculate', 'face', 'ratio', 'comprise', 'one', 'face', 'subtract', 'area', 'occupy', 'face', 'correspond', 'photo', 'album', 'face', 'region', 'obtain', 'remain', 'area', 'calculate', 'ratio', 'remain', 'area', 'area', 'obtain', 'face', 'ratio', 'accord', 'wherein', 'collect', 'face', 'information', 'respective', 'photo', 'album', 'comprise', 'acquire', 'identification', 'photo', 'album', 'extract', 'face', 'information', 'correspond', 'identification', 'face', 'database', 'face', 'database', 'store', 'face', 'result', 'face', 'result', 'include', 'face', 'information', 'processing', 'apparatus', 'comprise', 'memory', 'configure', 'store', 'instruction', 'executable', 'wherein', 'configure', 'run', 'program', 'corresponding', 'instruction', 'read', 'instruction', 'store', 'memory', 'perform', 'acquire', 'photo', 'album', 'obtain', 'face', 'cluster', 'collect', 'face', 'information', 'photo', 'album', 'acquire', 'face', 'parameter', 'accord', 'face', 'information', 'select', 'cover', 'accord', 'face', 'parameter', 'take', 'face-region', 'cover', 'set', 'face-region', 'cover', 'photo', 'album', 'wherein', 'configure', 'perform', 'calculation', 'face', 'parameter', 'preset', 'way', 'obtain', 'cover', 'score', 'select', 'high', 'cover', 'score', 'cover', 'wherein', 'configure', 'acquire', 'source', 'select', 'high', 'cover', 'score', 'come', 'preset', 'source', 'cover', 'apparatus', 'accord', 'wherein', 'configure', 'acquire', 'number', 'contain', 'determine', 'single-person', 'accord', 'number', 'select', 'single-person', 'high', 'cover', 'score', 'cover', 'apparatus', 'accord', 'wherein', 'configure', 'single-person', 'photo', 'album', 'determine', 'include', 'two', 'photo', 'album', 'select', 'high', 'cover', 'score', 'include', 'two', 'cover', 'apparatus', 'accord', 'wherein', 'face', 'information', 'comprise', 'face', 'feature', 'point', 'face', 'parameter', 'comprises', 'face', 'turn', 'angle', 'configure', 'acquire', 'coordinate', 'value', 'face', 'feature', 'point', 'determine', 'distance', 'angle', 'face', 'feature', 'point', 'determine', 'face', 'turn', 'angle', 'accord', 'distance', 'angle', 'apparatus', 'accord', 'wherein', 'face', 'parameter', 'comprise', 'face', 'ratio', 'configure', 'determine', 'face', 'region', 'accord', 'face', 'information', 'calculate', 'ratio', 'area', 'face', 'region', 'area', 'obtain', 'face', 'ratio', 'apparatus', 'accord', 'wherein', 'configure', 'one', 'face', 'subtract', 'area', 'occupy', 'face', 'correspond', 'photo', 'album', 'face', 'region', 'obtain', 'remain', 'area', 'calculate', 'ratio', 'remain', 'area', 'area', 'obtain', 'face', 'ratio', 'apparatus', 'accord', 'wherein', 'configure', 'acquire', 'identification', 'photo', 'album', 'extract', 'face', 'information', 'correspond', 'identification', 'face', 'database', 'face', 'database', 'store', 'face', 'result', 'face', 'result', 'include', 'face', 'information', 'comprise', 'memory', 'display', 'screen', 'input', 'device', 'connect', 'via', 'system', 'bus', 'wherein', 'memory', 'store', 'computer', 'program', 'execute', 'cause', 'implement', 'process', 'process', 'comprise', 'acquire', 'photo', 'album', 'obtain', 'face', 'cluster', 'collect', 'face', 'information', 'respective', 'photo', 'album', 'acquire', 'face', 'parameter', 'accord', 'face', 'information', 'select', 'cover', 'accord', 'face', 'parameter', 'take', 'face-region', 'cover', 'set', 'face-region', 'cover', 'photo', 'album', 'wherein', 'select', 'cover', 'accord', 'face', 'parameter', 'comprise', 'perform', 'calculation', 'face', 'parameter', 'preset', 'way', 'obtain', 'cover', 'score', 'select', 'high', 'cover', 'score', 'cover', 'wherein', 'select', 'high', 'cover', 'score', 'cover', 'comprise', 'acquire', 'source', 'select', 'high', 'cover', 'score', 'come', 'preset', 'source', 'cover', 'accord', 'wherein', 'comprises', 'least', 'one', 'mobile', 'phone', 'tablet', 'computer', 'personal', 'digital', 'assistant', 'wearable', 'device', 'computer-implemented', 'comprising', 'receive', 'compute', 'device', 'meeting', 'invitation', 'identify', 'location', 'least', 'one', 'invitee', 'meeting', 'invitation', 'configure', 'provide', 'least', 'one', 'invitee', 'physical', 'access', 'location', 'wherein', 'meeting', 'invitation', 'cause', 'system', 'control', 'pathway', 'allow', 'physical', 'access', 'location', 'provide', 'base', 'meet', 'invitation', 'least', 'one', 'invitee', 'physical', 'access', 'location', 'control', 'pathway', 'allow', 'least', 'one', 'invitee', 'physically', 'access', 'location', 'pathway', 'response', 'position', 'data', 'indicate', 'least', 'one', 'invitee', 'predetermine', 'location', 'near', 'location', 'wherein', 'position', 'data', 'base', 'part', 'face', 'camera', 'system', 'identify', 'least', 'one', 'invitee', 'receive', 'position', 'data', 'face', 'camera', 'system', 'identify', 'least', 'one', 'invitee', 'wherein', 'position', 'data', 'indicate', 'pattern', 'movement', 'least', 'one', 'invitee', 'determine', 'pattern', 'movement', 'indicate', 'least', 'one', 'invitee', 'exit', 'location', 'revoke', 'physical', 'access', 'location', 'identify', 'meet', 'invitation', 'control', 'pathway', 'restrict', 'least', 'one', 'invitee', 'identify', 'meet', 'invitation', 'physical', 'access', 'location', 'pathway', 'response', 'determine', 'pattern', 'movement', 'indicate', 'least', 'one', 'invitee', 'exit', 'location', 'computer-implemented', 'wherein', 'determine', 'least', 'one', 'invitee', 'exit', 'location', 'comprise', 'determine', 'least', 'one', 'invitee', 'pass', 'egress', 'associate', 'location', 'predetermine', 'direction', 'computer-implemented', 'wherein', 'determine', 'least', 'one', 'invitee', 'exit', 'location', 'comprise', 'determine', 'least', 'one', 'invitee', 'move', 'area', 'predetermine', 'direction', 'computer-implemented', 'wherein', 'position', 'data', 'indicate', 'second', 'pattern', 'movement', 'least', 'one', 'invitee', 'wherein', 'access', 'secure', 'data', 'associate', 'location', 'provide', 'response', 'detect', 'second', 'pattern', 'movement', 'computer-implemented', 'comprise', 'collating', 'secure', 'data', 'public', 'data', 'generate', 'resource', 'data', 'communicate', 'resource', 'data', 'client', 'compute', 'device', 'associate', 'least', 'one', 'invitee', 'access', 'location', 'provide', 'computer-implemented', 'wherein', 'position', 'data', 'indicate', 'least', 'one', 'invitee', 'predetermine', 'location', 'least', 'one', 'invitee', 'pass', 'predetermine', 'location', 'computer-implemented', 'wherein', 'position', 'data', 'indicate', 'least', 'one', 'invitee', 'predetermine', 'location', 'least', 'one', 'invitee', 'pass', 'predetermine', 'location', 'near', 'location', 'predetermine', 'direction', 'system', 'comprise', 'memory', 'communication', 'memory', 'computer-readable', 'instruction', 'store', 'thereupon', 'execute', 'cause', 'receive', 'meeting', 'invitation', 'indicate', 'location', 'identity', 'meeting', 'invitation', 'configure', 'provide', 'least', 'one', 'invitee', 'physical', 'access', 'location', 'wherein', 'meeting', 'invitation', 'cause', 'system', 'control', 'pathway', 'allow', 'physical', 'access', 'location', 'provide', 'least', 'one', 'invitee', 'associate', 'identity', 'access', 'location', 'control', 'pathway', 'allow', 'least', 'one', 'invitee', 'physically', 'access', 'location', 'pathway', 'response', 'position', 'data', 'indicate', 'least', 'one', 'invitee', 'predetermine', 'location', 'near', 'location', 'wherein', 'position', 'data', 'base', 'part', 'face', 'camera', 'system', 'identify', 'least', 'one', 'invitee', 'receive', 'position', 'data', 'face', 'camera', 'system', 'identify', 'least', 'one', 'invitee', 'wherein', 'position', 'data', 'indicate', 'pattern', 'movement', 'least', 'one', 'invitee', 'determine', 'pattern', 'movement', 'indicate', 'least', 'one', 'invitee', 'exit', 'location', 'revoke', 'physical', 'access', 'location', 'identify', 'meet', 'invitation', 'control', 'pathway', 'restrict', 'least', 'one', 'invitee', 'identify', 'meet', 'invitation', 'physical', 'access', 'location', 'pathway', 'response', 'determine', 'pattern', 'movement', 'indicate', 'least', 'one', 'invitee', 'exit', 'location', 'system', 'wherein', 'determine', 'least', 'one', 'invitee', 'exit', 'location', 'comprise', 'determine', 'least', 'one', 'invitee', 'pass', 'egress', 'associate', 'location', 'system', 'wherein', 'determine', 'least', 'one', 'invitee', 'exit', 'location', 'comprise', 'determine', 'least', 'one', 'invitee', 'move', 'area', 'predetermine', 'direction', 'system', 'wherein', 'position', 'data', 'indicate', 'second', 'pattern', 'movement', 'least', 'one', 'invitee', 'wherein', 'access', 'secure', 'data', 'associate', 'location', 'provide', 'response', 'detect', 'second', 'pattern', 'movement', 'system', 'wherein', 'instruction', 'cause', 'collate', 'secure', 'data', 'public', 'data', 'generate', 'resource', 'data', 'communicate', 'resource', 'data', 'client', 'compute', 'device', 'associate', 'least', 'one', 'invitee', 'access', 'location', 'provide', 'non-transitory', 'computer-readable', 'storage', 'medium', 'computer-executable', 'instruction', 'store', 'thereupon', 'execute', 'one', 'compute', 'device', 'cause', 'one', 'computing', 'device', 'receive', 'meeting', 'invitation', 'indicate', 'location', 'identity', 'meeting', 'invitation', 'configure', 'provide', 'least', 'one', 'invitee', 'physical', 'access', 'location', 'wherein', 'meeting', 'invitation', 'cause', 'system', 'control', 'pathway', 'allow', 'physical', 'access', 'location', 'provide', 'least', 'one', 'invitee', 'associate', 'identity', 'access', 'location', 'control', 'pathway', 'allow', 'least', 'one', 'invitee', 'physically', 'access', 'location', 'pathway', 'response', 'position', 'data', 'indicate', 'least', 'one', 'invitee', 'predetermine', 'location', 'near', 'location', 'wherein', 'position', 'data', 'base', 'part', 'face', 'camera', 'system', 'identify', 'least', 'one', 'invitee', 'receive', 'position', 'data', 'face', 'camera', 'system', 'identify', 'least', 'one', 'invitee', 'wherein', 'position', 'data', 'indicate', 'pattern', 'movement', 'least', 'one', 'invitee', 'determine', 'pattern', 'movement', 'indicate', 'least', 'one', 'invitee', 'exit', 'location', 'revoke', 'physical', 'access', 'location', 'identify', 'meet', 'invitation', 'control', 'pathway', 'restrict', 'least', 'one', 'invitee', 'identify', 'meet', 'invitation', 'physical', 'access', 'location', 'pathway', 'response', 'determine', 'pattern', 'movement', 'indicate', 'least', 'one', 'invitee', 'exit', 'location', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'determine', 'least', 'one', 'invitee', 'exit', 'location', 'comprise', 'determine', 'least', 'one', 'invitee', 'pass', 'egress', 'associate', 'location', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'position', 'data', 'indicate', 'second', 'pattern', 'movement', 'least', 'one', 'invitee', 'wherein', 'access', 'secure', 'data', 'associate', 'location', 'provide', 'response', 'detect', 'second', 'pattern', 'movement', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'instruction', 'cause', 'one', 'collate', 'secure', 'data', 'public', 'data', 'generate', 'resource', 'data', 'communicate', 'resource', 'data', 'client', 'compute', 'device', 'associate', 'least', 'one', 'invitee', 'access', 'location', 'provide', 'comprise', 'receive', 'piece', 'content', 'salient', 'data', 'piece', 'content', 'base', 'salient', 'data', 'determine', 'first', 'path', 'viewport', 'piece', 'content', 'wherein', 'first', 'path', 'viewport', 'include', 'different', 'salient', 'event', 'occur', 'piece', 'content', 'different', 'time', 'playback', 'piece', 'content', 'provide', 'viewport', 'display', 'device', 'wherein', 'movement', 'viewport', 'base', 'first', 'path', 'viewport', 'salient', 'data', 'playback', 'detect', 'additional', 'salient', 'event', 'piece', 'content', 'include', 'first', 'path', 'viewport', 'provide', 'indication', 'additional', 'salient', 'event', 'viewport', 'playback', 'wherein', 'salient', 'data', 'identifies', 'salient', 'event', 'piece', 'content', 'salient', 'data', 'indicate', 'salient', 'event', 'piece', 'content', 'correspond', 'point', 'location', 'salient', 'event', 'piece', 'content', 'correspond', 'time', 'salient', 'event', 'occur', 'playback', 'wherein', 'salient', 'data', 'indicate', 'salient', 'event', 'piece', 'content', 'correspond', 'type', 'salient', 'event', 'correspond', 'strength', 'value', 'salient', 'event', 'wherein', 'first', 'path', 'viewport', 'control', 'movement', 'viewport', 'put', 'different', 'salient', 'event', 'view', 'viewport', 'different', 'time', 'playback', 'comprise', 'detect', 'one', 'salient', 'event', 'piece', 'content', 'base', 'least', 'one', 'follow', 'visual', 'data', 'piece', 'content', 'audio', 'data', 'piece', 'content', 'content', 'consumption', 'experience', 'data', 'piece', 'content', 'wherein', 'salient', 'data', 'indicative', 'salient', 'event', 'detect', 'comprise', 'detect', 'one', 'salient', 'event', 'piece', 'content', 'base', 'least', 'one', 'follow', 'face', 'facial', 'emotion', 'object', 'motion', 'metadata', 'piece', 'content', 'wherein', 'salient', 'data', 'indicative', 'salient', 'event', 'detect', 'comprise', 'detect', 'interaction', 'indication', 'wherein', 'indication', 'comprise', 'interactive', 'hint', 'response', 'detect', 'interaction', 'adapt', 'first', 'path', 'viewport', 'second', 'path', 'viewport', 'base', 'interaction', 'wherein', 'second', 'path', 'viewport', 'include', 'additional', 'salient', 'event', 'provide', 'updated', 'viewport', 'piece', 'content', 'display', 'device', 'wherein', 'movement', 'update', 'viewport', 'base', 'second', 'path', 'viewport', 'salient', 'data', 'playback', 'second', 'path', 'viewport', 'control', 'movement', 'update', 'viewport', 'put', 'additional', 'salient', 'event', 'view', 'update', 'viewport', 'comprise', 'change', 'weight', 'assign', 'additional', 'salient', 'event', 'one', 'salient', 'event', 'piece', 'content', 'type', 'additional', 'salient', 'event', 'wherein', 'second', 'path', 'viewport', 'include', 'one', 'salient', 'event', 'piece', 'content', 'type', 'additional', 'salient', 'event', 'system', 'comprise', 'least', 'one', 'non-transitory', '-readable', 'memory', 'device', 'store', 'instruction', 'execute', 'least', 'one', 'cause', 'least', 'one', 'perform', 'operation', 'include', 'receive', 'piece', 'content', 'salient', 'data', 'piece', 'content', 'base', 'salient', 'data', 'determine', 'first', 'path', 'viewport', 'piece', 'content', 'wherein', 'first', 'path', 'viewport', 'include', 'different', 'salient', 'event', 'occur', 'piece', 'content', 'different', 'time', 'playback', 'piece', 'content', 'provide', 'viewport', 'display', 'device', 'wherein', 'movement', 'viewport', 'base', 'first', 'path', 'viewport', 'salient', 'data', 'playback', 'detect', 'additional', 'salient', 'event', 'piece', 'content', 'include', 'first', 'path', 'viewport', 'provide', 'indication', 'additional', 'salient', 'event', 'viewport', 'playback', 'system', 'wherein', 'salient', 'data', 'identifies', 'salient', 'event', 'piece', 'content', 'salient', 'data', 'indicate', 'salient', 'event', 'piece', 'content', 'correspond', 'point', 'location', 'salient', 'event', 'piece', 'content', 'correspond', 'time', 'salient', 'event', 'occur', 'playback', 'system', 'wherein', 'salient', 'data', 'indicate', 'salient', 'event', 'piece', 'content', 'correspond', 'type', 'salient', 'event', 'correspond', 'strength', 'value', 'salient', 'event', 'system', 'wherein', 'salient', 'data', 'generate', 'offline', 'server', 'system', 'operation', 'comprise', 'detect', 'one', 'salient', 'event', 'piece', 'content', 'base', 'least', 'one', 'follow', 'visual', 'data', 'piece', 'content', 'audio', 'data', 'piece', 'content', 'content', 'consumption', 'experience', 'data', 'piece', 'content', 'wherein', 'salient', 'data', 'indicative', 'salient', 'event', 'detect', 'system', 'operation', 'comprise', 'detect', 'one', 'salient', 'event', 'piece', 'content', 'base', 'least', 'one', 'follow', 'face', 'facial', 'emotion', 'object', 'motion', 'metadata', 'piece', 'content', 'wherein', 'salient', 'data', 'indicative', 'salient', 'event', 'detect', 'system', 'operation', 'comprise', 'detect', 'interaction', 'indication', 'wherein', 'indication', 'comprise', 'interactive', 'hint', 'response', 'detect', 'interaction', 'adapt', 'first', 'path', 'viewport', 'second', 'path', 'viewport', 'base', 'interaction', 'wherein', 'second', 'path', 'viewport', 'include', 'additional', 'salient', 'event', 'provide', 'updated', 'viewport', 'piece', 'content', 'display', 'device', 'wherein', 'movement', 'update', 'viewport', 'base', 'second', 'path', 'viewport', 'salient', 'data', 'playback', 'second', 'path', 'viewport', 'control', 'movement', 'update', 'viewport', 'put', 'additional', 'salient', 'event', 'view', 'update', 'viewport', 'system', 'operation', 'comprise', 'change', 'weight', 'assign', 'additional', 'salient', 'event', 'one', 'salient', 'event', 'piece', 'content', 'type', 'additional', 'salient', 'event', 'system', 'wherein', 'second', 'path', 'viewport', 'include', 'one', 'salient', 'event', 'piece', 'content', 'type', 'additional', 'salient', 'event', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'include', 'instruction', 'perform', 'comprise', 'receive', 'piece', 'content', 'salient', 'data', 'piece', 'content', 'base', 'salient', 'data', 'determine', 'first', 'path', 'viewport', 'piece', 'content', 'wherein', 'first', 'path', 'viewport', 'include', 'different', 'salient', 'event', 'occur', 'piece', 'content', 'different', 'time', 'playback', 'piece', 'content', 'provide', 'viewport', 'display', 'device', 'wherein', 'movement', 'viewport', 'base', 'first', 'path', 'viewport', 'salient', 'data', 'playback', 'detect', 'additional', 'salient', 'event', 'piece', 'content', 'include', 'first', 'path', 'viewport', 'provide', 'indication', 'additional', 'salient', 'event', 'viewport', 'playback', 'computer', 'readable', 'storage', 'medium', 'comprise', 'detect', 'interaction', 'indication', 'wherein', 'indication', 'comprise', 'interactive', 'hint', 'response', 'detect', 'interaction', 'adapt', 'first', 'path', 'viewport', 'second', 'path', 'viewport', 'base', 'interaction', 'wherein', 'second', 'path', 'viewport', 'include', 'additional', 'salient', 'event', 'provide', 'updated', 'viewport', 'piece', 'content', 'display', 'device', 'wherein', 'movement', 'update', 'viewport', 'base', 'second', 'path', 'viewport', 'salient', 'data', 'playback', 'second', 'path', 'viewport', 'control', 'movement', 'update', 'viewport', 'put', 'additional', 'salient', 'event', 'view', 'update', 'viewport', 'mobile', 'device', 'facial', 'mobile', 'device', 'comprise', 'one', 'cameras', 'device', 'memory', 'couple', 'device', 'processing', 'system', 'program', 'receive', 'one', 'camera', 'extract', 'feature', 'extractor', 'utilizing', 'convolutional', 'neural', 'network', 'cnn', 'enlarge', 'intra-class', 'variance', 'long-tail', 'class', 'feature', 'vector', 'generate', 'feature', 'generator', 'discriminative', 'feature', 'vector', 'feature', 'vector', 'classify', 'fully', 'connect', 'classifier', 'identity', 'discriminative', 'feature', 'vector', 'control', 'operation', 'mobile', 'device', 'react', 'accordance', 'identity', 'mobile', 'device', 'recite', 'include', 'communication', 'system', 'mobile', 'device', 'recite', 'wherein', 'operation', 'tag', 'video', 'identity', 'uploads', 'video', 'social', 'medium', 'mobile', 'device', 'recite', 'wherein', 'operation', 'tag', 'video', 'identity', 'sends', 'video', 'mobile', 'device', 'recite', 'wherein', 'mobile', 'device', 'smart', 'phone', 'mobile', 'device', 'recite', 'wherein', 'mobile', 'device', 'body', 'cam', 'mobile', 'device', 'recite', 'programmed', 'train', 'feature', 'extractor', 'feature', 'generator', 'fully', 'connect', 'classifier', 'alternative', 'bi-stage', 'strategy', 'mobile', 'device', 'recite', 'wherein', 'feature', 'extractor', 'share', 'covariance', 'matrix', 'across', 'class', 'transfer', 'intra-class', 'variance', 'regular', 'class', 'long-tail', 'class', 'mobile', 'device', 'recite', 'wherein', 'feature', 'generator', 'optimize', 'softmax', 'loss', 'joint', 'regularization', 'weight', 'feature', 'magnitude', 'inner', 'product', 'weight', 'feature', 'mobile', 'device', 'recite', 'wherein', 'feature', 'extractor', 'average', 'feature', 'vector', 'flip', 'feature', 'vector', 'flip', 'feature', 'vector', 'generate', 'horizontally', 'flip', 'frame', 'one', 'mobile', 'device', 'recite', 'wherein', 'select', 'group', 'consist', 'video', 'frame', 'video', 'mobile', 'device', 'recite', 'wherein', 'communication', 'system', 'connect', 'remote', 'server', 'include', 'facial', 'network', 'mobile', 'device', 'recite', 'wherein', 'one', 'stage', 'alternative', 'bi-stage', 'strategy', 'fix', 'feature', 'extractor', 'applies', 'feature', 'generator', 'generate', 'new', 'transfer', 'feature', 'diverse', 'violate', 'decision', 'boundary', 'mobile', 'device', 'recite', 'wherein', 'one', 'stage', 'alternative', 'bi-stage', 'strategy', 'fix', 'fully', 'connect', 'classifier', 'updates', 'feature', 'extractor', 'feature', 'generator', 'computer', 'program', 'product', 'mobile', 'device', 'facial', 'computer', 'program', 'product', 'comprise', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'program', 'instruction', 'embody', 'therewith', 'program', 'instruction', 'executable', 'computer', 'cause', 'computer', 'perform', 'comprise', 'receive', 'device', 'extract', 'device', 'feature', 'extractor', 'utilizing', 'convolutional', 'neural', 'network', 'cnn', 'enlarge', 'intra-class', 'variance', 'long-tail', 'class', 'feature', 'vector', 'generate', 'device', 'feature', 'generator', 'discriminative', 'feature', 'vector', 'feature', 'vector', 'classify', 'device', 'utilize', 'fully', 'connect', 'classifier', 'identity', 'discriminative', 'feature', 'vector', 'control', 'operation', 'mobile', 'device', 'react', 'accordance', 'identity', 'computer-implemented', 'facial', 'mobile', 'device', 'comprise', 'receive', 'device', 'extract', 'device', 'feature', 'extractor', 'utilizing', 'convolutional', 'neural', 'network', 'cnn', 'enlarge', 'intra-class', 'variance', 'long-tail', 'class', 'feature', 'vector', 'generate', 'device', 'feature', 'generator', 'discriminative', 'feature', 'vector', 'feature', 'vector', 'classify', 'device', 'utilize', 'fully', 'connect', 'classifier', 'identity', 'discriminative', 'feature', 'vector', 'control', 'operation', 'mobile', 'device', 'react', 'accordance', 'identity', 'computer-implemented', 'recite', 'wherein', 'control', 'include', 'tag', 'video', 'identity', 'upload', 'video', 'social', 'medium', 'computer-implemented', 'recite', 'wherein', 'control', 'include', 'tag', 'video', 'identity', 'send', 'video', 'computer-implemented', 'recite', 'wherein', 'extract', 'include', 'share', 'covariance', 'matrix', 'across', 'class', 'transfer', 'intra-class', 'variance', 'regular', 'class', 'long-tail', 'class', 'compute', 'device', 'comprise', 'non-transitory', 'machine', 'readable', 'medium', 'store', 'machine', 'train', 'mt', 'network', 'comprise', 'layer', 'process', 'node', 'process', 'node', 'configure', 'compute', 'first', 'output', 'value', 'combine', 'set', 'output', 'value', 'set', 'process', 'node', 'use', 'piecewise', 'linear', 'cup', 'function', 'compute', 'second', 'output', 'value', 'first', 'output', 'value', 'process', 'node', 'wherein', 'piecewise', 'linear', 'cup', 'function', 'prior', 'train', 'mt', 'network', 'comprises', 'least', 'first', 'linear', 'section', 'first', 'slope', 'follow', 'ii', 'second', 'linear', 'section', 'negative', 'second', 'slope', 'follow', 'iii', 'third', 'linear', 'section', 'negative', 'third', 'slope', 'different', 'second', 'slope', 'follow', 'iv', 'fourth', 'linear', 'section', 'positive', 'fourth', 'slope', 'follow', 'v', 'fifth', 'linear', 'section', 'positive', 'fifth', 'slope', 'different', 'fourth', 'slope', 'follow', 'vi', 'sixth', 'linear', 'section', 'sixth', 'slope', 'wherein', 'piecewise', 'linear', 'cup', 'function', 'symmetric', 'vertical', 'axis', 'third', 'fourth', 'linear', 'section', 'prior', 'train', 'mt', 'network', 'content', 'capturing', 'circuit', 'capture', 'content', 'processing', 'mt', 'network', 'set', 'process', 'unit', 'execute', 'process', 'node', 'process', 'content', 'capture', 'content', 'capture', 'circuit', 'wherein', 'training', 'set', 'parameter', 'define', 'piecewise', 'linear', 'cup', 'function', 'node', 'first', 'second', 'plurality', 'process', 'node', 'process', 'node', 'first', 'process', 'node', 'configured', 'emulate', 'boolean', 'operator', 'output', 'value', 'processing', 'node', 'range', 'associate', '``', \"''\", 'value', 'set', 'input', 'process', 'node', 'set', 'value', 'range', 'associate', '``', \"''\", 'ii', 'processing', 'node', 'second', 'processing', 'node', 'configure', 'emulate', 'boolean', 'xnor', 'operator', 'output', 'value', 'processing', 'node', 'range', 'associate', '``', \"''\", 'set', 'input', 'node', 'set', 'value', 'range', 'associate', '``', \"''\", 'b', 'set', 'input', 'node', 'set', 'value', 'range', 'associate', '``', \"''\", 'value', 'compute', 'device', 'wherein', 'third', 'linear', 'section', 'piecewise', 'linear', 'cup', 'function', 'first', 'process', 'node', 'mt', 'network', 'different', 'slope', 'third', 'linear', 'section', 'second', 'processing', 'node', 'mt', 'network', 'compute', 'device', 'wherein', 'length', 'third', 'section', 'piecewise', 'linear', 'cup', 'function', 'first', 'process', 'node', 'mt', 'network', 'different', 'length', 'third', 'section', 'piecewise', 'linear', 'cup', 'function', 'second', 'processing', 'node', 'mt', 'network', 'compute', 'device', 'wherein', 'set', 'parameter', 'train', 'part', 'back', 'propagate', 'module', 'back', 'propagating', 'error', 'output', 'value', 'later', 'layer', 'process', 'node', 'earlier', 'layer', 'process', 'node', 'adjust', 'set', 'parameter', 'define', 'piecewise', 'linear', 'cup', 'function', 'earlier', 'layer', 'process', 'node', 'compute', 'device', 'wherein', 'processing', 'node', 'us', 'linear', 'function', 'define', 'set', 'parameter', 'compute', 'first', 'output', 'value', 'process', 'node', 'wherein', 'back', 'propagate', 'module', 'back', 'propagate', 'error', 'output', 'value', 'later', 'layer', 'process', 'node', 'earlier', 'layer', 'process', 'node', 'adjust', 'set', 'parameter', 'define', 'linear', 'function', 'earlier', 'layer', 'process', 'node', 'compute', 'device', 'wherein', 'first', 'processing', 'node', 'emulate', 'boolean', 'operator', 'second', 'processing', 'node', 'emulate', 'boolean', 'operator', 'enable', 'mt', 'network', 'implement', 'mathematical', 'problem', 'compute', 'device', 'wherein', 'process', 'node', 'layer', 'process', 'node', 'receive', 'input', 'value', 'output', 'value', 'process', 'node', 'set', 'prior', 'layer', 'compute', 'device', 'wherein', 'processing', 'node', 'us', 'linear', 'function', 'compute', 'first', 'output', 'value', 'process', 'node', 'wherein', 'process', 'node', \"'s\", 'piecewise', 'linear', 'cup', 'function', 'define', 'along', 'first', 'second', 'ax', 'first', 'axis', 'define', 'range', 'output', 'value', 'process', 'node', \"'s\", 'linear', 'function', 'second', 'axis', 'define', 'range', 'output', 'value', 'produce', 'piecewise', 'linear', 'cup', 'function', 'range', 'output', 'value', 'process', 'node', \"'s\", 'linear', 'function', 'compute', 'device', 'comprise', 'content', 'output', 'circuit', 'present', 'output', 'base', 'processing', 'content', 'mt', 'network', 'compute', 'device', 'wherein', 'capture', 'content', 'one', 'audio', 'segment', 'wherein', 'present', 'output', 'output', 'display', 'display', 'screen', 'compute', 'device', 'audio', 'presentation', 'output', 'speaker', 'compute', 'device', 'compute', 'device', 'wherein', 'compute', 'device', 'mobile', 'device', 'compute', 'device', 'wherein', 'mt', 'network', 'mt', 'neural', 'network', 'process', 'node', 'mt', 'neuron', 'compute', 'device', 'wherein', 'set', 'parameter', 'configure', 'train', 'process', 'node', 'comprise', 'least', 'one', 'negative', 'second', 'third', 'slope', 'second', 'third', 'linear', 'section', 'positive', 'fourth', 'fifth', 'slope', 'fourth', 'fifth', 'linear', 'section', 'first', 'intercept', 'second', 'linear', 'section', 'second', 'intercept', 'fifth', 'linear', 'section', 'set', 'length', 'least', 'second', 'third', 'fourth', 'fifth', 'section', 'compute', 'device', 'wherein', 'train', 'set', 'parameter', 'define', 'piecewise', 'linear', 'cup', 'function', 'node', 'comprise', 'output', 'value', 'compute', 'device', 'wherein', 'first', 'sixth', 'slope', 'zerowe', 'system', 'comprise', 'memory', 'device', 'store', 'input', 'include', 'input', 'interface', 'receive', 'input', 'pre-', 'model', 'input', 'yield', 'multi-channel', 'feature', 'extractor', 'extract', 'set', 'feature', 'base', 'multi-channel', 'feature', 'selector', 'select', 'one', 'feature', 'set', 'feature', 'multi-channel', 'wherein', 'one', 'feature', 'select', 'base', 'ability', 'differentiate', 'feature', 'feature', 'matcher', 'match', 'one', 'feature', 'learn', 'feature', 'set', 'similarity', 'detector', 'determine', 'whether', 'one', 'feature', 'meet', 'pre-defined', 'similarity', 'threshold', 'system', 'wherein', 'pre-', 'activate', 'one', 'channel', 'multi-channel', 'yield', 'one', 'activate', 'channel', 'system', 'wherein', 'one', 'activate', 'channel', 'determine', 'base', 'ability', 'differentiate', 'feature', 'system', 'wherein', 'pre-', 'activate', 'one', 'local', 'patch', 'one', 'activate', 'channel', 'system', 'wherein', 'one', 'local', 'patch', 'determine', 'base', 'ability', 'differentiate', 'feature', 'system', 'wherein', 'feature', 'matcher', 'utilize', 'large-scale', 'data', 'learn', 'process', 'perform', 'feature', 'match', 'apparatus', 'comprise', 'input', 'interface', 'receive', 'input', 'pre-', 'model', 'input', 'yield', 'multi-channel', 'feature', 'extractor', 'extract', 'set', 'feature', 'base', 'multi-channel', 'feature', 'selector', 'select', 'one', 'feature', 'set', 'feature', 'multi-channel', 'wherein', 'one', 'feature', 'select', 'base', 'ability', 'differentiate', 'feature', 'feature', 'matcher', 'match', 'one', 'feature', 'learn', 'feature', 'set', 'similarity', 'detector', 'determine', 'whether', 'one', 'feature', 'meet', 'pre-defined', 'similarity', 'threshold', 'apparatus', 'wherein', 'pre-', 'activate', 'one', 'channel', 'multi-channel', 'yield', 'one', 'activate', 'channel', 'apparatus', 'wherein', 'one', 'activated', 'channel', 'determine', 'base', 'ability', 'differentiate', 'feature', 'apparatus', 'wherein', 'pre-', 'activate', 'one', 'local', 'patch', 'one', 'activated', 'channel', 'apparatus', 'wherein', 'one', 'local', 'patch', 'determine', 'base', 'ability', 'differentiate', 'feature', 'apparatus', 'wherein', 'feature', 'matcher', 'utilize', 'large-scale', 'data', 'learn', 'process', 'perform', 'feature', 'match', 'comprise', 'model', 'input', 'yield', 'multi-channel', 'extract', 'set', 'feature', 'base', 'multi-channel', 'select', 'one', 'feature', 'set', 'feature', 'multi-channel', 'wherein', 'one', 'feature', 'select', 'base', 'ability', 'differentiate', 'feature', 'match', 'one', 'feature', 'learn', 'feature', 'set', 'determine', 'whether', 'one', 'feature', 'meet', 'pre-defined', 'similarity', 'threshold', 'wherein', 'model', 'input', 'include', 'activate', 'one', 'channel', 'multi-channel', 'yield', 'one', 'activate', 'channel', 'wherein', 'one', 'activate', 'channel', 'determine', 'base', 'ability', 'differentiate', 'feature', 'wherein', 'extract', 'feature', 'input', 'include', 'activate', 'one', 'local', 'patch', 'one', 'activated', 'channel', 'wherein', 'one', 'local', 'patch', 'determine', 'base', 'ability', 'differentiate', 'feature', 'wherein', 'feature', 'matcher', 'utilizes', 'large-scale', 'data', 'learn', 'process', 'perform', 'feature', 'match', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'comprise', 'set', 'instruction', 'execute', 'compute', 'device', 'cause', 'compute', 'device', 'model', 'input', 'yield', 'multi-channel', 'extract', 'set', 'feature', 'base', 'multi-channel', 'select', 'one', 'feature', 'set', 'feature', 'multi-channel', 'wherein', 'feature', 'select', 'base', 'ability', 'differentiate', 'feature', 'match', 'one', 'feature', 'learn', 'feature', 'set', 'determine', 'whether', 'one', 'feature', 'meet', 'pre-defined', 'similarity', 'threshold', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'wherein', 'instruction', 'execute', 'cause', 'compute', 'device', 'activate', 'one', 'channel', 'multi-channel', 'yield', 'one', 'activate', 'channel', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'wherein', 'instruction', 'execute', 'cause', 'compute', 'device', 'determine', 'one', 'activate', 'channel', 'base', 'ability', 'differentiate', 'feature', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'wherein', 'extract', 'feature', 'input', 'include', 'activate', 'one', 'local', 'patch', 'one', 'activated', 'channel', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'wherein', 'one', 'local', 'patch', 'determine', 'base', 'ability', 'differentiate', 'feature', 'least', 'one', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'wherein', 'feature', 'matcher', 'utilize', 'large-scale', 'data', 'learn', 'process', 'perform', 'feature', 'match', 'apparatus', 'comprise', 'mean', 'model', 'input', 'yield', 'multi-channel', 'mean', 'extract', 'set', 'feature', 'base', 'multi-channel', 'mean', 'select', 'one', 'feature', 'set', 'feature', 'multi-channel', 'wherein', 'one', 'feature', 'select', 'base', 'ability', 'differentiate', 'feature', 'mean', 'match', 'one', 'feature', 'learn', 'feature', 'set', 'mean', 'determine', 'whether', 'one', 'feature', 'meet', 'pre-defined', 'similarity', 'threshold', 'control', 'terminal', 'terminal', 'comprise', 'capture', 'apparatus', 'least', 'one', 'comprise', 'acquire', 'capture', 'apparatus', 'obtain', 'least', 'one', 'motion', 'parameter', 'terminal', 'motion', 'parameter', 'comprise', 'least', 'one', 'motion', 'frequency', 'motion', 'time', 'two', 'parameter', 'among', 'acceleration', 'angular', 'velocity', 'motion', 'amplitude', 'motion', 'frequency', 'motion', 'time', 'transmit', 'least', 'one', 'parameter', 'threshold', 'obtain', 'request', 'data', 'management', 'server', 'parameter', 'threshold', 'obtain', 'request', 'comprise', 'configuration', 'information', 'terminal', 'receiving', 'correspond', 'preset', 'threshold', 'correspond', 'configuration', 'information', 'response', 'parameter', 'threshold', 'obtain', 'request', 'compare', 'two', 'parameter', 'correspond', 'preset', 'threshold', 'control', 'least', 'one', 'perform', 'processing', 'acquire', 'base', 'least', 'one', 'two', 'parameter', 'motion', 'parameter', 'great', 'correspond', 'preset', 'threshold', 'base', 'two', 'parameter', 'motion', 'parameter', 'respectively', 'great', 'correspond', 'preset', 'threshold', 'wherein', 'acquire', 'comprises', 'acquire', 'real', 'time', 'obtain', 'comprises', 'obtain', 'motion', 'parameter', 'terminal', 'real', 'time', 'comprise', 'response', 'least', 'one', 'two', 'parameter', 'motion', 'parameter', 'great', 'correspond', 'preset', 'threshold', 'obtain', 'motion', 'parameter', 'terminal', 'response', 'two', 'parameter', 'motion', 'parameter', 'obtain', 'late', 'time', 'le', 'equal', 'corresponding', 'preset', 'threshold', 'perform', 'processing', 'acquire', 'late', 'time', 'accord', 'wherein', 'acquire', 'comprises', 'control', 'least', 'one', 'turn', 'capture', 'apparatus', 'base', 'face', 'instruction', 'acquire', 'capture', 'apparatus', 'face', 'capture', 'apparatus', 'turn', 'accord', 'wherein', 'control', 'perform', 'processing', 'comprise', 'skip', 'perform', 'face', 'acquire', 'face', 'base', 'least', 'one', 'two', 'parameter', 'motion', 'parameter', 'great', 'correspond', 'preset', 'threshold', 'base', 'two', 'parameter', 'motion', 'parameter', 'respectively', 'great', 'correspond', 'preset', 'threshold', 'accord', 'wherein', 'obtain', 'comprises', 'least', 'one', 'obtain', 'acceleration', 'terminal', 'use', 'acceleration', 'sensor', 'obtain', 'angular', 'velocity', 'terminal', 'use', 'gyro', 'sensor', 'accord', 'wherein', 'transmitting', 'comprises', 'transmit', 'parameter', 'threshold', 'obtain', 'request', 'data', 'management', 'server', 'accord', 'preset', 'time', 'period', 'accord', 'comprise', 'generate', 'prompt', 'information', 'base', 'least', 'one', 'two', 'parameter', 'motion', 'parameter', 'great', 'correspond', 'preset', 'threshold', 'prompt', 'information', 'use', 'prompt', 'terminal', 'stop', 'move', 'accord', 'wherein', 'motion', 'parameter', 'comprise', 'motion', 'frequency', 'motion', 'time', 'terminal', 'comprise', 'capture', 'apparatus', 'least', 'one', 'memory', 'configure', 'store', 'program', 'code', 'least', 'one', 'configured', 'access', 'least', 'one', 'memory', 'operate', 'accord', 'program', 'code', 'program', 'code', 'comprise', 'motion', 'parameter', 'obtain', 'code', 'configure', 'cause', 'least', 'one', 'acquire', 'use', 'capture', 'apparatus', 'obtain', 'motion', 'parameter', 'terminal', 'motion', 'parameter', 'comprise', 'least', 'one', 'motion', 'frequency', 'motion', 'time', 'two', 'parameter', 'among', 'acceleration', 'angular', 'velocity', 'motion', 'amplitude', 'motion', 'frequency', 'motion', 'time', 'request', 'transmit', 'code', 'configure', 'cause', 'least', 'one', 'transmit', 'parameter', 'threshold', 'obtain', 'request', 'data', 'management', 'server', 'parameter', 'threshold', 'obtain', 'request', 'comprise', 'configuration', 'information', 'terminal', 'parameter', 'threshold', 'receive', 'code', 'configure', 'cause', 'least', 'one', 'receive', 'corresponding', 'preset', 'threshold', 'correspond', 'configuration', 'information', 'response', 'parameter', 'threshold', 'obtain', 'request', 'compare', 'code', 'configure', 'cause', 'least', 'one', 'compare', 'two', 'parameter', 'correspond', 'preset', 'threshold', 'control', 'code', 'configure', 'cause', 'least', 'one', 'perform', 'processing', 'acquire', 'base', 'least', 'one', 'two', 'parameter', 'motion', 'parameter', 'great', 'correspond', 'preset', 'threshold', 'base', 'two', 'parameter', 'motion', 'parameter', 'respectively', 'great', 'correspond', 'preset', 'threshold', 'wherein', 'motion', 'parameter', 'obtain', 'code', 'cause', 'least', 'one', 'acquire', 'real', 'time', 'obtain', 'motion', 'parameter', 'terminal', 'real', 'time', 'response', 'least', 'one', 'two', 'parameter', 'motion', 'parameter', 'great', 'correspond', 'preset', 'threshold', 'obtain', 'motion', 'parameter', 'terminal', 'wherein', 'control', 'code', 'cause', 'least', 'one', 'response', 'two', 'parameter', 'motion', 'parameter', 'obtain', 'late', 'time', 'le', 'equal', 'corresponding', 'preset', 'threshold', 'perform', 'process', 'acquire', 'late', 'time', 'terminal', 'accord', 'wherein', 'program', 'code', 'comprise', 'face', 'instruction', 'receive', 'code', 'configure', 'cause', 'least', 'one', 'receive', 'face', 'instruction', 'wherein', 'motion', 'parameter', 'obtain', 'code', 'cause', 'least', 'one', 'control', 'accord', 'face', 'instruction', 'capture', 'apparatus', 'turn', 'acquire', 'face', 'use', 'capture', 'apparatus', 'capture', 'apparatus', 'turn', 'wherein', 'control', 'code', 'cause', 'least', 'one', 'skip', 'perform', 'face', 'acquire', 'face', 'base', 'least', 'one', 'two', 'parameter', 'motion', 'parameter', 'great', 'correspond', 'preset', 'threshold', 'base', 'two', 'parameter', 'motion', 'parameter', 'respectively', 'great', 'correspond', 'preset', 'threshold', 'terminal', 'accord', 'wherein', 'request', 'transmit', 'code', 'cause', 'least', 'one', 'transmit', 'parameter', 'threshold', 'obtain', 'request', 'data', 'management', 'server', 'accord', 'preset', 'time', 'period', 'terminal', 'accord', 'wherein', 'program', 'code', 'comprise', 'prompt', 'information', 'generation', 'code', 'configure', 'cause', 'least', 'one', 'generate', 'prompt', 'information', 'base', 'least', 'one', 'two', 'parameter', 'motion', 'parameter', 'great', 'correspond', 'preset', 'threshold', 'prompt', 'information', 'use', 'prompt', 'terminal', 'stop', 'move', 'terminal', 'accord', 'wherein', 'motion', 'parameter', 'comprise', 'motion', 'frequency', 'motion', 'time', 'non-transitory', 'computer-readable', 'storage', 'medium', 'store', 'machine', 'instruction', 'execute', 'one', 'cause', 'one', 'perform', 'obtain', 'acquire', 'capture', 'apparatus', 'obtain', 'motion', 'parameter', 'terminal', 'terminal', 'comprise', 'capture', 'apparatus', 'motion', 'parameter', 'comprise', 'least', 'one', 'motion', 'frequency', 'motion', 'time', 'two', 'parameter', 'among', 'acceleration', 'angular', 'velocity', 'motion', 'amplitude', 'motion', 'frequency', 'motion', 'time', 'transmit', 'parameter', 'threshold', 'obtain', 'request', 'data', 'management', 'server', 'parameter', 'threshold', 'obtain', 'request', 'comprise', 'configuration', 'information', 'terminal', 'receiving', 'correspond', 'preset', 'threshold', 'correspond', 'configuration', 'information', 'response', 'parameter', 'threshold', 'obtain', 'request', 'compare', 'two', 'parameter', 'correspond', 'preset', 'threshold', 'control', 'perform', 'processing', 'acquire', 'base', 'least', 'one', 'two', 'parameter', 'motion', 'parameter', 'great', 'correspond', 'preset', 'threshold', 'base', 'two', 'parameter', 'motion', 'parameter', 'respectively', 'great', 'correspond', 'preset', 'threshold', 'wherein', 'acquire', 'comprises', 'acquire', 'real', 'time', 'obtain', 'comprises', 'obtain', 'motion', 'parameter', 'terminal', 'real', 'time', 'comprise', 'response', 'least', 'one', 'two', 'parameter', 'motion', 'parameter', 'great', 'correspond', 'preset', 'threshold', 'obtain', 'motion', 'parameter', 'terminal', 'response', 'two', 'parameter', 'motion', 'parameter', 'obtain', 'late', 'time', 'le', 'equal', 'corresponding', 'preset', 'threshold', 'perform', 'processing', 'acquire', 'late', 'time', 'non-transitory', 'computer-readable', 'storage', 'medium', 'accord', 'wherein', 'acquire', 'face', 'processing', 'comprise', 'perform', 'face', 'non-transitory', 'computer-readable', 'storage', 'medium', 'accord', 'wherein', 'obtain', 'motion', 'parameter', 'comprise', 'least', 'one', 'obtain', 'acceleration', 'terminal', 'use', 'acceleration', 'sensor', 'obtain', 'angular', 'velocity', 'terminal', 'use', 'gyro', 'sensor', 'non-transitory', 'computer-readable', 'storage', 'medium', 'accord', 'wherein', 'motion', 'parameter', 'comprise', 'motion', 'frequency', 'motion', 'time', 'process', 'drive-through', 'order', 'comprise', 'receive', 'customer', 'information', 'detect', 'vision', 'provide', 'product', 'information', 'customer', 'base', 'customer', 'information', 'process', 'product', 'order', 'customer', 'accord', 'wherein', 'receive', 'customer', 'information', 'comprise', 'least', 'one', 'receive', 'customer', 'information', 'associate', 'vehicle', 'information', 'detect', 'vehicle', 'receive', 'customer', 'information', 'associate', 'identification', 'information', 'detect', 'face', 'accord', 'comprise', 'determine', 'whether', 'customer', 'pre-order', 'customer', 'base', 'customer', 'information', 'wherein', 'customer', 'determine', 'pre-order', 'customer', 'provide', 'product', 'information', 'base', 'customer', 'information', 'comprise', 'provide', 'pre-order', 'information', 'use', 'least', 'one', 'audio', 'video', 'process', 'product', 'order', 'customer', 'comprise', 'provide', 'information', 'promptly', 'guide', 'vehicle', 'pickup', 'stand', 'use', 'least', 'one', 'audio', 'video', 'provide', 'information', 'additional', 'order', 'available', 'accord', 'wherein', 'product', 'information', 'base', 'customer', 'information', 'comprise', 'recently', 'order', 'product', 'component', 'frequently', 'order', 'product', 'component', 'order', 'history', 'customer', 'information', 'accord', 'wherein', 'receive', 'customer', 'information', 'comprise', 'receive', 'information', 'age', 'gender', 'passenger', 'detect', 'face', 'provide', 'product', 'information', 'customer', 'base', 'customer', 'information', 'comprise', 'provide', 'recommend', 'menu', 'information', 'differentiate', 'accord', 'age', 'gender', 'accord', 'wherein', 'processing', 'product', 'order', 'customer', 'comprise', 'determine', 'product', 'component', 'past', 'order', 'history', 'component', 'modify', 'product', 'component', 'product', 'order', 'accord', 'wherein', 'processing', 'product', 'order', 'customer', 'comprise', 'pay', 'product', 'price', 'accord', 'biometrics-based', 'authentication', 'communication', 'system', 'vehicle', 'mobile', 'terminal', 'accord', 'wherein', 'processing', 'product', 'order', 'customer', 'comprise', 'issue', 'payment', 'number', 'divide', 'payment', 'perform', 'divide', 'payment', 'accord', 'payment', 'request', 'mobile', 'terminal', 'payment', 'number', 'inputted', 'accord', 'wherein', 'processing', 'product', 'order', 'customer', 'comprise', 'accumulate', 'mileage', 'account', 'correspond', 'mobile', 'terminal', 'undergoing', 'payment', 'accord', 'wherein', 'processing', 'product', 'order', 'customer', 'comprise', 'suggest', 'takeout', 'package', 'accord', 'temperature', 'product', 'atmospheric', 'temperature', 'weather', 'vehicle', 'type', 'apparatus', 'configure', 'process', 'drive-through', 'order', 'apparatus', 'comprise', 'transceiver', 'configure', 'receive', 'customer', 'information', 'detect', 'vision', 'digital', 'signage', 'configure', 'provide', 'product', 'information', 'customer', 'base', 'customer', 'information', 'configure', 'process', 'product', 'order', 'customer', 'apparatus', 'accord', 'wherein', 'transceiver', 'receive', 'least', 'one', 'customer', 'information', 'associate', 'vehicle', 'information', 'detect', 'vehicle', 'customer', 'information', 'associate', 'identification', 'information', 'detect', 'face', 'apparatus', 'accord', 'wherein', 'configure', 'determine', 'whether', 'customer', 'pre-order', 'customer', 'base', 'customer', 'information', 'customer', 'determine', 'pre-order', 'customer', 'perform', 'control', 'operation', 'provide', 'pre-order', 'information', 'control', 'digital', 'signage', 'output', 'information', 'promptly', 'guide', 'vehicle', 'pickup', 'stand', 'provide', 'information', 'additional', 'order', 'available', 'apparatus', 'accord', 'wherein', 'product', 'information', 'base', 'customer', 'information', 'comprise', 'recently', 'order', 'product', 'component', 'frequently', 'order', 'product', 'component', 'order', 'history', 'customer', 'information', 'apparatus', 'accord', 'wherein', 'transceiver', 'configure', 'receive', 'information', 'age', 'gender', 'passenger', 'detect', 'face', 'configure', 'control', 'digital', 'signage', 'provide', 'recommend', 'menu', 'information', 'differentiate', 'accord', 'age', 'gender', 'apparatus', 'accord', 'wherein', 'configure', 'determine', 'product', 'component', 'past', 'order', 'history', 'component', 'modify', 'product', 'component', 'product', 'order', 'apparatus', 'accord', 'wherein', 'configured', 'pay', 'product', 'price', 'accord', 'biometrics-based', 'authentication', 'communication', 'system', 'vehicle', 'mobile', 'terminal', 'apparatus', 'accord', 'wherein', 'configure', 'issue', 'payment', 'number', 'divide', 'payment', 'perform', 'divide', 'payment', 'accord', 'request', 'mobile', 'terminal', 'payment', 'number', 'inputted', 'apparatus', 'accord', 'wherein', 'configure', 'accumulate', 'mileage', 'account', 'correspond', 'mobile', 'terminal', 'undergoing', 'payment', 'apparatus', 'accord', 'wherein', 'configure', 'control', 'digital', 'signage', 'suggest', 'takeout', 'package', 'accord', 'temperature', 'product', 'atmospheric', 'temperature', 'weather', 'vehicle', 'type', 'information', 'processing', 'perform', 'compute', 'device', 'one', 'memory', 'store', 'program', 'execute', 'one', 'comprise', 'identify', 'use', 'face', 'one', 'face', 'correspond', 'respective', 'person', 'capture', 'first', 'identify', 'face', 'extract', 'set', 'profile', 'parameter', 'correspond', 'person', 'first', 'select', 'tile', 'first', 'tile', 'match', 'face', 'correspond', 'person', 'first', 'accordance', 'predefined', 'correspondence', 'set', 'profile', 'parameter', 'correspond', 'person', 'set', 'pre-stored', 'description', 'parameter', 'first', 'tile', 'generate', 'second', 'cover', 'respective', 'person', 'first', 'correspond', 'first', 'tile', 'share', 'first', 'second', 'predefined', 'order', 'via', 'group', 'chat', 'session', 'wherein', 'first', 'second', 'display', 'group', 'chat', 'session', 'one', 'time', 'one', 'two', 'replace', 'two', 'periodically', 'wherein', 'extract', 'set', 'profile', 'parameter', 'correspond', 'person', 'first', 'include', 'determine', 'one', 'descriptive', 'label', 'correspond', 'identified', 'face', 'correspond', 'person', 'use', 'first', 'machine', 'learn', 'model', 'wherein', 'first', 'machine', 'learn', 'model', 'train', 'facial', 'correspond', 'descriptive', 'label', 'wherein', 'extract', 'set', 'profile', 'parameter', 'correspond', 'person', 'first', 'include', 'determine', 'identity', 'correspond', 'person', 'base', 'identified', 'face', 'correspond', 'person', 'locate', 'respective', 'profile', 'information', 'first', 'person', 'base', 'determine', 'identity', 'correspond', 'person', 'use', 'one', 'characteristic', 'respective', 'profile', 'information', 'first', 'person', 'set', 'profile', 'parameter', 'correspond', 'identified', 'face', 'correspond', 'person', 'wherein', 'least', 'first', 'one', 'first', 'tile', 'dynamic', 'tile', 'least', 'second', 'one', 'first', 'tile', 'static', 'tile', 'include', 'receive', 'comment', 'different', 'group', 'chat', 'session', 'comment', 'include', 'descriptive', 'term', 'respective', 'person', 'identify', 'first', 'choose', 'descriptive', 'label', 'respective', 'person', 'accord', 'comment', 'update', 'second', 'add', 'descriptive', 'label', 'adjacent', 'first', 'tile', 'respective', 'person', 'compute', 'device', 'information', 'process', 'comprise', 'one', 'memory', 'store', 'instruction', 'execute', 'one', 'cause', 'perform', 'operation', 'comprise', 'identify', 'use', 'face', 'one', 'face', 'correspond', 'respective', 'person', 'capture', 'first', 'identify', 'face', 'extract', 'set', 'profile', 'parameter', 'correspond', 'person', 'first', 'select', 'tile', 'first', 'tile', 'match', 'face', 'correspond', 'person', 'first', 'accordance', 'predefined', 'correspondence', 'set', 'profile', 'parameter', 'correspond', 'person', 'set', 'pre-stored', 'description', 'parameter', 'first', 'tile', 'generate', 'second', 'cover', 'respective', 'person', 'first', 'correspond', 'first', 'tile', 'share', 'first', 'second', 'predefined', 'order', 'via', 'group', 'chat', 'session', 'compute', 'device', 'wherein', 'first', 'second', 'display', 'group', 'chat', 'session', 'one', 'time', 'one', 'two', 'replace', 'two', 'periodically', 'compute', 'device', 'wherein', 'extract', 'set', 'profile', 'parameter', 'correspond', 'person', 'first', 'include', 'determine', 'one', 'descriptive', 'label', 'correspond', 'identified', 'face', 'correspond', 'person', 'use', 'first', 'machine', 'learn', 'model', 'wherein', 'first', 'machine', 'learn', 'model', 'train', 'facial', 'correspond', 'descriptive', 'label', 'compute', 'device', 'wherein', 'extract', 'set', 'profile', 'parameter', 'correspond', 'person', 'first', 'include', 'determine', 'identity', 'correspond', 'person', 'base', 'identified', 'face', 'correspond', 'person', 'locate', 'respective', 'profile', 'information', 'first', 'person', 'base', 'determine', 'identity', 'correspond', 'person', 'use', 'one', 'characteristic', 'respective', 'profile', 'information', 'first', 'person', 'set', 'profile', 'parameter', 'correspond', 'identified', 'face', 'correspond', 'person', 'compute', 'device', 'wherein', 'least', 'first', 'one', 'first', 'tile', 'dynamic', 'tile', 'least', 'second', 'one', 'first', 'tile', 'static', 'tile', 'compute', 'device', 'wherein', 'operation', 'include', 'receive', 'comment', 'different', 'group', 'chat', 'session', 'comment', 'include', 'descriptive', 'term', 'respective', 'person', 'identify', 'first', 'choose', 'descriptive', 'label', 'respective', 'person', 'accord', 'comment', 'update', 'second', 'add', 'descriptive', 'label', 'adjacent', 'first', 'tile', 'respective', 'person', 'non-transitory', 'computer-readable', 'storage', 'medium', 'store', 'instruction', 'execute', 'compute', 'device', 'one', 'cause', 'compute', 'device', 'perform', 'operation', 'comprise', 'identify', 'use', 'face', 'one', 'face', 'correspond', 'respective', 'person', 'capture', 'first', 'identify', 'face', 'extract', 'set', 'profile', 'parameter', 'correspond', 'person', 'first', 'select', 'tile', 'first', 'tile', 'match', 'face', 'correspond', 'person', 'first', 'accordance', 'predefined', 'correspondence', 'set', 'profile', 'parameter', 'correspond', 'person', 'set', 'pre-stored', 'description', 'parameter', 'first', 'tile', 'generate', 'second', 'cover', 'respective', 'person', 'first', 'correspond', 'first', 'tile', 'share', 'first', 'second', 'predefined', 'order', 'via', 'group', 'chat', 'session', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'first', 'second', 'display', 'group', 'chat', 'session', 'one', 'time', 'one', 'two', 'replace', 'two', 'periodically', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'extract', 'set', 'profile', 'parameter', 'correspond', 'person', 'first', 'include', 'determine', 'one', 'descriptive', 'label', 'correspond', 'identified', 'face', 'correspond', 'person', 'use', 'first', 'machine', 'learn', 'model', 'wherein', 'first', 'machine', 'learn', 'model', 'train', 'facial', 'correspond', 'descriptive', 'label', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'extract', 'set', 'profile', 'parameter', 'correspond', 'person', 'first', 'include', 'determine', 'identity', 'correspond', 'person', 'base', 'identified', 'face', 'correspond', 'person', 'locate', 'respective', 'profile', 'information', 'first', 'person', 'base', 'determine', 'identity', 'correspond', 'person', 'use', 'one', 'characteristic', 'respective', 'profile', 'information', 'first', 'person', 'set', 'profile', 'parameter', 'correspond', 'identified', 'face', 'correspond', 'person', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'least', 'first', 'one', 'first', 'tile', 'dynamic', 'tile', 'least', 'second', 'one', 'first', 'tile', 'static', 'tile', 'non-transitory', 'computer-readable', 'storage', 'medium', 'wherein', 'operation', 'include', 'receive', 'comment', 'different', 'group', 'chat', 'session', 'comment', 'include', 'descriptive', 'term', 'respective', 'person', 'identify', 'first', 'choose', 'descriptive', 'label', 'respective', 'person', 'accord', 'comment', 'update', 'second', 'add', 'descriptive', 'label', 'adjacent', 'first', 'tile', 'respective', 'person', 'comprise', 'compute', 'system', 'determine', 'performance', 'metric', 'eye', 'system', 'first', 'performance', 'threshold', 'wherein', 'eye', 'system', 'associate', 'head-mounted', 'display', 'worn', 'base', 'determination', 'performance', 'metric', 'eye', 'system', 'first', 'performance', 'threshold', 'computer', 'system', 'perform', 'receive', 'one', 'first', 'input', 'associate', 'body', 'estimate', 'region', 'look', 'within', 'field', 'view', 'head-mounted', 'display', 'base', 'receive', 'one', 'first', 'input', 'associate', 'body', 'determine', 'vergence', 'distance', 'base', 'least', 'one', 'first', 'input', 'associate', 'body', 'estimate', 'region', 'look', 'location', 'one', 'object', 'scene', 'display', 'head-mounted', 'display', 'adjust', 'one', 'configuration', 'head-mounted', 'display', 'base', 'determined', 'vergence', 'distance', 'wherein', 'one', 'configuration', 'head-mounted', 'display', 'comprise', 'one', 'rendering', 'position', 'display', 'screen', 'position', 'optic', 'block', 'comprise', 'determine', 'performance', 'metric', 'eye', 'system', 'second', 'performance', 'threshold', 'receive', 'eye', 'data', 'eye', 'system', 'determine', 'vergence', 'distance', 'base', 'eye', 'data', 'one', 'first', 'input', 'associate', 'body', 'comprise', 'receive', 'one', 'second', 'input', 'associate', 'one', 'displaying', 'element', 'scene', 'display', 'head-mounted', 'display', 'determine', 'vergence', 'distance', 'base', 'least', 'eye', 'data', 'one', 'first', 'input', 'associate', 'body', 'one', 'second', 'input', 'associate', 'one', 'displaying', 'element', 'scene', 'comprise', 'feed', 'one', 'first', 'input', 'associate', 'body', 'fusion', 'algorithm', 'wherein', 'fusion', 'algorithm', 'assigns', 'weight', 'score', 'input', 'one', 'first', 'input', 'determine', 'vergence', 'distance', 'use', 'fusion', 'algorithm', 'base', 'one', 'first', 'input', 'associate', 'body', 'determine', 'z-depth', 'display', 'screen', 'confidence', 'score', 'base', 'one', 'first', 'input', 'associate', 'body', 'comprise', 'compare', 'confidence', 'score', 'confidence', 'level', 'threshold', 'response', 'determination', 'confidence', 'score', 'confidence', 'level', 'threshold', 'feed', 'one', 'second', 'input', 'associate', 'one', 'displaying', 'element', 'scene', 'fusion', 'algorithm', 'determine', 'z-depth', 'display', 'screen', 'use', 'fusion', 'algorithm', 'base', 'one', 'first', 'input', 'associate', 'body', 'one', 'second', 'input', 'associate', 'one', 'displaying', 'element', 'scene', 'compare', 'compare', 'fusion', 'algorithm', 'confidence', 'score', 'associate', 'combination', 'input', 'determine', 'fusion', 'algorithm', 'z-depth', 'display', 'screen', 'base', 'combination', 'input', 'associate', 'high', 'confidence', 'score', 'wherein', 'z-depth', 'confidence', 'score', 'determine', 'fusion', 'algorithm', 'use', 'piecewise', 'comparison', 'one', 'first', 'input', 'one', 'second', 'input', 'wherein', 'z-depth', 'confidence', 'score', 'determine', 'base', 'correlation', 'two', 'input', 'one', 'first', 'input', 'one', 'second', 'input', 'wherein', 'fusion', 'algorithm', 'comprise', 'machine', 'learn', 'ml', 'algorithm', 'wherein', 'machine', 'learn', 'ml', 'algorithm', 'determines', 'combination', 'first', 'input', 'fed', 'fusion', 'algorithm', 'wherein', 'one', 'first', 'input', 'associate', 'body', 'comprise', 'one', 'hand', 'position', 'hand', 'direction', 'hand', 'movement', 'hand', 'gesture', 'head', 'position', 'head', 'direction', 'head', 'movement', 'head', 'gesture', 'gaze', 'angle', 'rea', 'body', 'gesture', 'body', 'posture', 'body', 'movement', 'behavior', 'weight', 'combination', 'one', 'related', 'parameter', 'wherein', 'one', 'first', 'input', 'associate', 'body', 'receive', 'one', 'controller', 'sensor', 'camera', 'microphone', 'accelerometer', 'headset', 'wear', 'mobile', 'device', 'wherein', 'one', 'second', 'input', 'associate', 'one', 'displaying', 'element', 'comprise', 'one', 'z-buffer', 'value', 'associate', 'display', 'element', 'display', 'element', 'mark', 'developer', 'analysis', 'result', 'shape', 'display', 'element', 'face', 'result', 'object', 'result', 'person', 'identify', 'display', 'content', 'object', 'identify', 'display', 'content', 'correlation', 'two', 'display', 'element', 'weight', 'combination', 'one', 'second', 'input', 'comprise', 'determine', 'performance', 'metric', 'eye', 'system', 'second', 'performance', 'threshold', 'receive', 'one', 'second', 'input', 'associate', 'one', 'displaying', 'element', 'scene', 'display', 'head-mounted', 'display', 'determine', 'vergence', 'distance', 'base', 'least', 'one', 'first', 'input', 'associate', 'body', 'one', 'second', 'input', 'associate', 'one', 'displaying', 'element', 'wherein', 'determine', 'performance', 'metric', 'eye', 'system', 'second', 'performance', 'threshold', 'comprise', 'determine', 'eye', 'system', 'exist', 'fails', 'provide', 'eye', 'data', 'wherein', 'performance', 'metric', 'eye', 'system', 'comprise', 'one', 'accuracy', 'parameter', 'eye', 'system', 'precision', 'parameter', 'eye', 'system', 'value', 'parameter', 'eye', 'system', 'detectability', 'pupil', 'metric', 'base', 'one', 'parameter', 'associated', 'parameter', 'change', 'parameter', 'change', 'trend', 'data', 'availability', 'weight', 'combination', 'one', 'performance', 'related', 'parameter', 'wherein', 'one', 'parameter', 'associate', 'comprise', 'one', 'eye', 'distance', 'pupil', 'position', 'pupil', 'status', 'correlation', 'two', 'pupil', 'head', 'size', 'position', 'headset', 'worn', 'angle', 'headset', 'worn', 'direction', 'headset', 'worn', 'alignment', 'eye', 'weight', 'combination', 'one', 'related', 'parameter', 'associate', 'wherein', 'first', 'performance', 'threshold', 'comprise', 'one', 'pre-determined', 'value', 'pre-determined', 'range', 'state', 'data', 'change', 'speed', 'data', 'trend', 'data', 'change', 'one', 'non-transitory', 'computer-readable', 'storage', 'medium', 'embody', 'software', 'operable', 'execute', 'computing', 'system', 'determine', 'performance', 'metric', 'eye', 'system', 'first', 'performance', 'threshold', 'wherein', 'eye', 'system', 'associate', 'head-mounted', 'display', 'worn', 'base', 'determination', 'performance', 'metric', 'eye', 'system', 'first', 'performance', 'threshold', 'medium', 'embody', 'software', 'operable', 'execute', 'compute', 'system', 'receive', 'one', 'first', 'input', 'associate', 'body', 'estimate', 'region', 'look', 'within', 'field', 'view', 'head-mounted', 'display', 'base', 'receive', 'one', 'first', 'input', 'associate', 'body', 'determine', 'vergence', 'distance', 'base', 'least', 'one', 'first', 'input', 'associate', 'body', 'estimate', 'region', 'look', 'location', 'one', 'object', 'scene', 'display', 'head-mounted', 'display', 'adjust', 'one', 'configuration', 'head-mounted', 'display', 'base', 'determined', 'vergence', 'distance', 'system', 'comprise', 'one', 'non-transitory', 'computer-readable', 'storage', 'medium', 'embody', 'instruction', 'one', 'couple', 'storage', 'medium', 'operable', 'execute', 'instruction', 'determine', 'performance', 'metric', 'eye', 'system', 'first', 'performance', 'threshold', 'wherein', 'eye', 'system', 'associate', 'head-mounted', 'display', 'worn', 'base', 'determination', 'performance', 'metric', 'eye', 'system', 'first', 'performance', 'threshold', 'system', 'configure', 'receive', 'one', 'first', 'input', 'associate', 'body', 'estimate', 'region', 'look', 'within', 'field', 'view', 'head-mounted', 'display', 'base', 'receive', 'one', 'first', 'input', 'associate', 'body', 'determine', 'vergence', 'distance', 'base', 'least', 'one', 'first', 'input', 'associate', 'body', 'estimate', 'region', 'look', 'location', 'one', 'object', 'scene', 'display', 'head-mounted', 'display', 'adjust', 'one', 'configuration', 'head-mounted', 'display', 'base', 'determined', 'vergence', 'distance', 'computer-implemented', '-based', 'self-guided', 'object', 'detection', 'comprise', 'receive', 'device', 'set', 'respective', 'grid', 'thereon', 'label', 'regard', 'respective', 'object', 'detect', 'use', 'grid', 'level', 'label', 'data', 'training', 'device', 'grid-based', 'object', 'detector', 'use', 'grid', 'level', 'label', 'data', 'determine', 'device', 'respective', 'bounding', 'box', 'respective', 'object', 'apply', 'local', 'segmentation', 'training', 'device', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'joint', 'object', 'localization', 'object', 'classification', 'use', 'respective', 'bounding', 'box', 'respective', 'object', 'input', 'rcnn', 'computer-implemented', 'comprising', 'perform', 'action', 'responsive', 'object', 'localization', 'object', 'classification', 'respective', 'new', 'object', 'new', 'rcnn', 'apply', 'computer-implemented', 'wherein', 'action', 'comprise', 'autonomously', 'control', 'motor', 'vehicle', 'avoid', 'collision', 'new', 'object', 'responsive', 'object', 'localization', 'object', 'classification', 'respective', 'new', 'object', 'computer-implemented', 'wherein', 'local', 'segmentation', 'perform', 'use', 'self-similarity', 'search', 'template', 'match', 'provide', 'respective', 'bound', 'box', 'around', 'respective', 'object', 'set', 'computer-implemented', 'wherein', 'local', 'segmentation', 'apply', 'segment', 'respective', 'target', 'region', 'therein', 'computer-implemented', 'wherein', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'form', 'model', 'object', 'training', 'stage', 'detect', 'object', 'new', 'inference', 'stage', 'computer-implemented', 'wherein', 'perform', 'system', 'select', 'group', 'consist', 'surveillance', 'system', 'face', 'detection', 'system', 'face', 'system', 'cancer', 'detection', 'system', 'object', 'system', 'advance', 'driver-assistance', 'system', 'computer', 'program', 'product', '-based', 'self-guided', 'object', 'detection', 'computer', 'program', 'product', 'comprise', 'non-transitory', 'computer', 'readable', 'storage', 'medium', 'program', 'instruction', 'embody', 'therewith', 'program', 'instruction', 'executable', 'computer', 'cause', 'computer', 'perform', 'comprise', 'receive', 'device', 'set', 'respective', 'grid', 'thereon', 'label', 'regard', 'respective', 'object', 'detect', 'use', 'grid', 'level', 'label', 'data', 'training', 'device', 'grid-based', 'object', 'detector', 'use', 'grid', 'level', 'label', 'data', 'determine', 'device', 'respective', 'bounding', 'box', 'respective', 'object', 'apply', 'local', 'segmentation', 'training', 'device', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'joint', 'object', 'localization', 'object', 'classification', 'use', 'respective', 'bounding', 'box', 'respective', 'object', 'input', 'rcnn', 'computer', 'program', 'product', 'wherein', 'comprise', 'perform', 'action', 'responsive', 'object', 'localization', 'object', 'classification', 'respective', 'new', 'object', 'new', 'rcnn', 'apply', 'computer', 'program', 'product', 'wherein', 'action', 'comprise', 'autonomously', 'control', 'motor', 'vehicle', 'avoid', 'collision', 'new', 'object', 'responsive', 'object', 'localization', 'object', 'classification', 'respective', 'new', 'object', 'computer', 'program', 'product', 'wherein', 'local', 'segmentation', 'perform', 'use', 'self-similarity', 'search', 'template', 'match', 'provide', 'respective', 'bound', 'box', 'around', 'respective', 'object', 'set', 'computer', 'program', 'product', 'wherein', 'local', 'segmentation', 'apply', 'segment', 'respective', 'target', 'region', 'therein', 'computer', 'program', 'product', 'wherein', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'form', 'model', 'object', 'training', 'stage', 'detect', 'object', 'new', 'inference', 'stage', 'computer', 'program', 'product', 'wherein', 'perform', 'system', 'select', 'group', 'consist', 'surveillance', 'system', 'face', 'detection', 'system', 'face', 'system', 'cancer', 'detection', 'system', 'object', 'system', 'advance', 'driver-assistance', 'system', 'computer', 'process', 'system', '-based', 'self-guided', 'object', 'detection', 'comprise', 'memory', 'device', 'store', 'program', 'code', 'device', 'run', 'program', 'code', 'receive', 'set', 'respective', 'grid', 'thereon', 'label', 'regard', 'respective', 'object', 'detect', 'use', 'grid', 'level', 'label', 'data', 'train', 'grid-based', 'object', 'detector', 'use', 'grid', 'level', 'label', 'data', 'determine', 'respective', 'bounding', 'box', 'respective', 'object', 'apply', 'local', 'segmentation', 'train', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'joint', 'object', 'localization', 'object', 'classification', 'use', 'respective', 'bounding', 'box', 'respective', 'object', 'input', 'rcnn', 'computer', 'processing', 'system', 'wherein', 'device', 'run', 'program', 'code', 'perform', 'action', 'responsive', 'object', 'localization', 'object', 'classification', 'respective', 'new', 'object', 'new', 'rcnn', 'apply', 'computer', 'process', 'system', 'wherein', 'action', 'comprise', 'autonomously', 'control', 'motor', 'vehicle', 'avoid', 'collision', 'new', 'object', 'responsive', 'object', 'localization', 'object', 'classification', 'respective', 'new', 'object', 'computer', 'processing', 'system', 'wherein', 'local', 'segmentation', 'perform', 'use', 'self-similarity', 'search', 'template', 'match', 'provide', 'respective', 'bound', 'box', 'around', 'respective', 'object', 'set', 'computer', 'process', 'system', 'wherein', 'region-based', 'convolutional', 'neural', 'network', 'rcnn', 'form', 'model', 'object', 'training', 'stage', 'detect', 'object', 'new', 'inference', 'stage', 'computer', 'processing', 'system', 'wherein', 'computer', 'processing', 'system', 'comprise', 'system', 'select', 'group', 'consist', 'surveillance', 'system', 'face', 'detection', 'system', 'face', 'system', 'cancer', 'detection', 'system', 'object', 'system', 'advance', 'driver-assistance', 'system', 'scalable', 'parallel', 'cloud-based', 'face', 'utilize', 'database', 'normalize', 'store', 'comprise', 'capture', 'use', 'camera', 'detect', 'face', 'capture', 'normalizing', 'detected', 'facial', 'match', 'normalize', 'store', 'identify', 'facial', 'feature', 'normalize', 'detected', 'facial', 'generate', 'facial', 'metric', 'facial', 'feature', 'calculate', 'euclidean', 'distance', 'facial', 'metric', 'normalize', 'detected', 'facial', 'correspond', 'facial', 'metric', 'store', 'compare', 'euclidean', 'distance', 'predetermine', 'threshold', 'responsive', 'euclidean', 'distance', 'comparison', 'produce', 'reduced', 'candidate', 'list', 'best', 'possible', 'match', 'normalize', 'store', 'compare', 'parallel', 'normalize', 'detect', 'facial', 'normalize', 'store', 'reduced', 'candidate', 'list', 'utilize', 'face', 'algorithm', 'parallel', 'process', 'system', 'use', 'different', 'face', 'algorithm', 'responsive', 'comparison', 'produce', 'best', 'match', 'result', 'parallel', 'subset', 'reduce', 'candidate', 'list', 'select', 'final', 'match', 'best', 'match', 'result', 'use', 'deep', 'learn', 'neural', 'network', 'face', 'algorithm', 'train', 'output', 'individual', 'face', 'algorithm', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'detect', 'face', 'capture', 'comprises', 'utilize', 'opencv', 'detect', 'face', 'capture', 'extract', 'location', 'eye', 'tip', 'nose', 'face', 'determine', 'distance', 'eye', 'crop', 'face', 'capture', 'width', 'height', 'crop', 'face', 'function', 'distance', 'eye', 'rotate', 'face', 'angle', 'rotation', 'function', 'distance', 'eye', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'width', 'crop', 'face', 'time', 'distance', 'eye', 'height', 'crop', 'face', 'time', 'distance', 'eye', 'angle', 'rotation', 'angle', 'form', 'straight', 'line', 'join', 'eye', 'x-axis', 'face', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'rotate', 'face', 'comprises', 'rotate', 'face', 'provide', 'frontal', 'face', 'pattern', 'scalable', 'parallel', 'cloud-based', 'face', 'comprise', 'step', 'proportionally', 'rescale', 'crop', 'rotate', 'scalable', 'parallel', 'cloud-based', 'face', 'proportional', 'rescaling', 'yield', 'crop', 'rotated', 'size', '=', 'pixel', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'facial', 'feature', 'identify', 'normalized', 'detect', 'facial', 'comprise', 'pair', 'eye', 'tip', 'nose', 'mouth', 'center', 'mouth', 'chin', 'area', 'comprise', 'bottom', 'top', 'leave', 'landmark', 'top', 'right', 'landmark', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'generate', 'facial', 'metric', 'comprises', 'calculate', 'distance', 'pair', 'eye', 'distance', 'eye', 'tip', 'nose', 'distance', 'equal', 'width', 'mouth', 'distance', 'tip', 'nose', 'center', 'mouth', 'distance', 'bottom', 'chin', 'center', 'mouth', 'distance', 'top', 'leave', 'landmark', 'chin', 'tip', 'nose', 'distance', 'top', 'right', 'landmark', 'chin', 'tip', 'nose', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'perform', 'euclidean', 'distance', 'match', 'comprise', 'partition', 'normalize', 'store', 'substantially', 'equal', 'subset', 'perform', 'euclidean', 'distance', 'match', 'facial', 'metric', 'normalize', 'detected', 'facial', 'correspond', 'facial', 'metric', 'store', 'subset', 'normalize', 'stored', 'separate', 'parallel', 'processing', 'system', 'generate', 'euclidean', 'distance', 'store', 'subset', 'compare', 'euclidean', 'distance', 'predetermine', 'threshold', 'separate', 'responsive', 'euclidean', 'distance', 'comparison', 'produce', 'reduced', 'candidate', 'list', 'best', 'possible', 'match', 'normalize', 'stored', 'subset', 'combining', 'reduced', 'candidate', 'list', 'subset', 'produce', 'single', 'reduce', 'candidate', 'list', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'face', 'algorithms', 'utilized', 'comparing', 'parallel', 'normalize', 'detect', 'facial', 'normalize', 'store', 'reduced', 'candidate', 'list', 'consist', 'face', 'algorithms', 'select', 'group', 'consist', 'principle', 'component', 'analysis', 'pca-based', 'algorithm', 'linear', 'discriminant', 'analysis', 'lda', 'algorithms', 'independent', 'component', 'analysis', 'ica', 'algorithm', 'kernel-based', 'algorithms', 'feature-based', 'technique', 'algorithms', 'base', 'neural', 'network', 'algorithms', 'base', 'transforms', 'model-based', 'face', 'algorithm', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'pca-based', 'algorithm', 'include', 'eigen', 'face', 'detection', 'lda', 'algorithms', 'include', 'fisher', 'face', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'compare', 'parallel', 'capture', 'normalized', 'store', 'reduced', 'candidate', 'list', 'comprise', 'partition', 'reduce', 'candidate', 'list', 'substantially', 'equal', 'subset', 'process', 'subset', 'different', 'parallel', 'process', 'system', 'use', 'unique', 'face', 'algorithm', 'produce', 'best', 'match', 'result', 'use', 'reduce', 'function', 'mapreduce', 'program', 'combine', 'best', 'match', 'result', 'subset', 'produce', 'single', 'set', 'best', 'match', 'result', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'partition', 'reduce', 'candidate', 'list', 'comprises', 'select', 'comprise', 'subset', 'optimize', 'variance', 'accord', 'follow', 'equation', 'n', 'number', 'row', 'columns', 'face', 'vector', 'n', 'number', 'group', 'σij', 'standard', 'deviation', 'dimension', 'group', 'j', 'face', 'vector', 'scalable', 'parallel', 'cloud-based', 'face', 'wherein', 'select', 'comprise', 'subset', 'optimize', 'variance', 'accord', 'follow', 'equation', 'dμi', 'μj', 'euclidean', 'distance', 'mean', 'group', 'mean', 'group', 'j', 'face', 'vector', 'l', 'number', 'group', 'level', 'scalable', 'parallel', 'cloud-based', 'face', 'select', 'final', 'match', 'best', 'match', 'result', 'utilizing', 'deep', 'learning', 'neural', 'network', 'face', 'algorithm', 'comprise', 'utilize', 'either', 'adaboost', 'machine-learning', 'algorithm', 'neural', 'network', 'machine-learning', 'model', 'scalable', 'parallel', 'cloud-based', 'face', 'normalize', 'detect', 'facial', 'match', 'normalize', 'store', 'include', 'normalizing', 'detect', 'facial', 'size', 'illumination', 'normalize', 'stored', 'non-transitory', 'computer-readable', 'medium', 'contain', 'executable', 'program', 'instruction', 'cause', 'computer', 'perform', 'face', 'comprise', 'detect', 'face', 'capture', 'camera', 'normalizing', 'detect', 'facial', 'match', 'normalize', 'store', 'identify', 'facial', 'feature', 'normalize', 'detected', 'facial', 'generate', 'facial', 'metric', 'facial', 'feature', 'calculate', 'euclidean', 'distance', 'facial', 'metric', 'normalize', 'detected', 'facial', 'correspond', 'facial', 'metric', 'store', 'compare', 'euclidean', 'distance', 'predetermine', 'threshold', 'responsive', 'euclidean', 'distance', 'comparison', 'produce', 'reduced', 'candidate', 'list', 'best', 'possible', 'match', 'normalize', 'store', 'compare', 'parallel', 'capture', 'normalized', 'store', 'reduced', 'candidate', 'list', 'utilize', 'face', 'algorithm', 'parallel', 'process', 'system', 'use', 'different', 'face', 'algorithm', 'responsive', 'comparison', 'produce', 'best', 'match', 'result', 'parallel', 'subset', 'reduce', 'candidate', 'list', 'select', 'final', 'match', 'best', 'match', 'result', 'use', 'deep', 'learn', 'neural', 'network', 'face', 'algorithm', 'train', 'output', 'individual', 'face', 'algorithm', 'non-transitory', 'computer-readable', 'medium', 'contain', 'executable', 'program', 'instruction', 'wherein', 'face', 'algorithm', 'utilized', 'comparing', 'parallel', 'normalize', 'detect', 'facial', 'normalize', 'store', 'reduced', 'candidate', 'list', 'consist', 'face', 'algorithms', 'select', 'group', 'consist', 'principle', 'component', 'analysis', 'pca-based', 'algorithm', 'linear', 'discriminant', 'analysis', 'lda', 'algorithms', 'independent', 'component', 'analysis', 'ica', 'algorithm', 'kernel-based', 'algorithms', 'feature-based', 'technique', 'algorithms', 'base', 'neural', 'network', 'algorithms', 'base', 'transforms', 'model-based', 'face', 'algorithm', 'non-transitory', 'computer-readable', 'medium', 'contain', 'executable', 'program', 'instruction', 'wherein', 'pca-based', 'algorithm', 'include', 'eigen', 'face', 'detection', 'lda', 'algorithms', 'include', 'fisher', 'face', 'non-transitory', 'computer-readable', 'medium', 'contain', 'executable', 'program', 'instruction', 'select', 'final', 'match', 'best', 'match', 'result', 'utilizing', 'deep', 'learning', 'neural', 'network', 'face', 'algorithm', 'comprise', 'utilize', 'either', 'adaboost', 'machine-learning', 'algorithm', 'neural', 'network', 'machine-learning', 'model', 'image', 'device', 'comprise', 'condense', 'lens', 'sensor', 'configure', 'detect', 'light', 'pass', 'condense', 'lens', 'comprise', 'pixel', 'matrix', 'wherein', 'pixel', 'matrix', 'comprise', 'phase', 'detection', 'pixel', 'pair', 'regular', 'pixel', 'configure', 'turn', 'phase', 'detection', 'pixel', 'pair', 'autofocusing', 'output', 'autofocused', 'pixel', 'data', 'complete', 'autofocusing', 'divide', 'autofocused', 'pixel', 'data', 'first', 'subframe', 'second', 'subframe', 'calculate', 'feature', 'least', 'one', 'first', 'subframe', 'second', 'subframe', 'wherein', 'feature', 'comprise', 'module', 'width', 'finder', 'pattern', 'finder', 'pattern', 'predetermine', 'ratio', 'harr-like', 'feature', 'gabor', 'feature', 'determine', 'operate', 'resolution', 'regular', 'pixel', 'accord', 'feature', 'calculate', 'least', 'one', 'first', 'subframe', 'second', 'subframe', 'divide', 'autofocused', 'pixel', 'data', 'image', 'device', 'ed', 'wherein', 'phase', 'detection', 'pixel', 'pair', 'comprise', 'first', 'pixel', 'second', 'pixel', 'cover', 'layer', 'cover', 'upon', 'first', 'region', 'first', 'pixel', 'upon', 'second', 'region', 'second', 'pixel', 'wherein', 'first', 'region', 'second', 'region', 'mirror', 'symmetrical', 'microlens', 'align', 'least', 'one', 'first', 'pixel', 'second', 'pixel', 'image', 'device', 'ed', 'wherein', 'first', 'region', 'second', 'region', '%', '%', 'area', 'single', 'pixel', 'image', 'device', 'ed', 'wherein', 'configure', 'perform', 'autofocusing', 'use', 'dual', 'pixel', 'autofocus', 'technique', 'accord', 'pixel', 'data', 'phase', 'detection', 'pixel', 'pair', 'complete', 'autofocusing', 'imaging', 'device', 'ed', 'wherein', 'configure', 'divide', 'pixel', 'data', 'phase', 'detection', 'pixel', 'pair', 'third', 'subframe', 'fourth', 'subframe', 'complete', 'autofocusing', 'perform', 'autofocusing', 'accord', 'third', 'subframe', 'fourth', 'subframe', 'image', 'device', 'ed', 'wherein', 'configure', 'calibrate', 'brightness', 'third', 'subframe', 'fourth', 'subframe', 'identical', 'use', 'shade', 'algorithm', 'image', 'device', 'ed', 'wherein', 'operate', 'resolution', 'select', 'first', 'resolution', 'small', 'number', 'regular', 'pixel', 'second', 'resolution', 'large', 'first', 'resolution', 'image', 'device', 'ed', 'wherein', 'regular', 'pixel', 'turn', 'autofocusing', 'image', 'device', 'ed', 'wherein', 'number', 'phase', 'detection', 'pixel', 'pair', 'small', 'regular', 'pixel', 'image', 'device', 'comprise', 'condense', 'lens', 'sensor', 'configure', 'detect', 'light', 'pass', 'condense', 'lens', 'comprise', 'pixel', 'matrix', 'wherein', 'pixel', 'matrix', 'comprise', 'phase', 'detection', 'pixel', 'pair', 'regular', 'pixel', 'configure', 'turn', 'phase', 'detection', 'pixel', 'pair', 'autofocusing', 'output', 'autofocused', 'pixel', 'data', 'complete', 'autofocusing', 'divide', 'autofocused', 'pixel', 'data', 'first', 'subframe', 'second', 'subframe', 'calculate', 'feature', 'least', 'one', 'first', 'subframe', 'second', 'subframe', 'wherein', 'feature', 'comprise', 'module', 'width', 'finder', 'pattern', 'finder', 'pattern', 'predetermine', 'ratio', 'harr-like', 'feature', 'gabor', 'feature', 'select', 'decode', 'use', 'pixel', 'data', 'regular', 'pixel', 'accord', 'feature', 'calculate', 'least', 'one', 'first', 'subframe', 'second', 'subframe', 'divide', 'autofocused', 'pixel', 'data', 'image', 'device', 'ed', 'wherein', 'phase', 'detection', 'pixel', 'pair', 'comprise', 'first', 'pixel', 'second', 'pixel', 'cover', 'layer', 'cover', 'upon', 'first', 'region', 'first', 'pixel', 'upon', 'second', 'region', 'second', 'pixel', 'wherein', 'first', 'region', 'second', 'region', 'mirror', 'symmetrical', 'microlens', 'align', 'least', 'one', 'first', 'pixel', 'second', 'pixel', 'image', 'device', 'ed', 'wherein', 'configure', 'perform', 'autofocusing', 'use', 'dual', 'pixel', 'autofocus', 'technique', 'accord', 'pixel', 'data', 'phase', 'detection', 'pixel', 'pair', 'complete', 'autofocusing', 'imaging', 'device', 'ed', 'wherein', 'configure', 'divide', 'pixel', 'data', 'phase', 'detection', 'pixel', 'pair', 'third', 'subframe', 'fourth', 'subframe', 'complete', 'autofocusing', 'calibrate', 'brightness', 'third', 'subframe', 'fourth', 'subframe', 'identical', 'use', 'shade', 'algorithm', 'perform', 'autofocusing', 'accord', 'third', 'subframe', 'fourth', 'subframe', 'image', 'device', 'ed', 'wherein', 'configure', 'calculate', 'feature', 'use', 'least', 'one', 'rule', 'base', 'algorithm', 'machine', 'learn', 'algorithm', 'image', 'device', 'ed', 'wherein', 'decode', 'decode', 'qr', 'code', 'face', 'operate', 'image', 'device', 'image', 'device', 'comprise', 'phase', 'detection', 'pixel', 'pair', 'regular', 'pixel', 'operate', 'comprise', 'turn', 'phase', 'detection', 'pixel', 'pair', 'autofocusing', 'output', 'autofocused', 'frame', 'complete', 'autofocusing', 'divide', 'autofocused', 'frame', 'acquire', 'phase', 'detection', 'pixel', 'pair', 'first', 'subframe', 'second', 'subframe', 'calculate', 'feature', 'least', 'one', 'first', 'subframe', 'second', 'subframe', 'wherein', 'feature', 'comprise', 'module', 'width', 'finder', 'pattern', 'finder', 'pattern', 'predetermine', 'ratio', 'harr-like', 'feature', 'gabor', 'feature', 'selectively', 'activate', 'least', 'part', 'regular', 'pixel', 'accord', 'feature', 'calculate', 'least', 'one', 'first', 'subframe', 'second', 'subframe', 'divide', 'autofocused', 'frame', 'operate', 'ed', 'wherein', 'selectively', 'activate', 'comprises', 'activate', 'first', 'part', 'regular', 'pixel', 'perform', 'decode', 'accord', 'pixel', 'data', 'first', 'part', 'regular', 'pixel', 'activate', 'regular', 'pixel', 'perform', 'accord', 'pixel', 'data', 'regular', 'pixel', 'operate', 'ed', 'wherein', 'pixel', 'data', 'phase', 'detection', 'pixel', 'pair', 'capture', 'frame', 'pixel', 'data', 'regular', 'pixel', 'also', 'use', 'perform', 'decode', 'operate', 'ed', 'wherein', 'decode', 'decode', 'qr', 'code', 'face', 'operate', 'ed', 'wherein', 'phase', 'detection', 'pixel', 'pair', 'partially', 'cover', 'pixel', 'structure', 'dual', 'pixel', 'apparatus', 'comprise', 'first', 'camera', 'module', 'configure', 'obtain', 'first', 'object', 'first', 'field', 'view', 'second', 'camera', 'module', 'configure', 'obtain', 'second', 'object', 'second', 'field', 'view', 'different', 'first', 'field', 'view', 'first', 'depth', 'map', 'generator', 'configure', 'generate', 'first', 'depth', 'map', 'first', 'base', 'first', 'second', 'second', 'depth', 'map', 'generator', 'configure', 'generate', 'second', 'depth', 'map', 'second', 'base', 'first', 'second', 'first', 'depth', 'map', 'apparatus', 'wherein', 'first', 'field', 'view', 'narrow', 'angle', 'second', 'field', 'view', 'wider', 'angle', 'apparatus', 'wherein', 'second', 'divide', 'primary', 'region', 'residual', 'region', 'second', 'depth', 'map', 'generator', 'comprise', 'relationship', 'estimate', 'module', 'configure', 'estimate', 'relationship', 'primary', 'region', 'residual', 'region', 'base', 'first', 'second', 'depth', 'map', 'estimate', 'module', 'configure', 'estimate', 'depth', 'map', 'residual', 'region', 'base', 'estimate', 'relationship', 'first', 'depth', 'map', 'apparatus', 'wherein', 'least', 'one', 'relationship', 'estimate', 'module', 'depth', 'map', 'estimate', 'module', 'performs', 'estimate', 'operation', 'base', 'neural', 'network', 'module', 'apparatus', 'comprise', 'depth', 'map', 'fusion', 'unit', 'configure', 'generate', 'third', 'depth', 'map', 'second', 'perform', 'fusion', 'operation', 'base', 'first', 'depth', 'map', 'second', 'depth', 'map', 'apparatus', 'wherein', 'depth', 'map', 'fusion', 'unit', 'comprise', 'tone', 'mapping', 'module', 'configure', 'generate', 'tone-mapped', 'second', 'depth', 'map', 'correspond', 'first', 'depth', 'map', 'perform', 'bias', 'remove', 'operation', 'second', 'depth', 'map', 'fusion', 'module', 'configure', 'generate', 'third', 'depth', 'map', 'fuse', 'tone-mapped', 'second', 'depth', 'map', 'first', 'depth', 'map', 'apparatus', 'wherein', 'depth', 'map', 'fusion', 'unit', 'comprise', 'propagate', 'module', 'configure', 'generate', 'propagate', 'first', 'depth', 'map', 'second', 'iterate', 'propagate', 'first', 'depth', 'map', 'base', 'first', 'depth', 'map', 'second', 'fusion', 'module', 'generate', 'third', 'depth', 'map', 'fuse', 'tone-mapped', 'second', 'depth', 'map', 'propagate', 'first', 'depth', 'map', 'apparatus', 'wherein', 'depth', 'map', 'fusion', 'unit', 'comprise', 'post-processing', 'module', 'configure', 'perform', 'post-processing', 'operation', 'third', 'depth', 'map', 'generate', 'fusion', 'module', 'provide', 'post-processed', 'third', 'depth', 'map', 'apparatus', 'wherein', 'post-processing', 'module', 'performs', 'post-processing', 'operation', 'filter', 'interface', 'generate', 'third', 'depth', 'map', 'accordance', 'fusion', 'fusion', 'module', 'apparatus', 'wherein', 'post-processing', 'module', 'remove', 'artifact', 'generate', 'third', 'depth', 'map', 'accordance', 'fusion', 'fusion', 'module', 'apparatus', 'wherein', 'first', 'depth', 'map', 'generator', 'analyse', 'distance', 'relationship', 'first', 'second', 'generates', 'first', 'depth', 'map', 'first', 'base', 'distance', 'relationship', 'process', 'electronic', 'apparatus', 'comprise', 'obtain', 'first', 'object', 'use', 'first', 'camera', 'module', 'obtain', 'second', 'object', 'use', 'second', 'camera', 'module', 'generate', 'first', 'depth', 'map', 'first', 'base', 'first', 'second', 'estimate', 'relationship', 'primary', 'region', 'second', 'residual', 'region', 'second', 'base', 'first', 'second', 'generate', 'second', 'depth', 'map', 'second', 'base', 'estimate', 'relationship', 'primary', 'region', 'residual', 'region', 'first', 'depth', 'map', 'wherein', 'electronic', 'apparatus', 'comprises', 'first', 'camera', 'module', 'include', 'first', 'lens', 'first', 'field', 'view', 'second', 'camera', 'module', 'include', 'second', 'lens', 'second', 'field', 'view', 'wider', 'first', 'field', 'view', 'wherein', 'generate', 'second', 'depth', 'map', 'comprise', 'estimate', 'depth', 'map', 'residual', 'region', 'base', 'estimate', 'relationship', 'primary', 'region', 'residual', 'region', 'first', 'depth', 'map', 'generate', 'second', 'depth', 'map', 'base', 'depth', 'map', 'residual', 'region', 'first', 'depth', 'map', 'wherein', 'estimate', 'relationship', 'primary', 'region', 'second', 'perform', 'use', 'neural', 'network', 'model', 'comprise', 'perform', 'pre-processing', 'operation', 'second', 'depth', 'map', 'generate', 'third', 'depth', 'map', 'residual', 'fuse', 'second', 'depth', 'map', 'pre-processing', 'operation', 'perform', 'first', 'depth', 'map', 'wherein', 'perform', 'pre-processing', 'operation', 'comprises', 'perform', 'tone', 'mapping', 'operation', 'depth', 'map', 'primary', 'region', 'depth', 'map', 'residual', 'region', 'base', 'second', 'depth', 'map', 'operate', 'electronic', 'apparatus', 'electronic', 'apparatus', 'include', 'first', 'camera', 'module', 'provide', 'first', 'object', 'use', 'first', 'field', 'view', 'second', 'camera', 'module', 'provide', 'second', 'object', 'use', 'second', 'field', 'view', 'wider', 'first', 'field', 'view', 'generate', 'depth', 'map', 'second', 'base', 'primary', 'region', 'second', 'residual', 'region', 'second', 'operating', 'comprise', 'generate', 'first', 'depth', 'map', 'primary', 'region', 'estimate', 'relationship', 'first', 'second', 'estimate', 'relationship', 'primary', 'region', 'residual', 'region', 'base', 'first', 'second', 'generate', 'second', 'depth', 'map', 'second', 'estimate', 'depth', 'map', 'second', 'region', 'base', 'estimate', 'relationship', 'primary', 'region', 'residual', 'region', 'generate', 'depth', 'map', 'second', 'fuse', 'first', 'depth', 'map', 'second', 'depth', 'map', 'operation', 'comprise', 'execute', 'application', 'applies', 'effect', 'second', 'base', 'depth', 'map', 'residual', 'operation', 'wherein', 'application', 'apply', 'least', 'one', 'effect', 'auto-focusing', 'out-focusing', 'forebackground', 'separation', 'face', 'object', 'detection', 'within', 'frame', 'augmented', 'reality', 'second', 'base', 'depth', 'map', 'second', 'payment', 'base', 'face', 'comprise', 'acquire', 'first', 'face', 'information', 'target', 'extract', 'first', 'characteristic', 'information', 'first', 'face', 'information', 'wherein', 'first', 'characteristic', 'information', 'include', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'determine', 'whether', 'target', 'willingness', 'pay', 'accord', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'include', 'determine', 'whether', 'angle', 'rotation', 'preset', 'direction', 'less', 'angle', 'threshold', 'wherein', 'head', 'posture', 'information', 'include', 'angle', 'rotation', 'preset', 'direction', 'determine', 'whether', 'probability', 'value', 'gaze', 'payment', 'screen', 'great', 'probability', 'threshold', 'wherein', 'gaze', 'information', 'include', 'probability', 'value', 'gaze', 'payment', 'screen', 'response', 'determine', 'angle', 'rotation', 'preset', 'direction', 'less', 'angle', 'threshold', 'probability', 'value', 'gaze', 'payment', 'screen', 'great', 'probability', 'threshold', 'determine', 'target', 'willingness', 'pay', 'response', 'determine', 'target', 'willingness', 'pay', 'complete', 'payment', 'operation', 'base', 'face', 'ed', 'wherein', 'complete', 'payment', 'operation', 'base', 'face', 'comprise', 'trigger', 'perform', 'payment', 'initiating', 'operation', 'acquire', 'second', 'face', 'information', 'base', 'face', 'determine', 'whether', 'second', 'characteristic', 'information', 'extract', 'second', 'face', 'information', 'indicate', 'willingness', 'pay', 'response', 'determine', 'second', 'characteristic', 'information', 'indicate', 'willingness', 'pay', 'trigger', 'perform', 'payment', 'confirmation', 'operation', 'complete', 'payment', 'operation', 'base', 'payment', 'account', 'information', 'correspond', 'target', 'ed', 'wherein', 'determine', 'whether', 'second', 'characteristic', 'information', 'extract', 'second', 'face', 'information', 'indicate', 'willingness', 'pay', 'comprises', 'determine', 'whether', 'current', 'correspond', 'second', 'face', 'information', 'consistent', 'target', 'response', 'determine', 'current', 'consistent', 'target', 'determine', 'whether', 'target', 'willingness', 'pay', 'accord', 'second', 'characteristic', 'information', 'extract', 'second', 'face', 'information', 'ed', 'wherein', 'extract', 'first', 'characteristic', 'information', 'first', 'face', 'information', 'comprise', 'determine', 'head', 'posture', 'information', 'target', 'use', 'head', 'posture', 'model', 'base', 'first', 'face', 'information', 'determine', 'gaze', 'information', 'target', 'use', 'gaze', 'information', 'model', 'base', 'characteristic', 'eye', 'region', 'first', 'face', 'information', 'ed', 'wherein', 'head', 'posture', 'model', 'obtain', 'train', 'acquire', 'first', 'sample', 'data', 'set', 'wherein', 'first', 'sample', 'data', 'set', 'include', 'piece', 'first', 'sample', 'data', 'piece', 'first', 'sample', 'data', 'include', 'correspondence', 'sample', 'face', 'head', 'posture', 'information', 'determine', 'mean', 'data', 'variance', 'data', 'sample', 'face', 'piece', 'first', 'sample', 'data', 'preprocessing', 'sample', 'face', 'contain', 'piece', 'first', 'sample', 'data', 'base', 'mean', 'data', 'variance', 'data', 'obtain', 'preprocessed', 'sample', 'face', 'set', 'preprocessed', 'sample', 'face', 'correspond', 'head', 'posture', 'information', 'first', 'model', 'train', 'sample', 'perform', 'training', 'use', 'machine', 'learning', 'base', 'first', 'model', 'training', 'sample', 'obtain', 'head', 'posture', 'model', 'ed', 'wherein', 'gaze', 'information', 'model', 'obtain', 'train', 'acquire', 'second', 'sample', 'data', 'set', 'wherein', 'second', 'sample', 'data', 'set', 'include', 'piece', 'second', 'sample', 'data', 'piece', 'second', 'sample', 'data', 'include', 'correspondence', 'sample', 'eye', 'gaze', 'information', 'determine', 'mean', 'data', 'variance', 'data', 'sample', 'eye', 'piece', 'second', 'sample', 'data', 'preprocessing', 'sample', 'eye', 'contain', 'piece', 'second', 'sample', 'data', 'base', 'mean', 'data', 'variance', 'data', 'obtain', 'preprocessed', 'sample', 'eye', 'set', 'preprocessed', 'sample', 'eye', 'correspond', 'gaze', 'information', 'second', 'model', 'train', 'sample', 'perform', 'training', 'use', 'machine', 'learn', 'base', 'second', 'model', 'training', 'sample', 'obtain', 'gaze', 'information', 'model', 'ed', 'wherein', 'angle', 'rotation', 'preset', 'direction', 'comprise', 'pitch', 'angle', 'yaw', 'angle', 'roll', 'angle', 'wherein', 'pitch', 'angle', 'refers', 'angle', 'rotation', 'around', 'x-axis', 'yaw', 'angle', 'refers', 'angle', 'rotation', 'around', 'y-axis', 'roll', 'angle', 'refers', 'angle', 'rotation', 'around', 'z-axis', 'payment', 'device', 'base', 'face', 'comprise', 'non-transitory', 'computer-readable', 'storage', 'medium', 'store', 'instruction', 'executable', 'cause', 'device', 'perform', 'operation', 'comprise', 'acquire', 'first', 'face', 'information', 'target', 'extract', 'first', 'characteristic', 'information', 'first', 'face', 'information', 'wherein', 'first', 'characteristic', 'information', 'include', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'determine', 'whether', 'target', 'willingness', 'pay', 'accord', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'include', 'determine', 'whether', 'angle', 'rotation', 'preset', 'direction', 'less', 'angle', 'threshold', 'wherein', 'head', 'posture', 'information', 'include', 'angle', 'rotation', 'preset', 'direction', 'determine', 'whether', 'probability', 'value', 'gaze', 'payment', 'screen', 'great', 'probability', 'threshold', 'wherein', 'gaze', 'information', 'include', 'probability', 'value', 'gaze', 'payment', 'screen', 'response', 'determine', 'angle', 'rotation', 'preset', 'direction', 'less', 'angle', 'threshold', 'probability', 'value', 'gaze', 'payment', 'screen', 'great', 'probability', 'threshold', 'determine', 'target', 'willingness', 'pay', 'response', 'determine', 'target', 'willingness', 'pay', 'complete', 'payment', 'operation', 'base', 'face', 'device', 'ed', 'wherein', 'complete', 'payment', 'operation', 'base', 'face', 'comprise', 'trigger', 'perform', 'payment', 'initiating', 'operation', 'acquire', 'second', 'face', 'information', 'base', 'face', 'determine', 'whether', 'second', 'characteristic', 'information', 'extract', 'second', 'face', 'information', 'indicate', 'willingness', 'pay', 'response', 'determine', 'second', 'characteristic', 'information', 'indicate', 'willingness', 'pay', 'trigger', 'perform', 'payment', 'confirmation', 'operation', 'complete', 'payment', 'operation', 'base', 'payment', 'account', 'information', 'correspond', 'target', 'device', 'ed', 'wherein', 'determine', 'whether', 'second', 'characteristic', 'information', 'extract', 'second', 'face', 'information', 'indicate', 'willingness', 'pay', 'comprises', 'determine', 'whether', 'current', 'correspond', 'second', 'face', 'information', 'consistent', 'target', 'response', 'determine', 'current', 'consistent', 'target', 'determine', 'whether', 'target', 'willingness', 'pay', 'accord', 'second', 'characteristic', 'information', 'extract', 'second', 'face', 'information', 'device', 'ed', 'wherein', 'extract', 'first', 'characteristic', 'information', 'first', 'face', 'information', 'comprise', 'determine', 'head', 'posture', 'information', 'target', 'use', 'head', 'posture', 'model', 'base', 'first', 'face', 'information', 'determine', 'gaze', 'information', 'target', 'use', 'gaze', 'information', 'model', 'base', 'characteristic', 'eye', 'region', 'first', 'face', 'information', 'device', 'ed', 'wherein', 'head', 'posture', 'model', 'obtain', 'train', 'acquire', 'first', 'sample', 'data', 'set', 'wherein', 'first', 'sample', 'data', 'set', 'include', 'piece', 'first', 'sample', 'data', 'piece', 'first', 'sample', 'data', 'include', 'correspondence', 'sample', 'face', 'head', 'posture', 'information', 'determine', 'mean', 'data', 'variance', 'data', 'sample', 'face', 'piece', 'first', 'sample', 'data', 'preprocessing', 'sample', 'face', 'contain', 'piece', 'first', 'sample', 'data', 'base', 'mean', 'data', 'variance', 'data', 'obtain', 'preprocessed', 'sample', 'face', 'set', 'preprocessed', 'sample', 'face', 'correspond', 'head', 'posture', 'information', 'first', 'model', 'train', 'sample', 'perform', 'training', 'use', 'machine', 'learning', 'base', 'first', 'model', 'training', 'sample', 'obtain', 'head', 'posture', 'model', 'device', 'ed', 'wherein', 'gaze', 'information', 'model', 'obtain', 'train', 'acquire', 'second', 'sample', 'data', 'set', 'wherein', 'second', 'sample', 'data', 'set', 'include', 'piece', 'second', 'sample', 'data', 'piece', 'second', 'sample', 'data', 'include', 'correspondence', 'sample', 'eye', 'gaze', 'information', 'determine', 'mean', 'data', 'variance', 'data', 'sample', 'eye', 'piece', 'second', 'sample', 'data', 'preprocessing', 'sample', 'eye', 'contain', 'piece', 'second', 'sample', 'data', 'base', 'mean', 'data', 'variance', 'data', 'obtain', 'preprocessed', 'sample', 'eye', 'set', 'preprocessed', 'sample', 'eye', 'correspond', 'gaze', 'information', 'second', 'model', 'train', 'sample', 'perform', 'training', 'use', 'machine', 'learn', 'second', 'model', 'training', 'sample', 'obtain', 'gaze', 'information', 'model', 'device', 'ed', 'wherein', 'angle', 'rotation', 'preset', 'direction', 'comprise', 'pitch', 'angle', 'yaw', 'angle', 'roll', 'angle', 'wherein', 'pitch', 'angle', 'refers', 'angle', 'rotation', 'around', 'x-axis', 'yaw', 'angle', 'refers', 'angle', 'rotation', 'around', 'y-axis', 'roll', 'angle', 'refers', 'angle', 'rotation', 'around', 'z-axis', 'non-transitory', 'computer-readable', 'storage', 'medium', 'payment', 'base', 'face', 'configure', 'instruction', 'executable', 'one', 'cause', 'one', 'perform', 'operation', 'comprise', 'acquire', 'first', 'face', 'information', 'target', 'extract', 'first', 'characteristic', 'information', 'first', 'face', 'information', 'wherein', 'first', 'characteristic', 'information', 'include', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'determine', 'whether', 'target', 'willingness', 'pay', 'accord', 'head', 'posture', 'information', 'target', 'gaze', 'information', 'target', 'include', 'determine', 'whether', 'angle', 'rotation', 'preset', 'direction', 'less', 'angle', 'threshold', 'wherein', 'head', 'posture', 'information', 'include', 'angle', 'rotation', 'preset', 'direction', 'determine', 'whether', 'probability', 'value', 'gaze', 'payment', 'screen', 'great', 'probability', 'threshold', 'wherein', 'gaze', 'information', 'include', 'probability', 'value', 'gaze', 'payment', 'screen', 'response', 'determine', 'angle', 'rotation', 'preset', 'direction', 'less', 'angle', 'threshold', 'probability', 'value', 'gaze', 'payment', 'screen', 'great', 'probability', 'threshold', 'determine', 'target', 'willingness', 'pay', 'response', 'determine', 'target', 'willingness', 'pay', 'complete', 'payment', 'operation', 'base', 'face', 'storage', 'medium', 'ed', 'wherein', 'complete', 'payment', 'operation', 'base', 'face', 'comprise', 'trigger', 'perform', 'payment', 'initiating', 'operation', 'acquire', 'second', 'face', 'information', 'base', 'face', 'determine', 'whether', 'second', 'characteristic', 'information', 'extract', 'second', 'face', 'information', 'indicate', 'willingness', 'pay', 'response', 'determine', 'second', 'characteristic', 'information', 'indicate', 'willingness', 'pay', 'trigger', 'perform', 'payment', 'confirmation', 'operation', 'complete', 'payment', 'operation', 'base', 'payment', 'account', 'information', 'correspond', 'target', 'storage', 'medium', 'ed', 'wherein', 'determine', 'whether', 'second', 'characteristic', 'information', 'extract', 'second', 'face', 'information', 'indicate', 'willingness', 'pay', 'comprises', 'determine', 'whether', 'current', 'correspond', 'second', 'face', 'information', 'consistent', 'target', 'response', 'determine', 'current', 'consistent', 'target', 'determine', 'whether', 'target', 'willingness', 'pay', 'accord', 'second', 'characteristic', 'information', 'extract', 'second', 'face', 'information', 'storage', 'medium', 'ed', 'wherein', 'extract', 'first', 'characteristic', 'information', 'first', 'face', 'information', 'comprise', 'determine', 'head', 'posture', 'information', 'target', 'use', 'head', 'posture', 'model', 'base', 'first', 'face', 'information', 'determine', 'gaze', 'information', 'target', 'use', 'gaze', 'information', 'model', 'base', 'characteristic', 'eye', 'region', 'first', 'face', 'information', 'storage', 'medium', 'ed', 'wherein', 'head', 'posture', 'model', 'obtain', 'train', 'acquire', 'first', 'sample', 'data', 'set', 'wherein', 'first', 'sample', 'data', 'set', 'include', 'piece', 'first', 'sample', 'data', 'piece', 'first', 'sample', 'data', 'include', 'correspondence', 'sample', 'face', 'head', 'posture', 'information', 'determine', 'mean', 'data', 'variance', 'data', 'sample', 'face', 'piece', 'first', 'sample', 'data', 'preprocessing', 'sample', 'face', 'contain', 'piece', 'first', 'sample', 'data', 'base', 'mean', 'data', 'variance', 'data', 'obtain', 'preprocessed', 'sample', 'face', 'set', 'preprocessed', 'sample', 'face', 'correspond', 'head', 'posture', 'information', 'first', 'model', 'train', 'sample', 'perform', 'training', 'use', 'machine', 'learning', 'base', 'first', 'model', 'training', 'sample', 'obtain', 'head', 'posture', 'model', 'wherein', 'gaze', 'information', 'model', 'obtain', 'train', 'acquire', 'second', 'sample', 'data', 'set', 'wherein', 'second', 'sample', 'data', 'set', 'include', 'piece', 'second', 'sample', 'data', 'piece', 'second', 'sample', 'data', 'include', 'correspondence', 'sample', 'eye', 'gaze', 'information', 'determine', 'mean', 'data', 'variance', 'data', 'sample', 'eye', 'piece', 'second', 'sample', 'data', 'preprocessing', 'sample', 'eye', 'contain', 'piece', 'second', 'sample', 'data', 'base', 'mean', 'data', 'variance', 'data', 'obtain', 'preprocessed', 'sample', 'eye', 'set', 'preprocessed', 'sample', 'eye', 'correspond', 'gaze', 'information', 'second', 'model', 'train', 'sample', 'perform', 'training', 'use', 'machine', 'learn', 'base', 'second', 'model', 'training', 'sample', 'obtain', 'gaze', 'information', 'model', 'storage', 'medium', 'ed', 'wherein', 'angle', 'rotation', 'preset', 'direction', 'comprise', 'pitch', 'angle', 'yaw', 'angle', 'roll', 'angle', 'wherein', 'pitch', 'angle', 'refers', 'angle', 'rotation', 'around', 'x-axis', 'yaw', 'angle', 'refers', 'angle', 'rotation', 'around', 'y-axis', 'roll', 'angle', 'refers', 'angle', 'rotation', 'around', 'z-axis', 'comprise', 'detect', 'motion', 'detection', 'module', 'motion', 'subject', 'within', 'predetermined', 'area', 'view', 'assign', 'unique', 'session', 'identification', 'number', 'subject', 'detect', 'within', 'predetermined', 'area', 'view', 'detect', 'facial', 'area', 'subject', 'detect', 'within', 'predetermined', 'area', 'view', 'generate', 'facial', 'area', 'subject', 'assess', 'quality', 'facial', 'area', 'subject', 'determine', 'identity', 'subject', 'base', 'facial', 'area', 'subject', 'identify', 'intent', 'subject', 'authorize', 'access', 'point', 'entry', 'base', 'determine', 'identity', 'subject', 'base', 'intent', 'subject', 'comprise', 'determine', 'one', 'additional', 'subject', 'within', 'predetermined', 'area', 'view', 'assign', 'unique', 'session', 'identification', 'number', 'one', 'additional', 'subject', 'detect', 'within', 'predetermined', 'area', 'view', 'wherein', 'assess', 'quality', 'facial', 'area', 'subject', 'comprise', 'assess', 'whether', 'quality', 'facial', 'area', 'object', 'equates', 'predetermine', 'metric', 'quality', 'upon', 'determine', 'quality', 'facial', 'area', 'object', 'inferior', 'predetermine', 'metric', 'quality', 'discard', 'facial', 'area', 'subject', 'generate', 'second', 'facial', 'area', 'subject', 'comprise', 'detect', 'whether', 'facial', 'area', 'subject', 'photographic', 'upon', 'detect', 'facial', 'area', 'subject', 'photographic', 'generate', 'warn', 'restrict', 'access', 'point', 'entry', 'comprise', 'conduce', 'incremental', 'training', 'facial', 'area', 'subject', 'wherein', 'conduce', 'incremental', 'training', 'facial', 'area', 'subject', 'comprises', 'capture', 'first', 'facial', 'area', 'facial', 'landmark', 'convert', 'first', 'facial', 'area', 'first', 'numeric', 'vector', 'capture', 'second', 'facial', 'area', 'facial', 'landmark', 'convert', 'second', 'facial', 'area', 'second', 'numeric', 'vector', 'calculate', 'weight', 'mean', 'first', 'numeric', 'vector', 'second', 'numeric', 'vector', 'wherein', 'weight', 'mean', 'represent', 'change', 'facial', 'area', 'store', 'weighted', 'mean', 'database', 'wherein', 'determine', 'identity', 'subject', 'base', 'facial', 'area', 'subject', 'comprise', 'compare', 'facial', 'area', 'subject', 'store', 'database', 'authenticate', 'subject', 'wherein', 'identify', 'intent', 'subject', 'comprise', 'upon', 'detect', 'facial', 'area', 'bound', 'box', 'commence', 'authentication', 'subject', 'calculate', 'directional', 'vector', 'face', 'subject', 'determine', 'intent', 'subject', 'gain', 'access', 'point', 'entry', 'base', 'directional', 'vector', 'face', 'subject', 'grant', 'access', 'point', 'entry', 'base', 'authentication', 'subject', 'base', 'determine', 'intent', 'subject', 'non-transitory', 'computer', 'readable', 'medium', 'program', 'instruction', 'store', 'thereon', 'response', 'execution', 'compute', 'device', 'cause', 'compute', 'device', 'perform', 'operation', 'comprise', 'detect', 'motion', 'subject', 'within', 'predetermined', 'area', 'view', 'assign', 'unique', 'session', 'identification', 'number', 'subject', 'detect', 'within', 'predetermined', 'area', 'view', 'detect', 'facial', 'area', 'subject', 'detect', 'within', 'predetermined', 'area', 'view', 'generate', 'facial', 'area', 'subject', 'assess', 'quality', 'facial', 'area', 'subject', 'determine', 'identity', 'subject', 'base', 'facial', 'area', 'subject', 'identify', 'intent', 'subject', 'authorize', 'access', 'point', 'entry', 'base', 'determine', 'identity', 'subject', 'base', 'intent', 'subject', 'non-transitory', 'computer', 'readable', 'medium', 'comprise', 'determine', 'one', 'additional', 'subject', 'within', 'predetermined', 'area', 'view', 'assign', 'unique', 'session', 'identification', 'number', 'one', 'additional', 'subject', 'detect', 'within', 'predetermined', 'area', 'view', 'non-transitory', 'computer', 'readable', 'medium', 'wherein', 'assess', 'quality', 'facial', 'area', 'subject', 'comprise', 'assess', 'whether', 'quality', 'facial', 'area', 'object', 'equates', 'predetermine', 'metric', 'quality', 'upon', 'determine', 'quality', 'facial', 'area', 'object', 'inferior', 'predetermine', 'metric', 'quality', 'discard', 'facial', 'area', 'subject', 'generate', 'second', 'facial', 'area', 'subject', 'non-transitory', 'computer', 'readable', 'medium', 'comprise', 'detect', 'whether', 'facial', 'area', 'subject', 'photographic', 'upon', 'detect', 'facial', 'area', 'subject', 'photographic', 'generate', 'warn', 'restrict', 'access', 'access', 'point', 'non-transitory', 'computer', 'readable', 'medium', 'comprise', 'conduce', 'incremental', 'training', 'facial', 'area', 'subject', 'non-transitory', 'computer', 'readable', 'medium', 'wherein', 'conduce', 'incremental', 'training', 'facial', 'area', 'subject', 'comprises', 'capture', 'first', 'facial', 'area', 'facial', 'landmark', 'convert', 'first', 'facial', 'area', 'first', 'numeric', 'vector', 'capture', 'second', 'facial', 'area', 'facial', 'landmark', 'convert', 'second', 'facial', 'area', 'second', 'numeric', 'vector', 'calculate', 'weight', 'mean', 'first', 'numeric', 'vector', 'second', 'numeric', 'vector', 'wherein', 'weight', 'mean', 'represent', 'change', 'facial', 'area', 'store', 'weighted', 'mean', 'database', 'apparatus', 'face', 'comprise', 'memory', 'store', 'computer', 'program', 'instruction', 'computer', 'program', 'instruction', 'execute', 'cause', 'perform', 'operation', 'comprise', 'detect', 'motion', 'subject', 'within', 'predetermined', 'area', 'view', 'assign', 'unique', 'session', 'identification', 'number', 'subject', 'detect', 'within', 'predetermined', 'area', 'view', 'detect', 'facial', 'area', 'subject', 'detect', 'within', 'predetermined', 'area', 'view', 'generate', 'facial', 'area', 'subject', 'assess', 'quality', 'facial', 'area', 'subject', 'determine', 'identity', 'subject', 'base', 'facial', 'area', 'subject', 'identify', 'intent', 'subject', 'authorize', 'access', 'point', 'entry', 'base', 'determine', 'identity', 'subject', 'base', 'intent', 'subject', 'apparatus', 'comprise', 'determine', 'one', 'additional', 'subject', 'within', 'predetermined', 'area', 'view', 'assign', 'unique', 'session', 'identification', 'number', 'one', 'additional', 'subject', 'detect', 'within', 'predetermined', 'area', 'view', 'apparatus', 'wherein', 'assess', 'quality', 'facial', 'area', 'subject', 'comprise', 'assess', 'whether', 'quality', 'facial', 'area', 'object', 'equates', 'predetermine', 'metric', 'quality', 'upon', 'determine', 'quality', 'facial', 'area', 'object', 'inferior', 'predetermine', 'metric', 'quality', 'discard', 'facial', 'area', 'subject', 'generate', 'second', 'facial', 'area', 'subject', 'apparatus', 'comprise', 'detect', 'whether', 'facial', 'area', 'subject', 'photographic', 'upon', 'detect', 'facial', 'area', 'subject', 'photographic', 'generate', 'warn', 'restrict', 'access', 'access', 'point', 'apparatus', 'comprise', 'conduce', 'incremental', 'training', 'facial', 'area', 'subject', 'apparatus', 'wherein', 'conduce', 'incremental', 'training', 'facial', 'area', 'subject', 'comprises', 'capture', 'first', 'facial', 'area', 'facial', 'landmark', 'convert', 'first', 'facial', 'area', 'first', 'numeric', 'vector', 'capture', 'second', 'facial', 'area', 'facial', 'landmark', 'convert', 'second', 'facial', 'area', 'second', 'numeric', 'vector', 'calculate', 'weight', 'mean', 'first', 'numeric', 'vector', 'second', 'numeric', 'vector', 'wherein', 'weight', 'mean', 'represent', 'change', 'facial', 'area', 'store', 'weighted', 'mean', 'database', 'robot', 'comprise', 'body', 'configure', 'rotate', 'tilt', 'camera', 'couple', 'body', 'configured', 'rotate', 'tilt', 'accord', 'rotate', 'tilt', 'body', 'wherein', 'camera', 'configure', 'acquire', 'video', 'space', 'face', 'unit', 'configure', 'recognize', 'respective', 'one', 'person', 'video', 'unit', 'configure', 'track', 'motion', 'recognize', 'one', 'person', 'controller', 'configure', 'calculate', 'respective', 'size', 'one', 'person', 'select', 'first', 'person', 'among', 'one', 'person', 'base', 'calculated', 'size', 'control', 'least', 'one', 'direction', 'rotation', 'camera', 'angle', 'tilt', 'camera', 'focal', 'distance', 'camera', 'base', 'tracked', 'motion', 'recognize', 'face', 'first', 'person', 'robot', 'wherein', 'controller', 'configure', 'control', 'direction', 'rotation', 'camera', 'angle', 'tilt', 'camera', 'achieve', 'particular', 'camera', 'relative', 'face', 'first', 'person', 'control', 'focal', 'distance', 'camera', 'compare', 'respective', 'size', 'face', 'first', 'person', 'motion', 'first', 'person', 'robot', 'wherein', 'particular', 'occurs', 'camera', 'general', 'direction', 'face', 'first', 'person', 'robot', 'wherein', 'controller', 'configure', 'normalize', 'size', 'one', 'person', 'base', 'interocular', 'distance', 'select', 'first', 'person', 'base', 'normalized', 'size', 'one', 'person', 'robot', 'wherein', 'controller', 'configure', 'select', 'person', 'large', 'face', 'size', 'among', 'one', 'person', 'first', 'person', 'robot', 'comprise', 'microphone', 'configure', 'receive', 'spoken', 'audio', 'present', 'space', 'wherein', 'controller', 'configure', 'select', 'first', 'person', 'base', 'receive', 'spoken', 'audio', 'robot', 'wherein', 'controller', 'configure', 'control', 'gain', 'microphone', 'compare', 'respective', 'size', 'face', 'first', 'person', 'motion', 'first', 'person', 'robot', 'wherein', 'controller', 'configure', 'calculate', 'position', 'speak', 'audio', 'provide', 'select', 'first', 'person', 'base', 'whether', 'one', 'person', 'position', 'voice', 'signal', 'provide', 'robot', 'wherein', 'controller', 'configure', 'select', 'second', 'person', 'first', 'person', 'among', 'one', 'person', 'second', 'person', 'locate', 'position', 'speak', 'audio', 'provide', 'robot', 'wherein', 'controller', 'configure', 'select', 'second', 'person', 'large', 'face', 'size', 'first', 'person', 'among', 'one', 'person', 'none', 'one', 'person', 'locate', 'position', 'speak', 'audio', 'provide', 'robot', 'wherein', 'controller', 'configure', 'select', 'second', 'person', 'large', 'face', 'size', 'first', 'person', 'among', 'one', 'person', 'person', 'among', 'one', 'person', 'locate', 'position', 'speak', 'audio', 'provide', 'robot', 'comprise', 'speaker', 'wherein', 'controller', 'configure', 'control', 'volume', 'speaker', 'compare', 'respective', 'size', 'face', 'first', 'person', 'motion', 'first', 'person', 'robot', 'wherein', 'body', 'configure', 'rotate', 'lateral', 'direction', 'tilt', 'vertical', 'direction', 'comprise', 'camera', 'couple', 'body', 'configured', 'rotate', 'tilt', 'wherein', 'camera', 'configure', 'acquire', 'video', 'space', 'within', 'one', 'person', 'position', 'configured', 'recognize', 'respective', 'one', 'person', 'video', 'track', 'motion', 'recognize', 'one', 'person', 'calculate', 'respective', 'size', 'one', 'person', 'select', 'first', 'person', 'among', 'one', 'person', 'base', 'calculated', 'size', 'control', 'least', 'one', 'direction', 'rotation', 'camera', 'angle', 'tilt', 'camera', 'focal', 'distance', 'camera', 'base', 'tracked', 'motion', 'recognize', 'face', 'first', 'person', 'comprise', 'acquire', 'camera', 'video', 'space', 'within', 'one', 'person', 'position', 'respective', 'one', 'person', 'video', 'motion', 'recognize', 'one', 'person', 'calculate', 'respective', 'size', 'one', 'person', 'select', 'first', 'person', 'among', 'one', 'person', 'base', 'calculated', 'size', 'control', 'least', 'one', 'direction', 'rotation', 'camera', 'angle', 'tilt', 'camera', 'focal', 'distance', 'camera', 'base', 'tracked', 'motion', 'recognize', 'face', 'first', 'person', 'infer', 'topic', 'multimodal', 'file', 'comprise', 'receive', 'multimodal', 'file', 'extract', 'set', 'entity', 'multimodal', 'file', 'link', 'set', 'entity', 'produce', 'set', 'link', 'entity', 'obtain', 'reference', 'information', 'set', 'entity', 'base', 'least', 'reference', 'information', 'generate', 'graph', 'set', 'link', 'entity', 'graph', 'comprise', 'node', 'edge', 'base', 'least', 'nodes', 'edge', 'graph', 'determine', 'cluster', 'graph', 'base', 'least', 'cluster', 'graph', 'identify', 'topic', 'candidate', 'extract', 'feature', 'cluster', 'graph', 'base', 'least', 'extracted', 'feature', 'select', 'least', 'one', 'topicid', 'among', 'topic', 'candidate', 'represent', 'least', 'one', 'cluster', 'index', 'multimodal', 'file', 'least', 'one', 'topicid', 'wherein', 'multimodal', 'file', 'comprise', 'video', 'portion', 'audio', 'portion', 'wherein', 'extract', 'set', 'entity', 'multimodal', 'file', 'comprises', 'detect', 'object', 'video', 'portion', 'multimodal', 'file', 'detect', 'text', 'audio', 'portion', 'multimodal', 'file', 'wherein', 'detect', 'object', 'comprise', 'perform', 'face', 'wherein', 'detect', 'text', 'comprises', 'perform', 'speech', 'text', 'process', 'comprise', 'identify', 'language', 'use', 'audio', 'portion', 'multimodal', 'file', 'wherein', 'perform', 'speech', 'text', 'process', 'comprise', 'perform', 'speech', 'text', 'process', 'identify', 'language', 'comprise', 'translate', 'detect', 'text', 'comprise', 'determine', 'significant', 'cluster', 'insignificant', 'cluster', 'determine', 'cluster', 'wherein', 'extract', 'feature', 'cluster', 'graph', 'comprises', 'extract', 'feature', 'significant', 'cluster', 'graph', 'wherein', 'extract', 'feature', 'cluster', 'graph', 'comprises', 'least', 'one', 'process', 'select', 'list', 'consist', 'determine', 'graph', 'diameter', 'determine', 'jaccard', 'coefficient', 'wherein', 'select', 'least', 'one', 'topicid', 'represent', 'least', 'one', 'cluster', 'comprises', 'base', 'least', 'extracted', 'feature', 'map', 'topic', 'candidate', 'probability', 'interval', 'base', 'least', 'map', 'rank', 'topic', 'candidate', 'within', 'least', 'one', 'cluster', 'select', 'least', 'one', 'topicid', 'base', 'least', 'ranking', 'comprise', 'translate', 'least', 'one', 'topicid', 'wherein', 'index', 'multimodal', 'file', 'least', 'one', 'topicid', 'comprise', 'index', 'multimodal', 'file', 'least', 'one', 'translate', 'topicid', 'system', 'infer', 'topic', 'multimodal', 'file', 'system', 'comprise', 'entity', 'extraction', 'component', 'comprise', 'object', 'detection', 'component', 'speech', 'text', 'component', 'operative', 'extract', 'set', 'entity', 'multimodal', 'file', 'comprise', 'video', 'portion', 'audio', 'portion', 'entity', 'link', 'component', 'operative', 'link', 'extract', 'set', 'entity', 'produce', 'set', 'link', 'entity', 'information', 'retrieval', 'component', 'operative', 'obtain', 'reference', 'information', 'extract', 'set', 'entity', 'graph', 'analysis', 'component', 'operative', 'generate', 'graph', 'set', 'link', 'entity', 'graph', 'comprise', 'node', 'edge', 'base', 'least', 'nodes', 'edge', 'graph', 'determine', 'cluster', 'graph', 'base', 'least', 'cluster', 'graph', 'identify', 'topic', 'candidate', 'extract', 'feature', 'cluster', 'graph', 'topicid', 'selection', 'component', 'operative', 'rank', 'topic', 'candidate', 'within', 'least', 'one', 'cluster', 'base', 'least', 'ranking', 'select', 'least', 'one', 'topicid', 'among', 'topic', 'candidate', 'represent', 'least', 'one', 'cluster', 'video', 'indexer', 'operative', 'index', 'multimodal', 'file', 'least', 'one', 'topicid', 'system', 'wherein', 'object', 'detection', 'component', 'operative', 'perform', 'face', 'system', 'wherein', 'speech', 'text', 'component', 'operative', 'extract', 'entity', 'information', 'least', 'two', 'different', 'language', 'one', 'computer', 'storage', 'device', 'computer-executable', 'instruction', 'store', 'thereon', 'infer', 'topic', 'multimodal', 'file', 'execution', 'computer', 'cause', 'computer', 'perform', 'operation', 'comprise', 'receive', 'multimodal', 'file', 'comprise', 'video', 'portion', 'audio', 'portion', 'extract', 'set', 'entity', 'multimodal', 'file', 'wherein', 'extract', 'set', 'entity', 'multimodal', 'file', 'comprises', 'detect', 'object', 'video', 'portion', 'multimodal', 'file', 'face', 'detect', 'text', 'audio', 'portion', 'multimodal', 'file', 'speech', 'text', 'process', 'disambiguate', 'among', 'set', 'detect', 'entity', 'name', 'link', 'set', 'entity', 'produce', 'set', 'link', 'entity', 'obtain', 'reference', 'information', 'set', 'entity', 'base', 'least', 'reference', 'information', 'generate', 'graph', 'set', 'link', 'entity', 'graph', 'comprise', 'node', 'edge', 'base', 'least', 'nodes', 'edge', 'graph', 'determine', 'cluster', 'graph', 'determine', 'significant', 'cluster', 'insignificant', 'cluster', 'determine', 'cluster', 'base', 'least', 'significant', 'cluster', 'graph', 'identify', 'topic', 'candidate', 'extract', 'feature', 'significant', 'cluster', 'graph', 'base', 'least', 'extracted', 'feature', 'map', 'topic', 'candidate', 'probability', 'interval', 'base', 'least', 'map', 'rank', 'topic', 'candidate', 'within', 'least', 'one', 'significant', 'cluster', 'base', 'rank', 'select', 'least', 'one', 'topicid', 'among', 'topic', 'candidate', 'represent', 'least', 'one', 'significant', 'cluster', 'index', 'multimodal', 'file', 'least', 'one', 'topicid', 'one', 'computer', 'storage', 'device', 'wherein', 'operation', 'comprise', 'identify', 'language', 'use', 'audio', 'portion', 'multimodal', 'file', 'detect', 'text', 'audio', 'portion', 'multimodal', 'file', 'speech', 'text', 'process', 'comprise', 'perform', 'speech', 'text', 'process', 'identify', 'language权利要求', 'system', 'alert', 'vision', 'impairment', 'say', 'system', 'comprise', 'process', 'unit', 'configure', 'operable', 'receive', 'scene', 'data', 'indicative', 'scene', 'least', 'one', 'consumer', 'environment', 'identify', 'scene', 'data', 'certain', 'consumer', 'identify', 'event', 'indicative', 'behavioral', 'compensation', 'vision', 'impairment', 'upon', 'identification', 'event', 'send', 'notification', 'relate', 'vision', 'impairment', 'system', 'comprise', 'least', 'one', 'sense', 'unit', 'configure', 'operable', 'detect', 'scene', 'data', 'system', 'wherein', 'say', 'least', 'one', 'sense', 'unit', 'comprise', 'least', 'one', 'least', 'one', 'image', 'unit', 'configure', 'operable', 'capture', 'least', 'one', 'least', 'portion', 'consumer', \"'s\", 'body', 'least', 'one', 'motion', 'detector', 'configure', 'operable', 'detect', 'consumer', 'data', 'indicative', 'motion', 'consumer', 'least', 'one', 'eye', 'tracker', 'configure', 'operable', 'eye', 'motion', 'consumer', 'system', 'wherein', 'least', 'one', 'image', 'unit', 'comprise', 'camera', 'place', 'different', 'height', 'system', 'one', 'wherein', 'say', 'sense', 'unit', 'accommodate', 'optical', 'digital', 'eyewear', 'frame', 'display', 'system', 'one', 'wherein', 'say', 'processing', 'unit', 'configure', 'operable', 'identify', 'consumer', \"'s\", 'condition', 'say', 'consumer', \"'s\", 'condition', 'comprise', 'consumer', 'data', 'indicative', 'consumer', \"'s\", 'position', 'location', 'relative', 'least', 'one', 'object', 'consumer', \"'s\", 'environment', 'say', 'consumer', 'data', 'comprises', 'least', 'one', 'consumer', \"'s\", 'face', 'eyewear', 'posture', 'position', 'sound', 'motion', 'system', 'one', 'wherein', 'say', 'event', 'comprises', 'least', 'one', 'position', 'head', 'increase', 'decrease', 'view', 'distance', 'consumer', 'view', 'object', 'change', 'position', 'eyeglass', 'worn', 'consumer', 'system', 'one', 'wherein', 'say', 'event', 'identify', 'identifying', 'feature', 'indicative', 'behavioral', 'compensation', 'perform', 'bruckner', 'test', 'perform', 'hirschberg', 'test', 'measure', 'blink', 'count', 'frequency', 'system', 'wherein', 'feature', 'indicative', 'behavioral', 'compensation', 'comprises', 'squint', 'head', 'certain', 'distance', 'object', 'consumer', \"'s\", 'eye', 'certain', 'position', 'eyeglasses', 'consumer', \"'s\", 'face', 'strabismus', 'cataracts', 'reflection', 'eye', 'system', 'one', 'wherein', 'notification', 'include', 'least', 'one', 'data', 'indicative', 'identify', 'event', 'data', 'indicative', 'identified', 'consumer', 'ophthalmologic', 'recommendation', 'base', 'identified', 'event', 'lack', 'event', 'appointment', 'vision', 'test', 'system', 'one', 'wherein', 'say', 'processing', 'unit', 'comprise', 'memory', 'store', 'least', 'one', 'reference', 'data', 'indicative', 'behavioral', 'compensation', 'vision', 'impairment', 'data', 'indicative', 'notification', 'data', 'indicative', 'follow-up', 'notification', 'system', 'wherein', 'say', 'processing', 'unit', 'configure', 'least', 'one', 'identify', 'event', 'upon', 'comparison', 'detect', 'data', 'reference', 'data', 'determine', 'probability', 'vision', 'impairment', 'consumer', 'base', 'comparison', 'system', 'one', 'wherein', 'say', 'processing', 'unit', 'comprise', 'communication', 'interface', 'configure', 'send', 'notification', 'least', 'one', 'identified', 'consumer', 'party', 'system', 'one', 'wherein', 'say', 'processing', 'unit', 'configure', 'provide', 'frame', 'recommendation', 'system', 'one', 'wherein', 'say', 'memory', 'configure', 'storing', 'database', 'include', 'multiplicity', 'data', 'set', 'related', 'spectacle', 'frame', 'model', 'size', 'system', 'accord', 'wherein', 'say', 'process', 'unit', 'configure', 'operable', 'correlate', 'frame', 'parameter', 'ophthalmic', 'prescription', 'system', 'accord', 'wherein', 'say', 'process', 'unit', 'configure', 'operable', 'correlate', 'frame', 'parameter', 'facial', 'feature', 'system', 'accord', 'wherein', 'say', 'process', 'unit', 'configure', 'operable', 'correlate', 'frame', 'parameter', 'eyewear', 'preference', 'system', 'accord', 'comprise', 'server', 'least', 'one', 'computer', 'entity', 'link', 'server', 'via', 'network', 'wherein', 'say', 'network', 'configure', 'receive', 'respond', 'request', 'send', 'across', 'network', 'transmit', 'one', 'module', 'computer', 'executable', 'program', 'instruction', 'displayable', 'data', 'network', 'connect', 'computer', 'platform', 'response', 'request', 'wherein', 'say', 'module', 'include', 'module', 'configure', 'receive', 'transmit', 'information', 'transmit', 'frame', 'recommendation', 'optical', 'lens', 'option', 'recommendation', 'base', 'receive', 'information', 'display', 'network', 'connect', 'computer', 'platform', 'computer', 'program', 'instruction', 'store', 'local', 'storage', 'execute', 'process', 'unit', 'cause', 'process', 'unit', 'receive', 'data', 'indicative', 'scene', 'least', 'one', 'consumer', 'environment', 'identify', 'data', 'certain', 'consumer', 'identify', 'event', 'indicative', 'behavioral', 'compensation', 'vision', 'impairment', 'upon', 'identification', 'event', 'send', 'notification', 'relate', 'vision', 'impairment', 'computer', 'program', 'product', 'store', 'tangible', 'computer', 'readable', 'medium', 'comprise', 'library', 'software', 'module', 'cause', 'computer', 'execute', 'prompt', 'information', 'pertinent', 'least', 'one', 'eyeglasses', 'recommendation', 'optical', 'lens', 'option', 'recommendation', 'store', 'say', 'information', 'display', 'eyewear', 'recommendation', 'computer', 'program', 'product', 'wherein', 'say', 'library', 'comprises', 'module', 'frame', 'selection', 'point', 'sale', 'advertise', 'computer', 'platform', 'facilitate', 'eye', 'glass', 'market', 'selection', 'comprise', 'camera', 'configure', 'execute', 'computer', 'program', 'instruction', 'cause', 'take', 'consumer', 'identify', 'certain', 'consumer', 'identify', 'event', 'indicative', 'behavioral', 'compensation', 'vision', 'impairment', 'upon', 'identification', 'event', 'send', 'notification', 'relate', 'vision', 'impairment', 'local', 'storage', 'executable', 'instruction', 'carry', 'storage', 'information', 'alert', 'vision', 'impairment', 'say', 'comprise', 'identify', 'certain', 'individual', 'scene', 'data', 'indicative', 'scene', 'least', 'one', 'consumer', 'environment', 'identify', 'event', 'indicative', 'behavioral', 'compensation', 'vision', 'impairment', 'upon', 'identification', 'event', 'send', 'notification', 'vision', 'impairment', 'comprise', 'detect', 'data', 'indicative', 'scene', 'least', 'one', 'consumer', 'retail', 'environment', 'wherein', 'detect', 'data', 'indicative', 'least', 'one', 'consumer', 'comprise', 'least', 'one', 'capture', 'least', 'one', 'least', 'one', 'consumer', 'detect', 'data', 'indicative', 'motion', 'consumer', 'eye', 'motion', 'consumer', 'wherein', 'capture', 'least', 'one', 'least', 'one', 'consumer', 'comprise', 'continuously', 'record', 'scene', 'one', 'comprising', 'identify', 'data', 'consumer', \"'\", 'condition', 'include', 'data', 'indicative', 'consumer', \"'s\", 'position', 'location', 'relative', 'consumer', \"'s\", 'environment', 'say', 'data', 'comprise', 'least', 'one', 'consumer', \"'s\", 'face', 'posture', 'position', 'sound', 'motion', 'one', 'wherein', 'say', 'event', 'comprises', 'least', 'one', 'position', 'head', 'increase', 'decrease', 'view', 'distance', 'consumer', 'view', 'object', 'change', 'position', 'eyeglass', 'worn', 'consumer', 'one', 'wherein', 'identify', 'event', 'comprises', 'identify', 'feature', 'indicative', 'behavioral', 'compensation', 'perform', 'bruckner', 'test', 'perform', 'hirschberg', 'test', 'measure', 'blink', 'countfrequency', 'wherein', 'feature', 'indicative', 'behavioral', 'compensation', 'comprises', 'squint', 'head', 'certain', 'distance', 'object', 'consumer', \"'s\", 'eye', 'certain', 'position', 'eyeglasses', 'consumer', \"'s\", 'face', 'strabismus', 'cataracts', 'reflection', 'eye', 'one', 'wherein', 'identify', 'least', 'one', 'consumer', 'retail', 'environment', 'comprise', 'least', 'one', 'receive', 'data', 'characterize', 'retail', 'environment', 'perform', 'face', 'one', 'wherein', 'send', 'notification', 'comprise', 'send', 'notification', 'least', 'one', 'identified', 'consumer', 'third', 'party', 'one', 'wherein', 'notification', 'include', 'least', 'one', 'data', 'indicative', 'identify', 'event', 'data', 'indicative', 'identified', 'consumer', 'ophthalmologic', 'recommendation', 'base', 'identified', 'event', 'lack', 'event', 'appointment', 'vision', 'test', 'one', 'comprise', 'store', 'least', 'one', 'reference', 'data', 'indicative', 'behavioral', 'compensation', 'vision', 'impairment', 'data', 'indicative', 'notification', 'data', 'indicative', 'follow-up', 'notification', 'comprise', 'identify', 'event', 'upon', 'comparison', 'detect', 'data', 'reference', 'data', 'determine', 'probability', 'vision', 'impairment', 'consumer', 'base', 'comparison', 'computer', 'program', 'intend', 'stored', 'memory', 'unit', 'computer', 'system', 'removable', 'memory', 'medium', 'adapt', 'cooperate', 'reader', 'unit', 'comprise', 'instruction', 'implement', 'accord']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_text_c = []\n",
    "\n",
    "for tuple in simpler_POS_text_c:\n",
    "    if (tuple[1] == None):\n",
    "        lemmatized_text_c.append(lemmatizer.lemmatize(tuple[0]))\n",
    "    else:\n",
    "        lemmatized_text_c.append(lemmatizer.lemmatize(tuple[0], pos=tuple[1]))\n",
    "    \n",
    "print(lemmatized_text_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17060a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"electronic apparatus include capture storage operation method thereof provide capture capture storage record module couple capture storage configure configure capture capture head perform obtain detect facial landmark within estimate head posture angle accord facial landmark calculate gaze position gaze screen accord head posture angle rotation reference angle predetermine calibration position configure screen display correspond visual effect accord gaze positionthe present disclosure provide product thereof adopts fusion method perform machine learn computation technical effect present disclosure include few computation less power consumptiona method detect body information passenger vehicle base human ' status provide method include step passenger body information-detecting inputting interior vehicle face network detect face passenger output passenger feature information inputting interior body network detect body output body-part length information b retrieve specific height mapping information refer height map table ratio segment body portion human group height per human group acquire specific height specific passenger retrieve specific weight map information weight mapping table correlation height weight per human group acquire weight specific passenger refer specific heighttechniques relate improved video cod base face detection region extraction track discuss technique may include perform facial search video frame determine candidate video frame test candidate base skin tone information determine valid invalid rejecting invalid encode video frame base valid generate cod bitstreama method manage smart database store facial face provide method include step manage count specific facial corresponding specific person smart database new facial continuously store determine whether first count value represent count specific facial satisfies first set value b first count value satisfies first set value inputting specific facial neural aggregation network generate quality score specific facial aggregation specific facial second count value represent count specific quality score among quality score high count thereof satisfies second set value delete part specific facial corresponding uncounted quality score smart databasea capable determine algorithm apply region interest within digital representation present preprocessing module utilizes one feature identification algorithm determine region interest base feature density preprocessing module leverage feature density signature region determine diverse module operate region interest specific embodiment focus structure document also present disclosed approach enhance addition object classifier classifies type object find region interestdisclosed mobile terminal mobile terminal may include front camera obtain face glance sensor tilt certain angle dispose adjacent front camera obtain metadata face controller obtain distance glance sensor front camera distance enable area overlap region first region represent range photographable front camera overlap second region represent range photographable glance sensor maximumthis disclosure provide method apparatus include computer program encode computer storage medium intelligent rout notification related medium program one aspect smart television tv implement track 's tv watch behavior anticipate programming base behavior aspect smart tv implement detect 's presence base detection automatically change tv channel medium program analyze desirable aspect smart tv implement transmit notification instruction electronic within network attempt alert upcoming medium program additionally smart tv implement transmit detection instruction electronic within network whereby electronic attempt detect 's presence voice configure output test depth+multi-spectral include pixel pixel correspond one sensor sensor array camera include least depth value spectral value spectral light sub-band spectral illuminators camera face machine previously train set labeled training depth+multi-spectral structure test depth+multi-spectral face machine configure output confidence value indicate likelihood test depth+multi-spectral include faceembodiments present disclosure relate processing method apparatus electronic method include acquire photo album obtain face cluster collect face information respective photo album acquire face parameter accord face information select cover accord face parameter take face-region cover set face-region cover photo albumtechniques describe herein provide location-based access control secure resource generally describe configuration disclose herein enable dynamically modify access secure resource base one location-related action example technique disclose herein enable compute control access resource compute display secure location secure data configuration technique disclose herein enable controlled access secure resource base least part invitation associate location position data indicate location one embodiment provide method comprise receive piece content salient moment data piece content method comprises base salient moment data determine first path viewport piece content method comprise display viewport display movement viewport base first path playback piece content method comprise generate augmentation salient moment occur piece content present augmentation viewport portion playback augmentation comprise interactive hint guide viewport salient momenta computer-implemented method computer program product provide facial method include receive method also include extracting feature extractor utilizing convolutional neural network cnn enlarge intra-class variance long-tail class feature vector method additionally include generating feature generator discriminative feature vector feature vector method include classify utilize fully connect classifier identity discriminative feature vector method also include control operation -based machine react accordance identitysome embodiment invention provide efficient expressive machine-trained network perform machine learn machine-trained mt network embodiment use novel processing node novel activation function allow mt network efficiently define few processing node layer complex mathematical expression solve particular problem eg face speech etc embodiment activation function eg cup function use numerous process node mt network machine learn activation function configure differently different process node different node emulate implement two different function eg two boolean logical operator xor activation function embodiment periodic function configure implement different function eg different sinusoidal functionsmethods may provide facial least one input utilize hierarchical feature learn pair-wise receptive field theory may use input generate pre-processed multi-channel channel pre-processed may activate base amount feature rich detail within channel similarly local patch may activate base discriminant within local patch may extract local patch discriminant may select order perform feature match pair set may utilize patch feature pool pair-wise matching large-scale training order quickly accurately perform facial low cost memory computationa method control terminal provide terminal include capture apparatus least one acquire capture apparatus motion parameter terminal obtain processing acquire control performed base motion parameter equal less preset parameter threshold skip base motion parameter great preset parameter thresholda drive-through order processing method apparatus disclose drive-through order processing method include receive customer information detect vision provide product information base customer information process product order customer accord present disclosure possible rapidly process order use customer information base customer use artificial intelligence ai model machine learn g networkan processing method perform compute include identify use face one face face correspond respective person capture first identify face extract set profile parameter correspond person first select tile first tile match face correspond person first accordance predefined correspondence set profile parameter correspond person set pre-stored description parameter first tile generate second cover face respective person first correspond first tile share first second predefined order via group chat sessionin one embodiment artificial reality determines performance metric eye track first performance threshold eye track associate head-mounted display wear artificial reality receives first inputs associate body determine region look within field view head-mounted display base receive first input determines vergence distance base least first input associate body region look location one object scene display head-mounted display adjust one configuration head-mounted display base determined vergence distance computer-implemented method provide -based self-guided object detection method include receive set respective grid thereon label regard respective object detect use grid level label data method include train grid-based object detector use grid level label data method also include determine respective bounding box respective object apply local segmentation method additionally include train region-based convolutional neural network rcnn joint object localization object use respective bounding box respective object input rcnna method face comprise multiple phase implement parallel architecture first phase normalization phase whereby capture normalized size orientation illumination store preexisting database second phase feature extractiondistance matrix phase distance matrix generate captured coarse phase generate distance matrix compare distance matrix database use euclidean distance match create candidate list detail phase multiple face algorithm applied candidate list produce final result distance matrix normalized database may break parallel list parallelization feature extractiondistance matrix phase candidate list may also group accord dissimilarity algorithm parallel processing detailed phasean image include pixel matrix provide pixel matrix include phase detection pixel regular pixel performs autofocusing accord pixel data phase detection pixel determine operate resolution regular pixel accord autofocused pixel data phase detection pixel wherein phase detection pixel always-on pixel regular pixel selectively turn autofocusing accomplishedan apparatus include first camera module provide first object first field view second camera module provide second object second field view different first field view first depth map generator generates first depth map first base first second second depth map generator generate second depth map second base first second first depth mapmethods apparatus include computer program encode computer storage medium payment base face provide one method include acquire first face information target extract first characteristic information first face information wherein first characteristic information include head posture information target gaze information target determine whether target willingness pay accord head posture information target gaze information target include determine whether angle rotation preset direction less angle threshold whether probability value gaze payment screen great probability threshold response determine target willingness pay complete payment operation base face novel method apparatus face authentication disclose disclose method comprise detect motion subject within predetermined area view assign unique session identification number subject detect within predetermined area view detect facial area subject detect within predetermined area view generate facial area subject assess quality facial area subject conduce incremental training facial area subject determine identity subject base facial area subject identify intent subject authorize access point entry base determine identity subject base intent subjectdisclosed herein robot electronic acquire video method acquire video use robot robot include camera configure rotate lateral direction tilt vertical direction control least one direction rotation camera angle tilt camera focal distance camera track video acquire camera method disclose infer topic file contain audio video example multimodal multimedia file order facilitate video index set entity extract file link produce graph reference information also obtain set entity entity may draw example wikipedia category large ontological data source analysis graph use unsupervised learn permit determine cluster graph extract cluster possibly use supervise learn provide selection topic identifier topic identifier use index filea face method neural network train method apparatus electronic method comprises obtain first face mean first camera extract first face feature first face compare first face feature pre-stored second face feature obtain reference similarity second face feature obtain extracting feature second face obtain second camera second camera first camera different type camera determine accord reference similarity whether first face feature second face feature correspond person present invention disclose technique alert vision impairment comprise process unit configure operable receive scene data indicative scene least one consumer environment identify scene data certain consumer identify event indicative behavioral compensation vision impairment upon identification event send notification relate vision impairment \""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_text_a = \"\"\n",
    "for abstract_text in lemmatized_text_a:\n",
    "    lem_text_a += abstract_text + \" \" #we pick each word and add to a variable, which will contain all the text\n",
    "lem_text_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a8831ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"configure make screen display frame comprise capture device storage device store module couple capture device storage device configure execute module storage device configure screen display marker object predetermine position configure capture device capture first head look predetermined position perform first face operation first head obtain first face region correspond predetermined position detect first facial landmark correspond first face region calculate rotation reference angle look predetermine position accord first facial landmark configure capture device capture second head perform second head obtain second face region detect second facial landmark within second face region estimate head posture angle accord second facial landmark calculate gaze position screen accord head posture angle rotation reference angle predetermine position configure screen display correspond visual effect accord gaze position accord wherein gaze position comprises first coordinate value first axial direction second coordinate value second axial direction accord wherein head posture angle comprise head pitch angle head yaw angle rotation reference angle comprise first pitch angle second pitch angle first yaw angle second yaw angle correspond predetermine position accord wherein performs interpolation operation extrapolation operation accord first yaw angle second yaw angle first position correspond first yaw angle among predetermined position second position correspond second yaw angle among predetermined position head yaw angle thereby obtain first coordinate value gaze position performs interpolation operation extrapolation operation accord first pitch angle second pitch angle third position correspond first pitch angle among predetermined position fourth position correspond second pitch angle among predetermined position head pitch angle thereby obtain second coordinate value gaze position accord wherein calculates first view distance screen accord first facial landmark estimate second view distance screen accord second facial landmark adjust rotation reference angle gaze position accord second viewing distance first view distance accord wherein map two-dimensional position coordinate second facial landmark plane coordinate system three-dimensional position coordinate three-dimensional coordinate system estimate head posture angle accord three-dimensional position coordinate second facial landmark accord wherein second head comprise wearable device second facial landmark comprise third facial landmark cover wearable device accord wherein second head comprise wearable device second facial landmark comprise one simulate landmark mark wearable device operate adapt comprise capture device make screen display frame comprise configure screen display marker object predetermine position configure capture device capture first head look predetermined position perform first face operation first head obtain first face region correspond predetermined position detect first facial landmark correspond first face region calculate rotation reference angle look predetermine position accord first facial landmark configure capture device capture second head perform second head obtain second face region detect second facial landmark within second face region estimate head posture angle accord second facial landmark calculate gaze position screen accord head posture angle rotation reference angle predetermine position configure screen display correspond visual effect accord gaze position operation accord wherein gaze position comprises first coordinate value first axial direction second coordinate value second axial direction operation accord wherein head posture angle comprise head pitch angle head yaw angle rotation reference angle comprise first pitch angle second pitch angle first yaw angle second yaw angle correspond predetermined position operation accord wherein step calculate gaze position screen accord head posture angle rotation reference angle predetermine position comprises perform interpolation operation extrapolation operation accord first yaw angle second yaw angle first position correspond first yaw angle among predetermined position second position correspond second yaw angle among predetermined position head yaw angle thereby obtain first coordinate value gaze position perform interpolation operation extrapolation operation accord first pitch angle second pitch angle third position correspond first pitch angle among predetermined position fourth position correspond second pitch angle among predetermined position head pitch angle thereby obtain second coordinate value gaze position operation accord wherein comprises calculate first view distance screen accord first facial landmark estimate second view distance screen accord second facial landmark adjust rotation reference angle gaze position accord second viewing distance first view distance operation accord wherein comprises map two-dimensional position coordinate second facial landmark plane coordinate system three-dimensional position coordinate three-dimensional coordinate system estimate head posture angle accord three-dimensional position coordinate second facial landmark operation accord wherein second head comprise wearable device second facial landmark comprise third facial landmark cover wearable device operation accord wherein second head comprise wearable device second facial landmark comprise one simulate landmark mark wearable device computation apply compute system wherein compute system comprise control unit computation group general storage unit wherein control unit comprise first memory decode logic controller wherein computation group comprise group controller compute unit general storage unit configure store data computation comprise receive controller first level instruction sequence partition decode logic first level instruction sequence second level instruction sequence create controller thread second level instruction sequence allocate controller independent register well configure independent address function thread thread wherein integer great equal obtain group controller computation type second level instruction sequence obtain correspond fusion computation manner computation type accord computation type adopt compute unit fusion computation manner call thread perform computation second level instruction sequence obtain final result wherein obtain group controller computation type second level instruction sequence obtain correspond fusion computation manner computation type accord computation type adopt compute unit fusion computation manner call thread perform computation second instruction sequence obtain final result computation type represent computation operation type group controller call combine computation manner single instruction multiple data type combination single instruction multiple thread use thread perform combine computation manner obtain final result include partition decode logic thread n wrap allocate compute unit convert group controller second instruction sequence second control signal send second control signal compute unit call compute unit wrap allocate compute unit second control signal fetch correspond data accord independent address function perform compute unit computation data obtain intermediate result splice intermediate result obtain final result wherein obtain group controller computation type second level instruction sequence obtain correspond fusion computation manner computation type accord computation type adopt compute unit fusion computation manner call thread perform computation second instruction sequence obtain final result computation type represent computation operation different type group controller call simultaneous multi-threading thread perform computation obtain final result include partition decode logic thread n wrap convert second instruction sequence second control signal obtain group controller computation type support compute unit allocate controller n wrap second control signal correspond compute unit support computation type wrap second control signal call compute unit wrap allocate compute unit second control signal fetch compute unit correspond data perform compute unit computation data obtain intermediate result splice intermediate result obtain final result comprise wrap wrap block add wrap wait queue data wrap already fetch add wrap preparation queue wherein preparation queue queue wrap schedule execute locate compute resource idle wherein first level instruction sequence include long instruction second level instruction sequence include instruction sequence wherein compute system include tree module wherein tree module include root port branch port wherein root port tree module connect group controller branch port tree module connect compute unit compute unit respectively tree module configure forward data block wrap instruction sequence group controller compute unit wherein tree module n-ary tree wherein n integer great equal wherein compute system include branch process circuit wherein branch process circuit connect group controller compute unit branch process circuit configure forward data wrap instruction sequence group controller compute unit compute system comprise control unit computation group general storage unit wherein control unit include first memory decode logic controller computation group include group controller compute unit general storage unit configure store data controller configure receive first level instruction sequence control first memory decode logic decode logic configure partition first level instruction sequence second level instruction sequence controller configured create thread second level instruction sequence allocate independent register configure independent address function thread thread integer great equal controller configure convert second instruction sequence control signal send group controller group controller configure receive control signal obtain computational type control signal divide thread n wrap allocate n wrap control signal compute unit accord computational type compute unit configure fetch data general storage unit allocate wrap control signal perform computation obtain intermediate result group controller configure splice intermediate result obtain final computation result compute system wherein compute unit include addition compute unit multiplication compute unit activation compute unit dedicate compute unit compute system wherein dedicate compute unit include face compute unit graphic compute unit fingerprint compute unit neural network compute unit compute system wherein group controller configure computation type control signal graphic computation fingerprint identification face neural network operation allocate control signal face compute unit graphic compute unit fingerprint compute unit neural network compute unit respectively compute system wherein first level instruction sequence include long instruction second level instruction sequence include instruction sequence compute system comprise tree module wherein tree module include root port branch port wherein root port tree module connect group controller branch port tree module connect compute unit compute unit respectively tree module configure forward data block wrap instruction sequence group controller compute unit compute system wherein tree module n-ary tree wherein n integer great equal computing system wherein compute system include branch processing circuit branch process circuit connect group controller compute unit branch process circuit configure forward data wrap instruction sequence group controller compute unit computer program product comprise non-instant computer readable storage medium wherein computer program store non-instant computer readable storage medium computer program capable cause computer perform operation detect body information one passenger vehicle base human ' status comprise step least one interior interior vehicle acquire passenger body information-detecting device perform process inputting interior face network thereby allow face network detect passenger interior thus output multiple piece passenger feature information correspond detect ii process inputting interior body network thereby allow body network detect body passenger interior thus output body-part length information detect body b passenger body information-detecting device perform process retrieve specific height mapping information correspond specific passenger feature information specific passenger height map table store height map information represent respective one predetermine ratio one segment body portion human group height per human group process acquire specific height specific passenger specific height map information refer specific body-part length information specific passenger process retrieve specific weight map information correspond specific passenger feature information weight map table store multiple piece weight map information represent predetermine correlation height weight per human group process acquire weight specific passenger specific weight map information refer specific height specific passenger wherein step passenger body information-detecting device performs process inputting interior body network thereby allow body network output one one channel correspond interior via feature extraction network ii generate least one keypoint heatmap least one part affinity field one channel correspond via keypoint heatmap & part affinity field extractor iii extract keypoints keypoint heatmap via keypoint detector group extract keypoints refer part affinity field thus generate body part per passenger result allow body network output multiple piece body-part length information passenger refer body part per passenger wherein feature extraction network include least one convolutional layer applies least one convolution operation interior thereby output wherein keypoint heatmap & part affinity field extractor include one fully convolutional network convolutional layer apply fully-convolution operation convolution operation thereby generate keypoint heatmap part affinity field wherein keypoint detector connects refer part affinity field pair respectively high mutual connection probability connect among extracted keypoints thereby group extract keypoints wherein feature extraction network keypoint heatmap & part affinity field extractor learn learning device perform process inputting least one training include one object train feature extraction network thereby allow feature extraction network generate one train one channel apply least one convolutional operation train ii process inputting train keypoint heatmap & part affinity field extractor thereby allow keypoint heatmap & part affinity field extractor generate one keypoint heatmaps train one part affinity field train one channel train iii process inputting keypoint heatmaps train part affinity field train keypoint detector thereby allow keypoint detector extract keypoints train keypoint heatmaps train process grouping extract keypoints train refer part affinity field train thereby detect keypoints per object train iv process allow loss layer calculate one loss refer keypoints per object train correspond ground truth thereby adjust one parameter feature extraction network keypoint heatmap & part affinity field extractor loss minimize backpropagation use loss wherein step passenger body information-detecting device performs process inputting interior face network thereby allow face network detect passenger locate interior via face detector output multiple piece passenger feature information facial via facial feature classifier wherein step passenger body information-detecting device performs process inputting interior face network thereby allow face network apply least one convolution operation interior thus output least one feature map correspond interior via least one convolutional layer ii output one proposal box passenger estimate located feature map via region proposal network iii apply pool operation one region correspond proposal box feature map thus output least one feature vector via pool layer iv apply fully-connected operation feature vector thus output multiple piece passenger feature information correspond passenger correspond proposal box via fully connect layer wherein multiple piece passenger feature information include age gender race correspond passenger passenger body information-detecting device detect body information one passenger vehicle base human ' status comprise least one memory store instruction least one configure execute instruction perform support another device perform least one interior interior vehicle acquire process inputting interior face network thereby allow face network detect passenger interior thus output multiple piece passenger feature information correspond detect ii process inputting interior body network thereby allow body network detect body passenger interior thus output body-part length information detect body ii process retrieve specific height mapping information correspond specific passenger feature information specific passenger height map table store height map information represent respective one predetermine ratio one segment body portion human group height per human group process acquire specific height specific passenger specific height map information refer specific body-part length information specific passenger process retrieve specific weight map information correspond specific passenger feature information weight map table store multiple piece weight map information represent predetermine correlation height weight per human group process acquire weight specific passenger specific weight map information refer specific height specific passenger passenger body information-detecting device wherein process performs process inputting interior body network thereby allow body network output one one channel correspond interior via feature extraction network ii generate least one keypoint heatmap least one part affinity field one channel correspond via keypoint heatmap & part affinity field extractor iii extract keypoints keypoint heatmap via keypoint detector group extract keypoints refer part affinity field thus generate body part per passenger result allow body network output multiple piece body-part length information passenger refer body part per passenger passenger body information-detecting device wherein keypoint heatmap & part affinity field extractor include one fully convolutional network convolutional layer apply fully-convolution operation convolution operation thereby generate keypoint heatmap part affinity field passenger body information-detecting device wherein keypoint detector connects refer part affinity field pair respectively high mutual connection probability connect among extracted keypoints thereby group extract keypoints passenger body information-detecting device wherein feature extraction network keypoint heatmap & part affinity field extractor learn learning device perform process inputting least one training include one object train feature extraction network thereby allow feature extraction network generate one train one channel apply least one convolutional operation train ii process inputting train keypoint heatmap & part affinity field extractor thereby allow keypoint heatmap & part affinity field extractor generate one keypoint heatmaps train one part affinity field train one channel train iii process inputting keypoint heatmaps train part affinity field train keypoint detector thereby allow keypoint detector extract keypoints train keypoint heatmaps train process grouping extract keypoints train refer part affinity field train thereby detect keypoints per object train iv process allow loss layer calculate one loss refer keypoints per object train correspond ground truth thereby adjust one parameter feature extraction network keypoint heatmap & part affinity field extractor loss minimize backpropagation use loss passenger body information-detecting device wherein process performs process inputting interior face network thereby allow face network apply least one convolution operation interior thus output least one feature map correspond interior via least one convolutional layer ii output one proposal box passenger estimate located feature map via region proposal network iii apply pool operation one region correspond proposal box feature map thus output least one feature vector via pool layer iv apply fully-connected operation feature vector thus output multiple piece passenger feature information correspond passenger correspond proposal box via fully connect layer computer implement perform video cod base face detection comprise receive video frame comprise one video frame video sequence determine video frame key frame video sequence perform response video frame key frame video sequence multi-stage facial search video frame base predetermined feature template predetermine number stage determine first candidate face region second candidate face region video frame test first second candidate face region base skin tone information determine first candidate face region valid face region second candidate face region invalid face region reject second candidate face region output first candidate face region encode video frame base least part first candidate face region valid face region generate cod bitstream wherein skin tone information comprise skin probability map wherein say test first second candidate face region base skin tone information perform response video frame key frame video sequence wherein first candidate face region comprise rectangular region comprise determine free form shape face region correspond first candidate face region wherein free form shape face region least one pixel accuracy small block pixel accuracy wherein determine free form shape face region comprise generate enhance skip probability map correspond first candidate face region binarizing enhance skip probability map overlay binarized enhanced skip probability map least portion video frame provide free form shape face region wherein second video frame comprise non-key frame video sequence comprise perform face detection second video frame video sequence base free form shape face region comprise second free form shape face region second video frame base free form shape face region video frame wherein second free form shape face region comprise determine location second valid face region second video frame base displacement offset respect first candidate face region comprise determine displacement offset base offset centroid bound box around skin enhance region correspond first candidate face region second centroid second bounding box around second skin enhance region second video frame wherein encode video frame base least part first candidate face region valid face region comprise least one reduce quantization parameter correspond first candidate face region adjust lambda value first candidate face region disable skip cod first candidate face region wherein bitstream comprise least one hadvanced video cod avc compliant bitstream hhigh efficiency video cod hevc compliant bitstream vp compliant bitstream vp compliant bitstream alliance open medium aom av compliant bitstream computer implement perform face detection comprise receive video frame sequence video frame perform multi-stage facial search video frame base predetermined feature template predetermine number stage determine first candidate face region second candidate face region video frame test first second candidate face region base skin tone information determine first candidate face region valid face region second candidate face region invalid face region reject second candidate face region output first candidate face region valid face region process provide index indicative person present video frame base valid face region wherein sequence video frame comprises sequence surveillance video frame comprise perform face surveillance video frame base valid face region wherein sequence video frame comprises sequence decode video frame comprise add marker correspond receive video frame perform face receive video frame base valid face region wherein sequence video frame receive device login attempt comprise perform face base valid face region allow access device secure face recognize wherein sequence video frame comprises sequence videoconferencing frame comprise encode video frame base least part valid face region generate cod bitstream wherein encode video frame comprise encode background region video frame bitstream comprise encode video frame base least part valid face region generate cod bitstream wherein encode video frame comprise include metadata correspond valid face region bitstream comprise decode cod bitstream generate decode video frame determine metadata correspond valid face region bitstream comprise least one replace valid face region base decode metadata crop display data correspond valid face region base decode metadata index decode video frame base decode metadata system perform video cod base face detection comprise memory configure store video frame comprise one video frame video sequence couple memory receive video frame determine video frame key frame video sequence perform response video frame key frame video sequence multi-stage facial search video frame base predetermined feature template predetermine number stage determine first candidate face region second candidate face region video frame test first second candidate face region base skin tone information determine first candidate face region valid face region second candidate face region invalid face region reject second candidate face region output first candidate face region encode video frame base least part first candidate face region valid face region generate cod bitstream system wherein skin tone information comprise skin probability map system wherein first candidate face region comprise rectangular region determine free form shape face region correspond first candidate face region wherein free form shape face region least one pixel accuracy small block pixel accuracy system wherein determine free form shape face region comprise generate enhance skip probability map correspond first candidate face region binarize enhance skip probability map overlay binarized enhanced skip probability map least portion video frame provide free form shape face region system wherein second video frame comprise non-key frame video sequence perform face detection second video frame video sequence base free form shape face region system wherein track second free form shape face region second video frame base free form shape face region video frame system wherein encode video frame base least part first candidate face region valid face region comprise reduce quantization parameter correspond first candidate face region adjust lambda value first candidate face region disable skip cod first candidate face region least one non-transitory machine readable medium comprise instruction response execute device cause device perform video cod base face detection receive video frame comprise one video frame video sequence determine video frame key frame video sequence perform response video frame key frame video sequence multi-stage facial search video frame base predetermined feature template predetermine number stage determine first candidate face region second candidate face region video frame test first second candidate face region base skin tone information determine first candidate face region valid face region second candidate face region invalid face region reject second candidate face region output first candidate face region encode video frame base least part first candidate face region valid face region generate cod bitstream non-transitory machine readable medium wherein skin tone information comprise skin probability map non-transitory machine readable medium wherein first candidate face region comprise rectangular region machine readable medium comprise instruction response execute device cause device perform video cod base face detection determine free form shape face region correspond first candidate face region wherein free form shape face region least one pixel accuracy small block pixel accuracy non-transitory machine readable medium wherein determine free form shape face region comprise generate enhance skip probability map correspond first candidate face region binarizing enhance skip probability map overlay binarized enhanced skip probability map least portion video frame provide free form shape face region non-transitory machine readable medium wherein second video frame comprise non-key frame video sequence machine readable medium comprise instruction response execute device cause device perform video cod base face detection perform face detection second video frame video sequence base free form shape face region non-transitory machine readable medium machine readable medium comprise instruction response execute device cause device perform video cod base face detection second free form shape face region second video frame base free form shape face region video frame non-transitory machine readable medium wherein encode video frame base least part first candidate face region valid face region comprise least one reduce quantization parameter correspond first candidate face region adjust lambda value first candidate face region disable skip cod first candidate face region manage smart database store facial face comprise step manage device perform process count one specific facial corresponding least one specific person store smart database new facial face continuously store process determine whether first count value represent count specific facial satisfies preset first set value b first count value determine satisfy first set value manage device perform process inputting specific facial neural aggregation network thereby allow neural aggregation network generate quality score specific facial aggregation specific facial process sort quality score correspond specific facial descending order quality score process counting sort specific facial descending order second count value represent number count part specific facial becomes equal preset second set value process delete uncounted part specific facial smart database comprise step c manage device perform process generate least one optimal feature weight summation one feature specific facial use counted part quality score process set optimal feature representative face correspond specific person wherein step b manage device performs process inputting specific facial cnn neural aggregation network thereby allow cnn generate one feature correspond specific facial process inputting least one feature vector feature embed aggregation module include least two attention block thereby allow aggregation module generate quality score feature wherein step b manage device performs process match i- one feature correspond specific facial store smart database i- quality score ii specific person process store match feature match quality score smart database comprise step manage device perform one process learn face system use specific facial corresponding specific person store smart database ii process transmit specific facial corresponding specific person learning device correspond face system thereby allow learn device learn face system use specific facial wherein neural aggregation network learn learning device repeat process inputting multiple facial training correspond set single face video single face cnn neural aggregation network thereby allow cnn generate one feature train apply least one convolution operation facial training ii process inputting least one feature vector training feature train embedded aggregation module include least two attention block neural aggregation network thereby allow aggregation module generate quality score train feature train aggregation feature train use one attention parameter learn previous iteration iii process output least one optimal feature training weighted summation feature train use quality score train iv process update attention parameter learn previous iteration least two attention block one loss minimize outputted loss layer refer optimal feature train correspond ground truth manage device manage smart database store facial face comprise least one memory store instruction least one configure execute instruction perform support another device perform process count one specific facial corresponding least one specific person store smart database new facial face continuously store process determine whether first count value represent count specific facial satisfies preset first set value ii first count value determine satisfy first set value process inputting specific facial neural aggregation network thereby allow neural aggregation network generate quality score specific facial aggregation specific facial process sort quality score correspond specific facial descending order quality score process counting sort specific facial descending order second count value represent number count part specific facial becomes equal preset second set value process delete uncounted part specific facial smart database manage device wherein perform iii process generate least one optimal feature weight summation one feature specific facial use counted part quality score process set optimal feature representative face correspond specific person manage device wherein process ii performs process inputting specific facial cnn neural aggregation network thereby allow cnn generate one feature correspond specific facial process inputting least one feature vector feature embed aggregation module include least two attention block thereby allow aggregation module generate quality score feature manage device wherein process ii performs process match i- one feature correspond specific facial store smart database i- quality score ii specific person process store match feature match quality score smart database manage device wherein perform iv one process learn face system use specific facial corresponding specific person store smart database ii process transmit specific facial corresponding specific person learning device correspond face system thereby allow learn device learn face system use specific facial managing device wherein neural aggregation network learn learning device repeat process inputting multiple facial training correspond set single face video single face cnn neural aggregation network thereby allow cnn generate one feature train apply least one convolution operation facial training ii process inputting least one feature vector training feature train embedded aggregation module include least two attention block neural aggregation network thereby allow aggregation module generate quality score train feature train aggregation feature train use one attention parameter learn previous iteration iii process output least one optimal feature training weighted summation feature train use quality score train iv process update attention parameter learn previous iteration least two attention block one loss minimize outputted loss layer refer optimal feature train correspond ground truth object data process system comprise least one configure execute least one implementation algorithm store least one non-transitory computer-readable storage medium algorithm feature density selection criterion data preprocessing code execute least one data preprocessing code comprise invariant feature identification algorithm configure obtain digital representation scene scene comprise one textual medium generate set invariant feature apply invariant feature identification algorithm digital representation cluster set invariant feature region interest digital representation scene region interest region feature density classify region classifier code least one region interest accord object type function attribute derived region feature density digital representation wherein least one classified region interest corresponds text use classification result correspond least one region interest classify another region interest accord object type wherein another region interest correspond region interest system wherein preprocessing code base feature density selection criterion determine ocr algorithm applicable text algorithm applicable aspect photograph logos system wherein create profile camera-equipped smartphone include information visually impaired cause prioritized execution ocr algorithm text reader program begin read text quickly possible system comprise audio tactile feedback mechanism help position smart phone relative text system comprise `` hold still '' audio feedback signal send text center capture scene system wherein digital representation comprises least one follow type digital data data video data audio data system wherein invariant feature identification algorithm comprise least one follow feature identification algorithm fast sift freak brisk harris daisy mser system wherein invariant feature identification algorithm include least one follow edge detection algorithm corner detection algorithm saliency map algorithm curve detection algorithm texton identification algorithm wavelet algorithm system wherein least one region interest represent least one physical object scene system wherein least one region interest represent least one textual medium scene system wherein region interest represent document textual medium system wherein region interest represent financial document system wherein region interest represent structure document system wherein least one implementation algorithm include least one follow template drive algorithm face algorithm optical character algorithm speech algorithm object algorithm system wherein data preprocessing code configure assign region interest least one algorithm function scene context derive digital representation system wherein scene context include least one follow type data location position time identity news event medical event promotion system comprise mobile device comprise least one implementation algorithm data preprocessing code system wherein mobile device comprises least one follow smart phone tablet wearable glass toy vehicle computer phablet system comprise network-accessible server device comprise least one implementation algorithm data preprocessing code system wherein object type include least one follow face animal vehicle document plant building appliance clothing body part toy object data processing system comprise least one configure execute least one implementation algorithm store least one non-transitory computer-readable storage medium algorithm feature density selection criterion data preprocessing code execute least one data preprocessing code comprise invariant feature identification algorithm configure obtain digital representation scene scene comprise one textual medium generate set invariant feature apply invariant feature identification algorithm digital representation cluster set invariant feature region interest digital representation scene region interest region feature density classify region classifier code least one region interest accord object type function attribute derived region feature density digital representation wherein least one classified region interest corresponds text use classification result correspond least one region interest classify another region interest accord object type wherein another region interest correspond region interest assign region interest least one algorithm least one implementation diverse algorithm function region feature density region interest feature density selection criterion least one implementation diverse algorithms configure assign algorithms process respective region interest wherein preprocessing code base feature density selection criterion determine ocr algorithm applicable text algorithm applicable aspect photograph logos device comprise least one configure execute least one implementation algorithm store least one non-transitory computer-readable storage medium algorithm feature density selection criterion data preprocessing code execute least one data preprocessing code comprise invariant feature identification algorithm configure obtain digital representation scene scene comprise one textual medium generate set invariant feature apply invariant feature identification algorithm digital representation cluster set invariant feature region interest digital representation scene region interest region feature density classify region classifier code least one region interest accord object type function attribute derived region feature density digital representation wherein least one classified region interest corresponds text use classification result correspond least one region interest classify another region interest accord object type wherein another region interest correspond region interest mobile terminal comprise front camera configure obtain two-dimensional face glance sensor tilt certain angle dispose adjacent front camera obtain metadata face controller obtain distance glance sensor front camera distance enable area overlap region first region represent range photographable front camera overlap second region represent range photographable glance sensor maximum mobile terminal wherein controller configure obtain distance enable area overlap region maximum glance sensor front camera vary tilt angle glance sensor mobile terminal wherein controller configure set distance enable area overlap region maximum glance sensor front camera tilt angle glance sensor optimal disposition location glance sensor mobile terminal wherein controller configure set disposition location front camera original point calculates coordinate first triangle represent first region base field view front camera maximum photograph distance front camera mobile terminal wherein controller configure calculate coordinate second triangle represent second region base field view glance sensor maximum photograph distance glance sensor distance front camera glance sensor tilt angle glance sensor mobile terminal wherein glance sensor tilt controller configured calculate coordinate third triangle represent third region photographable glance sensor controller configure rotation-convert coordinate third triangle base tilt angle glance sensor calculate coordinate second triangle mobile terminal wherein controller configure calculate coordinate overlap region base coordinate first triangle coordinate second triangle calculates area overlap region base coordinate overlap region mobile terminal wherein controller configure generate three-dimensional face information base face obtain front camera metadata obtain glance sensor mobile terminal wherein metadata comprise one angle face size face location face mobile terminal wherein angle face comprises angle face rotate one pitch axis roll axis yaw axis mobile terminal comprise memory store generate face information wherein controller configure performs authentication process compare store face information face information obtain authentication mobile terminal wherein glance sensor control permanently activate low power obtain front metadata front mobile terminal wherein front camera glance sensor dispose line upper end mobile terminal mobile terminal wherein glance sensor tilt one direction direction direction leave direction right direction mobile terminal wherein metadata data change mobile terminal tilt external physical force comprise receive smart television tv indication upcoming medium program wherein upcoming medium program base profile identify one device communication smart tv one device include least one microphone camera instruct least one identified device detect audio signal use respective microphone detect visual signal use respective camera select least one device one device base detected audio signal detect visual signal provide instruction select device output notification relate upcoming medium program wherein upcoming medium program one live television program record television program broadcast television program application-provided program wherein select first device base detected audio signal include voice comprise determine distance recognize voice wherein select first device base determined distance wherein select first device base detect visual signal include face wherein face include face technique comprise present smart tv upcoming medium program favorite channel list comprise obtain medium program view data wherein medium program view data include least one historical time historical date one medium program view obtain least one current time current date processing medium program view data determine probability one medium program view base least one current time current date presenting favorite channel list base determined probability one medium program view wherein processing medium program view data include employ neural network model wherein employ neural network model comprise determine duration one medium program view least one historical time historical date set threshold time duration compare determine duration threshold time duration filter one medium program view threshold time duration smart television tv comprise network interface non-transitory computer-readable medium communication network interface non-transitory computer-readable medium capable execute -executable program code store non-transitory computer-readable medium cause smart tv receive indication upcoming medium program wherein upcoming medium program base profile identify one device communication smart tv one device include least one microphone camera instruct least one identified device detect audio signal use respective microphone detect visual signal use respective camera select least one device one device base detected audio signal detect visual signal provide instruction select device output notification relate upcoming medium program smart tv wherein select first device base detected audio signal include voice smart tv wherein capable execute -executable program code determine distance recognize voice wherein select first device base determined distance smart tv wherein select first device base detect visual signal include detect presence smart tv wherein detect presence include employ one camera microphone fingerprint sensor associate least one smart tv mobile device smartphone laptop computer tablet device wearable device internet thing iot device internet everything ioe device iot hub ioe hub smart television tv comprising mean receive indication upcoming medium program wherein upcoming medium program base profile mean identify one device communication smart tv one device include least one microphone camera mean instruct least one identified device detect audio signal use respective microphone detect visual signal use respective camera mean select least one device one device base detected audio signal detect visual signal mean provide instruction select device output notification relate upcoming medium program smart tv wherein one device include least one mobile device smartphone laptop computer tablet device wearable device internet thing iot device internet everything ioe device iot hub ioe hub another smart tv smart tv wherein upcoming medium program one live television program record television program broadcast television program application-provided program smart tv wherein notification include least one push message sms message waysms message audio alert audio message email message smart tv comprise present upcoming medium program favorite channel list smart tv comprising mean obtain medium program view data wherein medium program view data include least one historical time historical date one medium program view smart tv mean obtain least one current time current date mean process medium program view data determine probability one medium program view smart tv base least one current time current date mean present favorite channel list base determined probability one medium program view smart tv wherein mean process medium program view data include employ neural network model smart tv wherein employ neural network model comprise determine duration one medium program view smart tv least one historical time historical date set threshold time duration compare determine duration threshold time duration filter one medium program view threshold time duration smart tv comprising mean adjust least one volume brightness smart tv wherein adjust base least one historical time historical date smart tv comprising mean restrict access one medium program non-transitory computer-readable medium comprise -executable program code configure cause smart television tv receive indication upcoming medium program wherein upcoming medium program base profile identify one device communication smart tv one device include least one microphone camera instruct least one identified device detect audio signal use respective microphone detect visual signal use respective camera select least one device one device base detected audio signal detect visual signal provide instruction select device output notification relate upcoming medium program non-transitory computer-readable medium wherein select first device base detected audio signal include voice non-transitory computer-readable medium wherein capable execute -executable program code determine distance recognize voice wherein select first device base determined distance non-transitory computer-readable medium wherein select first device base detect visual signal include face non-transitory computer-readable medium wherein face include face technique camera comprise sensor array include sensor infrared ir illuminator configure emit active ir light ir light sub-band spectral illuminators spectral illuminator configure emit active spectral light different spectral light sub-band depth controller machine configure determine depth value sensor base active ir light spectral controller machine configure sensor determine spectral value spectral light sub-band spectral illuminators output machine configure output test depth+multi-spectral include pixel pixel correspond one sensor sensor array include least depth value spectral value spectral light sub-band spectral illuminators face machine previously train set labeled training depth+multi-spectral structure test depth+multi-spectral face machine configure output confidence value indicate likelihood test depth+multi-spectral include face camera wherein spectral value calculate base depth value determine sensor corresponds pixel camera wherein face machine configure use convolutional neural network determine confidence value camera wherein face machine include input node wherein input node configure receive pixel value array correspond different pixel pixel test depth+multi-spectral wherein pixel value array include depth value multi-spectral value pixel camera wherein multi-spectral value pixel include three spectral value camera wherein output machine configure output surface normal pixel test depth+multi-spectral wherein pixel value array include surface normal camera wherein output machine configure output curvature pixel test depth+multi-spectral wherein pixel value array include curvature camera wherein face machine configure use model determine confidence value wherein model include channel-specific model wherein channel-specific model configure process different pixel parameter pixel test depth+multi-spectral wherein channel-specific model include input node wherein channel-specific model input node configure receive pixel parameter value different pixel pixel test depth+multi-spectral camera wherein face machine configure use statistical model determine confidence value camera wherein statistical model include near neighbor algorithm camera wherein statistical model include support vector machine camera wherein face machine configure output location test depth+multi-spectral bounding box around recognize face camera wherein face machine configure output location test depth+multi-spectral identify two-dimensional facial feature recognize face camera wherein face machine configure output location test depth+multi-spectral identify three-dimensional facial feature recognize face camera wherein face machine configure output location test depth+multi-spectral identified spectral feature recognize face camera wherein face machine configure output pixel test depth+multi-spectral confidence value indicate likelihood pixel include face camera wherein face machine configure output identity face recognize test depth+multi-spectral camera wherein sensor sensor array differential sensor wherein spectral value determine base depth value differential measurement differential sensor camera comprise sensor array include sensor infrared ir illuminator configure emit active ir light ir light sub-band spectral illuminators spectral illuminator configure emit active spectral light different spectral light sub-band depth controller machine configure determine depth value sensor base active ir light spectral controller machine configure sensor determine spectral value spectral light sub-band spectral illuminators wherein spectral value calculate base depth value determine sensor corresponds pixel output machine configure output test depth+multi-spectral include pixel pixel correspond one sensor sensor array include least depth value spectral value spectral light sub-band spectral illuminators face machine include convolutional neural network previously train set labeled training depth+multi-spectral structure test depth+multi-spectral face machine configure output confidence value indicate likelihood test depth+multi-spectral include face process comprise acquire photo album obtain face cluster collect face information respective photo album acquire face parameter accord face information select cover accord face parameter take face-region cover set face-region cover photo album wherein select cover accord face parameter comprise perform calculation face parameter preset way obtain cover score select high cover score cover wherein select high cover score cover comprise acquire source select high cover score come preset source cover accord wherein select high cover score cover comprise acquire number contain determine single-person accord number select single-person high cover score cover accord wherein select high cover score cover comprise single-person photo album determine include two photo album select high cover score include two cover accord wherein face information comprise face feature point face parameter comprises face turn angle acquire face parameter accord face information comprise acquire coordinate value face feature point determine distance angle face feature point determine face turn angle accord distance angle accord wherein face parameter comprise face ratio acquire face parameter accord face information comprise determine face region accord face information calculate ratio area face region area obtain face ratio accord wherein calculate face ratio comprise one face subtract area occupy face correspond photo album face region obtain remain area calculate ratio remain area area obtain face ratio accord wherein collect face information respective photo album comprise acquire identification photo album extract face information correspond identification face database face database store face result face result include face information processing apparatus comprise memory configure store instruction executable wherein configure run program corresponding instruction read instruction store memory perform acquire photo album obtain face cluster collect face information photo album acquire face parameter accord face information select cover accord face parameter take face-region cover set face-region cover photo album wherein configure perform calculation face parameter preset way obtain cover score select high cover score cover wherein configure acquire source select high cover score come preset source cover apparatus accord wherein configure acquire number contain determine single-person accord number select single-person high cover score cover apparatus accord wherein configure single-person photo album determine include two photo album select high cover score include two cover apparatus accord wherein face information comprise face feature point face parameter comprises face turn angle configure acquire coordinate value face feature point determine distance angle face feature point determine face turn angle accord distance angle apparatus accord wherein face parameter comprise face ratio configure determine face region accord face information calculate ratio area face region area obtain face ratio apparatus accord wherein configure one face subtract area occupy face correspond photo album face region obtain remain area calculate ratio remain area area obtain face ratio apparatus accord wherein configure acquire identification photo album extract face information correspond identification face database face database store face result face result include face information comprise memory display screen input device connect via system bus wherein memory store computer program execute cause implement process process comprise acquire photo album obtain face cluster collect face information respective photo album acquire face parameter accord face information select cover accord face parameter take face-region cover set face-region cover photo album wherein select cover accord face parameter comprise perform calculation face parameter preset way obtain cover score select high cover score cover wherein select high cover score cover comprise acquire source select high cover score come preset source cover accord wherein comprises least one mobile phone tablet computer personal digital assistant wearable device computer-implemented comprising receive compute device meeting invitation identify location least one invitee meeting invitation configure provide least one invitee physical access location wherein meeting invitation cause system control pathway allow physical access location provide base meet invitation least one invitee physical access location control pathway allow least one invitee physically access location pathway response position data indicate least one invitee predetermine location near location wherein position data base part face camera system identify least one invitee receive position data face camera system identify least one invitee wherein position data indicate pattern movement least one invitee determine pattern movement indicate least one invitee exit location revoke physical access location identify meet invitation control pathway restrict least one invitee identify meet invitation physical access location pathway response determine pattern movement indicate least one invitee exit location computer-implemented wherein determine least one invitee exit location comprise determine least one invitee pass egress associate location predetermine direction computer-implemented wherein determine least one invitee exit location comprise determine least one invitee move area predetermine direction computer-implemented wherein position data indicate second pattern movement least one invitee wherein access secure data associate location provide response detect second pattern movement computer-implemented comprise collating secure data public data generate resource data communicate resource data client compute device associate least one invitee access location provide computer-implemented wherein position data indicate least one invitee predetermine location least one invitee pass predetermine location computer-implemented wherein position data indicate least one invitee predetermine location least one invitee pass predetermine location near location predetermine direction system comprise memory communication memory computer-readable instruction store thereupon execute cause receive meeting invitation indicate location identity meeting invitation configure provide least one invitee physical access location wherein meeting invitation cause system control pathway allow physical access location provide least one invitee associate identity access location control pathway allow least one invitee physically access location pathway response position data indicate least one invitee predetermine location near location wherein position data base part face camera system identify least one invitee receive position data face camera system identify least one invitee wherein position data indicate pattern movement least one invitee determine pattern movement indicate least one invitee exit location revoke physical access location identify meet invitation control pathway restrict least one invitee identify meet invitation physical access location pathway response determine pattern movement indicate least one invitee exit location system wherein determine least one invitee exit location comprise determine least one invitee pass egress associate location system wherein determine least one invitee exit location comprise determine least one invitee move area predetermine direction system wherein position data indicate second pattern movement least one invitee wherein access secure data associate location provide response detect second pattern movement system wherein instruction cause collate secure data public data generate resource data communicate resource data client compute device associate least one invitee access location provide non-transitory computer-readable storage medium computer-executable instruction store thereupon execute one compute device cause one computing device receive meeting invitation indicate location identity meeting invitation configure provide least one invitee physical access location wherein meeting invitation cause system control pathway allow physical access location provide least one invitee associate identity access location control pathway allow least one invitee physically access location pathway response position data indicate least one invitee predetermine location near location wherein position data base part face camera system identify least one invitee receive position data face camera system identify least one invitee wherein position data indicate pattern movement least one invitee determine pattern movement indicate least one invitee exit location revoke physical access location identify meet invitation control pathway restrict least one invitee identify meet invitation physical access location pathway response determine pattern movement indicate least one invitee exit location non-transitory computer-readable storage medium wherein determine least one invitee exit location comprise determine least one invitee pass egress associate location non-transitory computer-readable storage medium wherein position data indicate second pattern movement least one invitee wherein access secure data associate location provide response detect second pattern movement non-transitory computer-readable storage medium wherein instruction cause one collate secure data public data generate resource data communicate resource data client compute device associate least one invitee access location provide comprise receive piece content salient data piece content base salient data determine first path viewport piece content wherein first path viewport include different salient event occur piece content different time playback piece content provide viewport display device wherein movement viewport base first path viewport salient data playback detect additional salient event piece content include first path viewport provide indication additional salient event viewport playback wherein salient data identifies salient event piece content salient data indicate salient event piece content correspond point location salient event piece content correspond time salient event occur playback wherein salient data indicate salient event piece content correspond type salient event correspond strength value salient event wherein first path viewport control movement viewport put different salient event view viewport different time playback comprise detect one salient event piece content base least one follow visual data piece content audio data piece content content consumption experience data piece content wherein salient data indicative salient event detect comprise detect one salient event piece content base least one follow face facial emotion object motion metadata piece content wherein salient data indicative salient event detect comprise detect interaction indication wherein indication comprise interactive hint response detect interaction adapt first path viewport second path viewport base interaction wherein second path viewport include additional salient event provide updated viewport piece content display device wherein movement update viewport base second path viewport salient data playback second path viewport control movement update viewport put additional salient event view update viewport comprise change weight assign additional salient event one salient event piece content type additional salient event wherein second path viewport include one salient event piece content type additional salient event system comprise least one non-transitory -readable memory device store instruction execute least one cause least one perform operation include receive piece content salient data piece content base salient data determine first path viewport piece content wherein first path viewport include different salient event occur piece content different time playback piece content provide viewport display device wherein movement viewport base first path viewport salient data playback detect additional salient event piece content include first path viewport provide indication additional salient event viewport playback system wherein salient data identifies salient event piece content salient data indicate salient event piece content correspond point location salient event piece content correspond time salient event occur playback system wherein salient data indicate salient event piece content correspond type salient event correspond strength value salient event system wherein salient data generate offline server system operation comprise detect one salient event piece content base least one follow visual data piece content audio data piece content content consumption experience data piece content wherein salient data indicative salient event detect system operation comprise detect one salient event piece content base least one follow face facial emotion object motion metadata piece content wherein salient data indicative salient event detect system operation comprise detect interaction indication wherein indication comprise interactive hint response detect interaction adapt first path viewport second path viewport base interaction wherein second path viewport include additional salient event provide updated viewport piece content display device wherein movement update viewport base second path viewport salient data playback second path viewport control movement update viewport put additional salient event view update viewport system operation comprise change weight assign additional salient event one salient event piece content type additional salient event system wherein second path viewport include one salient event piece content type additional salient event non-transitory computer readable storage medium include instruction perform comprise receive piece content salient data piece content base salient data determine first path viewport piece content wherein first path viewport include different salient event occur piece content different time playback piece content provide viewport display device wherein movement viewport base first path viewport salient data playback detect additional salient event piece content include first path viewport provide indication additional salient event viewport playback computer readable storage medium comprise detect interaction indication wherein indication comprise interactive hint response detect interaction adapt first path viewport second path viewport base interaction wherein second path viewport include additional salient event provide updated viewport piece content display device wherein movement update viewport base second path viewport salient data playback second path viewport control movement update viewport put additional salient event view update viewport mobile device facial mobile device comprise one cameras device memory couple device processing system program receive one camera extract feature extractor utilizing convolutional neural network cnn enlarge intra-class variance long-tail class feature vector generate feature generator discriminative feature vector feature vector classify fully connect classifier identity discriminative feature vector control operation mobile device react accordance identity mobile device recite include communication system mobile device recite wherein operation tag video identity uploads video social medium mobile device recite wherein operation tag video identity sends video mobile device recite wherein mobile device smart phone mobile device recite wherein mobile device body cam mobile device recite programmed train feature extractor feature generator fully connect classifier alternative bi-stage strategy mobile device recite wherein feature extractor share covariance matrix across class transfer intra-class variance regular class long-tail class mobile device recite wherein feature generator optimize softmax loss joint regularization weight feature magnitude inner product weight feature mobile device recite wherein feature extractor average feature vector flip feature vector flip feature vector generate horizontally flip frame one mobile device recite wherein select group consist video frame video mobile device recite wherein communication system connect remote server include facial network mobile device recite wherein one stage alternative bi-stage strategy fix feature extractor applies feature generator generate new transfer feature diverse violate decision boundary mobile device recite wherein one stage alternative bi-stage strategy fix fully connect classifier updates feature extractor feature generator computer program product mobile device facial computer program product comprise non-transitory computer readable storage medium program instruction embody therewith program instruction executable computer cause computer perform comprise receive device extract device feature extractor utilizing convolutional neural network cnn enlarge intra-class variance long-tail class feature vector generate device feature generator discriminative feature vector feature vector classify device utilize fully connect classifier identity discriminative feature vector control operation mobile device react accordance identity computer-implemented facial mobile device comprise receive device extract device feature extractor utilizing convolutional neural network cnn enlarge intra-class variance long-tail class feature vector generate device feature generator discriminative feature vector feature vector classify device utilize fully connect classifier identity discriminative feature vector control operation mobile device react accordance identity computer-implemented recite wherein control include tag video identity upload video social medium computer-implemented recite wherein control include tag video identity send video computer-implemented recite wherein extract include share covariance matrix across class transfer intra-class variance regular class long-tail class compute device comprise non-transitory machine readable medium store machine train mt network comprise layer process node process node configure compute first output value combine set output value set process node use piecewise linear cup function compute second output value first output value process node wherein piecewise linear cup function prior train mt network comprises least first linear section first slope follow ii second linear section negative second slope follow iii third linear section negative third slope different second slope follow iv fourth linear section positive fourth slope follow v fifth linear section positive fifth slope different fourth slope follow vi sixth linear section sixth slope wherein piecewise linear cup function symmetric vertical axis third fourth linear section prior train mt network content capturing circuit capture content processing mt network set process unit execute process node process content capture content capture circuit wherein training set parameter define piecewise linear cup function node first second plurality process node process node first process node configured emulate boolean operator output value processing node range associate `` '' value set input process node set value range associate `` '' ii processing node second processing node configure emulate boolean xnor operator output value processing node range associate `` '' set input node set value range associate `` '' b set input node set value range associate `` '' value compute device wherein third linear section piecewise linear cup function first process node mt network different slope third linear section second processing node mt network compute device wherein length third section piecewise linear cup function first process node mt network different length third section piecewise linear cup function second processing node mt network compute device wherein set parameter train part back propagate module back propagating error output value later layer process node earlier layer process node adjust set parameter define piecewise linear cup function earlier layer process node compute device wherein processing node us linear function define set parameter compute first output value process node wherein back propagate module back propagate error output value later layer process node earlier layer process node adjust set parameter define linear function earlier layer process node compute device wherein first processing node emulate boolean operator second processing node emulate boolean operator enable mt network implement mathematical problem compute device wherein process node layer process node receive input value output value process node set prior layer compute device wherein processing node us linear function compute first output value process node wherein process node 's piecewise linear cup function define along first second ax first axis define range output value process node 's linear function second axis define range output value produce piecewise linear cup function range output value process node 's linear function compute device comprise content output circuit present output base processing content mt network compute device wherein capture content one audio segment wherein present output output display display screen compute device audio presentation output speaker compute device compute device wherein compute device mobile device compute device wherein mt network mt neural network process node mt neuron compute device wherein set parameter configure train process node comprise least one negative second third slope second third linear section positive fourth fifth slope fourth fifth linear section first intercept second linear section second intercept fifth linear section set length least second third fourth fifth section compute device wherein train set parameter define piecewise linear cup function node comprise output value compute device wherein first sixth slope zerowe system comprise memory device store input include input interface receive input pre- model input yield multi-channel feature extractor extract set feature base multi-channel feature selector select one feature set feature multi-channel wherein one feature select base ability differentiate feature feature matcher match one feature learn feature set similarity detector determine whether one feature meet pre-defined similarity threshold system wherein pre- activate one channel multi-channel yield one activate channel system wherein one activate channel determine base ability differentiate feature system wherein pre- activate one local patch one activate channel system wherein one local patch determine base ability differentiate feature system wherein feature matcher utilize large-scale data learn process perform feature match apparatus comprise input interface receive input pre- model input yield multi-channel feature extractor extract set feature base multi-channel feature selector select one feature set feature multi-channel wherein one feature select base ability differentiate feature feature matcher match one feature learn feature set similarity detector determine whether one feature meet pre-defined similarity threshold apparatus wherein pre- activate one channel multi-channel yield one activate channel apparatus wherein one activated channel determine base ability differentiate feature apparatus wherein pre- activate one local patch one activated channel apparatus wherein one local patch determine base ability differentiate feature apparatus wherein feature matcher utilize large-scale data learn process perform feature match comprise model input yield multi-channel extract set feature base multi-channel select one feature set feature multi-channel wherein one feature select base ability differentiate feature match one feature learn feature set determine whether one feature meet pre-defined similarity threshold wherein model input include activate one channel multi-channel yield one activate channel wherein one activate channel determine base ability differentiate feature wherein extract feature input include activate one local patch one activated channel wherein one local patch determine base ability differentiate feature wherein feature matcher utilizes large-scale data learn process perform feature match least one non-transitory computer readable storage medium comprise set instruction execute compute device cause compute device model input yield multi-channel extract set feature base multi-channel select one feature set feature multi-channel wherein feature select base ability differentiate feature match one feature learn feature set determine whether one feature meet pre-defined similarity threshold least one non-transitory computer readable storage medium wherein instruction execute cause compute device activate one channel multi-channel yield one activate channel least one non-transitory computer readable storage medium wherein instruction execute cause compute device determine one activate channel base ability differentiate feature least one non-transitory computer readable storage medium wherein extract feature input include activate one local patch one activated channel least one non-transitory computer readable storage medium wherein one local patch determine base ability differentiate feature least one non-transitory computer readable storage medium wherein feature matcher utilize large-scale data learn process perform feature match apparatus comprise mean model input yield multi-channel mean extract set feature base multi-channel mean select one feature set feature multi-channel wherein one feature select base ability differentiate feature mean match one feature learn feature set mean determine whether one feature meet pre-defined similarity threshold control terminal terminal comprise capture apparatus least one comprise acquire capture apparatus obtain least one motion parameter terminal motion parameter comprise least one motion frequency motion time two parameter among acceleration angular velocity motion amplitude motion frequency motion time transmit least one parameter threshold obtain request data management server parameter threshold obtain request comprise configuration information terminal receiving correspond preset threshold correspond configuration information response parameter threshold obtain request compare two parameter correspond preset threshold control least one perform processing acquire base least one two parameter motion parameter great correspond preset threshold base two parameter motion parameter respectively great correspond preset threshold wherein acquire comprises acquire real time obtain comprises obtain motion parameter terminal real time comprise response least one two parameter motion parameter great correspond preset threshold obtain motion parameter terminal response two parameter motion parameter obtain late time le equal corresponding preset threshold perform processing acquire late time accord wherein acquire comprises control least one turn capture apparatus base face instruction acquire capture apparatus face capture apparatus turn accord wherein control perform processing comprise skip perform face acquire face base least one two parameter motion parameter great correspond preset threshold base two parameter motion parameter respectively great correspond preset threshold accord wherein obtain comprises least one obtain acceleration terminal use acceleration sensor obtain angular velocity terminal use gyro sensor accord wherein transmitting comprises transmit parameter threshold obtain request data management server accord preset time period accord comprise generate prompt information base least one two parameter motion parameter great correspond preset threshold prompt information use prompt terminal stop move accord wherein motion parameter comprise motion frequency motion time terminal comprise capture apparatus least one memory configure store program code least one configured access least one memory operate accord program code program code comprise motion parameter obtain code configure cause least one acquire use capture apparatus obtain motion parameter terminal motion parameter comprise least one motion frequency motion time two parameter among acceleration angular velocity motion amplitude motion frequency motion time request transmit code configure cause least one transmit parameter threshold obtain request data management server parameter threshold obtain request comprise configuration information terminal parameter threshold receive code configure cause least one receive corresponding preset threshold correspond configuration information response parameter threshold obtain request compare code configure cause least one compare two parameter correspond preset threshold control code configure cause least one perform processing acquire base least one two parameter motion parameter great correspond preset threshold base two parameter motion parameter respectively great correspond preset threshold wherein motion parameter obtain code cause least one acquire real time obtain motion parameter terminal real time response least one two parameter motion parameter great correspond preset threshold obtain motion parameter terminal wherein control code cause least one response two parameter motion parameter obtain late time le equal corresponding preset threshold perform process acquire late time terminal accord wherein program code comprise face instruction receive code configure cause least one receive face instruction wherein motion parameter obtain code cause least one control accord face instruction capture apparatus turn acquire face use capture apparatus capture apparatus turn wherein control code cause least one skip perform face acquire face base least one two parameter motion parameter great correspond preset threshold base two parameter motion parameter respectively great correspond preset threshold terminal accord wherein request transmit code cause least one transmit parameter threshold obtain request data management server accord preset time period terminal accord wherein program code comprise prompt information generation code configure cause least one generate prompt information base least one two parameter motion parameter great correspond preset threshold prompt information use prompt terminal stop move terminal accord wherein motion parameter comprise motion frequency motion time non-transitory computer-readable storage medium store machine instruction execute one cause one perform obtain acquire capture apparatus obtain motion parameter terminal terminal comprise capture apparatus motion parameter comprise least one motion frequency motion time two parameter among acceleration angular velocity motion amplitude motion frequency motion time transmit parameter threshold obtain request data management server parameter threshold obtain request comprise configuration information terminal receiving correspond preset threshold correspond configuration information response parameter threshold obtain request compare two parameter correspond preset threshold control perform processing acquire base least one two parameter motion parameter great correspond preset threshold base two parameter motion parameter respectively great correspond preset threshold wherein acquire comprises acquire real time obtain comprises obtain motion parameter terminal real time comprise response least one two parameter motion parameter great correspond preset threshold obtain motion parameter terminal response two parameter motion parameter obtain late time le equal corresponding preset threshold perform processing acquire late time non-transitory computer-readable storage medium accord wherein acquire face processing comprise perform face non-transitory computer-readable storage medium accord wherein obtain motion parameter comprise least one obtain acceleration terminal use acceleration sensor obtain angular velocity terminal use gyro sensor non-transitory computer-readable storage medium accord wherein motion parameter comprise motion frequency motion time process drive-through order comprise receive customer information detect vision provide product information customer base customer information process product order customer accord wherein receive customer information comprise least one receive customer information associate vehicle information detect vehicle receive customer information associate identification information detect face accord comprise determine whether customer pre-order customer base customer information wherein customer determine pre-order customer provide product information base customer information comprise provide pre-order information use least one audio video process product order customer comprise provide information promptly guide vehicle pickup stand use least one audio video provide information additional order available accord wherein product information base customer information comprise recently order product component frequently order product component order history customer information accord wherein receive customer information comprise receive information age gender passenger detect face provide product information customer base customer information comprise provide recommend menu information differentiate accord age gender accord wherein processing product order customer comprise determine product component past order history component modify product component product order accord wherein processing product order customer comprise pay product price accord biometrics-based authentication communication system vehicle mobile terminal accord wherein processing product order customer comprise issue payment number divide payment perform divide payment accord payment request mobile terminal payment number inputted accord wherein processing product order customer comprise accumulate mileage account correspond mobile terminal undergoing payment accord wherein processing product order customer comprise suggest takeout package accord temperature product atmospheric temperature weather vehicle type apparatus configure process drive-through order apparatus comprise transceiver configure receive customer information detect vision digital signage configure provide product information customer base customer information configure process product order customer apparatus accord wherein transceiver receive least one customer information associate vehicle information detect vehicle customer information associate identification information detect face apparatus accord wherein configure determine whether customer pre-order customer base customer information customer determine pre-order customer perform control operation provide pre-order information control digital signage output information promptly guide vehicle pickup stand provide information additional order available apparatus accord wherein product information base customer information comprise recently order product component frequently order product component order history customer information apparatus accord wherein transceiver configure receive information age gender passenger detect face configure control digital signage provide recommend menu information differentiate accord age gender apparatus accord wherein configure determine product component past order history component modify product component product order apparatus accord wherein configured pay product price accord biometrics-based authentication communication system vehicle mobile terminal apparatus accord wherein configure issue payment number divide payment perform divide payment accord request mobile terminal payment number inputted apparatus accord wherein configure accumulate mileage account correspond mobile terminal undergoing payment apparatus accord wherein configure control digital signage suggest takeout package accord temperature product atmospheric temperature weather vehicle type information processing perform compute device one memory store program execute one comprise identify use face one face correspond respective person capture first identify face extract set profile parameter correspond person first select tile first tile match face correspond person first accordance predefined correspondence set profile parameter correspond person set pre-stored description parameter first tile generate second cover respective person first correspond first tile share first second predefined order via group chat session wherein first second display group chat session one time one two replace two periodically wherein extract set profile parameter correspond person first include determine one descriptive label correspond identified face correspond person use first machine learn model wherein first machine learn model train facial correspond descriptive label wherein extract set profile parameter correspond person first include determine identity correspond person base identified face correspond person locate respective profile information first person base determine identity correspond person use one characteristic respective profile information first person set profile parameter correspond identified face correspond person wherein least first one first tile dynamic tile least second one first tile static tile include receive comment different group chat session comment include descriptive term respective person identify first choose descriptive label respective person accord comment update second add descriptive label adjacent first tile respective person compute device information process comprise one memory store instruction execute one cause perform operation comprise identify use face one face correspond respective person capture first identify face extract set profile parameter correspond person first select tile first tile match face correspond person first accordance predefined correspondence set profile parameter correspond person set pre-stored description parameter first tile generate second cover respective person first correspond first tile share first second predefined order via group chat session compute device wherein first second display group chat session one time one two replace two periodically compute device wherein extract set profile parameter correspond person first include determine one descriptive label correspond identified face correspond person use first machine learn model wherein first machine learn model train facial correspond descriptive label compute device wherein extract set profile parameter correspond person first include determine identity correspond person base identified face correspond person locate respective profile information first person base determine identity correspond person use one characteristic respective profile information first person set profile parameter correspond identified face correspond person compute device wherein least first one first tile dynamic tile least second one first tile static tile compute device wherein operation include receive comment different group chat session comment include descriptive term respective person identify first choose descriptive label respective person accord comment update second add descriptive label adjacent first tile respective person non-transitory computer-readable storage medium store instruction execute compute device one cause compute device perform operation comprise identify use face one face correspond respective person capture first identify face extract set profile parameter correspond person first select tile first tile match face correspond person first accordance predefined correspondence set profile parameter correspond person set pre-stored description parameter first tile generate second cover respective person first correspond first tile share first second predefined order via group chat session non-transitory computer-readable storage medium wherein first second display group chat session one time one two replace two periodically non-transitory computer-readable storage medium wherein extract set profile parameter correspond person first include determine one descriptive label correspond identified face correspond person use first machine learn model wherein first machine learn model train facial correspond descriptive label non-transitory computer-readable storage medium wherein extract set profile parameter correspond person first include determine identity correspond person base identified face correspond person locate respective profile information first person base determine identity correspond person use one characteristic respective profile information first person set profile parameter correspond identified face correspond person non-transitory computer-readable storage medium wherein least first one first tile dynamic tile least second one first tile static tile non-transitory computer-readable storage medium wherein operation include receive comment different group chat session comment include descriptive term respective person identify first choose descriptive label respective person accord comment update second add descriptive label adjacent first tile respective person comprise compute system determine performance metric eye system first performance threshold wherein eye system associate head-mounted display worn base determination performance metric eye system first performance threshold computer system perform receive one first input associate body estimate region look within field view head-mounted display base receive one first input associate body determine vergence distance base least one first input associate body estimate region look location one object scene display head-mounted display adjust one configuration head-mounted display base determined vergence distance wherein one configuration head-mounted display comprise one rendering position display screen position optic block comprise determine performance metric eye system second performance threshold receive eye data eye system determine vergence distance base eye data one first input associate body comprise receive one second input associate one displaying element scene display head-mounted display determine vergence distance base least eye data one first input associate body one second input associate one displaying element scene comprise feed one first input associate body fusion algorithm wherein fusion algorithm assigns weight score input one first input determine vergence distance use fusion algorithm base one first input associate body determine z-depth display screen confidence score base one first input associate body comprise compare confidence score confidence level threshold response determination confidence score confidence level threshold feed one second input associate one displaying element scene fusion algorithm determine z-depth display screen use fusion algorithm base one first input associate body one second input associate one displaying element scene compare compare fusion algorithm confidence score associate combination input determine fusion algorithm z-depth display screen base combination input associate high confidence score wherein z-depth confidence score determine fusion algorithm use piecewise comparison one first input one second input wherein z-depth confidence score determine base correlation two input one first input one second input wherein fusion algorithm comprise machine learn ml algorithm wherein machine learn ml algorithm determines combination first input fed fusion algorithm wherein one first input associate body comprise one hand position hand direction hand movement hand gesture head position head direction head movement head gesture gaze angle rea body gesture body posture body movement behavior weight combination one related parameter wherein one first input associate body receive one controller sensor camera microphone accelerometer headset wear mobile device wherein one second input associate one displaying element comprise one z-buffer value associate display element display element mark developer analysis result shape display element face result object result person identify display content object identify display content correlation two display element weight combination one second input comprise determine performance metric eye system second performance threshold receive one second input associate one displaying element scene display head-mounted display determine vergence distance base least one first input associate body one second input associate one displaying element wherein determine performance metric eye system second performance threshold comprise determine eye system exist fails provide eye data wherein performance metric eye system comprise one accuracy parameter eye system precision parameter eye system value parameter eye system detectability pupil metric base one parameter associated parameter change parameter change trend data availability weight combination one performance related parameter wherein one parameter associate comprise one eye distance pupil position pupil status correlation two pupil head size position headset worn angle headset worn direction headset worn alignment eye weight combination one related parameter associate wherein first performance threshold comprise one pre-determined value pre-determined range state data change speed data trend data change one non-transitory computer-readable storage medium embody software operable execute computing system determine performance metric eye system first performance threshold wherein eye system associate head-mounted display worn base determination performance metric eye system first performance threshold medium embody software operable execute compute system receive one first input associate body estimate region look within field view head-mounted display base receive one first input associate body determine vergence distance base least one first input associate body estimate region look location one object scene display head-mounted display adjust one configuration head-mounted display base determined vergence distance system comprise one non-transitory computer-readable storage medium embody instruction one couple storage medium operable execute instruction determine performance metric eye system first performance threshold wherein eye system associate head-mounted display worn base determination performance metric eye system first performance threshold system configure receive one first input associate body estimate region look within field view head-mounted display base receive one first input associate body determine vergence distance base least one first input associate body estimate region look location one object scene display head-mounted display adjust one configuration head-mounted display base determined vergence distance computer-implemented -based self-guided object detection comprise receive device set respective grid thereon label regard respective object detect use grid level label data training device grid-based object detector use grid level label data determine device respective bounding box respective object apply local segmentation training device region-based convolutional neural network rcnn joint object localization object classification use respective bounding box respective object input rcnn computer-implemented comprising perform action responsive object localization object classification respective new object new rcnn apply computer-implemented wherein action comprise autonomously control motor vehicle avoid collision new object responsive object localization object classification respective new object computer-implemented wherein local segmentation perform use self-similarity search template match provide respective bound box around respective object set computer-implemented wherein local segmentation apply segment respective target region therein computer-implemented wherein region-based convolutional neural network rcnn form model object training stage detect object new inference stage computer-implemented wherein perform system select group consist surveillance system face detection system face system cancer detection system object system advance driver-assistance system computer program product -based self-guided object detection computer program product comprise non-transitory computer readable storage medium program instruction embody therewith program instruction executable computer cause computer perform comprise receive device set respective grid thereon label regard respective object detect use grid level label data training device grid-based object detector use grid level label data determine device respective bounding box respective object apply local segmentation training device region-based convolutional neural network rcnn joint object localization object classification use respective bounding box respective object input rcnn computer program product wherein comprise perform action responsive object localization object classification respective new object new rcnn apply computer program product wherein action comprise autonomously control motor vehicle avoid collision new object responsive object localization object classification respective new object computer program product wherein local segmentation perform use self-similarity search template match provide respective bound box around respective object set computer program product wherein local segmentation apply segment respective target region therein computer program product wherein region-based convolutional neural network rcnn form model object training stage detect object new inference stage computer program product wherein perform system select group consist surveillance system face detection system face system cancer detection system object system advance driver-assistance system computer process system -based self-guided object detection comprise memory device store program code device run program code receive set respective grid thereon label regard respective object detect use grid level label data train grid-based object detector use grid level label data determine respective bounding box respective object apply local segmentation train region-based convolutional neural network rcnn joint object localization object classification use respective bounding box respective object input rcnn computer processing system wherein device run program code perform action responsive object localization object classification respective new object new rcnn apply computer process system wherein action comprise autonomously control motor vehicle avoid collision new object responsive object localization object classification respective new object computer processing system wherein local segmentation perform use self-similarity search template match provide respective bound box around respective object set computer process system wherein region-based convolutional neural network rcnn form model object training stage detect object new inference stage computer processing system wherein computer processing system comprise system select group consist surveillance system face detection system face system cancer detection system object system advance driver-assistance system scalable parallel cloud-based face utilize database normalize store comprise capture use camera detect face capture normalizing detected facial match normalize store identify facial feature normalize detected facial generate facial metric facial feature calculate euclidean distance facial metric normalize detected facial correspond facial metric store compare euclidean distance predetermine threshold responsive euclidean distance comparison produce reduced candidate list best possible match normalize store compare parallel normalize detect facial normalize store reduced candidate list utilize face algorithm parallel process system use different face algorithm responsive comparison produce best match result parallel subset reduce candidate list select final match best match result use deep learn neural network face algorithm train output individual face algorithm scalable parallel cloud-based face wherein detect face capture comprises utilize opencv detect face capture extract location eye tip nose face determine distance eye crop face capture width height crop face function distance eye rotate face angle rotation function distance eye scalable parallel cloud-based face wherein width crop face time distance eye height crop face time distance eye angle rotation angle form straight line join eye x-axis face scalable parallel cloud-based face wherein rotate face comprises rotate face provide frontal face pattern scalable parallel cloud-based face comprise step proportionally rescale crop rotate scalable parallel cloud-based face proportional rescaling yield crop rotated size = pixel scalable parallel cloud-based face wherein facial feature identify normalized detect facial comprise pair eye tip nose mouth center mouth chin area comprise bottom top leave landmark top right landmark scalable parallel cloud-based face wherein generate facial metric comprises calculate distance pair eye distance eye tip nose distance equal width mouth distance tip nose center mouth distance bottom chin center mouth distance top leave landmark chin tip nose distance top right landmark chin tip nose scalable parallel cloud-based face wherein perform euclidean distance match comprise partition normalize store substantially equal subset perform euclidean distance match facial metric normalize detected facial correspond facial metric store subset normalize stored separate parallel processing system generate euclidean distance store subset compare euclidean distance predetermine threshold separate responsive euclidean distance comparison produce reduced candidate list best possible match normalize stored subset combining reduced candidate list subset produce single reduce candidate list scalable parallel cloud-based face wherein face algorithms utilized comparing parallel normalize detect facial normalize store reduced candidate list consist face algorithms select group consist principle component analysis pca-based algorithm linear discriminant analysis lda algorithms independent component analysis ica algorithm kernel-based algorithms feature-based technique algorithms base neural network algorithms base transforms model-based face algorithm scalable parallel cloud-based face wherein pca-based algorithm include eigen face detection lda algorithms include fisher face scalable parallel cloud-based face wherein compare parallel capture normalized store reduced candidate list comprise partition reduce candidate list substantially equal subset process subset different parallel process system use unique face algorithm produce best match result use reduce function mapreduce program combine best match result subset produce single set best match result scalable parallel cloud-based face wherein partition reduce candidate list comprises select comprise subset optimize variance accord follow equation n number row columns face vector n number group σij standard deviation dimension group j face vector scalable parallel cloud-based face wherein select comprise subset optimize variance accord follow equation dμi μj euclidean distance mean group mean group j face vector l number group level scalable parallel cloud-based face select final match best match result utilizing deep learning neural network face algorithm comprise utilize either adaboost machine-learning algorithm neural network machine-learning model scalable parallel cloud-based face normalize detect facial match normalize store include normalizing detect facial size illumination normalize stored non-transitory computer-readable medium contain executable program instruction cause computer perform face comprise detect face capture camera normalizing detect facial match normalize store identify facial feature normalize detected facial generate facial metric facial feature calculate euclidean distance facial metric normalize detected facial correspond facial metric store compare euclidean distance predetermine threshold responsive euclidean distance comparison produce reduced candidate list best possible match normalize store compare parallel capture normalized store reduced candidate list utilize face algorithm parallel process system use different face algorithm responsive comparison produce best match result parallel subset reduce candidate list select final match best match result use deep learn neural network face algorithm train output individual face algorithm non-transitory computer-readable medium contain executable program instruction wherein face algorithm utilized comparing parallel normalize detect facial normalize store reduced candidate list consist face algorithms select group consist principle component analysis pca-based algorithm linear discriminant analysis lda algorithms independent component analysis ica algorithm kernel-based algorithms feature-based technique algorithms base neural network algorithms base transforms model-based face algorithm non-transitory computer-readable medium contain executable program instruction wherein pca-based algorithm include eigen face detection lda algorithms include fisher face non-transitory computer-readable medium contain executable program instruction select final match best match result utilizing deep learning neural network face algorithm comprise utilize either adaboost machine-learning algorithm neural network machine-learning model image device comprise condense lens sensor configure detect light pass condense lens comprise pixel matrix wherein pixel matrix comprise phase detection pixel pair regular pixel configure turn phase detection pixel pair autofocusing output autofocused pixel data complete autofocusing divide autofocused pixel data first subframe second subframe calculate feature least one first subframe second subframe wherein feature comprise module width finder pattern finder pattern predetermine ratio harr-like feature gabor feature determine operate resolution regular pixel accord feature calculate least one first subframe second subframe divide autofocused pixel data image device ed wherein phase detection pixel pair comprise first pixel second pixel cover layer cover upon first region first pixel upon second region second pixel wherein first region second region mirror symmetrical microlens align least one first pixel second pixel image device ed wherein first region second region % % area single pixel image device ed wherein configure perform autofocusing use dual pixel autofocus technique accord pixel data phase detection pixel pair complete autofocusing imaging device ed wherein configure divide pixel data phase detection pixel pair third subframe fourth subframe complete autofocusing perform autofocusing accord third subframe fourth subframe image device ed wherein configure calibrate brightness third subframe fourth subframe identical use shade algorithm image device ed wherein operate resolution select first resolution small number regular pixel second resolution large first resolution image device ed wherein regular pixel turn autofocusing image device ed wherein number phase detection pixel pair small regular pixel image device comprise condense lens sensor configure detect light pass condense lens comprise pixel matrix wherein pixel matrix comprise phase detection pixel pair regular pixel configure turn phase detection pixel pair autofocusing output autofocused pixel data complete autofocusing divide autofocused pixel data first subframe second subframe calculate feature least one first subframe second subframe wherein feature comprise module width finder pattern finder pattern predetermine ratio harr-like feature gabor feature select decode use pixel data regular pixel accord feature calculate least one first subframe second subframe divide autofocused pixel data image device ed wherein phase detection pixel pair comprise first pixel second pixel cover layer cover upon first region first pixel upon second region second pixel wherein first region second region mirror symmetrical microlens align least one first pixel second pixel image device ed wherein configure perform autofocusing use dual pixel autofocus technique accord pixel data phase detection pixel pair complete autofocusing imaging device ed wherein configure divide pixel data phase detection pixel pair third subframe fourth subframe complete autofocusing calibrate brightness third subframe fourth subframe identical use shade algorithm perform autofocusing accord third subframe fourth subframe image device ed wherein configure calculate feature use least one rule base algorithm machine learn algorithm image device ed wherein decode decode qr code face operate image device image device comprise phase detection pixel pair regular pixel operate comprise turn phase detection pixel pair autofocusing output autofocused frame complete autofocusing divide autofocused frame acquire phase detection pixel pair first subframe second subframe calculate feature least one first subframe second subframe wherein feature comprise module width finder pattern finder pattern predetermine ratio harr-like feature gabor feature selectively activate least part regular pixel accord feature calculate least one first subframe second subframe divide autofocused frame operate ed wherein selectively activate comprises activate first part regular pixel perform decode accord pixel data first part regular pixel activate regular pixel perform accord pixel data regular pixel operate ed wherein pixel data phase detection pixel pair capture frame pixel data regular pixel also use perform decode operate ed wherein decode decode qr code face operate ed wherein phase detection pixel pair partially cover pixel structure dual pixel apparatus comprise first camera module configure obtain first object first field view second camera module configure obtain second object second field view different first field view first depth map generator configure generate first depth map first base first second second depth map generator configure generate second depth map second base first second first depth map apparatus wherein first field view narrow angle second field view wider angle apparatus wherein second divide primary region residual region second depth map generator comprise relationship estimate module configure estimate relationship primary region residual region base first second depth map estimate module configure estimate depth map residual region base estimate relationship first depth map apparatus wherein least one relationship estimate module depth map estimate module performs estimate operation base neural network module apparatus comprise depth map fusion unit configure generate third depth map second perform fusion operation base first depth map second depth map apparatus wherein depth map fusion unit comprise tone mapping module configure generate tone-mapped second depth map correspond first depth map perform bias remove operation second depth map fusion module configure generate third depth map fuse tone-mapped second depth map first depth map apparatus wherein depth map fusion unit comprise propagate module configure generate propagate first depth map second iterate propagate first depth map base first depth map second fusion module generate third depth map fuse tone-mapped second depth map propagate first depth map apparatus wherein depth map fusion unit comprise post-processing module configure perform post-processing operation third depth map generate fusion module provide post-processed third depth map apparatus wherein post-processing module performs post-processing operation filter interface generate third depth map accordance fusion fusion module apparatus wherein post-processing module remove artifact generate third depth map accordance fusion fusion module apparatus wherein first depth map generator analyse distance relationship first second generates first depth map first base distance relationship process electronic apparatus comprise obtain first object use first camera module obtain second object use second camera module generate first depth map first base first second estimate relationship primary region second residual region second base first second generate second depth map second base estimate relationship primary region residual region first depth map wherein electronic apparatus comprises first camera module include first lens first field view second camera module include second lens second field view wider first field view wherein generate second depth map comprise estimate depth map residual region base estimate relationship primary region residual region first depth map generate second depth map base depth map residual region first depth map wherein estimate relationship primary region second perform use neural network model comprise perform pre-processing operation second depth map generate third depth map residual fuse second depth map pre-processing operation perform first depth map wherein perform pre-processing operation comprises perform tone mapping operation depth map primary region depth map residual region base second depth map operate electronic apparatus electronic apparatus include first camera module provide first object use first field view second camera module provide second object use second field view wider first field view generate depth map second base primary region second residual region second operating comprise generate first depth map primary region estimate relationship first second estimate relationship primary region residual region base first second generate second depth map second estimate depth map second region base estimate relationship primary region residual region generate depth map second fuse first depth map second depth map operation comprise execute application applies effect second base depth map residual operation wherein application apply least one effect auto-focusing out-focusing forebackground separation face object detection within frame augmented reality second base depth map second payment base face comprise acquire first face information target extract first characteristic information first face information wherein first characteristic information include head posture information target gaze information target determine whether target willingness pay accord head posture information target gaze information target include determine whether angle rotation preset direction less angle threshold wherein head posture information include angle rotation preset direction determine whether probability value gaze payment screen great probability threshold wherein gaze information include probability value gaze payment screen response determine angle rotation preset direction less angle threshold probability value gaze payment screen great probability threshold determine target willingness pay response determine target willingness pay complete payment operation base face ed wherein complete payment operation base face comprise trigger perform payment initiating operation acquire second face information base face determine whether second characteristic information extract second face information indicate willingness pay response determine second characteristic information indicate willingness pay trigger perform payment confirmation operation complete payment operation base payment account information correspond target ed wherein determine whether second characteristic information extract second face information indicate willingness pay comprises determine whether current correspond second face information consistent target response determine current consistent target determine whether target willingness pay accord second characteristic information extract second face information ed wherein extract first characteristic information first face information comprise determine head posture information target use head posture model base first face information determine gaze information target use gaze information model base characteristic eye region first face information ed wherein head posture model obtain train acquire first sample data set wherein first sample data set include piece first sample data piece first sample data include correspondence sample face head posture information determine mean data variance data sample face piece first sample data preprocessing sample face contain piece first sample data base mean data variance data obtain preprocessed sample face set preprocessed sample face correspond head posture information first model train sample perform training use machine learning base first model training sample obtain head posture model ed wherein gaze information model obtain train acquire second sample data set wherein second sample data set include piece second sample data piece second sample data include correspondence sample eye gaze information determine mean data variance data sample eye piece second sample data preprocessing sample eye contain piece second sample data base mean data variance data obtain preprocessed sample eye set preprocessed sample eye correspond gaze information second model train sample perform training use machine learn base second model training sample obtain gaze information model ed wherein angle rotation preset direction comprise pitch angle yaw angle roll angle wherein pitch angle refers angle rotation around x-axis yaw angle refers angle rotation around y-axis roll angle refers angle rotation around z-axis payment device base face comprise non-transitory computer-readable storage medium store instruction executable cause device perform operation comprise acquire first face information target extract first characteristic information first face information wherein first characteristic information include head posture information target gaze information target determine whether target willingness pay accord head posture information target gaze information target include determine whether angle rotation preset direction less angle threshold wherein head posture information include angle rotation preset direction determine whether probability value gaze payment screen great probability threshold wherein gaze information include probability value gaze payment screen response determine angle rotation preset direction less angle threshold probability value gaze payment screen great probability threshold determine target willingness pay response determine target willingness pay complete payment operation base face device ed wherein complete payment operation base face comprise trigger perform payment initiating operation acquire second face information base face determine whether second characteristic information extract second face information indicate willingness pay response determine second characteristic information indicate willingness pay trigger perform payment confirmation operation complete payment operation base payment account information correspond target device ed wherein determine whether second characteristic information extract second face information indicate willingness pay comprises determine whether current correspond second face information consistent target response determine current consistent target determine whether target willingness pay accord second characteristic information extract second face information device ed wherein extract first characteristic information first face information comprise determine head posture information target use head posture model base first face information determine gaze information target use gaze information model base characteristic eye region first face information device ed wherein head posture model obtain train acquire first sample data set wherein first sample data set include piece first sample data piece first sample data include correspondence sample face head posture information determine mean data variance data sample face piece first sample data preprocessing sample face contain piece first sample data base mean data variance data obtain preprocessed sample face set preprocessed sample face correspond head posture information first model train sample perform training use machine learning base first model training sample obtain head posture model device ed wherein gaze information model obtain train acquire second sample data set wherein second sample data set include piece second sample data piece second sample data include correspondence sample eye gaze information determine mean data variance data sample eye piece second sample data preprocessing sample eye contain piece second sample data base mean data variance data obtain preprocessed sample eye set preprocessed sample eye correspond gaze information second model train sample perform training use machine learn second model training sample obtain gaze information model device ed wherein angle rotation preset direction comprise pitch angle yaw angle roll angle wherein pitch angle refers angle rotation around x-axis yaw angle refers angle rotation around y-axis roll angle refers angle rotation around z-axis non-transitory computer-readable storage medium payment base face configure instruction executable one cause one perform operation comprise acquire first face information target extract first characteristic information first face information wherein first characteristic information include head posture information target gaze information target determine whether target willingness pay accord head posture information target gaze information target include determine whether angle rotation preset direction less angle threshold wherein head posture information include angle rotation preset direction determine whether probability value gaze payment screen great probability threshold wherein gaze information include probability value gaze payment screen response determine angle rotation preset direction less angle threshold probability value gaze payment screen great probability threshold determine target willingness pay response determine target willingness pay complete payment operation base face storage medium ed wherein complete payment operation base face comprise trigger perform payment initiating operation acquire second face information base face determine whether second characteristic information extract second face information indicate willingness pay response determine second characteristic information indicate willingness pay trigger perform payment confirmation operation complete payment operation base payment account information correspond target storage medium ed wherein determine whether second characteristic information extract second face information indicate willingness pay comprises determine whether current correspond second face information consistent target response determine current consistent target determine whether target willingness pay accord second characteristic information extract second face information storage medium ed wherein extract first characteristic information first face information comprise determine head posture information target use head posture model base first face information determine gaze information target use gaze information model base characteristic eye region first face information storage medium ed wherein head posture model obtain train acquire first sample data set wherein first sample data set include piece first sample data piece first sample data include correspondence sample face head posture information determine mean data variance data sample face piece first sample data preprocessing sample face contain piece first sample data base mean data variance data obtain preprocessed sample face set preprocessed sample face correspond head posture information first model train sample perform training use machine learning base first model training sample obtain head posture model wherein gaze information model obtain train acquire second sample data set wherein second sample data set include piece second sample data piece second sample data include correspondence sample eye gaze information determine mean data variance data sample eye piece second sample data preprocessing sample eye contain piece second sample data base mean data variance data obtain preprocessed sample eye set preprocessed sample eye correspond gaze information second model train sample perform training use machine learn base second model training sample obtain gaze information model storage medium ed wherein angle rotation preset direction comprise pitch angle yaw angle roll angle wherein pitch angle refers angle rotation around x-axis yaw angle refers angle rotation around y-axis roll angle refers angle rotation around z-axis comprise detect motion detection module motion subject within predetermined area view assign unique session identification number subject detect within predetermined area view detect facial area subject detect within predetermined area view generate facial area subject assess quality facial area subject determine identity subject base facial area subject identify intent subject authorize access point entry base determine identity subject base intent subject comprise determine one additional subject within predetermined area view assign unique session identification number one additional subject detect within predetermined area view wherein assess quality facial area subject comprise assess whether quality facial area object equates predetermine metric quality upon determine quality facial area object inferior predetermine metric quality discard facial area subject generate second facial area subject comprise detect whether facial area subject photographic upon detect facial area subject photographic generate warn restrict access point entry comprise conduce incremental training facial area subject wherein conduce incremental training facial area subject comprises capture first facial area facial landmark convert first facial area first numeric vector capture second facial area facial landmark convert second facial area second numeric vector calculate weight mean first numeric vector second numeric vector wherein weight mean represent change facial area store weighted mean database wherein determine identity subject base facial area subject comprise compare facial area subject store database authenticate subject wherein identify intent subject comprise upon detect facial area bound box commence authentication subject calculate directional vector face subject determine intent subject gain access point entry base directional vector face subject grant access point entry base authentication subject base determine intent subject non-transitory computer readable medium program instruction store thereon response execution compute device cause compute device perform operation comprise detect motion subject within predetermined area view assign unique session identification number subject detect within predetermined area view detect facial area subject detect within predetermined area view generate facial area subject assess quality facial area subject determine identity subject base facial area subject identify intent subject authorize access point entry base determine identity subject base intent subject non-transitory computer readable medium comprise determine one additional subject within predetermined area view assign unique session identification number one additional subject detect within predetermined area view non-transitory computer readable medium wherein assess quality facial area subject comprise assess whether quality facial area object equates predetermine metric quality upon determine quality facial area object inferior predetermine metric quality discard facial area subject generate second facial area subject non-transitory computer readable medium comprise detect whether facial area subject photographic upon detect facial area subject photographic generate warn restrict access access point non-transitory computer readable medium comprise conduce incremental training facial area subject non-transitory computer readable medium wherein conduce incremental training facial area subject comprises capture first facial area facial landmark convert first facial area first numeric vector capture second facial area facial landmark convert second facial area second numeric vector calculate weight mean first numeric vector second numeric vector wherein weight mean represent change facial area store weighted mean database apparatus face comprise memory store computer program instruction computer program instruction execute cause perform operation comprise detect motion subject within predetermined area view assign unique session identification number subject detect within predetermined area view detect facial area subject detect within predetermined area view generate facial area subject assess quality facial area subject determine identity subject base facial area subject identify intent subject authorize access point entry base determine identity subject base intent subject apparatus comprise determine one additional subject within predetermined area view assign unique session identification number one additional subject detect within predetermined area view apparatus wherein assess quality facial area subject comprise assess whether quality facial area object equates predetermine metric quality upon determine quality facial area object inferior predetermine metric quality discard facial area subject generate second facial area subject apparatus comprise detect whether facial area subject photographic upon detect facial area subject photographic generate warn restrict access access point apparatus comprise conduce incremental training facial area subject apparatus wherein conduce incremental training facial area subject comprises capture first facial area facial landmark convert first facial area first numeric vector capture second facial area facial landmark convert second facial area second numeric vector calculate weight mean first numeric vector second numeric vector wherein weight mean represent change facial area store weighted mean database robot comprise body configure rotate tilt camera couple body configured rotate tilt accord rotate tilt body wherein camera configure acquire video space face unit configure recognize respective one person video unit configure track motion recognize one person controller configure calculate respective size one person select first person among one person base calculated size control least one direction rotation camera angle tilt camera focal distance camera base tracked motion recognize face first person robot wherein controller configure control direction rotation camera angle tilt camera achieve particular camera relative face first person control focal distance camera compare respective size face first person motion first person robot wherein particular occurs camera general direction face first person robot wherein controller configure normalize size one person base interocular distance select first person base normalized size one person robot wherein controller configure select person large face size among one person first person robot comprise microphone configure receive spoken audio present space wherein controller configure select first person base receive spoken audio robot wherein controller configure control gain microphone compare respective size face first person motion first person robot wherein controller configure calculate position speak audio provide select first person base whether one person position voice signal provide robot wherein controller configure select second person first person among one person second person locate position speak audio provide robot wherein controller configure select second person large face size first person among one person none one person locate position speak audio provide robot wherein controller configure select second person large face size first person among one person person among one person locate position speak audio provide robot comprise speaker wherein controller configure control volume speaker compare respective size face first person motion first person robot wherein body configure rotate lateral direction tilt vertical direction comprise camera couple body configured rotate tilt wherein camera configure acquire video space within one person position configured recognize respective one person video track motion recognize one person calculate respective size one person select first person among one person base calculated size control least one direction rotation camera angle tilt camera focal distance camera base tracked motion recognize face first person comprise acquire camera video space within one person position respective one person video motion recognize one person calculate respective size one person select first person among one person base calculated size control least one direction rotation camera angle tilt camera focal distance camera base tracked motion recognize face first person infer topic multimodal file comprise receive multimodal file extract set entity multimodal file link set entity produce set link entity obtain reference information set entity base least reference information generate graph set link entity graph comprise node edge base least nodes edge graph determine cluster graph base least cluster graph identify topic candidate extract feature cluster graph base least extracted feature select least one topicid among topic candidate represent least one cluster index multimodal file least one topicid wherein multimodal file comprise video portion audio portion wherein extract set entity multimodal file comprises detect object video portion multimodal file detect text audio portion multimodal file wherein detect object comprise perform face wherein detect text comprises perform speech text process comprise identify language use audio portion multimodal file wherein perform speech text process comprise perform speech text process identify language comprise translate detect text comprise determine significant cluster insignificant cluster determine cluster wherein extract feature cluster graph comprises extract feature significant cluster graph wherein extract feature cluster graph comprises least one process select list consist determine graph diameter determine jaccard coefficient wherein select least one topicid represent least one cluster comprises base least extracted feature map topic candidate probability interval base least map rank topic candidate within least one cluster select least one topicid base least ranking comprise translate least one topicid wherein index multimodal file least one topicid comprise index multimodal file least one translate topicid system infer topic multimodal file system comprise entity extraction component comprise object detection component speech text component operative extract set entity multimodal file comprise video portion audio portion entity link component operative link extract set entity produce set link entity information retrieval component operative obtain reference information extract set entity graph analysis component operative generate graph set link entity graph comprise node edge base least nodes edge graph determine cluster graph base least cluster graph identify topic candidate extract feature cluster graph topicid selection component operative rank topic candidate within least one cluster base least ranking select least one topicid among topic candidate represent least one cluster video indexer operative index multimodal file least one topicid system wherein object detection component operative perform face system wherein speech text component operative extract entity information least two different language one computer storage device computer-executable instruction store thereon infer topic multimodal file execution computer cause computer perform operation comprise receive multimodal file comprise video portion audio portion extract set entity multimodal file wherein extract set entity multimodal file comprises detect object video portion multimodal file face detect text audio portion multimodal file speech text process disambiguate among set detect entity name link set entity produce set link entity obtain reference information set entity base least reference information generate graph set link entity graph comprise node edge base least nodes edge graph determine cluster graph determine significant cluster insignificant cluster determine cluster base least significant cluster graph identify topic candidate extract feature significant cluster graph base least extracted feature map topic candidate probability interval base least map rank topic candidate within least one significant cluster base rank select least one topicid among topic candidate represent least one significant cluster index multimodal file least one topicid one computer storage device wherein operation comprise identify language use audio portion multimodal file detect text audio portion multimodal file speech text process comprise perform speech text process identify language权利要求 system alert vision impairment say system comprise process unit configure operable receive scene data indicative scene least one consumer environment identify scene data certain consumer identify event indicative behavioral compensation vision impairment upon identification event send notification relate vision impairment system comprise least one sense unit configure operable detect scene data system wherein say least one sense unit comprise least one least one image unit configure operable capture least one least portion consumer 's body least one motion detector configure operable detect consumer data indicative motion consumer least one eye tracker configure operable eye motion consumer system wherein least one image unit comprise camera place different height system one wherein say sense unit accommodate optical digital eyewear frame display system one wherein say processing unit configure operable identify consumer 's condition say consumer 's condition comprise consumer data indicative consumer 's position location relative least one object consumer 's environment say consumer data comprises least one consumer 's face eyewear posture position sound motion system one wherein say event comprises least one position head increase decrease view distance consumer view object change position eyeglass worn consumer system one wherein say event identify identifying feature indicative behavioral compensation perform bruckner test perform hirschberg test measure blink count frequency system wherein feature indicative behavioral compensation comprises squint head certain distance object consumer 's eye certain position eyeglasses consumer 's face strabismus cataracts reflection eye system one wherein notification include least one data indicative identify event data indicative identified consumer ophthalmologic recommendation base identified event lack event appointment vision test system one wherein say processing unit comprise memory store least one reference data indicative behavioral compensation vision impairment data indicative notification data indicative follow-up notification system wherein say processing unit configure least one identify event upon comparison detect data reference data determine probability vision impairment consumer base comparison system one wherein say processing unit comprise communication interface configure send notification least one identified consumer party system one wherein say processing unit configure provide frame recommendation system one wherein say memory configure storing database include multiplicity data set related spectacle frame model size system accord wherein say process unit configure operable correlate frame parameter ophthalmic prescription system accord wherein say process unit configure operable correlate frame parameter facial feature system accord wherein say process unit configure operable correlate frame parameter eyewear preference system accord comprise server least one computer entity link server via network wherein say network configure receive respond request send across network transmit one module computer executable program instruction displayable data network connect computer platform response request wherein say module include module configure receive transmit information transmit frame recommendation optical lens option recommendation base receive information display network connect computer platform computer program instruction store local storage execute process unit cause process unit receive data indicative scene least one consumer environment identify data certain consumer identify event indicative behavioral compensation vision impairment upon identification event send notification relate vision impairment computer program product store tangible computer readable medium comprise library software module cause computer execute prompt information pertinent least one eyeglasses recommendation optical lens option recommendation store say information display eyewear recommendation computer program product wherein say library comprises module frame selection point sale advertise computer platform facilitate eye glass market selection comprise camera configure execute computer program instruction cause take consumer identify certain consumer identify event indicative behavioral compensation vision impairment upon identification event send notification relate vision impairment local storage executable instruction carry storage information alert vision impairment say comprise identify certain individual scene data indicative scene least one consumer environment identify event indicative behavioral compensation vision impairment upon identification event send notification vision impairment comprise detect data indicative scene least one consumer retail environment wherein detect data indicative least one consumer comprise least one capture least one least one consumer detect data indicative motion consumer eye motion consumer wherein capture least one least one consumer comprise continuously record scene one comprising identify data consumer ' condition include data indicative consumer 's position location relative consumer 's environment say data comprise least one consumer 's face posture position sound motion one wherein say event comprises least one position head increase decrease view distance consumer view object change position eyeglass worn consumer one wherein identify event comprises identify feature indicative behavioral compensation perform bruckner test perform hirschberg test measure blink countfrequency wherein feature indicative behavioral compensation comprises squint head certain distance object consumer 's eye certain position eyeglasses consumer 's face strabismus cataracts reflection eye one wherein identify least one consumer retail environment comprise least one receive data characterize retail environment perform face one wherein send notification comprise send notification least one identified consumer third party one wherein notification include least one data indicative identify event data indicative identified consumer ophthalmologic recommendation base identified event lack event appointment vision test one comprise store least one reference data indicative behavioral compensation vision impairment data indicative notification data indicative follow-up notification comprise identify event upon comparison detect data reference data determine probability vision impairment consumer base comparison computer program intend stored memory unit computer system removable memory medium adapt cooperate reader unit comprise instruction implement accord \""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_text_c = \"\"\n",
    "for claim_text in lemmatized_text_c:\n",
    "    lem_text_c += claim_text + \" \" #we pick each word and add to a variable, which will contain all the text\n",
    "lem_text_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68cbbd3",
   "metadata": {},
   "source": [
    "### KeyWord Extraction\n",
    "Finally we obtain the key words and phrases from the proccesed text to obtain the main themes of the patents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afaf2762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing extractor...\n",
      "Loading text...\n",
      "Candidate Selection...\n",
      "Weighting...\n",
      "Selecting 20 best candidates...\n",
      "fusion method\n",
      "order\n",
      "aspect smart television tv\n",
      "region interest\n",
      "detect\n",
      "local patch\n",
      "field view head-mounted display base\n",
      "different process\n",
      "human group\n",
      "vergence distance\n",
      "configuration\n",
      "customer information\n",
      "step manage count specific facial\n",
      "network\n",
      "motion subject\n",
      "input\n",
      "face cluster\n",
      "pre-processed multi-channel channel pre-processed\n",
      "electronic apparatus\n",
      "pixel pixel\n",
      "area view\n",
      "photo album\n",
      "feature extractor\n",
      "module\n",
      "interior vehicle face network\n",
      "location\n",
      "face face correspond respective person\n",
      "notification instruction electronic\n",
      "location-based access control secure resource\n",
      "regular pixel\n",
      "[('fusion method', 0.050166996687558967), ('order', 0.0227374855218926), ('aspect smart television tv', 0.020706482678782133), ('region interest', 0.01973492968607286), ('detect', 0.018785845382593425), ('local patch', 0.018697103953675444), ('field view head-mounted display base', 0.0175542834262552), ('different process', 0.016855921568266444), ('human group', 0.016724854481452806), ('vergence distance', 0.016505319329333502), ('configuration', 0.015534940401856807), ('customer information', 0.015213072264997822), ('step manage count specific facial', 0.014641968829486008), ('network', 0.014189100390174429), ('motion subject', 0.013939185235271148), ('input', 0.013673447347328541), ('face cluster', 0.013588766270018503), ('pre-processed multi-channel channel pre-processed', 0.013263357897256314), ('electronic apparatus', 0.013075347174824337), ('pixel pixel', 0.012871412601773494), ('area view', 0.012281648341841953), ('photo album', 0.012151449081686283), ('feature extractor', 0.01198137697423357), ('module', 0.011924991913970687), ('interior vehicle face network', 0.01171119148133469), ('location', 0.011653514216528816), ('face face correspond respective person', 0.011469777160905494), ('notification instruction electronic', 0.011396735713428911), ('location-based access control secure resource', 0.010442863864117784), ('regular pixel', 0.010326286134365878)]\n"
     ]
    }
   ],
   "source": [
    "# initialize keyphrase extraction model, here TopicRank\n",
    "print(\"Initializing extractor...\")\n",
    "extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "# load the content of the document, here document is expected to be in raw\n",
    "# format (i.e. a simple text file) and preprocessing is carried out using spacy\n",
    "print(\"Loading text...\");\n",
    "extractor.load_document(input=lem_text_a, language='en')\n",
    "\n",
    "# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
    "# and adjectives (i.e. `(Noun|Adj)*`)\n",
    "print(\"Candidate Selection...\")\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
    "print(\"Weighting...\")\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# N-best selection, keyphrases contains the 10 highest scored candidates as\n",
    "# (keyphrase, score) tuples\n",
    "print(\"Selecting 20 best candidates...\")\n",
    "keyphrases = extractor.get_n_best(n=30)\n",
    "for tuple in keyphrases:\n",
    "    print(tuple[0])\n",
    "print(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6491ad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing extractor...\n",
      "Loading text...\n",
      "Candidate Selection...\n",
      "Weighting...\n",
      "Selecting 60 best candidates...\n",
      "device\n",
      "feature map correspond interior\n",
      "face detection comprise\n",
      "process\n",
      "location\n",
      "keypoint heatmap\n",
      "upcoming medium program base profile\n",
      "tree module\n",
      "specific person\n",
      "parameter motion parameter\n",
      "compute system\n",
      "configure\n",
      "respective\n",
      "face correspond respective person\n",
      "invitee\n",
      "model\n",
      "position\n",
      "comprises\n",
      "specific facial neural aggregation network\n",
      "data\n",
      "count specific facial satisfies preset\n",
      "audio feedback signal\n",
      "operation\n",
      "facial area subject detect\n",
      "machine\n",
      "possible system\n",
      "information\n",
      "local patch\n",
      "attention parameter\n",
      "angle rotation preset direction\n",
      "face subtract area\n",
      "topic multimodal file comprise\n",
      "memory store instruction\n",
      "preset source cover accord\n",
      "feature extraction network ii generate\n",
      "layer computer implement\n",
      "second characteristic information\n",
      "consumer\n",
      "region classifier code\n",
      "interior face network\n",
      "convolutional layer\n",
      "historical time historical date\n",
      "second yaw angle\n",
      "training\n",
      "input interface\n",
      "photo album\n",
      "head-mounted display\n",
      "channel\n",
      "different salient event\n",
      "video frame\n",
      "body information\n",
      "facial feature classifier\n",
      "gaze information\n",
      "control unit\n",
      "salient data indicative salient event\n",
      "fusion algorithm\n",
      "target willingness pay response\n",
      "value\n",
      "response\n",
      "face cluster\n",
      "[('device', 0.018439748875149663), ('feature map correspond interior', 0.017590095650924526), ('face detection comprise', 0.01740069088789859), ('process', 0.01663266787213228), ('location', 0.013192440889538004), ('keypoint heatmap', 0.010963413772297806), ('upcoming medium program base profile', 0.00947540514102604), ('tree module', 0.009253837756865873), ('specific person', 0.008930700383876096), ('parameter motion parameter', 0.008900674776189662), ('compute system', 0.008889494721024464), ('configure', 0.008885912335865213), ('respective', 0.00873833180666451), ('face correspond respective person', 0.008639215262056424), ('invitee', 0.008607603254713018), ('model', 0.008227100064291584), ('position', 0.00819000599581195), ('comprises', 0.008148136241909503), ('specific facial neural aggregation network', 0.008009804335273353), ('data', 0.008006497505242731), ('count specific facial satisfies preset', 0.007878521647179877), ('audio feedback signal', 0.007717479162554067), ('operation', 0.007699634043339794), ('facial area subject detect', 0.007608021260836473), ('machine', 0.007555391812102028), ('possible system', 0.007324979563735124), ('information', 0.007110981184251781), ('local patch', 0.007042604861917578), ('attention parameter', 0.007032893024032304), ('angle rotation preset direction', 0.006843097197924269), ('face subtract area', 0.006793724067637818), ('topic multimodal file comprise', 0.006642899664200824), ('memory store instruction', 0.006642092621066658), ('preset source cover accord', 0.006592463722583592), ('feature extraction network ii generate', 0.00656912755856824), ('layer computer implement', 0.006475053638678861), ('second characteristic information', 0.006449848626839061), ('consumer', 0.006436659648096866), ('region classifier code', 0.006325092836515056), ('interior face network', 0.006289847028493265), ('convolutional layer', 0.006276408491125881), ('historical time historical date', 0.0060980221392337025), ('second yaw angle', 0.006043682755707173), ('training', 0.006032316402670475), ('input interface', 0.005987086248732472), ('photo album', 0.005911198952248034), ('head-mounted display', 0.0058906732286904875), ('channel', 0.005831893312180809), ('different salient event', 0.005745344752060435), ('video frame', 0.005660400755535077), ('body information', 0.005607017825514047), ('facial feature classifier', 0.005498695366452917), ('gaze information', 0.0052498246502931155), ('control unit', 0.005247348188471504), ('salient data indicative salient event', 0.005216504198345099), ('fusion algorithm', 0.005115298133052637), ('target willingness pay response', 0.005025495703739974), ('value', 0.004890389449670529), ('response', 0.00479521126126292), ('face cluster', 0.004782496014260783)]\n"
     ]
    }
   ],
   "source": [
    "# initialize keyphrase extraction model, here TopicRank\n",
    "print(\"Initializing extractor...\")\n",
    "extractor = pke.unsupervised.TopicRank()\n",
    "\n",
    "# load the content of the document, here document is expected to be in raw\n",
    "# format (i.e. a simple text file) and preprocessing is carried out using spacy\n",
    "print(\"Loading text...\");\n",
    "extractor.load_document(input=lem_text_c, language='en')\n",
    "\n",
    "# keyphrase candidate selection, in the case of TopicRank: sequences of nouns\n",
    "# and adjectives (i.e. `(Noun|Adj)*`)\n",
    "print(\"Candidate Selection...\")\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# candidate weighting, in the case of TopicRank: using a random walk algorithm\n",
    "print(\"Weighting...\")\n",
    "extractor.candidate_weighting()\n",
    "\n",
    "# N-best selection, keyphrases contains the 10 highest scored candidates as\n",
    "# (keyphrase, score) tuples\n",
    "print(\"Selecting 60 best candidates...\")\n",
    "keyphrases = extractor.get_n_best(n=60)\n",
    "for tuple in keyphrases:\n",
    "    print(tuple[0])\n",
    "print(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ffdaa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posture\n",
      "faceembodiments\n",
      "capture\n",
      "classify\n",
      "feature\n",
      "[('posture', 0.3497), ('faceembodiments', 0.2996), ('capture', 0.2826), ('classify', 0.2775), ('feature', 0.2763)]\n"
     ]
    }
   ],
   "source": [
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(lem_text_a)\n",
    "for tuple in keywords:\n",
    "    print(tuple[0])\n",
    "\n",
    "print(kw_model.extract_keywords(lem_text_a, keyphrase_ngram_range=(1, 1), stop_words=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "392e81bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posture\n",
      "alignment\n",
      "interpolation\n",
      "rotated\n",
      "tilt\n",
      "[('posture', 0.4437), ('alignment', 0.4106), ('interpolation', 0.3969), ('rotated', 0.3907), ('tilt', 0.3821)]\n"
     ]
    }
   ],
   "source": [
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(lem_text_c)\n",
    "for tuple in keywords:\n",
    "    print(tuple[0])\n",
    "\n",
    "print(kw_model.extract_keywords(lem_text_c, keyphrase_ngram_range=(1, 1), stop_words=None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
