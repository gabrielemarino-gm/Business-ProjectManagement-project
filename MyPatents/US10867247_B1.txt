<document>

<filing_date>
2016-08-09
</filing_date>

<publication_date>
2020-12-15
</publication_date>

<priority_date>
2016-02-14
</priority_date>

<ipc_classes>
G06N3/04,G06N3/08
</ipc_classes>

<assignee>
PERCEIVE CORPORATION
</assignee>

<inventors>
TEIG, STEVEN L.
</inventors>

<docdb_family_id>
73746942
</docdb_family_id>

<title>
Machine learning through multiple layers of novel machine trained processing nodes
</title>

<abstract>
Some embodiments of the invention provide efficient, expressive machine-trained networks for performing machine learning. The machine-trained (MT) networks of some embodiments use novel processing nodes with novel activation functions that allow the MT network to efficiently define with fewer processing node layers a complex mathematical expression that solves a particular problem (e.g., face recognition, speech recognition, etc.). In some embodiments, the same activation function (e.g., a cup function) is used for numerous processing nodes of the MT network, but through the machine learning, this activation function is configured differently for different processing nodes so that different nodes can emulate or implement two or more different functions (e.g., two or more Boolean logical operators, such as XOR and AND). The activation function in some embodiments is a periodic function that can be configured to implement different functions (e.g., different sinusoidal functions).
</abstract>

<claims>
1. A computing device comprising: a non-transitory machine readable medium storing a machine trained (MT) network comprising a plurality of layers of processing nodes, each processing node configured to: compute a first output value by combining a set of output values from a set of processing nodes, and use a piecewise linear cup function to compute a second output value from the first output value of the processing node, wherein the piecewise linear cup function prior to training of the MT network comprises at least (i) a first linear section with a first slope, followed by (ii) a second linear section with a negative second slope, followed by (iii) a third linear section with a negative third slope that is different from the second slope, followed by (iv) a fourth linear section with a positive fourth slope, followed by (v) a fifth linear section with a positive fifth slope that is different from the fourth slope, followed by (vi) a sixth linear section with a sixth slope, wherein the piecewise linear cup function is symmetric about a vertical axis between the third and fourth linear sections prior to training of the MT network; a content capturing circuit for capturing content for processing by the MT network; and a set of processing units for executing the processing nodes to process content captured by the content capturing circuit, wherein by training a set of parameters that define the piecewise linear cup function of each node in first and second pluralities of processing nodes, (i) each processing node in the first plurality of processing nodes is configured to emulate a Boolean AND operator such that an output value of the processing node is in a range associated with a "1" value only when a set of inputs to the processing node have a set of values in a range associated with "1" and (ii) each processing node in the second plurality of processing nodes is configured to emulate a Boolean XNOR operator such that an output value of the processing node is in the range associated with "1" only when (a) a set of inputs to the node have a set of values in a range associated with "1" or (b) the set of inputs to the node have a set of values in a range associated with a "0" value.
2. The computing device of claim 1, wherein the third linear section of the piecewise linear cup function of a first processing node in the MT network has a different slope from the third linear section of a second processing node in the MT network.
3. The computing device of claim 1, wherein the length of the third section of a piecewise linear cup function of a first processing node in the MT network is different from the length of the third section of a piecewise linear cup function of a second processing node in the MT network.
4. The computing device of claim 1, wherein the sets of parameters are trained in part by a back propagating module for back propagating errors in output values of later layers of processing nodes to earlier layers of processing nodes by adjusting the set of parameters that define the piecewise linear cup functions of the earlier layers of processing nodes.
5. The computing device of claim 4, wherein each processing node uses a linear function that is defined by a set of parameters to compute the first output value of the processing node, wherein the back propagating module back propagates errors in output values of later layers of processing nodes to earlier layers of processing nodes by adjusting the set of parameters that define the linear functions of the earlier layers of processing nodes.
6. The computing device of claim 1, wherein the first plurality of processing nodes that emulate the Boolean AND operator and the second plurality of processing nodes that emulate the Boolean XNOR operator enable the MT network to implement mathematical problems.
7. The computing device of claim 1, wherein each of a plurality of processing node layers has a plurality of processing nodes that receive as input values the output values from a plurality of processing nodes in a set of prior layers.
8. The computing device of claim 7, wherein each processing node uses a linear function to compute the first output value of the processing node, wherein each processing node's piecewise linear cup function is defined along first and second axes, the first axis defining a range of output values from the processing node's linear function, and the second axis defining a range of output values produced by the piecewise linear cup function for the range of output values from the processing node's linear function.
9. The computing device of claim 1, further comprising: a content output circuit for presenting an output based on the processing of the content by the MT network.
10. The computing device of claim 9, wherein the captured content is one of an image and an audio segment, and wherein the presented output is an output display on a display screen of the computing device or an audio presentation output on a speaker of the computing device.
11. The computing device of claim 10, wherein the computing device is a mobile device.
12. The computing device of claim 1, wherein the MT network is a MT neural network and the processing nodes are MT neurons.
13. The computing device of claim 1, wherein the set of parameters configured through training for a plurality of the processing nodes comprise at least one of the negative second and third slopes for the second and third linear sections, the positive fourth and fifth slopes for the fourth and fifth linear sections, a first intercept for the second linear section, a second intercept for the fifth linear section, and a set of lengths for at least the second, third, fourth, and fifth sections.
14. The computing device of claim 1, wherein the trained set of parameters that define the piecewise linear cup function of each node comprise a plurality of output values.
15. The computing device of claim 1, wherein the first and sixth slopes are zero.
</claims>
</document>
