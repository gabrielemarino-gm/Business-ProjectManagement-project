<document>

<filing_date>
2020-05-31
</filing_date>

<publication_date>
2020-09-17
</publication_date>

<priority_date>
2019-08-16
</priority_date>

<ipc_classes>
G06F3/01,G06K9/00,G06N3/08,G06Q20/40
</ipc_classes>

<assignee>
ALIBABA GROUP
</assignee>

<inventors>
CAO, Jiajiong
</inventors>

<docdb_family_id>
72423323
</docdb_family_id>

<title>
PAYMENT METHOD AND DEVICE
</title>

<abstract>
Methods, systems, and apparatus, including computer programs encoded on computer storage media, for a payment based on a face recognition are provided. One of the methods includes: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including determining whether an angle of rotation in each preset direction is less than an angle threshold and whether a probability value that a user gazes at a payment screen is greater than a probability threshold; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition.
</abstract>

<claims>
1. A payment method based on a face recognition, comprising: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including: determining whether an angle of rotation in each preset direction is less than an angle threshold, wherein the head posture information includes the angle of rotation in each preset direction; determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold, wherein the gaze information includes the probability value that a user gazes at a payment screen; and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold, determining that the target user has a willingness to pay; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition.
2. The method as claimed in claim 1, wherein the completing a payment operation based on the face recognition comprises: triggering and performing a payment initiating operation to acquire second face image information based on the face recognition; determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay; and in response to determining that the second characteristic information indicates that the user has a willingness to pay, triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user.
3. The method as claimed in claim 2, wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises: determining whether a current user corresponding to the second face image information is consistent with the target user; and in response to determining that the current user is consistent with the target user, determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information.
4. The method as claimed in claim 1, wherein the extracting first characteristic information from the first face image information comprises: determining the head posture information of the target user using a head posture recognition model based on the first face image information; and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information.
5. The method as claimed in claim 4, wherein the head posture recognition model is obtained through training by: acquiring a first sample data set, wherein the first sample data set includes a plurality of pieces of first sample data, and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information; determining mean image data and variance image data of a plurality of sample face images; for each of the plurality of pieces of first sample data, preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image; setting the preprocessed sample face image and the corresponding head posture information as a first model training sample; and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model.
6. The method as claimed in claim 4, wherein the gaze information recognition model is obtained through training by: acquiring a second sample data set, wherein the second sample data set includes a plurality of pieces of second sample data, and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information; determining mean image data and variance image data of a plurality of sample eye images; for each of the plurality of pieces of second sample data, preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image; setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample; and performing training using a machine learning method and based on a plurality of second model training samples to obtain the gaze information recognition model.
7. The method as claimed in claim 1, wherein the angle of rotation in each preset direction comprises a pitch angle, a yaw angle, and a roll angle, wherein the pitch angle refers to an angle of rotation around a X-axis, the yaw angle refers to an angle of rotation around a Y-axis, and the roll angle refers to an angle of rotation around a Z-axis.
8. A payment device based on a face recognition, comprising: a processor, and a non-transitory computer-readable storage medium storing instructions executable by the processor to cause the device to perform operations comprising: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including: determining whether an angle of rotation in each preset direction is less than an angle threshold, wherein the head posture information includes the angle of rotation in each preset direction; determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold, wherein the gaze information includes the probability value that a user gazes at a payment screen; and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold, determining that the target user has a willingness to pay; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition.
9. The device as claimed in claim 8, wherein the completing a payment operation based on the face recognition comprises: triggering and performing a payment initiating operation to acquire second face image information based on the face recognition; determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay; and in response to determining that the second characteristic information indicates that the user has a willingness to pay, triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user.
10. The device as claimed in claim 9, wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises: determining whether a current user corresponding to the second face image information is consistent with the target user; and in response to determining that the current user is consistent with the target user, determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information.
11. The device as claimed in claim 8, wherein the extracting first characteristic information from the first face image information comprises: determining the head posture information of the target user using a head posture recognition model based on the first face image information; and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information.
12. The device as claimed in claim 11, wherein the head posture recognition model is obtained through training by: acquiring a first sample data set, wherein the first sample data set includes a plurality of pieces of first sample data, and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information; determining mean image data and variance image data of a plurality of sample face images; for each of the plurality of pieces of first sample data, preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image; setting the preprocessed sample face image and the corresponding head posture information as a first model training sample; and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model.
13. The device as claimed in claim 11, wherein the gaze information recognition model is obtained through training by: acquiring a second sample data set, wherein the second sample data set includes a plurality of pieces of second sample data, and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information; determining mean image data and variance image data of a plurality of sample eye images; for each of the plurality of pieces of second sample data, preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image; setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample; and performing training using a machine learning method and on a plurality of second model training samples to obtain the gaze information recognition model.
14. The device as claimed in claim 11, wherein the angle of rotation in each preset direction comprises a pitch angle, a yaw angle, and a roll angle, wherein the pitch angle refers to an angle of rotation around a X-axis, the yaw angle refers to an angle of rotation around a Y-axis, and the roll angle refers to an angle of rotation around a Z-axis.
15. A non-transitory computer-readable storage medium for a payment based on a face recognition, configured with instructions executable by one or more processors to cause the one or more processors to perform operations comprising: acquiring first face image information of a target user; extracting first characteristic information from the first face image information, wherein the first characteristic information includes head posture information of the target user, and gaze information of the target user; determining whether the target user has a willingness to pay according to the head posture information of the target user and the gaze information of the target user, including: determining whether an angle of rotation in each preset direction is less than an angle threshold, wherein the head posture information includes the angle of rotation in each preset direction; determining whether a probability value that a user gazes at a payment screen is greater than a probability threshold, wherein the gaze information includes the probability value that a user gazes at a payment screen; and in response to determining that the angle of rotation in each preset direction is less than the angle threshold and that the probability value that a user gazes at a payment screen is greater than the probability threshold, determining that the target user has a willingness to pay; and in response to determining that the target user has a willingness to pay, completing a payment operation based on the face recognition.
16. The storage medium as claimed in claim 15, wherein the completing a payment operation based on the face recognition comprises: triggering and performing a payment initiating operation to acquire second face image information based on the face recognition; determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay; and in response to determining that the second characteristic information indicates that the user has a willingness to pay, triggering and performing a payment confirmation operation to complete the payment operation based on payment account information corresponding to the target user.
17. The storage medium as claimed in claim 16, wherein the determining whether second characteristic information extracted from the second face image information indicates that the user has a willingness to pay comprises: determining whether a current user corresponding to the second face image information is consistent with the target user; and in response to determining that the current user is consistent with the target user, determining whether the target user has a willingness to pay according to the second characteristic information extracted from the second face image information.
18. The storage medium as claimed in claim 15, wherein the extracting first characteristic information from the first face image information comprises: determining the head posture information of the target user using a head posture recognition model based on the first face image information; and determining the gaze information of the target user using a gaze information recognition model based on characteristics of an eye region in the first face image information.
19. The storage medium as claimed in claim 18, wherein the head posture recognition model is obtained through training by: acquiring a first sample data set, wherein the first sample data set includes a plurality of pieces of first sample data, and each of the plurality of pieces of first sample data includes a correspondence between a sample face image and head posture information; determining mean image data and variance image data of a plurality of sample face images; for each of the plurality of pieces of first sample data, preprocessing the sample face image contained in each of the plurality of pieces of first sample data based on the mean image data and the variance image data to obtain a preprocessed sample face image; setting the preprocessed sample face image and the corresponding head posture information as a first model training sample; and performing training using a machine learning method and based on a plurality of first model training samples to obtain the head posture recognition model; and wherein the gaze information recognition model is obtained through training by: acquiring a second sample data set, wherein the second sample data set includes a plurality of pieces of second sample data, and each of the plurality of pieces of second sample data includes a correspondence between a sample eye image and gaze information; determining mean image data and variance image data of a plurality of sample eye images; for each of the plurality of pieces of second sample data, preprocessing the sample eye image contained in each of the plurality of pieces of second sample data based on the mean image data and the variance image data to obtain a preprocessed sample eye image; setting the preprocessed sample eye image and the corresponding gaze information as a second model training sample; and performing training using a machine learning method and based on a plurality of second model training samples to obtain the gaze information recognition model.
20. The storage medium as claimed in claim 18, wherein the angle of rotation in each preset direction comprises a pitch angle, a yaw angle, and a roll angle, wherein the pitch angle refers to an angle of rotation around a X-axis, the yaw angle refers to an angle of rotation around a Y-axis, and the roll angle refers to an angle of rotation around a Z-axis.
</claims>
</document>
