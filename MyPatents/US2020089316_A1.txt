<document>

<filing_date>
2018-09-14
</filing_date>

<publication_date>
2020-03-19
</publication_date>

<priority_date>
2018-09-14
</priority_date>

<ipc_classes>
G02B27/01,G06F3/01,G06T19/00,G06T7/00,G06T7/11,G06T7/55
</ipc_classes>

<assignee>
FACEBOOK TECHNOLOGY COMPANY
</assignee>

<inventors>
CHOUBEY, NEERAJ
RASKAR, RAMESH
</inventors>

<docdb_family_id>
69774190
</docdb_family_id>

<title>
Vergence determination
</title>

<abstract>
In one embodiment, the artificial reality system determines that a performance metric of an eye tracking system is below a first performance threshold. The eye tracking system is associated with a head-mounted display worn by a user. The artificial reality system receives first inputs associated with the body of a user and determines a region that the user is looking at within a field of view of a head-mounted display based on the received first inputs. The system determines a vergence distance of the user based at least on the first inputs associated with the body of the user, the region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display. The system adjusts one or more configurations of the head-mounted display based on the determined vergence distance of the user.
</abstract>

<claims>
1. A method comprising, by a computing system: determining that a performance metric of an eye tracking system is below a first performance threshold, wherein the eye tracking system is associated with a head-mounted display worn by a user; based on the determination of the performance metric of the eye tracking system being below the first performance threshold, the computer system performing: receiving one or more first inputs associated with a body of the user; estimating a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user; determining a vergence distance of the user based at least on the one or more first inputs associated with the body of the user, the estimated region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display; and adjusting one or more configurations of the head-mounted display based on the determined vergence distance of the user.
2. The method of claim 1, wherein the one or more configurations of the head-mounted display comprise one or more of: a rendering image; a position of a display screen; or a position of an optics block.
3. The method of claim 1, further comprising: determining that the performance metric of the eye tracking system is above a second performance threshold; receiving eye tracking data from the eye tracking system; and determining the vergence distance of the user based on the eye tracking data and the one or more first inputs associated with the body of the user.
4. The method of claim 3, further comprising: receiving one or more second inputs associated with one or more displaying elements in the scene displayed by the head-mounted display; and determining the vergence distance of the user based at least on the eye tracking data, the one or more first inputs associated with the body of the user, and the one or more second inputs associated with the one or more displaying elements of the scene.
5. The method of claim 4, further comprising: feeding the one or more first inputs associated with the body of the user to a fusion algorithm, wherein the fusion algorithm assigns a weight score to each input of the one or more first inputs; determining the vergence distance of the user using the fusion algorithm based on the one or more first inputs associated with the body of the user; and determining a Z-depth of a display screen and a confidence score based on the one or more first inputs associated with the body of the user
6. The method of claim 5, further comprising: comparing the confidence score to a confidence level threshold; in response to a determination that the confidence score is below the confidence level threshold, feeding the one or more second inputs associated with the one or more displaying elements of the scene to the fusion algorithm; and determining the Z-depth of the display screen using the fusion algorithm based on the one or more first inputs associated with the body of the user and the one or more second inputs associated with the one or more displaying elements of the scene.
7. The method of claim 6, further comparing: comparing, by the fusion algorithm, confidence scores associated with a plurality of combinations of inputs; and determining, by the fusion algorithm, the Z-depth of the display screen based on a combination of inputs associated with a highest confidence score.
8. The method of claim 6, wherein the Z-depth and the confidence score are determined by the fusion algorithm using a piecewise comparison of the one or more first inputs and the one or more second inputs.
9. The method of claim 6, wherein the Z-depth and the confidence score are determined based on a correlation between two or more inputs of the one or more first inputs and the one or more second inputs.
10. The method of claim 5, wherein the fusion algorithm comprises a machine learning (ML) algorithm, and wherein the machine learning (ML) algorithm determines a combination of first inputs fed to the fusion algorithm.
11. The method of claim 4, wherein the one or more first inputs associated with the body of the user comprise one or more of: a hand position; a hand direction; a hand movement; a hand gesture; a head position; a head direction; a head movement; a head gesture; a gaze angle; rea body gesture; a body posture; a body movement; a behavior of the user; or a weighted combination of one or more related parameters.
12. The method of claim 11, wherein the one or more first inputs associated with the body of the user are received from one or more of: a controller; a sensor; a camera; a microphone; an accelerometer; a headset worn by the user; or a mobile device.
13. The method of claim 4, wherein the one or more second inputs associated with the one or more displaying elements comprise one or more of: a Z-buffer value associated with a displaying element; a displaying element marked by a developer; an image analysis result; a shape of a displaying element; a face recognition result; an object recognition result; a person identified in a displaying content; an object identified in a displaying content; a correlation of two or more displaying elements; or a weighted combination of the one or more second inputs.
14. The method of claim 1, further comprising: determining that the performance metric of the eye tracking system is below a second performance threshold; receiving one or more second inputs associated with one or more displaying elements in the scene displayed by the head-mounted display; and determining the vergence distance of the user based at least on the one or more first inputs associated with the body of the user and the one or more second inputs associated with the one or more displaying elements.
15. The method of claim 14, wherein determining that the performance metric of the eye tracking system is below the second performance threshold comprises determining that the eye tracking system does not exist or fails to provide eye tracking data.
16. The method of claim 1, wherein the performance metric of the eye tracking system comprises one or more of: an accuracy of a parameter from the eye tracking system; a precision of a parameter from the eye tracking system; a value of a parameter from the eye tracking system; a detectability of a pupil; a metric based on one or more parameters associated with the user; a parameter change; a parameter changing trend; a data availability; or a weighted combination of one or more performance related parameters.
17. The method of claim 16, wherein the one or more parameters associated with the user comprise one or more of: an eye distance of the user; a pupil position; a pupil status; a correlation of two pupils of the user; a head size of the user; a position of a headset worn by the user; an angle of the headset worn by the user; a direction of the headset worn by the user; an alignment of the eyes of the user; or a weighted combination of one or more related parameters associated with the user.
18. The method of claim 1, wherein the first performance threshold comprises one or more of: a pre-determined value; a pre-determined range; a state of a data; a changing speed of a data; or a trend of a data change.
19. One or more non-transitory computer-readable storage media embodying software that is operable when executed by a computing system to: determine that a performance metric of an eye tracking system is below a first performance threshold, wherein the eye tracking system is associated with a head-mounted display worn by a user; based on the determination of the performance metric of the eye tracking system being below the first performance threshold, the media embodying software operable when executed by the computing system to: receive one or more first inputs associated with a body of the user; estimate a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user; determine a vergence distance of the user based at least on the one or more first inputs associated with the body of the user, the estimated region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display; and adjust one or more configurations of the head-mounted display based on the determined vergence distance of the user.
20. A system comprising: one or more non-transitory computer-readable storage media embodying instructions; one or more processors coupled to the storage media and operable to execute the instructions to: determine that a performance metric of an eye tracking system is below a first performance threshold, wherein the eye tracking system is associated with a head-mounted display worn by a user; based on the determination of the performance metric of the eye tracking system being below the first performance threshold, the system is configured to: receive one or more first inputs associated with a body of the user; estimate a region that the user is looking at within a field of view of the head-mounted display based on the received one or more first inputs associated with the body of the user; determine a vergence distance of the user based at least on the one or more first inputs associated with the body of the user, the estimated region that the user is looking at, and locations of one or more objects in a scene displayed by the head-mounted display; and adjust one or more configurations of the head-mounted display based on the determined vergence distance of the user.
</claims>
</document>
